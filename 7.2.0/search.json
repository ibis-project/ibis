[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ibis",
    "section": "",
    "text": "Why Ibis?\n  \n  \n    \n     Installation\n  \n  \n    \n     Tutorial: getting started\n  \n  \n    \n     GitHub\n  \n  \n    \n     Chat\n  \n  \n    \n     RSS"
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "Ibis",
    "section": "Install",
    "text": "Install\nWe recommend starting with the default backend (DuckDB).\n1pip install 'ibis-framework[duckdb]'\n\n1\n\nInstall Ibis with optional dependencies (DuckDB in this case).\n\n\n\nShow supported backends"
  },
  {
    "objectID": "index.html#backends",
    "href": "index.html#backends",
    "title": "Ibis",
    "section": "Backends",
    "text": "Backends\nNeed to use Ibis with a backend that isn’t currently supported? Let us know!\nYou can install Ibis and a supported backend with pip, conda, mamba, or pixi.\n\npipcondamambapixi\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall with the bigquery extra:\npip install 'ibis-framework[bigquery]'\nConnect using ibis.bigquery.connect.\n\n\nInstall with the clickhouse extra:\npip install 'ibis-framework[clickhouse]'\nConnect using ibis.clickhouse.connect.\n\n\nInstall with the dask extra:\npip install 'ibis-framework[dask]'\nConnect using ibis.dask.connect.\n\n\nInstall with the datafusion extra:\npip install 'ibis-framework[datafusion]'\nConnect using ibis.datafusion.connect.\n\n\nInstall with the druid extra:\npip install 'ibis-framework[druid]'\nConnect using ibis.druid.connect.\n\n\nInstall with the duckdb extra:\npip install 'ibis-framework[duckdb]'\nConnect using ibis.duckdb.connect.\n\n\nInstall with the impala extra:\npip install 'ibis-framework[impala]'\nConnect using ibis.impala.connect.\n\n\nInstall with the mssql extra:\npip install 'ibis-framework[mssql]'\nConnect using ibis.mssql.connect.\n\n\nInstall with the mysql extra:\npip install 'ibis-framework[mysql]'\nConnect using ibis.mysql.connect.\n\n\nInstall with the oracle extra:\npip install 'ibis-framework[oracle]'\nConnect using ibis.oracle.connect.\n\n\nInstall with the pandas extra:\npip install 'ibis-framework[pandas]'\nConnect using ibis.pandas.connect.\n\n\nInstall with the polars extra:\npip install 'ibis-framework[polars]'\nConnect using ibis.polars.connect.\n\n\nInstall with the postgres extra:\npip install 'ibis-framework[postgres]'\nConnect using ibis.postgres.connect.\n\n\nInstall with the pyspark extra:\npip install 'ibis-framework[pyspark]'\nConnect using ibis.pyspark.connect.\n\n\nInstall with the snowflake extra:\npip install 'ibis-framework[snowflake]'\nConnect using ibis.snowflake.connect.\n\n\nInstall with the sqlite extra:\npip install 'ibis-framework[sqlite]'\nConnect using ibis.sqlite.connect.\n\n\nInstall with the trino extra:\npip install 'ibis-framework[trino]'\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nconda install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nconda install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nconda install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nconda install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nconda install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nconda install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nconda install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nconda install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nconda install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nconda install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nconda install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nconda install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nconda install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nconda install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nconda install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nconda install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nconda install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nmamba install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nmamba install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nmamba install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nmamba install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nmamba install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nmamba install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nmamba install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nmamba install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nmamba install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nmamba install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nmamba install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nmamba install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nmamba install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nmamba install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nmamba install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nmamba install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nmamba install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nAdd the ibis-bigquery package:\npixi add ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nAdd the ibis-clickhouse package:\npixi add ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nAdd the ibis-dask package:\npixi add ibis-dask\nConnect using ibis.dask.connect.\n\n\nAdd the ibis-datafusion package:\npixi add ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nAdd the ibis-druid package:\npixi add ibis-druid\nConnect using ibis.druid.connect.\n\n\nAdd the ibis-duckdb package:\npixi add ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nAdd the ibis-impala package:\npixi add ibis-impala\nConnect using ibis.impala.connect.\n\n\nAdd the ibis-mssql package:\npixi add ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nAdd the ibis-mysql package:\npixi add ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nAdd the ibis-oracle package:\npixi add ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nAdd the ibis-pandas package:\npixi add ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nAdd the ibis-polars package:\npixi add ibis-polars\nConnect using ibis.polars.connect.\n\n\nAdd the ibis-postgres package:\npixi add ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nAdd the ibis-pyspark package:\npixi add ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nAdd the ibis-snowflake package:\npixi add ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nAdd the ibis-sqlite package:\npixi add ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nAdd the ibis-trino package:\npixi add ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nSee the backend support matrix for details on operations supported. Open a feature request if you’d like to see support for an operation in a given backend. If the backend supports it, we’ll do our best to add it quickly!"
  },
  {
    "objectID": "index.html#quickstart",
    "href": "index.html#quickstart",
    "title": "Ibis",
    "section": "Quickstart",
    "text": "Quickstart\nSee the getting started tutorial for a more in-depth introduction to Ibis. Below is a quick overview.\n\n1import ibis\nimport ibis.selectors as s\n\n2ibis.options.interactive = True\n\n3t = ibis.examples.penguins.fetch()\n4t.head(3)\n\n\n1\n\nEnsure you install Ibis first.\n\n2\n\nUse interactive mode for exploratory data analysis (EDA) or demos.\n\n3\n\nLoad a dataset from the built-in examples.\n\n4\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nIbis is a dataframe library with familiar syntax.\n\n1t[10:15]\n\n\n1\n\nDisplay a slice of the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           37.8 │          17.1 │               186 │        3300 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           37.8 │          17.3 │               180 │        3700 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │               182 │        3200 │ female │  2007 │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ male   │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\nShow analytics\n\n\n\n\n\n\nAnalytics\nIbis is built for easy analytics at scale in Python.\n\n1(\n    t.filter(ibis._[\"body_mass_g\"] != None)\n    .group_by([\"species\", \"island\"])\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n\n\n1\n\nGroup by species and island, and compute the number of rows in each group.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ int64 │\n├───────────┼───────────┼───────┤\n│ Gentoo    │ Biscoe    │   123 │\n│ Chinstrap │ Dream     │    68 │\n│ Adelie    │ Dream     │    56 │\n│ Adelie    │ Torgersen │    51 │\n│ Adelie    │ Biscoe    │    44 │\n└───────────┴───────────┴───────┘\n\n\n\n\n\n\nShow EDA + visualization\n\n\n\n\n\n\nExploratory data analysis (EDA) and visualization\n\nExploratory data analysis\nIbis has built-in methods for exploration and visualization.\n\n1num_species = int(t.select(\"species\").nunique().to_pandas())\n2t[\"species\"].topk(num_species)\n\n\n1\n\nCompute the number of species in the dataset.\n\n2\n\nDisplay the top species by count.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species   ┃ Count(species) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ int64          │\n├───────────┼────────────────┤\n│ Adelie    │            152 │\n│ Gentoo    │            124 │\n│ Chinstrap │             68 │\n└───────────┴────────────────┘\n\n\n\n\n\nVisualization\nIbis works with any Python plotting library that supports the dataframe interchange protocol.\n\n\nCode\n1width = 640\nheight = 480\n\n\n\n1\n\nSet the width and height of the plots.\n\n\n\n\n\n1grouped = (\n    t.group_by(\"species\")\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n2grouped\n\n\n1\n\nSetup data to plot.\n\n2\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ int64 │\n├───────────┼───────┤\n│ Adelie    │   152 │\n│ Gentoo    │   124 │\n│ Chinstrap │    68 │\n└───────────┴───────┘\n\n\n\n\nAltairmatplotlibPlotlyplotnineseaborn\n\n\npip install altair\n\nimport altair as alt\n\nchart = (\n    alt.Chart(grouped.to_pandas())\n    .mark_bar()\n    .encode(\n        x=\"species\",\n        y=\"count\",\n        tooltip=[\"species\", \"count\"],\n    )\n    .properties(width=width, height=height)\n    .interactive()\n)\nchart\n\n\n\n\n\n\n\n\n\npip install matplotlib\n\nimport matplotlib.pyplot as plt\n\nchart = grouped.to_pandas().plot.bar(\n    x=\"species\",\n    y=\"count\",\n    figsize=(width / 100, height / 100),\n)\nplt.show()\n\n\n\n\n\n\npip install plotly\n\nimport plotly.express as px\n\nchart = px.bar(\n    grouped.to_pandas(),\n    x=\"species\",\n    y=\"count\",\n    width=width,\n    height=height,\n)\nchart\n\n                                                \n\n\n\n\npip install plotnine\n\nfrom plotnine import ggplot, aes, geom_bar, theme\n\nchart = (\n    ggplot(\n        grouped,\n        aes(x=\"species\", y=\"count\"),\n    )\n    + geom_bar(stat=\"identity\")\n    + theme(figure_size=(width / 100, height / 100))\n)\nchart\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\npip install seaborn\n\nimport seaborn as sns\n\nchart = sns.barplot(\n    data=grouped.to_pandas(),\n    x=\"species\",\n    y=\"count\",\n)\nchart.figure.set_size_inches(width / 100, height / 100)\n\n\n\n\n\n\n\n\n\n\n\nShow data science\n\n\n\n\n\n\nData science\nUse Ibis with your favorite data science libraries for concise and efficient workflows.\n\n1import ibis.selectors as s\n\n\n2def transform(t):\n    t = t.mutate(\n        s.across(s.numeric(), {\"zscore\": lambda x: (x - x.mean()) / x.std()})\n    ).dropna()\n    return t\n\n\n3f = transform(t.drop(\"year\"))\n4f.select(\"species\", \"island\", s.contains(\"zscore\"))\n\n\n1\n\nImport the selectors module.\n\n2\n\nDefine a function to transform the table for code reuse (compute z-scores on numeric columns).\n\n3\n\nApply the function to the table and assign it to a new variable.\n\n4\n\nDisplay the transformed table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64               │ float64              │ float64                  │ float64            │\n├─────────┼───────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ Adelie  │ Torgersen │             -0.883205 │             0.784300 │                -1.416272 │          -0.563317 │\n│ Adelie  │ Torgersen │             -0.809939 │             0.126003 │                -1.060696 │          -0.500969 │\n│ Adelie  │ Torgersen │             -0.663408 │             0.429833 │                -0.420660 │          -1.186793 │\n│ Adelie  │ Torgersen │             -1.322799 │             1.088129 │                -0.562890 │          -0.937403 │\n│ Adelie  │ Torgersen │             -0.846572 │             1.746426 │                -0.776236 │          -0.688012 │\n│ Adelie  │ Torgersen │             -0.919837 │             0.328556 │                -1.416272 │          -0.719186 │\n│ Adelie  │ Torgersen │             -0.864888 │             1.240044 │                -0.420660 │           0.590115 │\n│ Adelie  │ Torgersen │             -0.516876 │             0.227280 │                -1.345156 │          -1.249141 │\n│ Adelie  │ Torgersen │             -0.974787 │             2.050255 │                -0.705121 │          -0.500969 │\n│ Adelie  │ Torgersen │             -1.707443 │             1.999617 │                -0.207315 │           0.247203 │\n│ …       │ …         │                     … │                    … │                        … │                  … │\n└─────────┴───────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘\n\n\n\npip install scikit-learn\n\n1import plotly.express as px\nfrom sklearn.decomposition import PCA\n\n2X = f.select(s.contains(\"zscore\"))\n\n3n_components = 3\npca = PCA(n_components=n_components).fit(X)\n\n4t_pca = ibis.memtable(pca.transform(X)).relabel(\n    {\"col0\": \"pc1\", \"col1\": \"pc2\", \"col2\": \"pc3\"}\n)\n\n5f = f.mutate(row_number=ibis.row_number().over()).join(\n    t_pca.mutate(row_number=ibis.row_number().over()), \"row_number\"\n)\n\n6px.scatter_3d(\n    f.to_pandas(),\n    x=\"pc1\",\n    y=\"pc2\",\n    z=\"pc3\",\n    color=\"species\",\n    symbol=\"island\",\n)\n\n\n1\n\nImport data science libraries\n\n2\n\nSelect “features” (numeric columns) as X\n\n3\n\nCompute PCA\n\n4\n\nCreate a table from the PCA results\n\n5\n\nJoin the PCA results to the original table\n\n6\n\nPlot the results\n\n\n\n\n                                                \n\n\n\n\n\n\n\nShow input and output\n\n\n\n\n\n\n\nInput and output\nIbis supports a variety of input and output options.\n\n\nData platforms\nYou can connect Ibis to any supported backend to read and write data in backend-native tables.\n\n\nCode\ncon = ibis.duckdb.connect(\"penguins.ddb\")\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)\n\n\n\n1con = ibis.duckdb.connect(\"penguins.ddb\")\n2t = con.table(\"penguins\")\n3t.head(3)\n\n\n1\n\nConnect to a backend.\n\n2\n\nLoad a table.\n\n3\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n1grouped = (\n    t.group_by([\"species\", \"island\"])\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n2con.create_table(\"penguins_grouped\", grouped.to_pyarrow(), overwrite=True)\n\n\n1\n\nCreate a lazily evaluated Ibis expression.\n\n2\n\nWrite to a table.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ int64 │\n├───────────┼───────────┼───────┤\n│ Gentoo    │ Biscoe    │   124 │\n│ Chinstrap │ Dream     │    68 │\n│ Adelie    │ Dream     │    56 │\n│ Adelie    │ Torgersen │    52 │\n│ Adelie    │ Biscoe    │    44 │\n└───────────┴───────────┴───────┘\n\n\n\n\n\nFile formats\nDepending on the backend, you can read and write data in several file formats.\n\nCSVDelta LakeParquet\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_csv(\"penguins.csv\")\n2ibis.read_csv(\"penguins.csv\").head(3)\n\n\n1\n\nWrite the table to a CSV file. Dependent on backend.\n\n2\n\nRead the CSV file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb,deltalake]'\n\n1t.to_delta(\"penguins.delta\", mode=\"overwrite\")\n2ibis.read_delta(\"penguins.delta\").head(3)\n\n\n1\n\nWrite the table to a Delta Lake table. Dependent on backend.\n\n2\n\nRead the Delta Lake table into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_parquet(\"penguins.parquet\")\n2ibis.read_parquet(\"penguins.parquet\").head(3)\n\n\n1\n\nWrite the table to a Parquet file. Dependent on backend.\n\n2\n\nRead the Parquet file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\n\n\n\nWith other Python libraries\nIbis uses Apache Arrow for efficient data transfer to and from other libraries. Ibis tables implement the __dataframe__ and __array__ protocols, so you can pass them to any library that supports these protocols.\n\npandaspolarspyarrowtorch__dataframe____array__\n\n\nYou can convert Ibis tables to pandas dataframes.\npip install pandas\n\n1df = t.to_pandas()\ndf.head(3)\n\n\n1\n\nReturns a pandas dataframe.\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\nOr you can convert pandas dataframes to Ibis tables.\n\n1t = ibis.memtable(df)\nt.head(3)\n\n\n1\n\nReturns an Ibis table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │             195.0 │      3250.0 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to Polars dataframes.\npip install polars\n\nimport polars as pl\n\ndf = pl.from_arrow(t.to_pyarrow())\ndf.head(3)\n\n\nshape: (3, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\n\n\n\nOr Polars dataframes to Ibis tables.\n\nt = ibis.memtable(df)\nt.head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to PyArrow tables.\npip install pyarrow\n\nt.to_pyarrow()\n\npyarrow.Table\ncol0: string\ncol1: string\ncol2: double\ncol3: double\ncol4: double\ncol5: double\ncol6: string\ncol7: int64\n----\ncol0: [[\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",...,\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"]]\ncol1: [[\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",...,\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"]]\ncol2: [[39.1,39.5,40.3,null,36.7,...,55.8,43.5,49.6,50.8,50.2]]\ncol3: [[18.7,17.4,18,null,19.3,...,19.8,18.1,18.2,19,18.7]]\ncol4: [[181,186,195,null,193,...,207,202,193,210,198]]\ncol5: [[3750,3800,3250,null,3450,...,4000,3400,3775,4100,3775]]\ncol6: [[\"male\",\"female\",\"female\",null,\"female\",...,\"male\",\"female\",\"male\",\"male\",\"female\"]]\ncol7: [[2007,2007,2007,2007,2007,...,2009,2009,2009,2009,2009]]\n\n\nOr PyArrow batches:\n\nt.to_pyarrow_batches()\n\n&lt;pyarrow.lib.RecordBatchReader at 0x74580dbdf420&gt;\n\n\nAnd you can convert PyArrow tables to Ibis tables.\n\nibis.memtable(t.to_pyarrow()).head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to torch tensors.\npip install torch\nt.select(s.numeric()).limit(3).to_torch()\n{'col2': tensor([39.1000, 39.5000, 40.3000], dtype=torch.float64),\n 'col3': tensor([18.7000, 17.4000, 18.0000], dtype=torch.float64),\n 'col4': tensor([181., 186., 195.], dtype=torch.float64),\n 'col5': tensor([3750., 3800., 3250.], dtype=torch.float64),\n 'col7': tensor([2007, 2007, 2007], dtype=torch.int16)}\n\n\nYou can directly call the __dataframe__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__dataframe__()\n\n&lt;ibis.expr.types.dataframe_interchange.IbisDataFrame at 0x74580019a6e0&gt;\n\n\n\n\nYou can directly call the __array__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__array__()\n\narray([['Adelie', 'Torgersen', 39.1, ..., 3750.0, 'male', 2007],\n       ['Adelie', 'Torgersen', 39.5, ..., 3800.0, 'female', 2007],\n       ['Adelie', 'Torgersen', 40.3, ..., 3250.0, 'female', 2007],\n       ...,\n       ['Chinstrap', 'Dream', 49.6, ..., 3775.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.8, ..., 4100.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.2, ..., 3775.0, 'female', 2009]],\n      dtype=object)\n\n\n\n\n\n\n\n\nShow SQL + Python\n\n\n\nSQL + Python\nIbis has the ibis.to_sql to generate SQL strings and ibis.show_sql display them. Ibis uses SQLGlot under the hood to allow passing a dialect parameter to SQL methods.\n\nBigQuerySnowflakeOracleMySQLMSSQLPostgreSQLSQLiteTrino\n\n\n\n1dialect = \"bigquery\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.*\nFROM (\n  SELECT\n    t1.`species`,\n    t1.`island`,\n    count(1) AS `count`\n  FROM penguins AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.`count` DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"snowflake\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.\"count\"\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS \"count\"\n  FROM \"main\".\"penguins\" AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.\"count\" DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"oracle\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.\"count\"\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS \"count\"\n  FROM penguins t1\n  GROUP BY\n    t1.species,\n    t1.island\n) t0\nORDER BY\n  t0.\"count\" DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"mysql\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.count\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS count\n  FROM penguins AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.count DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"mssql\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.count\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS count\n  FROM penguins AS t1\n  GROUP BY\n    t1.species,\n    t1.island\n) AS t0\nORDER BY\n  t0.count DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"postgres\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.count\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS count\n  FROM penguins AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.count DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"sqlite\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.count\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS count\n  FROM penguins AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.count DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘\n\n\n\n\n\n\n1dialect = \"trino\"\n2sql = ibis.to_sql(\n    grouped,\n    dialect=dialect,\n)\n3sql\n\n\n1\n\nSet the dialect.\n\n2\n\nConvert the table to a SQL string.\n\n3\n\nDisplay the SQL string.\n\n\n\n\nSELECT\n  t0.species,\n  t0.island,\n  t0.count\nFROM (\n  SELECT\n    t1.species AS species,\n    t1.island AS island,\n    COUNT(*) AS count\n  FROM main.penguins AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nORDER BY\n  t0.count DESC\n\n\nYou can chain Ibis expressions and .sql together.\n\n1con.sql(sql, dialect=dialect).filter(ibis._[\"species\"] == \"Adelie\")\n\n\n1\n\nChain .sql calls and Ibis expressions together.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ count ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Dream     │    56 │\n│ Adelie  │ Torgersen │    52 │\n│ Adelie  │ Biscoe    │    44 │\n└─────────┴───────────┴───────┘"
  },
  {
    "objectID": "index.html#data-platforms",
    "href": "index.html#data-platforms",
    "title": "Ibis",
    "section": "Data platforms",
    "text": "Data platforms\nYou can connect Ibis to any supported backend to read and write data in backend-native tables.\n\n\nCode\ncon = ibis.duckdb.connect(\"penguins.ddb\")\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)\n\n\n\n1con = ibis.duckdb.connect(\"penguins.ddb\")\n2t = con.table(\"penguins\")\n3t.head(3)\n\n\n1\n\nConnect to a backend.\n\n2\n\nLoad a table.\n\n3\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n1grouped = (\n    t.group_by([\"species\", \"island\"])\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n2con.create_table(\"penguins_grouped\", grouped.to_pyarrow(), overwrite=True)\n\n\n1\n\nCreate a lazily evaluated Ibis expression.\n\n2\n\nWrite to a table.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ int64 │\n├───────────┼───────────┼───────┤\n│ Gentoo    │ Biscoe    │   124 │\n│ Chinstrap │ Dream     │    68 │\n│ Adelie    │ Dream     │    56 │\n│ Adelie    │ Torgersen │    52 │\n│ Adelie    │ Biscoe    │    44 │\n└───────────┴───────────┴───────┘"
  },
  {
    "objectID": "index.html#file-formats",
    "href": "index.html#file-formats",
    "title": "Ibis",
    "section": "File formats",
    "text": "File formats\nDepending on the backend, you can read and write data in several file formats.\n\nCSVDelta LakeParquet\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_csv(\"penguins.csv\")\n2ibis.read_csv(\"penguins.csv\").head(3)\n\n\n1\n\nWrite the table to a CSV file. Dependent on backend.\n\n2\n\nRead the CSV file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb,deltalake]'\n\n1t.to_delta(\"penguins.delta\", mode=\"overwrite\")\n2ibis.read_delta(\"penguins.delta\").head(3)\n\n\n1\n\nWrite the table to a Delta Lake table. Dependent on backend.\n\n2\n\nRead the Delta Lake table into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_parquet(\"penguins.parquet\")\n2ibis.read_parquet(\"penguins.parquet\").head(3)\n\n\n1\n\nWrite the table to a Parquet file. Dependent on backend.\n\n2\n\nRead the Parquet file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "index.html#with-other-python-libraries",
    "href": "index.html#with-other-python-libraries",
    "title": "Ibis",
    "section": "With other Python libraries",
    "text": "With other Python libraries\nIbis uses Apache Arrow for efficient data transfer to and from other libraries. Ibis tables implement the __dataframe__ and __array__ protocols, so you can pass them to any library that supports these protocols.\n\npandaspolarspyarrowtorch__dataframe____array__\n\n\nYou can convert Ibis tables to pandas dataframes.\npip install pandas\n\n1df = t.to_pandas()\ndf.head(3)\n\n\n1\n\nReturns a pandas dataframe.\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\nOr you can convert pandas dataframes to Ibis tables.\n\n1t = ibis.memtable(df)\nt.head(3)\n\n\n1\n\nReturns an Ibis table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │             195.0 │      3250.0 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to Polars dataframes.\npip install polars\n\nimport polars as pl\n\ndf = pl.from_arrow(t.to_pyarrow())\ndf.head(3)\n\n\nshape: (3, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\n\n\n\nOr Polars dataframes to Ibis tables.\n\nt = ibis.memtable(df)\nt.head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to PyArrow tables.\npip install pyarrow\n\nt.to_pyarrow()\n\npyarrow.Table\ncol0: string\ncol1: string\ncol2: double\ncol3: double\ncol4: double\ncol5: double\ncol6: string\ncol7: int64\n----\ncol0: [[\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",...,\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"]]\ncol1: [[\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",...,\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"]]\ncol2: [[39.1,39.5,40.3,null,36.7,...,55.8,43.5,49.6,50.8,50.2]]\ncol3: [[18.7,17.4,18,null,19.3,...,19.8,18.1,18.2,19,18.7]]\ncol4: [[181,186,195,null,193,...,207,202,193,210,198]]\ncol5: [[3750,3800,3250,null,3450,...,4000,3400,3775,4100,3775]]\ncol6: [[\"male\",\"female\",\"female\",null,\"female\",...,\"male\",\"female\",\"male\",\"male\",\"female\"]]\ncol7: [[2007,2007,2007,2007,2007,...,2009,2009,2009,2009,2009]]\n\n\nOr PyArrow batches:\n\nt.to_pyarrow_batches()\n\n&lt;pyarrow.lib.RecordBatchReader at 0x74580dbdf420&gt;\n\n\nAnd you can convert PyArrow tables to Ibis tables.\n\nibis.memtable(t.to_pyarrow()).head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to torch tensors.\npip install torch\nt.select(s.numeric()).limit(3).to_torch()\n{'col2': tensor([39.1000, 39.5000, 40.3000], dtype=torch.float64),\n 'col3': tensor([18.7000, 17.4000, 18.0000], dtype=torch.float64),\n 'col4': tensor([181., 186., 195.], dtype=torch.float64),\n 'col5': tensor([3750., 3800., 3250.], dtype=torch.float64),\n 'col7': tensor([2007, 2007, 2007], dtype=torch.int16)}\n\n\nYou can directly call the __dataframe__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__dataframe__()\n\n&lt;ibis.expr.types.dataframe_interchange.IbisDataFrame at 0x74580019a6e0&gt;\n\n\n\n\nYou can directly call the __array__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__array__()\n\narray([['Adelie', 'Torgersen', 39.1, ..., 3750.0, 'male', 2007],\n       ['Adelie', 'Torgersen', 39.5, ..., 3800.0, 'female', 2007],\n       ['Adelie', 'Torgersen', 40.3, ..., 3250.0, 'female', 2007],\n       ...,\n       ['Chinstrap', 'Dream', 49.6, ..., 3775.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.8, ..., 4100.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.2, ..., 3775.0, 'female', 2009]],\n      dtype=object)"
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "APIs for manipulating table, column and scalar expressions\n\n\n\nTable expressions\nTables are one of the core data structures in Ibis.\n\n\nGeneric expressions\nScalars and columns of any element type.\n\n\nNumeric and Boolean expressions\nInteger, floating point, decimal, and boolean expressions.\n\n\nString expressions\nAll string operations are valid for both scalars and columns.\n\n\nTemporal expressions\nDates, times, timestamps and intervals.\n\n\nCollection expressions\nArrays, maps and structs.\n\n\nGeospatial expressions\nPoints, Polygons, LineStrings, and other geospatial types.\n\n\nColumn selectors\nChoose Table columns based on dtype, regex, and other criteria\n\n\n\n\n\n\nData types and schemas\n\n\n\nData types\nScalar and column data types\n\n\nSchemas\nTable Schemas\n\n\n\n\n\n\n\n\n\nTop-level connection APIs\nCreate and manage backend connections.\n\n\n\n\n\n\nUser-defined function APIs\n\n\n\nScalar UDFs\nScalar user-defined function APIs\n\n\n\n\n\n\nIbis configuration\n\n\n\nContextAdjustment\nOptions related to time context adjustment.\n\n\nInteractive\nOptions controlling the interactive repr.\n\n\nOptions\nIbis configuration options.\n\n\nRepr\nExpression printing options.\n\n\nSQL\nSQL-related options."
  },
  {
    "objectID": "reference/index.html#expression-api",
    "href": "reference/index.html#expression-api",
    "title": "Reference",
    "section": "",
    "text": "APIs for manipulating table, column and scalar expressions\n\n\n\nTable expressions\nTables are one of the core data structures in Ibis.\n\n\nGeneric expressions\nScalars and columns of any element type.\n\n\nNumeric and Boolean expressions\nInteger, floating point, decimal, and boolean expressions.\n\n\nString expressions\nAll string operations are valid for both scalars and columns.\n\n\nTemporal expressions\nDates, times, timestamps and intervals.\n\n\nCollection expressions\nArrays, maps and structs.\n\n\nGeospatial expressions\nPoints, Polygons, LineStrings, and other geospatial types.\n\n\nColumn selectors\nChoose Table columns based on dtype, regex, and other criteria"
  },
  {
    "objectID": "reference/index.html#type-system",
    "href": "reference/index.html#type-system",
    "title": "Reference",
    "section": "",
    "text": "Data types and schemas\n\n\n\nData types\nScalar and column data types\n\n\nSchemas\nTable Schemas"
  },
  {
    "objectID": "reference/index.html#connection-apis",
    "href": "reference/index.html#connection-apis",
    "title": "Reference",
    "section": "",
    "text": "Top-level connection APIs\nCreate and manage backend connections."
  },
  {
    "objectID": "reference/index.html#udfs",
    "href": "reference/index.html#udfs",
    "title": "Reference",
    "section": "",
    "text": "User-defined function APIs\n\n\n\nScalar UDFs\nScalar user-defined function APIs"
  },
  {
    "objectID": "reference/index.html#configuration",
    "href": "reference/index.html#configuration",
    "title": "Reference",
    "section": "",
    "text": "Ibis configuration\n\n\n\nContextAdjustment\nOptions related to time context adjustment.\n\n\nInteractive\nOptions controlling the interactive repr.\n\n\nOptions\nIbis configuration options.\n\n\nRepr\nExpression printing options.\n\n\nSQL\nSQL-related options."
  },
  {
    "objectID": "reference/expression-tables.html",
    "href": "reference/expression-tables.html",
    "title": "Table expressions",
    "section": "",
    "text": "Tables are one of the core data structures in Ibis."
  },
  {
    "objectID": "reference/expression-tables.html#attributes",
    "href": "reference/expression-tables.html#attributes",
    "title": "Table expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ncolumns\nThe list of column names in this table."
  },
  {
    "objectID": "reference/expression-tables.html#methods",
    "href": "reference/expression-tables.html#methods",
    "title": "Table expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nAggregate a table with a given set of reductions grouping by by.\n\n\nalias\nCreate a table expression with a specific name alias.\n\n\nas_table\nPromote the expression to a table.\n\n\nasof_join\nPerform an “as-of” join between left and right.\n\n\ncache\nCache the provided expression.\n\n\ncast\nCast the columns of a table.\n\n\ncount\nCompute the number of rows in the table.\n\n\ncross_join\nCompute the cross join of a sequence of tables.\n\n\ndifference\nCompute the set difference of multiple table expressions.\n\n\ndistinct\nReturn a Table with duplicate rows removed.\n\n\ndrop\nRemove fields from a table.\n\n\ndropna\nRemove rows with null values from the table.\n\n\nfillna\nFill null values in a table expression.\n\n\nfilter\nSelect rows from table based on predicates.\n\n\nget_name\nReturn the fully qualified name of the table.\n\n\ngroup_by\nCreate a grouped table expression.\n\n\nhead\nSelect the first n rows of a table.\n\n\ninfo\nReturn summary information about a table.\n\n\nintersect\nCompute the set intersection of multiple table expressions.\n\n\njoin\nPerform a join between two tables.\n\n\nlimit\nSelect n rows from self starting at offset.\n\n\nmutate\nAdd columns to a table expression.\n\n\nnunique\nCompute the number of unique rows in the table.\n\n\norder_by\nSort a table by one or more expressions.\n\n\npivot_longer\nTransform a table from wider to longer.\n\n\npivot_wider\nPivot a table to a wider format.\n\n\nrelabel\nDeprecated in favor of Table.rename\n\n\nrelocate\nRelocate columns before or after other specified columns.\n\n\nrename\nRename columns in the table.\n\n\nrowid\nA unique integer per row.\n\n\nsample\nSample a fraction of rows from a table.\n\n\nschema\nReturn the Schema for this table.\n\n\nselect\nCompute a new table expression using exprs and named_exprs.\n\n\nsql\nRun a SQL query against a table expression.\n\n\nto_array\nView a single column table as an array.\n\n\nto_pandas\nConvert a table expression to a pandas DataFrame.\n\n\ntry_cast\nCast the columns of a table.\n\n\nunion\nCompute the set union of multiple table expressions.\n\n\nunpack\nProject the struct fields of each of columns into self.\n\n\nview\nCreate a new table expression distinct from the current one.\n\n\nwindow_by\nCreate a windowing table-valued function (TVF) expression.\n\n\n\n\naggregate\naggregate(metrics=None, by=None, having=None, **kwargs)\nAggregate a table with a given set of reductions grouping by by.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmetrics\nSequence[ir.Scalar] | None\nAggregate expressions. These can be any scalar-producing expression, including aggregation functions like sum or literal values like ibis.literal(1).\nNone\n\n\nby\nSequence[ir.Value] | None\nGrouping expressions.\nNone\n\n\nhaving\nSequence[ir.BooleanValue] | None\nPost-aggregation filters. The shape requirements are the same metrics, but the output type for having is boolean. ::: {.callout-warning} ## Expressions like x is None return bool and will not generate a SQL comparison to NULL :::\nNone\n\n\nkwargs\nir.Value\nNamed aggregate expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn aggregate table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"fruit\": [\"apple\", \"apple\", \"banana\", \"orange\"],\n...         \"price\": [0.5, 0.5, 0.25, 0.33],\n...     }\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┳━━━━━━━━━┓\n┃ fruit  ┃ price   ┃\n┡━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │\n├────────┼─────────┤\n│ apple  │    0.50 │\n│ apple  │    0.50 │\n│ banana │    0.25 │\n│ orange │    0.33 │\n└────────┴─────────┘\n\n\n\n\n&gt;&gt;&gt; t.aggregate(\n...     by=[\"fruit\"],\n...     total_cost=_.price.sum(),\n...     avg_cost=_.price.mean(),\n...     having=_.price.sum() &lt; 0.5,\n... )\n\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ fruit  ┃ total_cost ┃ avg_cost ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ float64    │ float64  │\n├────────┼────────────┼──────────┤\n│ banana │       0.25 │     0.25 │\n│ orange │       0.33 │     0.33 │\n└────────┴────────────┴──────────┘\n\n\n\n\n\n\nalias\nalias(alias)\nCreate a table expression with a specific name alias.\nThis method is useful for exposing an ibis expression to the underlying backend for use in the Table.sql method.\n\n\n\n\n\n\n.alias will create a temporary view\n\n\n\n.alias creates a temporary view in the database.\nThis side effect will be removed in a future version of ibis and is not part of the public API.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nalias\nstr\nName of the child expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; expr = t.alias(\"pingüinos\").sql('SELECT * FROM \"pingüinos\" LIMIT 5')\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\n\nas_table\nas_table()\nPromote the expression to a table.\nThis method is a no-op for table expressions.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\"), name=\"t\")\n&gt;&gt;&gt; s = t.as_table()\n&gt;&gt;&gt; t is s\n\nTrue\n\n\n\n\n\nasof_join\nasof_join(left, right, predicates=(), by=(), tolerance=None, *, lname='', rname='{name}_right')\nPerform an “as-of” join between left and right.\nSimilar to a left join except that the match is done on nearest key rather than equal keys.\nOptionally, match keys with by before joining with predicates.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nTable expression\nrequired\n\n\nright\nTable\nTable expression\nrequired\n\n\npredicates\nstr | ir.BooleanColumn | Sequence[str | ir.BooleanColumn]\nJoin expressions\n()\n\n\nby\nstr | ir.Column | Sequence[str | ir.Column]\ncolumn to group by before joining\n()\n\n\ntolerance\nstr | ir.IntervalScalar | None\nAmount of time to look behind when joining\nNone\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ncache\ncache()\nCache the provided expression.\nAll subsequent operations on the returned expression will be performed on the cached data. Use the with statement to limit the lifetime of a cached table.\nThis method is idempotent: calling it multiple times in succession will return the same value as the first call.\n\n\n\n\n\n\nThis method eagerly evaluates the expression prior to caching\n\n\n\nSubsequent evaluations will not recompute the expression so method chaining will not incur the overhead of caching more than once.\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCached table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cached_penguins = t.mutate(computation=\"Heavy Computation\").cache()\n&gt;&gt;&gt; cached_penguins\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ computation       ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ string            │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼───────────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │ Heavy Computation │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │ Heavy Computation │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │ …                 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴───────────────────┘\n\n\n\nExplicit cache cleanup\n\n&gt;&gt;&gt; with t.mutate(computation=\"Heavy Computation\").cache() as cached_penguins:\n...     cached_penguins\n...\n\n\n\n\ncast\ncast(schema)\nCast the columns of a table.\nSimilar to pandas.DataFrame.astype.\n\n\n\n\n\n\nIf you need to cast columns to a single type, use selectors.\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschema\nSupportsSchema\nMapping, schema or iterable of pairs to use for casting\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCasted table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.schema()\n\nibis.Schema {\n  species            string\n  island             string\n  bill_length_mm     float64\n  bill_depth_mm      float64\n  flipper_length_mm  int64\n  body_mass_g        int64\n  sex                string\n  year               int64\n}\n\n\n\n&gt;&gt;&gt; cols = [\"body_mass_g\", \"bill_length_mm\"]\n&gt;&gt;&gt; t[cols].head()\n\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ body_mass_g ┃ bill_length_mm ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int64       │ float64        │\n├─────────────┼────────────────┤\n│        3750 │           39.1 │\n│        3800 │           39.5 │\n│        3250 │           40.3 │\n│        NULL │           NULL │\n│        3450 │           36.7 │\n└─────────────┴────────────────┘\n\n\n\nColumns not present in the input schema will be passed through unchanged\n\n&gt;&gt;&gt; t.columns\n\n['species',\n 'island',\n 'bill_length_mm',\n 'bill_depth_mm',\n 'flipper_length_mm',\n 'body_mass_g',\n 'sex',\n 'year']\n\n\n\n&gt;&gt;&gt; expr = t.cast({\"body_mass_g\": \"float64\", \"bill_length_mm\": \"int\"})\n&gt;&gt;&gt; expr.select(*cols).head()\n\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ body_mass_g ┃ bill_length_mm ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ float64     │ int64          │\n├─────────────┼────────────────┤\n│      3750.0 │             39 │\n│      3800.0 │             40 │\n│      3250.0 │             40 │\n│        NULL │           NULL │\n│      3450.0 │             37 │\n└─────────────┴────────────────┘\n\n\n\nColumns that are in the input schema but not in the table raise an error\n\n&gt;&gt;&gt; t.cast({\"foo\": \"string\"})  \n\nIbisError: Cast schema has fields that are not in the table: ['foo']\n\n\n\n\n\ncount\ncount(where=None)\nCompute the number of rows in the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nOptional boolean expression to filter rows when counting.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of rows in the table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [\"foo\", \"bar\", \"baz\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ a      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ foo    │\n│ bar    │\n│ baz    │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.count()\n\n\n\n\n\n3\n\n\n\n\n&gt;&gt;&gt; t.count(t.a != \"foo\")\n\n\n\n\n\n2\n\n\n\n\n&gt;&gt;&gt; type(t.count())\n\nibis.expr.types.numeric.IntegerScalar\n\n\n\n\n\ncross_join\ncross_join(left, right, *rest, lname='', rname='{name}_right')\nCompute the cross join of a sequence of tables.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nLeft table\nrequired\n\n\nright\nTable\nRight table\nrequired\n\n\nrest\nTable\nAdditional tables to cross join\n()\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCross join of left, right and rest\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.count()\n\n\n\n\n\n344\n\n\n\n\n&gt;&gt;&gt; agg = t.drop(\"year\").agg(s.across(s.numeric(), _.mean()))\n&gt;&gt;&gt; expr = t.cross_join(agg)\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_mm_right ┃ bill_depth_mm_right ┃ flipper_length_mm_right ┃ body_mass_g_right ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ float64              │ float64             │ float64                 │ float64           │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼──────────────────────┼─────────────────────┼─────────────────────────┼───────────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │             43.92193 │            17.15117 │              200.915205 │       4201.754386 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │                    … │                   … │                       … │                 … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴──────────────────────┴─────────────────────┴─────────────────────────┴───────────────────┘\n\n\n\n\n&gt;&gt;&gt; expr.columns\n\n['species',\n 'island',\n 'bill_length_mm',\n 'bill_depth_mm',\n 'flipper_length_mm',\n 'body_mass_g',\n 'sex',\n 'year',\n 'bill_length_mm_right',\n 'bill_depth_mm_right',\n 'flipper_length_mm_right',\n 'body_mass_g_right']\n\n\n\n&gt;&gt;&gt; expr.count()\n\n\n\n\n\n344\n\n\n\n\n\n\ndifference\ndifference(table, *rest, distinct=True)\nCompute the set difference of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly diff distinct rows not occurring in the calling table\nTrue\n\n\n\n\n\nSee Also\nibis.difference\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe rows present in self that are not present in tables.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t1.difference(t2)\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n└───────┘\n\n\n\n\n\n\ndistinct\ndistinct(on=None, keep='first')\nReturn a Table with duplicate rows removed.\nSimilar to pandas.DataFrame.drop_duplicates().\n\n\n\n\n\n\nSome backends do not support keep='last'\n\n\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\non\nstr | Iterable[str] | s.Selector | None\nOnly consider certain columns for identifying duplicates. By default deduplicate all of the columns.\nNone\n\n\nkeep\nLiteral[‘first’, ‘last’] | None\nDetermines which duplicates to keep. - \"first\": Drop duplicates except for the first occurrence. - \"last\": Drop duplicates except for the last occurrence. - None: Drop all duplicates\n'first'\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.examples as ex\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ex.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nCompute the distinct rows of a subset of columns\n\n&gt;&gt;&gt; t[[\"species\", \"island\"]].distinct().order_by(s.all())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species   ┃ island    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Biscoe    │\n│ Adelie    │ Dream     │\n│ Adelie    │ Torgersen │\n│ Chinstrap │ Dream     │\n│ Gentoo    │ Biscoe    │\n└───────────┴───────────┘\n\n\n\nDrop all duplicate rows except the first\n\n&gt;&gt;&gt; t.distinct(on=[\"species\", \"island\"], keep=\"first\").order_by(s.all())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie    │ Biscoe    │           37.8 │          18.3 │               174 │        3400 │ female │  2007 │\n│ Adelie    │ Dream     │           39.5 │          16.7 │               178 │        3250 │ female │  2007 │\n│ Adelie    │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Chinstrap │ Dream     │           46.5 │          17.9 │               192 │        3500 │ female │  2007 │\n│ Gentoo    │ Biscoe    │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n└───────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nDrop all duplicate rows except the last\n\n&gt;&gt;&gt; t.distinct(on=[\"species\", \"island\"], keep=\"last\").order_by(s.all())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie    │ Biscoe    │           42.7 │          18.3 │               196 │        4075 │ male   │  2009 │\n│ Adelie    │ Dream     │           41.5 │          18.5 │               201 │        4000 │ male   │  2009 │\n│ Adelie    │ Torgersen │           43.1 │          19.2 │               197 │        3500 │ male   │  2009 │\n│ Chinstrap │ Dream     │           50.2 │          18.7 │               198 │        3775 │ female │  2009 │\n│ Gentoo    │ Biscoe    │           49.9 │          16.1 │               213 │        5400 │ male   │  2009 │\n└───────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nDrop all duplicated rows\n\n&gt;&gt;&gt; expr = t.distinct(\n...     on=[\"species\", \"island\", \"year\", \"bill_length_mm\"], keep=None\n... )\n&gt;&gt;&gt; expr.count()\n\n\n\n\n\n273\n\n\n\n\n&gt;&gt;&gt; t.count()\n\n\n\n\n\n344\n\n\n\nYou can pass selectors to on\n\n&gt;&gt;&gt; t.distinct(on=~s.numeric())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie    │ Biscoe    │           37.8 │          18.3 │               174 │        3400 │ female │  2007 │\n│ Gentoo    │ Biscoe    │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n│ Adelie    │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie    │ Dream     │           37.2 │          18.1 │               178 │        3900 │ male   │  2007 │\n│ Chinstrap │ Dream     │           46.5 │          17.9 │               192 │        3500 │ female │  2007 │\n│ Gentoo    │ Biscoe    │           44.5 │          14.3 │               216 │        4100 │ NULL   │  2007 │\n│ Adelie    │ Biscoe    │           37.7 │          18.7 │               180 │        3600 │ male   │  2007 │\n│ Gentoo    │ Biscoe    │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │\n│ Adelie    │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie    │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ …         │ …         │              … │             … │                 … │           … │ …      │     … │\n└───────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nThe only valid values of keep are \"first\", \"last\" and [`None][None]\n\n&gt;&gt;&gt; t.distinct(on=\"species\", keep=\"second\")  \n\nIbisError: Invalid value for `keep`: 'second', must be 'first', 'last' or None\n\n\n\n\n\ndrop\ndrop(*fields)\nRemove fields from a table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfields\nstr | Selector\nFields to drop. Strings and selectors are accepted.\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table with all columns matching fields removed.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nDrop one or more columns\n\n&gt;&gt;&gt; t.drop(\"species\").head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.drop(\"species\", \"bill_length_mm\").head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │\n│ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │\n│ Torgersen │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │\n└───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nDrop with selectors, mix and match\n\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.drop(\"species\", s.startswith(\"bill_\")).head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ int64             │ int64       │ string │ int64 │\n├───────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Torgersen │               181 │        3750 │ male   │  2007 │\n│ Torgersen │               186 │        3800 │ female │  2007 │\n│ Torgersen │               195 │        3250 │ female │  2007 │\n│ Torgersen │              NULL │        NULL │ NULL   │  2007 │\n│ Torgersen │               193 │        3450 │ female │  2007 │\n└───────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\n\ndropna\ndropna(subset=None, how='any')\nRemove rows with null values from the table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubset\nSequence[str] | str | None\nColumns names to consider when dropping nulls. By default all columns are considered.\nNone\n\n\nhow\nLiteral[‘any’, ‘all’]\nDetermine whether a row is removed if there is at least one null value in the row ('any'), or if all row values are null ('all').\n'any'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.count()\n\n\n\n\n\n344\n\n\n\n\n&gt;&gt;&gt; t.dropna([\"bill_length_mm\", \"body_mass_g\"]).count()\n\n\n\n\n\n342\n\n\n\n\n&gt;&gt;&gt; t.dropna(how=\"all\").count()  # no rows where all columns are null\n\n\n\n\n\n344\n\n\n\n\n\n\nfillna\nfillna(replacements)\nFill null values in a table expression.\n\n\n\n\n\n\nThere is potential lack of type stability with the fillna API\n\n\n\nFor example, different library versions may impact whether a given backend promotes integer replacement values to floats.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nreplacements\nir.Scalar | Mapping[str, ir.Scalar]\nValue with which to fill nulls. If replacements is a mapping, the keys are column names that map to their replacement value. If passed as a scalar all columns are filled with that value.\nrequired\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.sex\n\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n│ male   │\n│ female │\n│ male   │\n│ NULL   │\n│ NULL   │\n│ …      │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.fillna({\"sex\": \"unrecorded\"}).sex\n\n┏━━━━━━━━━━━━┓\n┃ sex        ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ male       │\n│ female     │\n│ female     │\n│ unrecorded │\n│ female     │\n│ male       │\n│ female     │\n│ male       │\n│ unrecorded │\n│ unrecorded │\n│ …          │\n└────────────┘\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nfilter\nfilter(predicates)\nSelect rows from table based on predicates.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicates\nir.BooleanValue | Sequence[ir.BooleanValue] | IfAnyAll\nBoolean value expressions used to select rows in table.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nFiltered table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.filter(\n...     [t.species == \"Adelie\", t.body_mass_g &gt; 3500]\n... ).sex.value_counts().dropna(\"sex\").order_by(\"sex\")\n\n┏━━━━━━━━┳━━━━━━━━━━━┓\n┃ sex    ┃ sex_count ┃\n┡━━━━━━━━╇━━━━━━━━━━━┩\n│ string │ int64     │\n├────────┼───────────┤\n│ female │        22 │\n│ male   │        68 │\n└────────┴───────────┘\n\n\n\n\n\n\nget_name\nget_name()\nReturn the fully qualified name of the table.\n\n\ngroup_by\ngroup_by(by=None, **key_exprs)\nCreate a grouped table expression.\nSimilar to SQL’s GROUP BY statement, or pandas .groupby() method.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nby\nstr | ir.Value | Iterable[str] | Iterable[ir.Value] | None\nGrouping expressions\nNone\n\n\nkey_exprs\nstr | ir.Value | Iterable[str] | Iterable[ir.Value]\nNamed grouping expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA grouped table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"fruit\": [\"apple\", \"apple\", \"banana\", \"orange\"],\n...         \"price\": [0.5, 0.5, 0.25, 0.33],\n...     }\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┳━━━━━━━━━┓\n┃ fruit  ┃ price   ┃\n┡━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │\n├────────┼─────────┤\n│ apple  │    0.50 │\n│ apple  │    0.50 │\n│ banana │    0.25 │\n│ orange │    0.33 │\n└────────┴─────────┘\n\n\n\n\n&gt;&gt;&gt; t.group_by(\"fruit\").agg(\n...     total_cost=_.price.sum(), avg_cost=_.price.mean()\n... ).order_by(\"fruit\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ fruit  ┃ total_cost ┃ avg_cost ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ float64    │ float64  │\n├────────┼────────────┼──────────┤\n│ apple  │       1.00 │     0.50 │\n│ banana │       0.25 │     0.25 │\n│ orange │       0.33 │     0.33 │\n└────────┴────────────┴──────────┘\n\n\n\n\n\n\nhead\nhead(n=5)\nSelect the first n rows of a table.\n\n\n\n\n\n\nThe result set is not deterministic without a call to order_by.\n\n\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint\nNumber of rows to include\n5\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nself limited to n rows\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 1, 2], \"b\": [\"c\", \"a\", \"a\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n│     2 │ a      │\n└───────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.head(2)\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n└───────┴────────┘\n\n\n\n\n\nSee Also\nTable.limit Table.order_by\n\n\n\ninfo\ninfo()\nReturn summary information about a table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nSummary of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.info()\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━┓\n┃ name              ┃ type    ┃ nullable ┃ nulls ┃ non_nulls ┃ null_frac ┃ pos  ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━┩\n│ string            │ string  │ boolean  │ int64 │ int64     │ float64   │ int8 │\n├───────────────────┼─────────┼──────────┼───────┼───────────┼───────────┼──────┤\n│ species           │ string  │ True     │     0 │       344 │  0.000000 │    0 │\n│ island            │ string  │ True     │     0 │       344 │  0.000000 │    1 │\n│ bill_length_mm    │ float64 │ True     │     2 │       342 │  0.005814 │    2 │\n│ bill_depth_mm     │ float64 │ True     │     2 │       342 │  0.005814 │    3 │\n│ flipper_length_mm │ int64   │ True     │     2 │       342 │  0.005814 │    4 │\n│ body_mass_g       │ int64   │ True     │     2 │       342 │  0.005814 │    5 │\n│ sex               │ string  │ True     │    11 │       333 │  0.031977 │    6 │\n│ year              │ int64   │ True     │     0 │       344 │  0.000000 │    7 │\n└───────────────────┴─────────┴──────────┴───────┴───────────┴───────────┴──────┘\n\n\n\n\n\n\nintersect\nintersect(table, *rest, distinct=True)\nCompute the set intersection of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the intersection of all input tables.\n\n\n\n\n\nSee Also\nibis.intersect\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t1.intersect(t2)\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n└───────┘\n\n\n\n\n\n\njoin\njoin(left, right, predicates=(), how='inner', *, lname='', rname='{name}_right')\nPerform a join between two tables.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nleft\nTable\nLeft table to join\nrequired\n\n\nright\nTable\nRight table to join\nrequired\n\n\npredicates\nstr | Sequence[str | ir.BooleanColumn | Literal[True] | Literal[False] | tuple[str | ir.Column | ir.Deferred, str | ir.Column | ir.Deferred]]\nCondition(s) to join on. See examples for details.\n()\n\n\nhow\nLiteral[‘inner’, ‘left’, ‘outer’, ‘right’, ‘semi’, ‘anti’, ‘any_inner’, ‘any_left’, ‘left_semi’]\nJoin method, e.g. \"inner\" or \"left\".\n'inner'\n\n\nlname\nstr\nA format string to use to rename overlapping columns in the left table (e.g. \"left_{name}\").\n''\n\n\nrname\nstr\nA format string to use to rename overlapping columns in the right table (e.g. \"right_{name}\").\n'{name}_right'\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; movies = ibis.examples.ml_latest_small_movies.fetch()\n&gt;&gt;&gt; movies.head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ movieId ┃ title                              ┃ genres                                      ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string                             │ string                                      │\n├─────────┼────────────────────────────────────┼─────────────────────────────────────────────┤\n│       1 │ Toy Story (1995)                   │ Adventure|Animation|Children|Comedy|Fantasy │\n│       2 │ Jumanji (1995)                     │ Adventure|Children|Fantasy                  │\n│       3 │ Grumpier Old Men (1995)            │ Comedy|Romance                              │\n│       4 │ Waiting to Exhale (1995)           │ Comedy|Drama|Romance                        │\n│       5 │ Father of the Bride Part II (1995) │ Comedy                                      │\n└─────────┴────────────────────────────────────┴─────────────────────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; ratings = ibis.examples.ml_latest_small_ratings.fetch().drop(\"timestamp\")\n&gt;&gt;&gt; ratings.head()\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n┃ userId ┃ movieId ┃ rating  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n│ int64  │ int64   │ float64 │\n├────────┼─────────┼─────────┤\n│      1 │       1 │     4.0 │\n│      1 │       3 │     4.0 │\n│      1 │       6 │     4.0 │\n│      1 │      47 │     5.0 │\n│      1 │      50 │     5.0 │\n└────────┴─────────┴─────────┘\n\n\n\nEquality left join on the shared movieId column. Note the _right suffix added to all overlapping columns from the right table (in this case only the “movieId” column).\n\n&gt;&gt;&gt; ratings.join(movies, \"movieId\", how=\"left\").head(5)\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ userId ┃ movieId ┃ rating  ┃ movieId_right ┃ title                       ┃ genres                                      ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64  │ int64   │ float64 │ int64         │ string                      │ string                                      │\n├────────┼─────────┼─────────┼───────────────┼─────────────────────────────┼─────────────────────────────────────────────┤\n│      1 │       1 │     4.0 │             1 │ Toy Story (1995)            │ Adventure|Animation|Children|Comedy|Fantasy │\n│      1 │       3 │     4.0 │             3 │ Grumpier Old Men (1995)     │ Comedy|Romance                              │\n│      1 │       6 │     4.0 │             6 │ Heat (1995)                 │ Action|Crime|Thriller                       │\n│      1 │      47 │     5.0 │            47 │ Seven (a.k.a. Se7en) (1995) │ Mystery|Thriller                            │\n│      1 │      50 │     5.0 │            50 │ Usual Suspects, The (1995)  │ Crime|Mystery|Thriller                      │\n└────────┴─────────┴─────────┴───────────────┴─────────────────────────────┴─────────────────────────────────────────────┘\n\n\n\nExplicit equality join using the default how value of \"inner\". Note how there is no _right suffix added to the movieId column since this is an inner join and the movieId column is part of the join condition.\n\n&gt;&gt;&gt; ratings.join(movies, ratings.movieId == movies.movieId).head(5)\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ userId ┃ movieId ┃ rating  ┃ title                       ┃ genres                                      ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64  │ int64   │ float64 │ string                      │ string                                      │\n├────────┼─────────┼─────────┼─────────────────────────────┼─────────────────────────────────────────────┤\n│      1 │       1 │     4.0 │ Toy Story (1995)            │ Adventure|Animation|Children|Comedy|Fantasy │\n│      1 │       3 │     4.0 │ Grumpier Old Men (1995)     │ Comedy|Romance                              │\n│      1 │       6 │     4.0 │ Heat (1995)                 │ Action|Crime|Thriller                       │\n│      1 │      47 │     5.0 │ Seven (a.k.a. Se7en) (1995) │ Mystery|Thriller                            │\n│      1 │      50 │     5.0 │ Usual Suspects, The (1995)  │ Crime|Mystery|Thriller                      │\n└────────┴─────────┴─────────┴─────────────────────────────┴─────────────────────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; tags = ibis.examples.ml_latest_small_tags.fetch()\n&gt;&gt;&gt; tags.head()\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ userId ┃ movieId ┃ tag             ┃ timestamp  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ int64  │ int64   │ string          │ int64      │\n├────────┼─────────┼─────────────────┼────────────┤\n│      2 │   60756 │ funny           │ 1445714994 │\n│      2 │   60756 │ Highly quotable │ 1445714996 │\n│      2 │   60756 │ will ferrell    │ 1445714992 │\n│      2 │   89774 │ Boxing story    │ 1445715207 │\n│      2 │   89774 │ MMA             │ 1445715200 │\n└────────┴─────────┴─────────────────┴────────────┘\n\n\n\nYou can join on multiple columns/conditions by passing in a sequence. Find all instances where a user both tagged and rated a movie:\n\n&gt;&gt;&gt; tags.join(ratings, [\"userId\", \"movieId\"]).head(5)\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ userId ┃ movieId ┃ tag            ┃ timestamp  ┃ rating  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━┩\n│ int64  │ int64   │ string         │ int64      │ float64 │\n├────────┼─────────┼────────────────┼────────────┼─────────┤\n│     62 │       2 │ Robin Williams │ 1528843907 │     4.0 │\n│     62 │     110 │ sword fight    │ 1528152535 │     4.5 │\n│     62 │     410 │ gothic         │ 1525636609 │     4.5 │\n│     62 │    2023 │ mafia          │ 1525636733 │     5.0 │\n│     62 │    2124 │ quirky         │ 1525636846 │     5.0 │\n└────────┴─────────┴────────────────┴────────────┴─────────┘\n\n\n\nTo self-join a table with itself, you need to call .view() on one of the arguments so the two tables are distinct from each other.\nFor crafting more complex join conditions, a valid form of a join condition is a 2-tuple like ({left_key}, {right_key}), where each key can be\n\na Column\nDeferred expression\nlambda of the form (Table) -&gt; Column\n\nFor example, to find all movies pairings that received the same (ignoring case) tags:\n\n&gt;&gt;&gt; movie_tags = tags[\"movieId\", \"tag\"]\n&gt;&gt;&gt; view = movie_tags.view()\n&gt;&gt;&gt; movie_tags.join(\n...     view,\n...     [\n...         movie_tags.movieId != view.movieId,\n...         (_.tag.lower(), lambda t: t.tag.lower()),\n...     ],\n... ).head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ movieId ┃ tag               ┃ movieId_right ┃ tag_right         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string            │ int64         │ string            │\n├─────────┼───────────────────┼───────────────┼───────────────────┤\n│   60756 │ funny             │          1732 │ funny             │\n│   60756 │ Highly quotable   │          1732 │ Highly quotable   │\n│   89774 │ Tom Hardy         │        139385 │ tom hardy         │\n│  106782 │ drugs             │          1732 │ drugs             │\n│  106782 │ Leonardo DiCaprio │          5989 │ Leonardo DiCaprio │\n└─────────┴───────────────────┴───────────────┴───────────────────┘\n\n\n\n\n\n\nlimit\nlimit(n, offset=0)\nSelect n rows from self starting at offset.\n\n\n\n\n\n\nThe result set is not deterministic without a call to order_by.\n\n\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | None\nNumber of rows to include. If None, the entire table is selected starting from offset.\nrequired\n\n\noffset\nint\nNumber of rows to skip first\n0\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe first n rows of self starting at offset\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 1, 2], \"b\": [\"c\", \"a\", \"a\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n│     2 │ a      │\n└───────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.limit(2)\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ c      │\n│     1 │ a      │\n└───────┴────────┘\n\n\n\nYou can use None with offset to slice starting from a particular row\n\n&gt;&gt;&gt; t.limit(None, offset=1)\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ a      │\n│     2 │ a      │\n└───────┴────────┘\n\n\n\n\n\nSee Also\nTable.order_by\n\n\n\nmutate\nmutate(exprs=None, **mutations)\nAdd columns to a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nSequence[ir.Expr] | None\nList of named expressions to add as columns\nNone\n\n\nmutations\nir.Value\nNamed expressions using keyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression with additional columns\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().select(\n...     \"species\", \"year\", \"bill_length_mm\"\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │\n├─────────┼───────┼────────────────┤\n│ Adelie  │  2007 │           39.1 │\n│ Adelie  │  2007 │           39.5 │\n│ Adelie  │  2007 │           40.3 │\n│ Adelie  │  2007 │           NULL │\n│ Adelie  │  2007 │           36.7 │\n│ Adelie  │  2007 │           39.3 │\n│ Adelie  │  2007 │           38.9 │\n│ Adelie  │  2007 │           39.2 │\n│ Adelie  │  2007 │           34.1 │\n│ Adelie  │  2007 │           42.0 │\n│ …       │     … │              … │\n└─────────┴───────┴────────────────┘\n\n\n\nAdd a new column from a per-element expression\n\n&gt;&gt;&gt; t.mutate(next_year=_.year + 1).head()\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃ next_year ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │ int64     │\n├─────────┼───────┼────────────────┼───────────┤\n│ Adelie  │  2007 │           39.1 │      2008 │\n│ Adelie  │  2007 │           39.5 │      2008 │\n│ Adelie  │  2007 │           40.3 │      2008 │\n│ Adelie  │  2007 │           NULL │      2008 │\n│ Adelie  │  2007 │           36.7 │      2008 │\n└─────────┴───────┴────────────────┴───────────┘\n\n\n\nAdd a new column based on an aggregation. Note the automatic broadcasting.\n\n&gt;&gt;&gt; t.select(\n...     \"species\", bill_demean=_.bill_length_mm - _.bill_length_mm.mean()\n... ).head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ species ┃ bill_demean ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string  │ float64     │\n├─────────┼─────────────┤\n│ Adelie  │    -4.82193 │\n│ Adelie  │    -4.42193 │\n│ Adelie  │    -3.62193 │\n│ Adelie  │        NULL │\n│ Adelie  │    -7.22193 │\n└─────────┴─────────────┘\n\n\n\nMutate across multiple columns\n\n&gt;&gt;&gt; t.mutate(s.across(s.numeric() & ~s.c(\"year\"), _ - _.mean())).head()\n\n┏━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ year  ┃ bill_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ int64 │ float64        │\n├─────────┼───────┼────────────────┤\n│ Adelie  │  2007 │       -4.82193 │\n│ Adelie  │  2007 │       -4.42193 │\n│ Adelie  │  2007 │       -3.62193 │\n│ Adelie  │  2007 │           NULL │\n│ Adelie  │  2007 │       -7.22193 │\n└─────────┴───────┴────────────────┘\n\n\n\n\n\n\nnunique\nnunique(where=None)\nCompute the number of unique rows in the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nOptional boolean expression to filter rows when counting.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of unique rows in the table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [\"foo\", \"bar\", \"bar\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ a      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ foo    │\n│ bar    │\n│ bar    │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.nunique()\n\n\n\n\n\n2\n\n\n\n\n&gt;&gt;&gt; t.nunique(t.a != \"foo\")\n\n\n\n\n\n1\n\n\n\n\n\n\norder_by\norder_by(by)\nSort a table by one or more expressions.\nSimilar to pandas.DataFrame.sort_values().\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nby\nstr | ir.Column | s.Selector | Sequence[str] | Sequence[ir.Column] | Sequence[s.Selector] | None\nExpressions to sort the table by.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nSorted table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"a\": [3, 2, 1, 3],\n...         \"b\": [\"a\", \"B\", \"c\", \"D\"],\n...         \"c\": [4, 6, 5, 7],\n...     }\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     3 │ a      │     4 │\n│     2 │ B      │     6 │\n│     1 │ c      │     5 │\n│     3 │ D      │     7 │\n└───────┴────────┴───────┘\n\n\n\nSort by b. Default is ascending. Note how capital letters come before lowercase\n\n&gt;&gt;&gt; t.order_by(\"b\")\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     2 │ B      │     6 │\n│     3 │ D      │     7 │\n│     3 │ a      │     4 │\n│     1 │ c      │     5 │\n└───────┴────────┴───────┘\n\n\n\nSort in descending order\n\n&gt;&gt;&gt; t.order_by(ibis.desc(\"b\"))\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     1 │ c      │     5 │\n│     3 │ a      │     4 │\n│     3 │ D      │     7 │\n│     2 │ B      │     6 │\n└───────┴────────┴───────┘\n\n\n\nYou can also use the deferred API to get the same result\n\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; t.order_by(_.b.desc())\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     1 │ c      │     5 │\n│     3 │ a      │     4 │\n│     3 │ D      │     7 │\n│     2 │ B      │     6 │\n└───────┴────────┴───────┘\n\n\n\nSort by multiple columns/expressions\n\n&gt;&gt;&gt; t.order_by([\"a\", _.c.desc()])\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     1 │ c      │     5 │\n│     2 │ B      │     6 │\n│     3 │ D      │     7 │\n│     3 │ a      │     4 │\n└───────┴────────┴───────┘\n\n\n\nYou can actually pass arbitrary expressions to use as sort keys. For example, to ignore the case of the strings in column b\n\n&gt;&gt;&gt; t.order_by(_.b.lower())\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     3 │ a      │     4 │\n│     2 │ B      │     6 │\n│     1 │ c      │     5 │\n│     3 │ D      │     7 │\n└───────┴────────┴───────┘\n\n\n\nThis means than shuffling a Table is super simple\n\n&gt;&gt;&gt; t.order_by(ibis.random())\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ string │ int64 │\n├───────┼────────┼───────┤\n│     3 │ D      │     7 │\n│     1 │ c      │     5 │\n│     3 │ a      │     4 │\n│     2 │ B      │     6 │\n└───────┴────────┴───────┘\n\n\n\n\n\n\npivot_longer\npivot_longer(col, *, names_to='name', names_pattern='(.+)', names_transform=None, values_to='value', values_transform=None)\nTransform a table from wider to longer.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncol\nstr | s.Selector\nString column name or selector.\nrequired\n\n\nnames_to\nstr | Iterable[str]\nA string or iterable of strings indicating how to name the new pivoted columns.\n'name'\n\n\nnames_pattern\nstr | re.Pattern\nPattern to use to extract column names from the input. By default the entire column name is extracted.\n'(.+)'\n\n\nnames_transform\nCallable[[str], ir.Value] | Mapping[str, Callable[[str], ir.Value]] | None\nFunction or mapping of a name in names_to to a function to transform a column name to a value.\nNone\n\n\nvalues_to\nstr\nName of the pivoted value column.\n'value'\n\n\nvalues_transform\nCallable[[ir.Value], ir.Value] | Deferred | None\nApply a function to the value column. This can be a lambda or deferred expression.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nPivoted table\n\n\n\n\n\nExamples\nBasic usage\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; relig_income = ibis.examples.relig_income_raw.fetch()\n&gt;&gt;&gt; relig_income\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ religion                ┃ &lt;$10k ┃ $10-20k ┃ $20-30k ┃ $30-40k ┃ $40-50k ┃ $50-75k ┃ $75-100k ┃ $100-150k ┃ &gt;150k ┃ Don't know/refused ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string                  │ int64 │ int64   │ int64   │ int64   │ int64   │ int64   │ int64    │ int64     │ int64 │ int64              │\n├─────────────────────────┼───────┼─────────┼─────────┼─────────┼─────────┼─────────┼──────────┼───────────┼───────┼────────────────────┤\n│ Agnostic                │    27 │      34 │      60 │      81 │      76 │     137 │      122 │       109 │    84 │                 96 │\n│ Atheist                 │    12 │      27 │      37 │      52 │      35 │      70 │       73 │        59 │    74 │                 76 │\n│ Buddhist                │    27 │      21 │      30 │      34 │      33 │      58 │       62 │        39 │    53 │                 54 │\n│ Catholic                │   418 │     617 │     732 │     670 │     638 │    1116 │      949 │       792 │   633 │               1489 │\n│ Don’t know/refused      │    15 │      14 │      15 │      11 │      10 │      35 │       21 │        17 │    18 │                116 │\n│ Evangelical Prot        │   575 │     869 │    1064 │     982 │     881 │    1486 │      949 │       723 │   414 │               1529 │\n│ Hindu                   │     1 │       9 │       7 │       9 │      11 │      34 │       47 │        48 │    54 │                 37 │\n│ Historically Black Prot │   228 │     244 │     236 │     238 │     197 │     223 │      131 │        81 │    78 │                339 │\n│ Jehovah's Witness       │    20 │      27 │      24 │      24 │      21 │      30 │       15 │        11 │     6 │                 37 │\n│ Jewish                  │    19 │      19 │      25 │      25 │      30 │      95 │       69 │        87 │   151 │                162 │\n│ …                       │     … │       … │       … │       … │       … │       … │        … │         … │     … │                  … │\n└─────────────────────────┴───────┴─────────┴─────────┴─────────┴─────────┴─────────┴──────────┴───────────┴───────┴────────────────────┘\n\n\n\nHere we convert column names not matching the selector for the religion column and convert those names into values\n\n&gt;&gt;&gt; relig_income.pivot_longer(\n...     ~s.c(\"religion\"), names_to=\"income\", values_to=\"count\"\n... )\n\n┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━┓\n┃ religion ┃ income             ┃ count ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━┩\n│ string   │ string             │ int64 │\n├──────────┼────────────────────┼───────┤\n│ Agnostic │ &lt;$10k              │    27 │\n│ Agnostic │ $10-20k            │    34 │\n│ Agnostic │ $20-30k            │    60 │\n│ Agnostic │ $30-40k            │    81 │\n│ Agnostic │ $40-50k            │    76 │\n│ Agnostic │ $50-75k            │   137 │\n│ Agnostic │ $75-100k           │   122 │\n│ Agnostic │ $100-150k          │   109 │\n│ Agnostic │ &gt;150k              │    84 │\n│ Agnostic │ Don't know/refused │    96 │\n│ …        │ …                  │     … │\n└──────────┴────────────────────┴───────┘\n\n\n\nSimilarly for a different example dataset, we convert names to values but using a different selector and the default values_to value.\n\n&gt;&gt;&gt; world_bank_pop = ibis.examples.world_bank_pop_raw.fetch()\n&gt;&gt;&gt; world_bank_pop.head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ country ┃ indicator   ┃ 2000         ┃ 2001         ┃ 2002         ┃ 2003         ┃ 2004         ┃ 2005         ┃ 2006         ┃ 2007         ┃ 2008         ┃ 2009         ┃ 2010         ┃ 2011         ┃ 2012         ┃ 2013         ┃ 2014         ┃ 2015         ┃ 2016         ┃ 2017         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ string  │ string      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │ float64      │\n├─────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┤\n│ ABW     │ SP.URB.TOTL │ 4.162500e+04 │ 4.202500e+04 │ 4.219400e+04 │ 4.227700e+04 │ 4.231700e+04 │ 4.239900e+04 │ 4.255500e+04 │ 4.272900e+04 │ 4.290600e+04 │ 4.307900e+04 │ 4.320600e+04 │ 4.349300e+04 │ 4.386400e+04 │ 4.422800e+04 │ 4.458800e+04 │ 4.494300e+04 │ 4.529700e+04 │ 4.564800e+04 │\n│ ABW     │ SP.URB.GROW │ 1.664222e+00 │ 9.563731e-01 │ 4.013352e-01 │ 1.965172e-01 │ 9.456936e-02 │ 1.935880e-01 │ 3.672580e-01 │ 4.080490e-01 │ 4.133830e-01 │ 4.023963e-01 │ 2.943735e-01 │ 6.620631e-01 │ 8.493932e-01 │ 8.264135e-01 │ 8.106692e-01 │ 7.930256e-01 │ 7.845785e-01 │ 7.718989e-01 │\n│ ABW     │ SP.POP.TOTL │ 8.910100e+04 │ 9.069100e+04 │ 9.178100e+04 │ 9.270100e+04 │ 9.354000e+04 │ 9.448300e+04 │ 9.560600e+04 │ 9.678700e+04 │ 9.799600e+04 │ 9.921200e+04 │ 1.003410e+05 │ 1.012880e+05 │ 1.021120e+05 │ 1.028800e+05 │ 1.035940e+05 │ 1.042570e+05 │ 1.048740e+05 │ 1.054390e+05 │\n│ ABW     │ SP.POP.GROW │ 2.539234e+00 │ 1.768757e+00 │ 1.194718e+00 │ 9.973955e-01 │ 9.009892e-01 │ 1.003077e+00 │ 1.181566e+00 │ 1.227711e+00 │ 1.241397e+00 │ 1.233231e+00 │ 1.131541e+00 │ 9.393559e-01 │ 8.102306e-01 │ 7.493010e-01 │ 6.916153e-01 │ 6.379592e-01 │ 5.900625e-01 │ 5.372957e-01 │\n│ AFE     │ SP.URB.TOTL │ 1.155517e+08 │ 1.197755e+08 │ 1.242275e+08 │ 1.288340e+08 │ 1.336475e+08 │ 1.387456e+08 │ 1.440267e+08 │ 1.492313e+08 │ 1.553838e+08 │ 1.617762e+08 │ 1.684561e+08 │ 1.754157e+08 │ 1.825587e+08 │ 1.901087e+08 │ 1.980733e+08 │ 2.065563e+08 │ 2.150833e+08 │ 2.237321e+08 │\n└─────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┘\n\n\n\n\n&gt;&gt;&gt; world_bank_pop.pivot_longer(s.matches(r\"\\d{4}\"), names_to=\"year\").head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┓\n┃ country ┃ indicator   ┃ year   ┃ value   ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━┩\n│ string  │ string      │ string │ float64 │\n├─────────┼─────────────┼────────┼─────────┤\n│ ABW     │ SP.URB.TOTL │ 2000   │ 41625.0 │\n│ ABW     │ SP.URB.TOTL │ 2001   │ 42025.0 │\n│ ABW     │ SP.URB.TOTL │ 2002   │ 42194.0 │\n│ ABW     │ SP.URB.TOTL │ 2003   │ 42277.0 │\n│ ABW     │ SP.URB.TOTL │ 2004   │ 42317.0 │\n└─────────┴─────────────┴────────┴─────────┘\n\n\n\npivot_longer has some preprocessing capabiltiies like stripping a prefix and applying a function to column names\n\n&gt;&gt;&gt; billboard = ibis.examples.billboard.fetch()\n&gt;&gt;&gt; billboard\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ artist         ┃ track                   ┃ date_entered ┃ wk1   ┃ wk2   ┃ wk3   ┃ wk4   ┃ wk5   ┃ wk6   ┃ wk7   ┃ wk8   ┃ wk9   ┃ wk10  ┃ wk11  ┃ wk12  ┃ wk13  ┃ wk14  ┃ wk15  ┃ wk16  ┃ wk17  ┃ wk18  ┃ wk19  ┃ wk20  ┃ wk21  ┃ wk22  ┃ wk23  ┃ wk24  ┃ wk25  ┃ wk26  ┃ wk27  ┃ wk28  ┃ wk29  ┃ wk30  ┃ wk31  ┃ wk32  ┃ wk33  ┃ wk34  ┃ wk35  ┃ wk36  ┃ wk37  ┃ wk38  ┃ wk39  ┃ wk40  ┃ wk41  ┃ wk42  ┃ wk43  ┃ wk44  ┃ wk45  ┃ wk46  ┃ wk47  ┃ wk48  ┃ wk49  ┃ wk50  ┃ wk51  ┃ wk52  ┃ wk53  ┃ wk54  ┃ wk55  ┃ wk56  ┃ wk57  ┃ wk58  ┃ wk59  ┃ wk60  ┃ wk61  ┃ wk62  ┃ wk63  ┃ wk64  ┃ wk65  ┃ wk66   ┃ wk67   ┃ wk68   ┃ wk69   ┃ wk70   ┃ wk71   ┃ wk72   ┃ wk73   ┃ wk74   ┃ wk75   ┃ wk76   ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ string         │ string                  │ date         │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │ string │ string │ string │ string │ string │ string │ string │ string │ string │ string │ string │\n├────────────────┼─────────────────────────┼──────────────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼───────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┼────────┤\n│ 2 Pac          │ Baby Don't Cry (Keep... │ 2000-02-26   │    87 │    82 │    72 │    77 │    87 │    94 │    99 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ 2Ge+her        │ The Hardest Part Of ... │ 2000-09-02   │    91 │    87 │    92 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ 3 Doors Down   │ Kryptonite              │ 2000-04-08   │    81 │    70 │    68 │    67 │    66 │    57 │    54 │    53 │    51 │    51 │    51 │    51 │    47 │    44 │    38 │    28 │    22 │    18 │    18 │    14 │    12 │     7 │     6 │     6 │     6 │     5 │     5 │     4 │     4 │     4 │     4 │     3 │     3 │     3 │     4 │     5 │     5 │     9 │     9 │    15 │    14 │    13 │    14 │    16 │    17 │    21 │    22 │    24 │    28 │    33 │    42 │    42 │    49 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ 3 Doors Down   │ Loser                   │ 2000-10-21   │    76 │    76 │    72 │    69 │    67 │    65 │    55 │    59 │    62 │    61 │    61 │    59 │    61 │    66 │    72 │    76 │    75 │    67 │    73 │    70 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ 504 Boyz       │ Wobble Wobble           │ 2000-04-15   │    57 │    34 │    25 │    17 │    17 │    31 │    36 │    49 │    53 │    57 │    64 │    70 │    75 │    76 │    78 │    85 │    92 │    96 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ 98^0           │ Give Me Just One Nig... │ 2000-08-19   │    51 │    39 │    34 │    26 │    26 │    19 │     2 │     2 │     3 │     6 │     7 │    22 │    29 │    36 │    47 │    67 │    66 │    84 │    93 │    94 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ A*Teens        │ Dancing Queen           │ 2000-07-08   │    97 │    97 │    96 │    95 │   100 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ Aaliyah        │ I Don't Wanna           │ 2000-01-29   │    84 │    62 │    51 │    41 │    38 │    35 │    35 │    38 │    38 │    36 │    37 │    37 │    38 │    49 │    61 │    63 │    62 │    67 │    83 │    86 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ Aaliyah        │ Try Again               │ 2000-03-18   │    59 │    53 │    38 │    28 │    21 │    18 │    16 │    14 │    12 │    10 │     9 │     8 │     6 │     1 │     2 │     2 │     2 │     2 │     3 │     4 │     5 │     5 │     6 │     9 │    13 │    14 │    16 │    23 │    22 │    33 │    36 │    43 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ Adams, Yolanda │ Open My Heart           │ 2000-08-26   │    76 │    76 │    74 │    69 │    68 │    67 │    61 │    58 │    57 │    59 │    66 │    68 │    61 │    67 │    59 │    63 │    67 │    71 │    79 │    89 │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │  NULL │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │ NULL   │\n│ …              │ …                       │ …            │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │     … │ …      │ …      │ …      │ …      │ …      │ …      │ …      │ …      │ …      │ …      │ …      │\n└────────────────┴─────────────────────────┴──────────────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; billboard.pivot_longer(\n...     s.startswith(\"wk\"),\n...     names_to=\"week\",\n...     names_pattern=r\"wk(.+)\",\n...     names_transform=int,\n...     values_to=\"rank\",\n...     values_transform=_.cast(\"int\"),\n... ).dropna(\"rank\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃ artist  ┃ track                   ┃ date_entered ┃ week ┃ rank  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│ string  │ string                  │ date         │ int8 │ int64 │\n├─────────┼─────────────────────────┼──────────────┼──────┼───────┤\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    1 │    87 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    2 │    82 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    3 │    72 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    4 │    77 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    5 │    87 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    6 │    94 │\n│ 2 Pac   │ Baby Don't Cry (Keep... │ 2000-02-26   │    7 │    99 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    1 │    91 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    2 │    87 │\n│ 2Ge+her │ The Hardest Part Of ... │ 2000-09-02   │    3 │    92 │\n│ …       │ …                       │ …            │    … │     … │\n└─────────┴─────────────────────────┴──────────────┴──────┴───────┘\n\n\n\nYou can use regular expression capture groups to extract multiple variables stored in column names\n\n&gt;&gt;&gt; who = ibis.examples.who.fetch()\n&gt;&gt;&gt; who\n\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ new_sp_m014 ┃ new_sp_m1524 ┃ new_sp_m2534 ┃ new_sp_m3544 ┃ new_sp_m4554 ┃ new_sp_m5564 ┃ new_sp_m65 ┃ new_sp_f014 ┃ new_sp_f1524 ┃ new_sp_f2534 ┃ new_sp_f3544 ┃ new_sp_f4554 ┃ new_sp_f5564 ┃ new_sp_f65 ┃ new_sn_m014 ┃ new_sn_m1524 ┃ new_sn_m2534 ┃ new_sn_m3544 ┃ new_sn_m4554 ┃ new_sn_m5564 ┃ new_sn_m65 ┃ new_sn_f014 ┃ new_sn_f1524 ┃ new_sn_f2534 ┃ new_sn_f3544 ┃ new_sn_f4554 ┃ new_sn_f5564 ┃ new_sn_f65 ┃ new_ep_m014 ┃ new_ep_m1524 ┃ new_ep_m2534 ┃ new_ep_m3544 ┃ new_ep_m4554 ┃ new_ep_m5564 ┃ new_ep_m65 ┃ new_ep_f014 ┃ new_ep_f1524 ┃ new_ep_f2534 ┃ new_ep_f3544 ┃ new_ep_f4554 ┃ new_ep_f5564 ┃ new_ep_f65 ┃ newrel_m014 ┃ newrel_m1524 ┃ newrel_m2534 ┃ newrel_m3544 ┃ newrel_m4554 ┃ newrel_m5564 ┃ newrel_m65 ┃ newrel_f014 ┃ newrel_f1524 ┃ newrel_f2534 ┃ newrel_f3544 ┃ newrel_f4554 ┃ newrel_f5564 ┃ newrel_f65 ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string      │ string │ string │ int64 │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │ int64       │ int64        │ int64        │ int64        │ int64        │ int64        │ int64      │\n├─────────────┼────────┼────────┼───────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┼─────────────┼──────────────┼──────────────┼──────────────┼──────────────┼──────────────┼────────────┤\n│ Afghanistan │ AF     │ AFG    │  1980 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1981 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1982 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1983 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1984 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1985 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1986 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1987 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1988 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ Afghanistan │ AF     │ AFG    │  1989 │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │        NULL │         NULL │         NULL │         NULL │         NULL │         NULL │       NULL │\n│ …           │ …      │ …      │     … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │           … │            … │            … │            … │            … │            … │          … │\n└─────────────┴────────┴────────┴───────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴────────────┘\n\n\n\n\n&gt;&gt;&gt; len(who.columns)\n\n60\n\n\n\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_(.)(.*)\",\n...     values_to=\"count\",\n... )\n\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ diagnosis ┃ gender ┃ age    ┃ count ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string      │ string │ string │ int64 │ string    │ string │ string │ int64 │\n├─────────────┼────────┼────────┼───────┼───────────┼────────┼────────┼───────┤\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 014    │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 1524   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 2534   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 3544   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 4554   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 5564   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ m      │ 65     │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 014    │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 1524   │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │ f      │ 2534   │  NULL │\n│ …           │ …      │ …      │     … │ …         │ …      │ …      │     … │\n└─────────────┴────────┴────────┴───────┴───────────┴────────┴────────┴───────┘\n\n\n\nnames_transform is flexible, and can be:\n1. A mapping of one or more names in `names_to` to callable\n2. A callable that will be applied to every name\nLet’s recode gender and age to numeric values using a mapping\n\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_(.)(.*)\",\n...     names_transform=dict(\n...         gender={\"m\": 1, \"f\": 2}.get,\n...         age=dict(\n...             zip(\n...                 [\"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"],\n...                 range(7),\n...             )\n...         ).get,\n...     ),\n...     values_to=\"count\",\n... )\n\n┏━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━┳━━━━━━━┓\n┃ country     ┃ iso2   ┃ iso3   ┃ year  ┃ diagnosis ┃ gender ┃ age  ┃ count ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━╇━━━━━━━┩\n│ string      │ string │ string │ int64 │ string    │ int8   │ int8 │ int64 │\n├─────────────┼────────┼────────┼───────┼───────────┼────────┼──────┼───────┤\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    0 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    1 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    2 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    3 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    4 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    5 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      1 │    6 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    0 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    1 │  NULL │\n│ Afghanistan │ AF     │ AFG    │  1980 │ sp        │      2 │    2 │  NULL │\n│ …           │ …      │ …      │     … │ …         │      … │    … │     … │\n└─────────────┴────────┴────────┴───────┴───────────┴────────┴──────┴───────┘\n\n\n\nThe number of match groups in names_pattern must match the length of names_to\n\n&gt;&gt;&gt; who.pivot_longer(  \n...     s.r[\"new_sp_m014\":\"newrel_f65\"],\n...     names_to=[\"diagnosis\", \"gender\", \"age\"],\n...     names_pattern=\"new_?(.*)_.(.*)\",\n... )\n\nIbisInputError: Number of match groups in `names_pattern`'new_?(.*)_.(.*)' (2 groups) doesn't match the length of `names_to` ['diagnosis', 'gender', 'age'] (length 3)\n\n\nnames_transform must be a mapping or callable\n\n&gt;&gt;&gt; who.pivot_longer(\n...     s.r[\"new_sp_m014\":\"newrel_f65\"], names_transform=\"upper\"\n... )  # quartodoc: +EXPECTED_FAILURE\n\nIbisTypeError: `names_transform` must be a mapping or callable. Got &lt;class 'str'&gt;\n\n\n\n\n\npivot_wider\npivot_wider(id_cols=None, names_from='name', names_prefix='', names_sep='_', names_sort=False, names=None, values_from='value', values_fill=None, values_agg='arbitrary')\nPivot a table to a wider format.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nid_cols\ns.Selector | None\nA set of columns that uniquely identify each observation.\nNone\n\n\nnames_from\nstr | Iterable[str] | s.Selector\nAn argument describing which column or columns to use to get the name of the output columns.\n'name'\n\n\nnames_prefix\nstr\nString added to the start of every column name.\n''\n\n\nnames_sep\nstr\nIf names_from or values_from contains multiple columns, this argument will be used to join their values together into a single string to use as a column name.\n'_'\n\n\nnames_sort\nbool\nIf True columns are sorted. If False column names are ordered by appearance.\nFalse\n\n\nnames\nIterable[str] | None\nAn explicit sequence of values to look for in columns matching names_from. * When this value is None, the values will be computed from names_from. * When this value is not None, each element’s length must match the length of names_from. See examples below for more detail.\nNone\n\n\nvalues_from\nstr | Iterable[str] | s.Selector\nAn argument describing which column or columns to get the cell values from.\n'value'\n\n\nvalues_fill\nint | float | str | ir.Scalar | None\nA scalar value that specifies what each value should be filled with when missing.\nNone\n\n\nvalues_agg\nstr | Callable[[ir.Value], ir.Scalar] | Deferred\nA function applied to the value in each cell in the output.\n'arbitrary'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nWider pivoted table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n\nBasic usage\n\n&gt;&gt;&gt; fish_encounters = ibis.examples.fish_encounters.fetch()\n&gt;&gt;&gt; fish_encounters\n\n┏━━━━━━━┳━━━━━━━━━┳━━━━━━━┓\n┃ fish  ┃ station ┃ seen  ┃\n┡━━━━━━━╇━━━━━━━━━╇━━━━━━━┩\n│ int64 │ string  │ int64 │\n├───────┼─────────┼───────┤\n│  4842 │ Release │     1 │\n│  4842 │ I80_1   │     1 │\n│  4842 │ Lisbon  │     1 │\n│  4842 │ Rstr    │     1 │\n│  4842 │ Base_TD │     1 │\n│  4842 │ BCE     │     1 │\n│  4842 │ BCW     │     1 │\n│  4842 │ BCE2    │     1 │\n│  4842 │ BCW2    │     1 │\n│  4842 │ MAE     │     1 │\n│     … │ …       │     … │\n└───────┴─────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; fish_encounters.pivot_wider(\n...     names_from=\"station\", values_from=\"seen\"\n... )\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ fish  ┃ Lisbon ┃ Base_TD ┃ MAE   ┃ MAW   ┃ Release ┃ BCE   ┃ BCE2  ┃ I80_1 ┃ Rstr  ┃ BCW   ┃ BCW2  ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ int64 │ int64  │ int64   │ int64 │ int64 │ int64   │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │\n├───────┼────────┼─────────┼───────┼───────┼─────────┼───────┼───────┼───────┼───────┼───────┼───────┤\n│  4848 │      1 │    NULL │  NULL │  NULL │       1 │  NULL │  NULL │     1 │     1 │  NULL │  NULL │\n│  4865 │      1 │    NULL │  NULL │  NULL │       1 │  NULL │  NULL │     1 │  NULL │  NULL │  NULL │\n│  4844 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4845 │      1 │       1 │  NULL │  NULL │       1 │  NULL │  NULL │     1 │     1 │  NULL │  NULL │\n│  4849 │   NULL │    NULL │  NULL │  NULL │       1 │  NULL │  NULL │     1 │  NULL │  NULL │  NULL │\n│  4859 │      1 │       1 │  NULL │  NULL │       1 │  NULL │  NULL │     1 │     1 │  NULL │  NULL │\n│  4861 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4847 │      1 │    NULL │  NULL │  NULL │       1 │  NULL │  NULL │     1 │  NULL │  NULL │  NULL │\n│  4850 │   NULL │       1 │  NULL │  NULL │       1 │     1 │  NULL │     1 │     1 │     1 │  NULL │\n│  4864 │   NULL │    NULL │  NULL │  NULL │       1 │  NULL │  NULL │     1 │  NULL │  NULL │  NULL │\n│     … │      … │       … │     … │     … │       … │     … │     … │     … │     … │     … │     … │\n└───────┴────────┴─────────┴───────┴───────┴─────────┴───────┴───────┴───────┴───────┴───────┴───────┘\n\n\n\nFill missing pivoted values using values_fill\n\n&gt;&gt;&gt; fish_encounters.pivot_wider(\n...     names_from=\"station\", values_from=\"seen\", values_fill=0\n... )\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ fish  ┃ Lisbon ┃ Base_TD ┃ MAE   ┃ MAW   ┃ Release ┃ BCE   ┃ BCE2  ┃ I80_1 ┃ Rstr  ┃ BCW   ┃ BCW2  ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ int64 │ int64  │ int64   │ int64 │ int64 │ int64   │ int64 │ int64 │ int64 │ int64 │ int64 │ int64 │\n├───────┼────────┼─────────┼───────┼───────┼─────────┼───────┼───────┼───────┼───────┼───────┼───────┤\n│  4843 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4848 │      1 │       0 │     0 │     0 │       1 │     0 │     0 │     1 │     1 │     0 │     0 │\n│  4865 │      1 │       0 │     0 │     0 │       1 │     0 │     0 │     1 │     0 │     0 │     0 │\n│  4844 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4845 │      1 │       1 │     0 │     0 │       1 │     0 │     0 │     1 │     1 │     0 │     0 │\n│  4849 │      0 │       0 │     0 │     0 │       1 │     0 │     0 │     1 │     0 │     0 │     0 │\n│  4859 │      1 │       1 │     0 │     0 │       1 │     0 │     0 │     1 │     1 │     0 │     0 │\n│  4861 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4842 │      1 │       1 │     1 │     1 │       1 │     1 │     1 │     1 │     1 │     1 │     1 │\n│  4854 │      0 │       0 │     0 │     0 │       1 │     0 │     0 │     1 │     0 │     0 │     0 │\n│     … │      … │       … │     … │     … │       … │     … │     … │     … │     … │     … │     … │\n└───────┴────────┴─────────┴───────┴───────┴─────────┴───────┴───────┴───────┴───────┴───────┴───────┘\n\n\n\nCompute multiple values columns\n\n&gt;&gt;&gt; us_rent_income = ibis.examples.us_rent_income.fetch()\n&gt;&gt;&gt; us_rent_income\n\n┏━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┓\n┃ geoid  ┃ name       ┃ variable ┃ estimate ┃ moe   ┃\n┡━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━┩\n│ string │ string     │ string   │ int64    │ int64 │\n├────────┼────────────┼──────────┼──────────┼───────┤\n│ 01     │ Alabama    │ income   │    24476 │   136 │\n│ 01     │ Alabama    │ rent     │      747 │     3 │\n│ 02     │ Alaska     │ income   │    32940 │   508 │\n│ 02     │ Alaska     │ rent     │     1200 │    13 │\n│ 04     │ Arizona    │ income   │    27517 │   148 │\n│ 04     │ Arizona    │ rent     │      972 │     4 │\n│ 05     │ Arkansas   │ income   │    23789 │   165 │\n│ 05     │ Arkansas   │ rent     │      709 │     5 │\n│ 06     │ California │ income   │    29454 │   109 │\n│ 06     │ California │ rent     │     1358 │     3 │\n│ …      │ …          │ …        │        … │     … │\n└────────┴────────────┴──────────┴──────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; us_rent_income.pivot_wider(\n...     names_from=\"variable\", values_from=[\"estimate\", \"moe\"]\n... )\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ geoid  ┃ name         ┃ estimate_income ┃ moe_income ┃ estimate_rent ┃ moe_rent ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ string       │ int64           │ int64      │ int64         │ int64    │\n├────────┼──────────────┼─────────────────┼────────────┼───────────────┼──────────┤\n│ 05     │ Arkansas     │           23789 │        165 │           709 │        5 │\n│ 16     │ Idaho        │           25298 │        208 │           792 │        7 │\n│ 30     │ Montana      │           26249 │        206 │           751 │        9 │\n│ 39     │ Ohio         │           27435 │         94 │           764 │        2 │\n│ 47     │ Tennessee    │           25453 │        102 │           808 │        4 │\n│ 06     │ California   │           29454 │        109 │          1358 │        3 │\n│ 13     │ Georgia      │           27024 │        106 │           927 │        3 │\n│ 15     │ Hawaii       │           32453 │        218 │          1507 │       18 │\n│ 38     │ North Dakota │           32336 │        245 │           775 │        9 │\n│ 40     │ Oklahoma     │           26207 │        101 │           766 │        3 │\n│ …      │ …            │               … │          … │             … │        … │\n└────────┴──────────────┴─────────────────┴────────────┴───────────────┴──────────┘\n\n\n\nThe column name separator can be changed using the names_sep parameter\n\n&gt;&gt;&gt; us_rent_income.pivot_wider(\n...     names_from=\"variable\",\n...     names_sep=\".\",\n...     values_from=(\"estimate\", \"moe\"),\n... )\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ geoid  ┃ name         ┃ estimate.income ┃ moe.income ┃ estimate.rent ┃ moe.rent ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string │ string       │ int64           │ int64      │ int64         │ int64    │\n├────────┼──────────────┼─────────────────┼────────────┼───────────────┼──────────┤\n│ 05     │ Arkansas     │           23789 │        165 │           709 │        5 │\n│ 16     │ Idaho        │           25298 │        208 │           792 │        7 │\n│ 30     │ Montana      │           26249 │        206 │           751 │        9 │\n│ 39     │ Ohio         │           27435 │         94 │           764 │        2 │\n│ 47     │ Tennessee    │           25453 │        102 │           808 │        4 │\n│ 06     │ California   │           29454 │        109 │          1358 │        3 │\n│ 13     │ Georgia      │           27024 │        106 │           927 │        3 │\n│ 15     │ Hawaii       │           32453 │        218 │          1507 │       18 │\n│ 38     │ North Dakota │           32336 │        245 │           775 │        9 │\n│ 40     │ Oklahoma     │           26207 │        101 │           766 │        3 │\n│ …      │ …            │               … │          … │             … │        … │\n└────────┴──────────────┴─────────────────┴────────────┴───────────────┴──────────┘\n\n\n\nSupply an alternative function to summarize values\n\n&gt;&gt;&gt; warpbreaks = ibis.examples.warpbreaks.fetch().select(\n...     \"wool\", \"tension\", \"breaks\"\n... )\n&gt;&gt;&gt; warpbreaks\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ wool   ┃ tension ┃ breaks ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ int64  │\n├────────┼─────────┼────────┤\n│ A      │ L       │     26 │\n│ A      │ L       │     30 │\n│ A      │ L       │     54 │\n│ A      │ L       │     25 │\n│ A      │ L       │     70 │\n│ A      │ L       │     52 │\n│ A      │ L       │     51 │\n│ A      │ L       │     26 │\n│ A      │ L       │     67 │\n│ A      │ M       │     18 │\n│ …      │ …       │      … │\n└────────┴─────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; warpbreaks.pivot_wider(\n...     names_from=\"wool\", values_from=\"breaks\", values_agg=\"mean\"\n... )\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ tension ┃ A         ┃ B         ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ float64   │ float64   │\n├─────────┼───────────┼───────────┤\n│ L       │ 44.555556 │ 28.222222 │\n│ M       │ 24.000000 │ 28.777778 │\n│ H       │ 24.555556 │ 18.777778 │\n└─────────┴───────────┴───────────┘\n\n\n\nPassing Deferred objects to values_agg is supported\n\n&gt;&gt;&gt; warpbreaks.pivot_wider(\n...     names_from=\"tension\",\n...     values_from=\"breaks\",\n...     values_agg=_.sum(),\n... ).order_by(\"wool\")\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ wool   ┃ L     ┃ M     ┃ H     ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │ int64 │\n├────────┼───────┼───────┼───────┤\n│ A      │   401 │   216 │   221 │\n│ B      │   254 │   259 │   169 │\n└────────┴───────┴───────┴───────┘\n\n\n\nUse a custom aggregate function\n\n&gt;&gt;&gt; warpbreaks.pivot_wider(\n...     names_from=\"wool\",\n...     values_from=\"breaks\",\n...     values_agg=lambda col: col.std() / col.mean(),\n... )\n\n┏━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ tension ┃ A        ┃ B        ┃\n┡━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ string  │ float64  │ float64  │\n├─────────┼──────────┼──────────┤\n│ L       │ 0.406183 │ 0.349325 │\n│ M       │ 0.360844 │ 0.327719 │\n│ H       │ 0.418344 │ 0.260590 │\n└─────────┴──────────┴──────────┘\n\n\n\nGenerate some random data, setting the random seed for reproducibility\n\n&gt;&gt;&gt; import random\n&gt;&gt;&gt; random.seed(0)\n&gt;&gt;&gt; raw = ibis.memtable(\n...     [\n...         dict(\n...             product=product,\n...             country=country,\n...             year=year,\n...             production=random.random(),\n...         )\n...         for product in \"AB\"\n...         for country in [\"AI\", \"EI\"]\n...         for year in range(2000, 2015)\n...     ]\n... )\n&gt;&gt;&gt; production = raw.filter(\n...     ((_.product == \"A\") & (_.country == \"AI\")) | (_.product == \"B\")\n... )\n&gt;&gt;&gt; production\n\n┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━┓\n┃ product ┃ country ┃ year  ┃ production ┃\n┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━┩\n│ string  │ string  │ int64 │ float64    │\n├─────────┼─────────┼───────┼────────────┤\n│ B       │ AI      │  2000 │   0.477010 │\n│ B       │ AI      │  2001 │   0.865310 │\n│ B       │ AI      │  2002 │   0.260492 │\n│ B       │ AI      │  2003 │   0.805028 │\n│ B       │ AI      │  2004 │   0.548699 │\n│ B       │ AI      │  2005 │   0.014042 │\n│ B       │ AI      │  2006 │   0.719705 │\n│ B       │ AI      │  2007 │   0.398824 │\n│ B       │ AI      │  2008 │   0.824845 │\n│ B       │ AI      │  2009 │   0.668153 │\n│ …       │ …       │     … │          … │\n└─────────┴─────────┴───────┴────────────┘\n\n\n\nPivoting with multiple name columns\n\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     values_from=\"production\",\n... )\n\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ B_AI     ┃ B_EI     ┃ A_AI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │ float64  │\n├───────┼──────────┼──────────┼──────────┤\n│  2001 │ 0.865310 │ 0.191067 │ 0.757954 │\n│  2003 │ 0.805028 │ 0.238616 │ 0.258917 │\n│  2009 │ 0.668153 │ 0.507941 │ 0.583382 │\n│  2006 │ 0.719705 │ 0.447970 │ 0.783799 │\n│  2008 │ 0.824845 │ 0.320055 │ 0.476597 │\n│  2000 │ 0.477010 │ 0.870471 │ 0.844422 │\n│  2010 │ 0.001143 │ 0.932834 │ 0.908113 │\n│  2014 │ 0.325204 │ 0.547441 │ 0.618369 │\n│  2004 │ 0.548699 │ 0.967540 │ 0.511275 │\n│  2007 │ 0.398824 │ 0.080446 │ 0.303313 │\n│     … │        … │        … │        … │\n└───────┴──────────┴──────────┴──────────┘\n\n\n\nSelect a subset of names. This call incurs no computation when constructing the expression.\n\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     names=[(\"A\", \"AI\"), (\"B\", \"AI\")],\n...     values_from=\"production\",\n... )\n\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ A_AI     ┃ B_AI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │\n├───────┼──────────┼──────────┤\n│  2001 │ 0.757954 │ 0.865310 │\n│  2003 │ 0.258917 │ 0.805028 │\n│  2009 │ 0.583382 │ 0.668153 │\n│  2004 │ 0.511275 │ 0.548699 │\n│  2007 │ 0.303313 │ 0.398824 │\n│  2011 │ 0.504687 │ 0.493578 │\n│  2000 │ 0.844422 │ 0.477010 │\n│  2010 │ 0.908113 │ 0.001143 │\n│  2014 │ 0.618369 │ 0.325204 │\n│  2006 │ 0.783799 │ 0.719705 │\n│     … │        … │        … │\n└───────┴──────────┴──────────┘\n\n\n\nSort the new columns’ names\n\n&gt;&gt;&gt; production.pivot_wider(\n...     names_from=[\"product\", \"country\"],\n...     values_from=\"production\",\n...     names_sort=True,\n... )\n\n┏━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┓\n┃ year  ┃ A_AI     ┃ B_AI     ┃ B_EI     ┃\n┡━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━┩\n│ int64 │ float64  │ float64  │ float64  │\n├───────┼──────────┼──────────┼──────────┤\n│  2012 │ 0.281838 │ 0.867603 │ 0.551267 │\n│  2004 │ 0.511275 │ 0.548699 │ 0.967540 │\n│  2007 │ 0.303313 │ 0.398824 │ 0.080446 │\n│  2011 │ 0.504687 │ 0.493578 │ 0.109058 │\n│  2002 │ 0.420572 │ 0.260492 │ 0.567511 │\n│  2006 │ 0.783799 │ 0.719705 │ 0.447970 │\n│  2008 │ 0.476597 │ 0.824845 │ 0.320055 │\n│  2000 │ 0.844422 │ 0.477010 │ 0.870471 │\n│  2001 │ 0.757954 │ 0.865310 │ 0.191067 │\n│  2003 │ 0.258917 │ 0.805028 │ 0.238616 │\n│     … │        … │        … │        … │\n└───────┴──────────┴──────────┴──────────┘\n\n\n\n\n\n\nrelabel\nrelabel(substitutions)\nDeprecated in favor of Table.rename\n\n\nrelocate\nrelocate(*columns, before=None, after=None, **kwargs)\nRelocate columns before or after other specified columns.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr | s.Selector\nColumns to relocate. Selectors are accepted.\n()\n\n\nbefore\nstr | s.Selector | None\nA column name or selector to insert the new columns before.\nNone\n\n\nafter\nstr | s.Selector | None\nA column name or selector. Columns in columns are relocated after the last column selected in after.\nNone\n\n\nkwargs\nstr\nAdditional column names to relocate, renaming argument values to keyword argument names.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table with the columns relocated.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.memtable(dict(a=[1], b=[1], c=[1], d=[\"a\"], e=[\"a\"], f=[\"a\"]))\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b     ┃ c     ┃ d      ┃ e      ┃ f      ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ int64 │ int64 │ int64 │ string │ string │ string │\n├───────┼───────┼───────┼────────┼────────┼────────┤\n│     1 │     1 │     1 │ a      │ a      │ a      │\n└───────┴───────┴───────┴────────┴────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(\"f\")\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ f      ┃ a     ┃ b     ┃ c     ┃ d      ┃ e      ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ string │ int64 │ int64 │ int64 │ string │ string │\n├────────┼───────┼───────┼───────┼────────┼────────┤\n│ a      │     1 │     1 │     1 │ a      │ a      │\n└────────┴───────┴───────┴───────┴────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(\"a\", after=\"c\")\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ b     ┃ c     ┃ a     ┃ d      ┃ e      ┃ f      ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ int64 │ int64 │ int64 │ string │ string │ string │\n├───────┼───────┼───────┼────────┼────────┼────────┤\n│     1 │     1 │     1 │ a      │ a      │ a      │\n└───────┴───────┴───────┴────────┴────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(\"f\", before=\"b\")\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ f      ┃ b     ┃ c     ┃ d      ┃ e      ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │ int64 │ int64 │ string │ string │\n├───────┼────────┼───────┼───────┼────────┼────────┤\n│     1 │ a      │     1 │     1 │ a      │ a      │\n└───────┴────────┴───────┴───────┴────────┴────────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(\"a\", after=s.last())\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ b     ┃ c     ┃ d      ┃ e      ┃ f      ┃ a     ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ int64 │ int64 │ string │ string │ string │ int64 │\n├───────┼───────┼────────┼────────┼────────┼───────┤\n│     1 │     1 │ a      │ a      │ a      │     1 │\n└───────┴───────┴────────┴────────┴────────┴───────┘\n\n\n\nRelocate allows renaming\n\n&gt;&gt;&gt; t.relocate(ff=\"f\")\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ ff     ┃ a     ┃ b     ┃ c     ┃ d      ┃ e      ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ string │ int64 │ int64 │ int64 │ string │ string │\n├────────┼───────┼───────┼───────┼────────┼────────┤\n│ a      │     1 │     1 │     1 │ a      │ a      │\n└────────┴───────┴───────┴───────┴────────┴────────┘\n\n\n\nYou can relocate based on any predicate selector, such as of_type\n\n&gt;&gt;&gt; t.relocate(s.of_type(\"string\"))\n\n┏━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ d      ┃ e      ┃ f      ┃ a     ┃ b     ┃ c     ┃\n┡━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ string │ string │ int64 │ int64 │ int64 │\n├────────┼────────┼────────┼───────┼───────┼───────┤\n│ a      │ a      │ a      │     1 │     1 │     1 │\n└────────┴────────┴────────┴───────┴───────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(s.numeric(), after=s.last())\n\n┏━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ d      ┃ e      ┃ f      ┃ a     ┃ b     ┃ c     ┃\n┡━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ string │ string │ int64 │ int64 │ int64 │\n├────────┼────────┼────────┼───────┼───────┼───────┤\n│ a      │ a      │ a      │     1 │     1 │     1 │\n└────────┴────────┴────────┴───────┴───────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(s.any_of(s.c(*\"ae\")))\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ e      ┃ b     ┃ c     ┃ d      ┃ f      ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │ int64 │ int64 │ string │ string │\n├───────┼────────┼───────┼───────┼────────┼────────┤\n│     1 │ a      │     1 │     1 │ a      │ a      │\n└───────┴────────┴───────┴───────┴────────┴────────┘\n\n\n\nWhen multiple columns are selected with before or after, those selected columns are moved before and after the selectors input\n\n&gt;&gt;&gt; t = ibis.memtable(dict(a=[1], b=[\"a\"], c=[1], d=[\"a\"]))\n&gt;&gt;&gt; t.relocate(s.numeric(), after=s.of_type(\"string\"))\n\n┏━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ b      ┃ d      ┃ a     ┃ c     ┃\n┡━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ string │ int64 │ int64 │\n├────────┼────────┼───────┼───────┤\n│ a      │ a      │     1 │     1 │\n└────────┴────────┴───────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.relocate(s.numeric(), before=s.of_type(\"string\"))\n\n┏━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ c     ┃ b      ┃ d      ┃\n┡━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ int64 │ int64 │ string │ string │\n├───────┼───────┼────────┼────────┤\n│     1 │     1 │ a      │ a      │\n└───────┴───────┴────────┴────────┘\n\n\n\nWhen there are duplicate renames in a call to relocate, the last one is preserved\n\n&gt;&gt;&gt; t.relocate(e=\"d\", f=\"d\")\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ f      ┃ a     ┃ b      ┃ c     ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ string │ int64 │\n├────────┼───────┼────────┼───────┤\n│ a      │     1 │ a      │     1 │\n└────────┴───────┴────────┴───────┘\n\n\n\nHowever, if there are duplicates that are not part of a rename, the order specified in the relocate call is preserved\n\n&gt;&gt;&gt; t.relocate(\n...     \"b\",\n...     s.of_type(\"string\"),  # \"b\" is a string column, so the selector matches\n... )\n\n┏━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ b      ┃ d      ┃ a     ┃ c     ┃\n┡━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ string │ int64 │ int64 │\n├────────┼────────┼───────┼───────┤\n│ a      │ a      │     1 │     1 │\n└────────┴────────┴───────┴───────┘\n\n\n\n\n\n\nrename\nrename(method=None, /, **substitutions)\nRename columns in the table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nmethod\nstr | Callable[[str], str | None] | Literal[‘snake_case’, ‘ALL_CAPS’] | Mapping[str, str] | None\nAn optional method for renaming columns. May be one of: - A format string to use to rename all columns, like \"prefix_{name}\". - A function from old name to new name. If the function returns None the old name is used. - The literal strings \"snake_case\" or \"ALL_CAPS\" to rename all columns using a snake_case or \"ALL_CAPS\" naming convention respectively. - A mapping from new name to old name. Existing columns not present in the mapping will passthrough with their original name.\nNone\n\n\nsubstitutions\nstr\nColumns to be explicitly renamed, expressed as new_name=old_name keyword arguments.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA renamed table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; first3 = s.r[:3]  # first 3 columns\n&gt;&gt;&gt; t = ibis.examples.penguins_raw_raw.fetch().select(first3)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ studyName ┃ Sample Number ┃ Species                             ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64         │ string                              │\n├───────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708   │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             2 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             3 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             4 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             5 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             6 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             7 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             8 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │             9 │ Adelie Penguin (Pygoscelis adeliae) │\n│ PAL0708   │            10 │ Adelie Penguin (Pygoscelis adeliae) │\n│ …         │             … │ …                                   │\n└───────────┴───────────────┴─────────────────────────────────────┘\n\n\n\nRename specific columns by passing keyword arguments like new_name=\"old_name\"\n\n&gt;&gt;&gt; t.rename(study_name=\"studyName\").head(1)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ study_name ┃ Sample Number ┃ Species                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\n\n\n\nRename all columns using a format string\n\n&gt;&gt;&gt; t.rename(\"p_{name}\").head(1)\n\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ p_studyName ┃ p_Sample Number ┃ p_Species                           ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string      │ int64           │ string                              │\n├─────────────┼─────────────────┼─────────────────────────────────────┤\n│ PAL0708     │               1 │ Adelie Penguin (Pygoscelis adeliae) │\n└─────────────┴─────────────────┴─────────────────────────────────────┘\n\n\n\nRename all columns using a snake_case convention\n\n&gt;&gt;&gt; t.rename(\"snake_case\").head(1)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ study_name ┃ sample_number ┃ species                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\n\n\n\nRename all columns using an ALL_CAPS convention\n\n&gt;&gt;&gt; t.rename(\"ALL_CAPS\").head(1)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ STUDY_NAME ┃ SAMPLE_NUMBER ┃ SPECIES                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ int64         │ string                              │\n├────────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708    │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└────────────┴───────────────┴─────────────────────────────────────┘\n\n\n\nRename all columns using a callable\n\n&gt;&gt;&gt; t.rename(str.upper).head(1)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ STUDYNAME ┃ SAMPLE NUMBER ┃ SPECIES                             ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64         │ string                              │\n├───────────┼───────────────┼─────────────────────────────────────┤\n│ PAL0708   │             1 │ Adelie Penguin (Pygoscelis adeliae) │\n└───────────┴───────────────┴─────────────────────────────────────┘\n\n\n\n\n\n\nrowid\nrowid()\nA unique integer per row.\n\n\n\n\n\n\nThis operation is only valid on physical tables\n\n\n\nAny further meaning behind this expression is backend dependent. Generally this corresponds to some index into the database storage (for example, SQLite and DuckDB’s rowid).\nFor a monotonically increasing row number, see ibis.row_number.\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nAn integer column\n\n\n\n\n\n\nsample\nsample(fraction, *, method='row', seed=None)\nSample a fraction of rows from a table.\n\n\n\n\n\n\nResults may be non-repeatable\n\n\n\nSampling is by definition a random operation. Some backends support specifying a seed for repeatable results, but not all backends support that option. And some backends (duckdb, for example) do support specifying a seed but may still not have repeatable results in all cases.\nIn all cases, results are backend-specific. An execution against one backend is unlikely to sample the same rows when executed against a different backend, even with the same seed set.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfraction\nfloat\nThe percentage of rows to include in the sample, expressed as a float between 0 and 1.\nrequired\n\n\nmethod\nLiteral[‘row’, ‘block’]\nThe sampling method to use. The default is “row”, which includes each row with a probability of fraction. If method is “block”, some backends may instead perform sampling a fraction of blocks of rows (where “block” is a backend dependent definition). This is identical to “row” for backends lacking a blockwise sampling implementation. For those coming from SQL, “row” and “block” correspond to “bernoulli” and “system” respectively in a TABLESAMPLE clause.\n'row'\n\n\nseed\nint | None\nAn optional random seed to use, for repeatable sampling. Backends that never support specifying a seed for repeatable sampling will error appropriately. Note that some backends (like DuckDB) do support specifying a seed, but may still not have repeatable results in all cases.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe input table, with fraction of rows selected.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"x\": [1, 2, 3, 4], \"y\": [\"a\", \"b\", \"c\", \"d\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ x     ┃ y      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ a      │\n│     2 │ b      │\n│     3 │ c      │\n│     4 │ d      │\n└───────┴────────┘\n\n\n\nSample approximately half the rows, with a seed specified for reproducibility.\n\n&gt;&gt;&gt; t.sample(0.5, seed=1234)\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ x     ┃ y      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     2 │ b      │\n│     3 │ c      │\n└───────┴────────┘\n\n\n\n\n\n\nschema\nschema()\nReturn the Schema for this table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe table’s schema.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.schema()\n\nibis.Schema {\n  species            string\n  island             string\n  bill_length_mm     float64\n  bill_depth_mm      float64\n  flipper_length_mm  int64\n  body_mass_g        int64\n  sex                string\n  year               int64\n}\n\n\n\n\n\nselect\nselect(*exprs, **named_exprs)\nCompute a new table expression using exprs and named_exprs.\nPassing an aggregate function to this method will broadcast the aggregate’s value over the number of rows in the table and automatically constructs a window function expression. See the examples section for more details.\nFor backwards compatibility the keyword argument exprs is reserved and cannot be used to name an expression. This behavior will be removed in v4.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nir.Value | str | Iterable[ir.Value | str]\nColumn expression, string, or list of column expressions and strings.\n()\n\n\nnamed_exprs\nir.Value | str\nColumn expressions\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nSimple projection\n\n&gt;&gt;&gt; t.select(\"island\", \"bill_length_mm\").head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ float64        │\n├───────────┼────────────────┤\n│ Torgersen │           39.1 │\n│ Torgersen │           39.5 │\n│ Torgersen │           40.3 │\n│ Torgersen │           NULL │\n│ Torgersen │           36.7 │\n└───────────┴────────────────┘\n\n\n\nIn that simple case, you could also just use python’s indexing syntax\n\n&gt;&gt;&gt; t[[\"island\", \"bill_length_mm\"]].head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ float64        │\n├───────────┼────────────────┤\n│ Torgersen │           39.1 │\n│ Torgersen │           39.5 │\n│ Torgersen │           40.3 │\n│ Torgersen │           NULL │\n│ Torgersen │           36.7 │\n└───────────┴────────────────┘\n\n\n\nProjection by zero-indexed column position\n\n&gt;&gt;&gt; t.select(0, 4).head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ int64             │\n├─────────┼───────────────────┤\n│ Adelie  │               181 │\n│ Adelie  │               186 │\n│ Adelie  │               195 │\n│ Adelie  │              NULL │\n│ Adelie  │               193 │\n└─────────┴───────────────────┘\n\n\n\nProjection with renaming and compute in one call\n\n&gt;&gt;&gt; t.select(next_year=t.year + 1).head()\n\n┏━━━━━━━━━━━┓\n┃ next_year ┃\n┡━━━━━━━━━━━┩\n│ int64     │\n├───────────┤\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n└───────────┘\n\n\n\nYou can do the same thing with a named expression, and using the deferred API\n\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; t.select((_.year + 1).name(\"next_year\")).head()\n\n┏━━━━━━━━━━━┓\n┃ next_year ┃\n┡━━━━━━━━━━━┩\n│ int64     │\n├───────────┤\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n│      2008 │\n└───────────┘\n\n\n\nProjection with aggregation expressions\n\n&gt;&gt;&gt; t.select(\"island\", bill_mean=t.bill_length_mm.mean()).head()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ island    ┃ bill_mean ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ float64   │\n├───────────┼───────────┤\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n│ Torgersen │  43.92193 │\n└───────────┴───────────┘\n\n\n\nProjection with a selector\n\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t.select(s.numeric() & ~s.c(\"year\")).head()\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float64        │ float64       │ int64             │ int64       │\n├────────────────┼───────────────┼───────────────────┼─────────────┤\n│           39.1 │          18.7 │               181 │        3750 │\n│           39.5 │          17.4 │               186 │        3800 │\n│           40.3 │          18.0 │               195 │        3250 │\n│           NULL │          NULL │              NULL │        NULL │\n│           36.7 │          19.3 │               193 │        3450 │\n└────────────────┴───────────────┴───────────────────┴─────────────┘\n\n\n\nProjection + aggregation across multiple columns\n\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; t.select(s.across(s.numeric() & ~s.c(\"year\"), _.mean())).head()\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64           │ float64     │\n├────────────────┼───────────────┼───────────────────┼─────────────┤\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n│       43.92193 │      17.15117 │        200.915205 │ 4201.754386 │\n└────────────────┴───────────────┴───────────────────┴─────────────┘\n\n\n\n\n\n\nsql\nsql(query, dialect=None)\nRun a SQL query against a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nQuery string\nrequired\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. Defaults to the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn opaque table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch(table_name=\"penguins\")\n&gt;&gt;&gt; expr = t.sql(\n...     \"\"\"\n...     SELECT island, mean(bill_length_mm) AS avg_bill_length\n...     FROM penguins\n...     GROUP BY 1\n...     ORDER BY 2 DESC\n...     \"\"\"\n... )\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64         │\n├───────────┼─────────────────┤\n│ Biscoe    │       45.257485 │\n│ Dream     │       44.167742 │\n│ Torgersen │       38.950980 │\n└───────────┴─────────────────┘\n\n\n\nMix and match ibis expressions with SQL queries\n\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch(table_name=\"penguins\")\n&gt;&gt;&gt; expr = t.sql(\n...     \"\"\"\n...     SELECT island, mean(bill_length_mm) AS avg_bill_length\n...     FROM penguins\n...     GROUP BY 1\n...     ORDER BY 2 DESC\n...     \"\"\"\n... )\n&gt;&gt;&gt; expr = expr.mutate(\n...     island=_.island.lower(),\n...     avg_bill_length=_.avg_bill_length.round(1),\n... )\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64         │\n├───────────┼─────────────────┤\n│ biscoe    │            45.3 │\n│ dream     │            44.2 │\n│ torgersen │            39.0 │\n└───────────┴─────────────────┘\n\n\n\nBecause ibis expressions aren’t named, they aren’t visible to subsequent .sql calls. Use the alias method to assign a name to an expression.\n\n&gt;&gt;&gt; expr.alias(\"b\").sql(\"SELECT * FROM b WHERE avg_bill_length &gt; 40\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ island ┃ avg_bill_length ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string │ float64         │\n├────────┼─────────────────┤\n│ biscoe │            45.3 │\n│ dream  │            44.2 │\n└────────┴─────────────────┘\n\n\n\n\n\nSee Also\nTable.alias\n\n\n\nto_array\nto_array()\nView a single column table as an array.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nA single column view of a table\n\n\n\n\n\n\nto_pandas\nto_pandas(**kwargs)\nConvert a table expression to a pandas DataFrame.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nSame as keyword arguments to execute\n{}\n\n\n\n\n\n\ntry_cast\ntry_cast(schema)\nCast the columns of a table.\nIf the cast fails for a row, the value is returned as NULL or NaN depending on backend behavior.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschema\nSupportsSchema\nMapping, schema or iterable of pairs to use for casting\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nCasted table\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [\"1\", \"2\", \"3\"], \"b\": [\"2.2\", \"3.3\", \"book\"]})\n&gt;&gt;&gt; t.try_cast({\"a\": \"int\", \"b\": \"float\"})\n\n┏━━━━━━━┳━━━━━━━━━┓\n┃ a     ┃ b       ┃\n┡━━━━━━━╇━━━━━━━━━┩\n│ int64 │ float64 │\n├───────┼─────────┤\n│     1 │     2.2 │\n│     2 │     3.3 │\n│     3 │    NULL │\n└───────┴─────────┘\n\n\n\n\n\n\nunion\nunion(table, *rest, distinct=False)\nCompute the set union of multiple table expressions.\nThe input tables must have identical schemas.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nTable\nA table expression\nrequired\n\n\n*rest\nTable\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the union of all input tables.\n\n\n\n\n\nSee Also\nibis.union\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t1.union(t2)  # union all by default\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t1.union(t2, distinct=True).order_by(\"a\")\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n\n\nunpack\nunpack(*columns)\nProject the struct fields of each of columns into self.\nExisting fields are retained in the projection.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncolumns\nstr\nString column names to project into self.\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe child table with struct fields of each of columns projected.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; lines = '''\n...     {\"name\": \"a\", \"pos\": {\"lat\": 10.1, \"lon\": 30.3}}\n...     {\"name\": \"b\", \"pos\": {\"lat\": 10.2, \"lon\": 30.2}}\n...     {\"name\": \"c\", \"pos\": {\"lat\": 10.3, \"lon\": 30.1}}\n... '''\n&gt;&gt;&gt; with open(\"/tmp/lines.json\", \"w\") as f:\n...     nbytes = f.write(lines)  # nbytes is unused\n...\n&gt;&gt;&gt; t = ibis.read_json(\"/tmp/lines.json\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ name   ┃ pos                                ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string │ struct&lt;lat: float64, lon: float64&gt; │\n├────────┼────────────────────────────────────┤\n│ a      │ {'lat': 10.1, 'lon': 30.3}         │\n│ b      │ {'lat': 10.2, 'lon': 30.2}         │\n│ c      │ {'lat': 10.3, 'lon': 30.1}         │\n└────────┴────────────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.unpack(\"pos\")\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n┃ name   ┃ lat     ┃ lon     ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n│ string │ float64 │ float64 │\n├────────┼─────────┼─────────┤\n│ a      │    10.1 │    30.3 │\n│ b      │    10.2 │    30.2 │\n│ c      │    10.3 │    30.1 │\n└────────┴─────────┴─────────┘\n\n\n\n\n\nSee Also\nStructValue.lift\n\n\n\nview\nview()\nCreate a new table expression distinct from the current one.\nUse this API for any self-referencing operations like a self-join.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nwindow_by\nwindow_by(time_col)\nCreate a windowing table-valued function (TVF) expression.\nWindowing table-valued functions (TVF) assign rows of a table to windows based on a time attribute column in the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntime_col\nir.Value\nColumn of the table that will be mapped to windows.\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nWindowedTable\nWindowedTable expression."
  },
  {
    "objectID": "reference/expression-tables.html#methods-1",
    "href": "reference/expression-tables.html#methods-1",
    "title": "Table expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\naggregate\nCompute aggregates over a group by.\n\n\ncount\nComputing the number of rows per group.\n\n\nhaving\nAdd a post-aggregation result filter expr.\n\n\nmutate\nReturn a table projection with window functions applied.\n\n\norder_by\nSort a grouped table expression by expr.\n\n\nover\nApply a window over the input expressions.\n\n\nselect\nProject new columns out of the grouped table.\n\n\n\n\naggregate\naggregate(metrics=None, **kwds)\nCompute aggregates over a group by.\n\n\ncount\ncount()\nComputing the number of rows per group.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe aggregated table\n\n\n\n\n\n\nhaving\nhaving(expr)\nAdd a post-aggregation result filter expr.\n\n\n\n\n\n\nExpressions like x is None return bool and will not generate a SQL comparison to NULL\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.BooleanScalar\nAn expression that filters based on an aggregate value.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA grouped table expression\n\n\n\n\n\n\nmutate\nmutate(*exprs, **kwexprs)\nReturn a table projection with window functions applied.\nAny arguments can be functions.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexprs\nir.Value | Sequence[ir.Value]\nList of expressions\n()\n\n\nkwexprs\nir.Value\nExpressions\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; (\n...     t.select(\"species\", \"bill_length_mm\")\n...     .group_by(\"species\")\n...     .mutate(\n...         centered_bill_len=ibis._.bill_length_mm\n...         - ibis._.bill_length_mm.mean()\n...     )\n...     .order_by(s.all())\n... )\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ bill_length_mm ┃ centered_bill_len ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ float64        │ float64           │\n├─────────┼────────────────┼───────────────────┤\n│ Adelie  │           32.1 │         -6.691391 │\n│ Adelie  │           33.1 │         -5.691391 │\n│ Adelie  │           33.5 │         -5.291391 │\n│ Adelie  │           34.0 │         -4.791391 │\n│ Adelie  │           34.1 │         -4.691391 │\n│ Adelie  │           34.4 │         -4.391391 │\n│ Adelie  │           34.5 │         -4.291391 │\n│ Adelie  │           34.6 │         -4.191391 │\n│ Adelie  │           34.6 │         -4.191391 │\n│ Adelie  │           35.0 │         -3.791391 │\n│ …       │              … │                 … │\n└─────────┴────────────────┴───────────────────┘\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression with window functions applied\n\n\n\n\n\n\norder_by\norder_by(expr)\nSort a grouped table expression by expr.\n\nNotes\nThis API call is ignored in aggregations.\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Value | Iterable[ir.Value]\nExpressions to order the results by\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA sorted grouped GroupedTable\n\n\n\n\n\n\nover\nover(window=None, *, rows=None, range=None, group_by=None, order_by=None)\nApply a window over the input expressions.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\n\nWindow to add to the input\nNone\n\n\nrows\n\nWhether to use the ROWS window clause\nNone\n\n\nrange\n\nWhether to use the RANGE window clause\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGroupedTable\nA new grouped table expression\n\n\n\n\n\n\nselect\nselect(*exprs, **kwexprs)\nProject new columns out of the grouped table.\n\nSee Also\nGroupedTable.mutate"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-36",
    "href": "reference/expression-tables.html#parameters-36",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nstr | Path | Sequence[str | Path]\nA filesystem path or URL or list of same. Supports CSV and TSV files.\nrequired\n\n\ntable_name\nstr | None\nA name to refer to the table. If not provided, a name will be generated.\nNone\n\n\nkwargs\nAny\nBackend-specific keyword arguments for the file type. For the DuckDB backend used by default, please refer to: * CSV/TSV: https://duckdb.org/docs/data/csv#parameters.\n{}"
  },
  {
    "objectID": "reference/expression-tables.html#returns-41",
    "href": "reference/expression-tables.html#returns-41",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nTable expression representing a file"
  },
  {
    "objectID": "reference/expression-tables.html#examples-34",
    "href": "reference/expression-tables.html#examples-34",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; lines = '''a,b\n... 1,d\n... 2,\n... ,f\n... '''\n&gt;&gt;&gt; with open(\"/tmp/lines.csv\", mode=\"w\") as f:\n...     nbytes = f.write(lines)  # nbytes is unused\n...\n&gt;&gt;&gt; t = ibis.read_csv(\"/tmp/lines.csv\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ d      │\n│     2 │ NULL   │\n│  NULL │ f      │\n└───────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-37",
    "href": "reference/expression-tables.html#parameters-37",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | Path\nA filesystem path or URL.\nrequired\n\n\ntable_name\nstr | None\nA name to refer to the table. If not provided, a name will be generated.\nNone\n\n\nkwargs\nAny\nBackend-specific keyword arguments for the file type.\n{}"
  },
  {
    "objectID": "reference/expression-tables.html#returns-42",
    "href": "reference/expression-tables.html#returns-42",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nTable expression representing a file"
  },
  {
    "objectID": "reference/expression-tables.html#examples-35",
    "href": "reference/expression-tables.html#examples-35",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"ghi\")})\n&gt;&gt;&gt; df\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\ng\n\n\n1\n2\nh\n\n\n2\n3\ni\n\n\n\n\n\n\n\n\n&gt;&gt;&gt; import deltalake as dl\n&gt;&gt;&gt; dl.write_deltalake(\"/tmp/data.delta\", df, mode=\"overwrite\")\n&gt;&gt;&gt; t = ibis.read_delta(\"/tmp/data.delta\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ g      │\n│     2 │ h      │\n│     3 │ i      │\n└───────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-38",
    "href": "reference/expression-tables.html#parameters-38",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nstr | Path | Sequence[str | Path]\nA filesystem path or URL or list of same.\nrequired\n\n\ntable_name\nstr | None\nA name to refer to the table. If not provided, a name will be generated.\nNone\n\n\nkwargs\nAny\nBackend-specific keyword arguments for the file type. See https://duckdb.org/docs/extensions/json.html for details.\n{}"
  },
  {
    "objectID": "reference/expression-tables.html#returns-43",
    "href": "reference/expression-tables.html#returns-43",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nTable expression representing a file"
  },
  {
    "objectID": "reference/expression-tables.html#examples-36",
    "href": "reference/expression-tables.html#examples-36",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; lines = '''\n... {\"a\": 1, \"b\": \"d\"}\n... {\"a\": 2, \"b\": null}\n... {\"a\": null, \"b\": \"f\"}\n... '''\n&gt;&gt;&gt; with open(\"/tmp/lines.json\", mode=\"w\") as f:\n...     nbytes = f.write(lines)  # nbytes is unused\n...\n&gt;&gt;&gt; t = ibis.read_json(\"/tmp/lines.json\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ d      │\n│     2 │ NULL   │\n│  NULL │ f      │\n└───────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-39",
    "href": "reference/expression-tables.html#parameters-39",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsources\nstr | Path | Sequence[str | Path]\nA filesystem path or URL or list of same.\nrequired\n\n\ntable_name\nstr | None\nA name to refer to the table. If not provided, a name will be generated.\nNone\n\n\nkwargs\nAny\nBackend-specific keyword arguments for the file type. For the DuckDB backend used by default, please refer to: * Parquet: https://duckdb.org/docs/data/parquet\n{}"
  },
  {
    "objectID": "reference/expression-tables.html#returns-44",
    "href": "reference/expression-tables.html#returns-44",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nTable expression representing a file"
  },
  {
    "objectID": "reference/expression-tables.html#examples-37",
    "href": "reference/expression-tables.html#examples-37",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; df = pd.DataFrame({\"a\": [1, 2, 3], \"b\": list(\"ghi\")})\n&gt;&gt;&gt; df\n\n\n\n\n\n\n\n\na\nb\n\n\n\n\n0\n1\ng\n\n\n1\n2\nh\n\n\n2\n3\ni\n\n\n\n\n\n\n\n\n&gt;&gt;&gt; df.to_parquet(\"/tmp/data.parquet\")\n&gt;&gt;&gt; t = ibis.read_parquet(\"/tmp/data.parquet\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ g      │\n│     2 │ h      │\n│     3 │ i      │\n└───────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-40",
    "href": "reference/expression-tables.html#parameters-40",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndata\n\nAny data accepted by the pandas.DataFrame constructor or a pyarrow.Table. Examples of acceptable objects are a pandas.DataFrame, a pyarrow.Table, a list of dicts of non-ibis Python objects, etc. ibis objects, like MapValue, will result in an error. Do not depend on the underlying storage type (e.g., pyarrow.Table), it’s subject to change across non-major releases.\nrequired\n\n\ncolumns\nIterable[str] | None\nOptional typing.Iterable of str column names.\nNone\n\n\nschema\nSupportsSchema | None\nOptional Schema. The functions use data to infer a schema if not passed.\nNone\n\n\nname\nstr | None\nOptional name of the table.\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-45",
    "href": "reference/expression-tables.html#returns-45",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression backed by in-memory data."
  },
  {
    "objectID": "reference/expression-tables.html#examples-38",
    "href": "reference/expression-tables.html#examples-38",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.memtable([{\"a\": 1}, {\"a\": 2}])\n&gt;&gt;&gt; t\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t = ibis.memtable([{\"a\": 1, \"b\": \"foo\"}, {\"a\": 2, \"b\": \"baz\"}])\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo    │\n│     2 │ baz    │\n└───────┴────────┘\n\n\n\nCreate a table literal without column names embedded in the data and pass columns\n\n&gt;&gt;&gt; t = ibis.memtable([(1, \"foo\"), (2, \"baz\")], columns=[\"a\", \"b\"])\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo    │\n│     2 │ baz    │\n└───────┴────────┘\n\n\n\nCreate a table literal without column names embedded in the data. Ibis generates column names if none are provided.\n\n&gt;&gt;&gt; t = ibis.memtable([(1, \"foo\"), (2, \"baz\")])\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━━┓\n┃ col0  ┃ col1   ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ foo    │\n│     2 │ baz    │\n└───────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-41",
    "href": "reference/expression-tables.html#parameters-41",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nschema\nSupportsSchema | None\nA schema for the table\nNone\n\n\nname\nstr | None\nName for the table. One is generated if this value is None.\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-46",
    "href": "reference/expression-tables.html#returns-46",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression"
  },
  {
    "objectID": "reference/expression-tables.html#examples-39",
    "href": "reference/expression-tables.html#examples-39",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\nCreate a table with no data backing it\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive\n\nTrue\n\n\n\n&gt;&gt;&gt; t = ibis.table(schema=dict(a=\"int\", b=\"string\"), name=\"t\")\n&gt;&gt;&gt; t\n\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.&lt;backend&gt;.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n\n\n\n\n\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.&lt;backend&gt;.execute(expr) or assign a backend instance to `ibis.options.default_backend`."
  },
  {
    "objectID": "reference/expression-tables.html#parameters-42",
    "href": "reference/expression-tables.html#parameters-42",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nir.Table\nA table expression\nrequired\n\n\n*rest\nir.Table\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly diff distinct rows not occurring in the calling table\nTrue"
  },
  {
    "objectID": "reference/expression-tables.html#returns-47",
    "href": "reference/expression-tables.html#returns-47",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe rows present in self that are not present in tables."
  },
  {
    "objectID": "reference/expression-tables.html#examples-40",
    "href": "reference/expression-tables.html#examples-40",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; ibis.difference(t1, t2)\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n└───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-43",
    "href": "reference/expression-tables.html#parameters-43",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nir.Table\nA table expression\nrequired\n\n\n*rest\nir.Table\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nTrue"
  },
  {
    "objectID": "reference/expression-tables.html#returns-48",
    "href": "reference/expression-tables.html#returns-48",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the intersection of all input tables."
  },
  {
    "objectID": "reference/expression-tables.html#examples-41",
    "href": "reference/expression-tables.html#examples-41",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; ibis.intersect(t1, t2)\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n└───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-44",
    "href": "reference/expression-tables.html#parameters-44",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable\nir.Table\nA table expression\nrequired\n\n\n*rest\nir.Table\nAdditional table expressions\n()\n\n\ndistinct\nbool\nOnly return distinct rows\nFalse"
  },
  {
    "objectID": "reference/expression-tables.html#returns-49",
    "href": "reference/expression-tables.html#returns-49",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA new table containing the union of all input tables."
  },
  {
    "objectID": "reference/expression-tables.html#examples-42",
    "href": "reference/expression-tables.html#examples-42",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t1 = ibis.memtable({\"a\": [1, 2]})\n&gt;&gt;&gt; t1\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"a\": [2, 3]})\n&gt;&gt;&gt; t2\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; ibis.union(t1, t2)  # union all by default\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     2 │\n│     3 │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; ibis.union(t1, t2, distinct=True).order_by(\"a\")\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     2 │\n│     3 │\n└───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#returns-50",
    "href": "reference/expression-tables.html#returns-50",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nA column expression enumerating rows"
  },
  {
    "objectID": "reference/expression-tables.html#examples-43",
    "href": "reference/expression-tables.html#examples-43",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rownum=ibis.row_number())\n\n┏━━━━━━━━┳━━━━━━━━┓\n┃ values ┃ rownum ┃\n┡━━━━━━━━╇━━━━━━━━┩\n│ int64  │ int64  │\n├────────┼────────┤\n│      1 │      0 │\n│      2 │      1 │\n│      1 │      2 │\n│      2 │      3 │\n│      3 │      4 │\n│      2 │      5 │\n└────────┴────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#returns-51",
    "href": "reference/expression-tables.html#returns-51",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nInt64Column\nThe min rank"
  },
  {
    "objectID": "reference/expression-tables.html#examples-44",
    "href": "reference/expression-tables.html#examples-44",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=ibis.rank().over(order_by=t.values))\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     2 │\n│      2 │     2 │\n│      2 │     2 │\n│      3 │     5 │\n└────────┴───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#returns-52",
    "href": "reference/expression-tables.html#returns-52",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nThe rank"
  },
  {
    "objectID": "reference/expression-tables.html#examples-45",
    "href": "reference/expression-tables.html#examples-45",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=ibis.dense_rank().over(order_by=t.values))\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     1 │\n│      2 │     1 │\n│      2 │     1 │\n│      3 │     2 │\n└────────┴───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#returns-53",
    "href": "reference/expression-tables.html#returns-53",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nFloatingColumn\nThe percent rank"
  },
  {
    "objectID": "reference/expression-tables.html#examples-46",
    "href": "reference/expression-tables.html#examples-46",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(pct_rank=ibis.percent_rank().over(order_by=t.values))\n\n┏━━━━━━━━┳━━━━━━━━━━┓\n┃ values ┃ pct_rank ┃\n┡━━━━━━━━╇━━━━━━━━━━┩\n│ int64  │ float64  │\n├────────┼──────────┤\n│      1 │      0.0 │\n│      1 │      0.0 │\n│      2 │      0.4 │\n│      2 │      0.4 │\n│      2 │      0.4 │\n│      3 │      1.0 │\n└────────┴──────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#returns-54",
    "href": "reference/expression-tables.html#returns-54",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nFloatingColumn\nThe cumulative distribution"
  },
  {
    "objectID": "reference/expression-tables.html#examples-47",
    "href": "reference/expression-tables.html#examples-47",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(dist=ibis.cume_dist().over(order_by=t.values))\n\n┏━━━━━━━━┳━━━━━━━━━━┓\n┃ values ┃ dist     ┃\n┡━━━━━━━━╇━━━━━━━━━━┩\n│ int64  │ float64  │\n├────────┼──────────┤\n│      1 │ 0.333333 │\n│      1 │ 0.333333 │\n│      2 │ 0.833333 │\n│      2 │ 0.833333 │\n│      2 │ 0.833333 │\n│      3 │ 1.000000 │\n└────────┴──────────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-45",
    "href": "reference/expression-tables.html#parameters-45",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuckets\nint | ir.IntegerValue\nNumber of buckets to partition into\nrequired"
  },
  {
    "objectID": "reference/expression-tables.html#examples-48",
    "href": "reference/expression-tables.html#examples-48",
    "title": "Table expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(ntile=ibis.ntile(2).over(order_by=t.values))\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ ntile ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     0 │\n│      2 │     1 │\n│      2 │     1 │\n│      3 │     1 │\n└────────┴───────┘"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-46",
    "href": "reference/expression-tables.html#parameters-46",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npreceding\n\nNumber of preceding rows in the window\nNone\n\n\nfollowing\n\nNumber of following rows in the window\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone\n\n\nrows\n\nWhether to use the ROWS window clause\nNone\n\n\nrange\n\nWhether to use the RANGE window clause\nNone\n\n\nbetween\n\nAutomatically infer the window kind based on the boundaries\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-55",
    "href": "reference/expression-tables.html#returns-55",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nWindow\nA window frame"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-47",
    "href": "reference/expression-tables.html#parameters-47",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-56",
    "href": "reference/expression-tables.html#returns-56",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nWindow\nA window frame"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-48",
    "href": "reference/expression-tables.html#parameters-48",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npreceding\n\nNumber of preceding rows in the window\nNone\n\n\nfollowing\n\nNumber of following rows in the window\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-57",
    "href": "reference/expression-tables.html#returns-57",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nWindow\nA window frame"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-49",
    "href": "reference/expression-tables.html#parameters-49",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npreceding\n\nA value expression\nrequired\n\n\norder_by\n\nOrdering key\nrequired\n\n\ngroup_by\n\nGrouping key\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-58",
    "href": "reference/expression-tables.html#returns-58",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nWindow\nA window frame"
  },
  {
    "objectID": "reference/expression-tables.html#parameters-50",
    "href": "reference/expression-tables.html#parameters-50",
    "title": "Table expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npreceding\n\nThe number of preceding rows\nrequired\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone"
  },
  {
    "objectID": "reference/expression-tables.html#returns-59",
    "href": "reference/expression-tables.html#returns-59",
    "title": "Table expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nWindow\nA window frame"
  },
  {
    "objectID": "reference/expression-geospatial.html",
    "href": "reference/expression-geospatial.html",
    "title": "Geospatial expressions",
    "section": "",
    "text": "Points, Polygons, LineStrings, and other geospatial types."
  },
  {
    "objectID": "reference/expression-geospatial.html#methods",
    "href": "reference/expression-geospatial.html#methods",
    "title": "Geospatial expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\narea\nCompute the area of a geospatial value.\n\n\nas_binary\nGet the geometry as well-known bytes (WKB) without the SRID data.\n\n\nas_ewkb\nGet the geometry as well-known bytes (WKB) with the SRID data.\n\n\nas_ewkt\nGet the geometry as well-known text (WKT) with the SRID data.\n\n\nas_text\nGet the geometry as well-known text (WKT) without the SRID data.\n\n\nazimuth\nReturn the angle in radians from the horizontal of the vector defined by the inputs.\n\n\nbuffer\nReturn all points whose distance from this geometry is less than or equal to radius.\n\n\ncentroid\nReturns the centroid of the geometry.\n\n\ncontains\nCheck if the geometry contains the right.\n\n\ncontains_properly\nCheck if the first geometry contains the second one.\n\n\ncovered_by\nCheck if the first geometry is covered by the second one.\n\n\ncovers\nCheck if the first geometry covers the second one.\n\n\ncrosses\nCheck if the geometries have at least one interior point in common.\n\n\nd_fully_within\nCheck if self is entirely within distance from right.\n\n\nd_within\nCheck if self is partially within distance from right.\n\n\ndifference\nReturn the difference of two geometries.\n\n\ndisjoint\nCheck if the geometries have no points in common.\n\n\ndistance\nCompute the distance between two geospatial expressions.\n\n\nend_point\nReturn the last point of a LINESTRING geometry as a POINT.\n\n\nenvelope\nReturns a geometry representing the bounding box of self.\n\n\ngeo_equals\nCheck if the geometries are equal.\n\n\ngeometry_n\nGet the 1-based Nth geometry of a multi geometry.\n\n\ngeometry_type\nGet the type of a geometry.\n\n\nintersection\nReturn the intersection of two geometries.\n\n\nintersects\nCheck if the geometries share any points.\n\n\nis_valid\nCheck if the geometry is valid.\n\n\nlength\nCompute the length of a geospatial expression.\n\n\nline_locate_point\nLocate the distance a point falls along the length of a line.\n\n\nline_merge\nMerge a MultiLineString into a LineString.\n\n\nline_substring\nClip a substring from a LineString.\n\n\nmax_distance\nReturns the 2-dimensional max distance between two geometries in projected units.\n\n\nn_points\nReturn the number of points in a geometry. Works for all geometries.\n\n\nn_rings\nReturn the number of rings for polygons and multipolygons.\n\n\nordering_equals\nCheck if two geometries are equal and have the same point ordering.\n\n\noverlaps\nCheck if the geometries share space, have the same dimension, and are not completely contained by each other.\n\n\nperimeter\nCompute the perimeter of a geospatial expression.\n\n\npoint_n\nReturn the Nth point in a single linestring in the geometry.\n\n\nset_srid\nSet the spatial reference identifier for the ST_Geometry.\n\n\nsimplify\nSimplify a given geometry.\n\n\nsrid\nReturn the spatial reference identifier for the ST_Geometry.\n\n\nstart_point\nReturn the first point of a LINESTRING geometry as a POINT.\n\n\ntouches\nCheck if the geometries have at least one point in common, but do not intersect.\n\n\ntransform\nTransform a geometry into a new SRID.\n\n\nunion\nMerge two geometries into a union geometry.\n\n\nwithin\nCheck if the first geometry is completely inside of the second.\n\n\nx\nReturn the X coordinate of self, or NULL if not available.\n\n\nx_max\nReturn the X maxima of a geometry.\n\n\nx_min\nReturn the X minima of a geometry.\n\n\ny\nReturn the Y coordinate of self, or NULL if not available.\n\n\ny_max\nReturn the Y maxima of a geometry.\n\n\ny_min\nReturn the Y minima of a geometry.\n\n\n\n\narea\narea()\nCompute the area of a geospatial value.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nThe area of self\n\n\n\n\n\n\nas_binary\nas_binary()\nGet the geometry as well-known bytes (WKB) without the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nBinary value\n\n\n\n\n\n\nas_ewkb\nas_ewkb()\nGet the geometry as well-known bytes (WKB) with the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nWKB value\n\n\n\n\n\n\nas_ewkt\nas_ewkt()\nGet the geometry as well-known text (WKT) with the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString value\n\n\n\n\n\n\nas_text\nas_text()\nGet the geometry as well-known text (WKT) without the SRID data.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString value\n\n\n\n\n\n\nazimuth\nazimuth(right)\nReturn the angle in radians from the horizontal of the vector defined by the inputs.\nAngle is computed clockwise from down-to-up on the clock: 12=0; 3=PI/2; 6=PI; 9=3PI/2.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nazimuth\n\n\n\n\n\n\nbuffer\nbuffer(radius)\nReturn all points whose distance from this geometry is less than or equal to radius.\nCalculations are in the Spatial Reference System of this Geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nradius\nfloat | ir.FloatingValue\nFloating expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nGeometry expression\n\n\n\n\n\n\ncentroid\ncentroid()\nReturns the centroid of the geometry.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nThe centroid\n\n\n\n\n\n\ncontains\ncontains(right)\nCheck if the geometry contains the right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self contains right\n\n\n\n\n\n\ncontains_properly\ncontains_properly(right)\nCheck if the first geometry contains the second one.\nExcludes common border points.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self contains right excluding border points.\n\n\n\n\n\n\ncovered_by\ncovered_by(right)\nCheck if the first geometry is covered by the second one.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is covered by right\n\n\n\n\n\n\ncovers\ncovers(right)\nCheck if the first geometry covers the second one.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self covers right\n\n\n\n\n\n\ncrosses\ncrosses(right)\nCheck if the geometries have at least one interior point in common.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right have at least one common interior point.\n\n\n\n\n\n\nd_fully_within\nd_fully_within(right, distance)\nCheck if self is entirely within distance from right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\ndistance\nir.FloatingValue\nDistance to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is within a specified distance from right.\n\n\n\n\n\n\nd_within\nd_within(right, distance)\nCheck if self is partially within distance from right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\ndistance\nir.FloatingValue\nDistance to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is partially within distance from right.\n\n\n\n\n\n\ndifference\ndifference(right)\nReturn the difference of two geometries.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nDifference of self and right\n\n\n\n\n\n\ndisjoint\ndisjoint(right)\nCheck if the geometries have no points in common.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right are disjoint\n\n\n\n\n\n\ndistance\ndistance(right)\nCompute the distance between two geospatial expressions.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry or geography\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nDistance between self and right\n\n\n\n\n\n\nend_point\nend_point()\nReturn the last point of a LINESTRING geometry as a POINT.\nReturn NULL if the input parameter is not a LINESTRING\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nEnd point\n\n\n\n\n\n\nenvelope\nenvelope()\nReturns a geometry representing the bounding box of self.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPolygonValue\nA polygon\n\n\n\n\n\n\ngeo_equals\ngeo_equals(right)\nCheck if the geometries are equal.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self equals right\n\n\n\n\n\n\ngeometry_n\ngeometry_n(n)\nGet the 1-based Nth geometry of a multi geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ir.IntegerValue\nNth geometry index\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nGeometry value\n\n\n\n\n\n\ngeometry_type\ngeometry_type()\nGet the type of a geometry.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nString representing the type of self.\n\n\n\n\n\n\nintersection\nintersection(right)\nReturn the intersection of two geometries.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nIntersection of self and right\n\n\n\n\n\n\nintersects\nintersects(right)\nCheck if the geometries share any points.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self intersects right\n\n\n\n\n\n\nis_valid\nis_valid()\nCheck if the geometry is valid.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is valid\n\n\n\n\n\n\nlength\nlength()\nCompute the length of a geospatial expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nLength of self\n\n\n\n\n\n\nline_locate_point\nline_locate_point(right)\nLocate the distance a point falls along the length of a line.\nReturns a float between zero and one representing the location of the closest point on the linestring to the given point, as a fraction of the total 2d line length.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nPointValue\nPoint geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nFraction of the total line length\n\n\n\n\n\n\nline_merge\nline_merge()\nMerge a MultiLineString into a LineString.\nReturns a (set of) LineString(s) formed by sewing together the constituent line work of a MultiLineString. If a geometry other than a LineString or MultiLineString is given, this will return an empty geometry collection.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nMerged linestrings\n\n\n\n\n\n\nline_substring\nline_substring(start, end)\nClip a substring from a LineString.\nReturns a linestring that is a substring of the input one, starting and ending at the given fractions of the total 2d length. The second and third arguments are floating point values between zero and one. This only works with linestrings.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nir.FloatingValue\nStart value\nrequired\n\n\nend\nir.FloatingValue\nEnd value\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nLineStringValue\nClipped linestring\n\n\n\n\n\n\nmax_distance\nmax_distance(right)\nReturns the 2-dimensional max distance between two geometries in projected units.\nIf self and right are the same geometry the function will return the distance between the two vertices most far from each other in that geometry.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nMaximum distance\n\n\n\n\n\n\nn_points\nn_points()\nReturn the number of points in a geometry. Works for all geometries.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nNumber of points\n\n\n\n\n\n\nn_rings\nn_rings()\nReturn the number of rings for polygons and multipolygons.\nOuter rings are counted as well.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nNumber of rings\n\n\n\n\n\n\nordering_equals\nordering_equals(right)\nCheck if two geometries are equal and have the same point ordering.\nReturns true if the two geometries are equal and the coordinates are in the same order.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether points and orderings are equal.\n\n\n\n\n\n\noverlaps\noverlaps(right)\nCheck if the geometries share space, have the same dimension, and are not completely contained by each other.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nOverlaps indicator\n\n\n\n\n\n\nperimeter\nperimeter()\nCompute the perimeter of a geospatial expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nPerimeter of self\n\n\n\n\n\n\npoint_n\npoint_n(n)\nReturn the Nth point in a single linestring in the geometry.\nNegative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nir.IntegerValue\nNth point index\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nNth point in self\n\n\n\n\n\n\nset_srid\nset_srid(srid)\nSet the spatial reference identifier for the ST_Geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsrid\nir.IntegerValue\nSRID integer value\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nself with SRID set to srid\n\n\n\n\n\n\nsimplify\nsimplify(tolerance, preserve_collapsed)\nSimplify a given geometry.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntolerance\nir.FloatingValue\nTolerance\nrequired\n\n\npreserve_collapsed\nir.BooleanValue\nWhether to preserve collapsed geometries\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nSimplified geometry\n\n\n\n\n\n\nsrid\nsrid()\nReturn the spatial reference identifier for the ST_Geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nSRID\n\n\n\n\n\n\nstart_point\nstart_point()\nReturn the first point of a LINESTRING geometry as a POINT.\nReturn NULL if the input parameter is not a LINESTRING\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nStart point\n\n\n\n\n\n\ntouches\ntouches(right)\nCheck if the geometries have at least one point in common, but do not intersect.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self and right are touching\n\n\n\n\n\n\ntransform\ntransform(srid)\nTransform a geometry into a new SRID.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsrid\nir.IntegerValue\nInteger expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nTransformed geometry\n\n\n\n\n\n\nunion\nunion(right)\nMerge two geometries into a union geometry.\nReturns the pointwise union of the two geometries. This corresponds to the non-aggregate version the PostGIS ST_Union.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialValue\nUnion of geometries\n\n\n\n\n\n\nwithin\nwithin(right)\nCheck if the first geometry is completely inside of the second.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nGeoSpatialValue\nRight geometry\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self is in right.\n\n\n\n\n\n\nx\nx()\nReturn the X coordinate of self, or NULL if not available.\nInput must be a point.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX coordinate of self\n\n\n\n\n\n\nx_max\nx_max()\nReturn the X maxima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX maxima\n\n\n\n\n\n\nx_min\nx_min()\nReturn the X minima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nX minima\n\n\n\n\n\n\ny\ny()\nReturn the Y coordinate of self, or NULL if not available.\nInput must be a point.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY coordinate of self\n\n\n\n\n\n\ny_max\ny_max()\nReturn the Y maxima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY maxima\n\n\n\n\n\n\ny_min\ny_min()\nReturn the Y minima of a geometry.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nFloatingValue\nY minima"
  },
  {
    "objectID": "reference/expression-geospatial.html#methods-1",
    "href": "reference/expression-geospatial.html#methods-1",
    "title": "Geospatial expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nunary_union\nAggregate a set of geometries into a union.\n\n\n\n\nunary_union\nunary_union()\nAggregate a set of geometries into a union.\nThis corresponds to the aggregate version of the PostGIS ST_Union. We give it a different name (following the corresponding method in GeoPandas) to avoid name conflicts with the non-aggregate version.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nGeoSpatialScalar\nUnion of geometries"
  },
  {
    "objectID": "reference/expression-collections.html",
    "href": "reference/expression-collections.html",
    "title": "Collection expressions",
    "section": "",
    "text": "Arrays, maps and structs."
  },
  {
    "objectID": "reference/expression-collections.html#methods",
    "href": "reference/expression-collections.html#methods",
    "title": "Collection expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nconcat\nConcatenate this array with one or more arrays.\n\n\ncontains\nReturn whether the array contains other.\n\n\nfilter\nFilter array elements using predicate.\n\n\nindex\nReturn the position of other in an array.\n\n\nintersect\nIntersect two arrays.\n\n\njoin\nJoin the elements of this array expression with sep.\n\n\nlength\nCompute the length of an array.\n\n\nmap\nApply a callable func to each element of this array expression.\n\n\nremove\nRemove other from self.\n\n\nrepeat\nRepeat this array n times.\n\n\nsort\nSort the elements in an array.\n\n\nunion\nUnion two arrays.\n\n\nunique\nReturn the unique values in an array.\n\n\nunnest\nFlatten an array into a column.\n\n\nzip\nZip two or more arrays together.\n\n\n\n\nconcat\nconcat(other, *args)\nConcatenate this array with one or more arrays.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nArrayValue\nOther array to concat with self\nrequired\n\n\nargs\nArrayValue\nOther arrays to concat with self\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nself concatenated with other and args\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7], [3], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ a            ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [7]          │\n│ [3]          │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.concat(t.a)\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayConcat() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [7, 7]        │\n│ [3, 3]        │\n│ NULL          │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.concat(ibis.literal([4], type=\"array&lt;int64&gt;\"))\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayConcat() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [7, 4]        │\n│ [3, 4]        │\n│ [4]           │\n└───────────────┘\n\n\n\nconcat is also available using the + operator\n\n&gt;&gt;&gt; [1] + t.a\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayConcat() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [1, 7]        │\n│ [1, 3]        │\n│ [1]           │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a + [1]\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayConcat() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [7, 1]        │\n│ [3, 1]        │\n│ [1]           │\n└───────────────┘\n\n\n\n\n\n\ncontains\ncontains(other)\nReturn whether the array contains other.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nir.Value\nIbis expression to check for existence of in self\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether other is contained in self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1], [], [42, 42], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ arr          ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [1]          │\n│ []           │\n│ [42, 42]     │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.contains(42)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(arr, 42) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ False                  │\n│ False                  │\n│ True                   │\n│ NULL                   │\n└────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.contains(None)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(arr, None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                  │\n├──────────────────────────┤\n│ NULL                     │\n│ NULL                     │\n│ NULL                     │\n│ NULL                     │\n└──────────────────────────┘\n\n\n\n\n\n\nfilter\nfilter(predicate)\nFilter array elements using predicate.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\nCallable[[ir.Value], bool | ir.BooleanValue]\nFunction to use to filter array elements\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nArray elements filtered using predicate\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[1, None, 2], [4], []]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ a                 ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;      │\n├───────────────────┤\n│ [1, None, ... +1] │\n│ [4]               │\n│ []                │\n└───────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.filter(lambda x: x &gt; 1)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayFilter(a, Greater(x, 1)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;                  │\n├───────────────────────────────┤\n│ [2]                           │\n│ [4]                           │\n│ []                            │\n└───────────────────────────────┘\n\n\n\n.filter() also supports more complex callables like functools.partial and lambdas with closures\n\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; def gt(x, y):\n...     return x &gt; y\n...\n&gt;&gt;&gt; gt1 = partial(gt, y=1)\n&gt;&gt;&gt; t.a.filter(gt1)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayFilter(a, Greater(x, 1)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;                  │\n├───────────────────────────────┤\n│ [2]                           │\n│ [4]                           │\n│ []                            │\n└───────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; y = 1\n&gt;&gt;&gt; t.a.filter(lambda x: x &gt; y)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayFilter(a, Greater(x, 1)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;                  │\n├───────────────────────────────┤\n│ [2]                           │\n│ [4]                           │\n│ []                            │\n└───────────────────────────────┘\n\n\n\n\n\n\nindex\nindex(other)\nReturn the position of other in an array.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nir.Value\nIbis expression to existence of in self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nThe position of other in self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1], [], [42, 42], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ arr          ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [1]          │\n│ []           │\n│ [42, 42]     │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.index(42)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, 42) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                  │\n├────────────────────────┤\n│                     -1 │\n│                     -1 │\n│                      0 │\n│                   NULL │\n└────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.index(800)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, 800) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                   │\n├─────────────────────────┤\n│                      -1 │\n│                      -1 │\n│                      -1 │\n│                    NULL │\n└─────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.index(None)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(arr, None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                    │\n├──────────────────────────┤\n│                     NULL │\n│                     NULL │\n│                     NULL │\n│                     NULL │\n└──────────────────────────┘\n\n\n\n\n\n\nintersect\nintersect(other)\nIntersect two arrays.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nArrayValue\nAnother array to intersect with self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nIntersected arrays\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"arr1\": [[3, 2], [], None], \"arr2\": [[1, 3], [None], [5]]}\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ arr1         ┃ arr2         ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │ array&lt;int64&gt; │\n├──────────────┼──────────────┤\n│ [3, 2]       │ [1, 3]       │\n│ []           │ [None]       │\n│ NULL         │ [5]          │\n└──────────────┴──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr1.intersect(t.arr2)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayIntersect(arr1, arr2) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;               │\n├────────────────────────────┤\n│ [3]                        │\n│ []                         │\n│ NULL                       │\n└────────────────────────────┘\n\n\n\n\n\n\njoin\njoin(sep)\nJoin the elements of this array expression with sep.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsep\nstr | ir.StringValue\nSeparator to use for joining array elements\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nElements of self joined with sep\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[\"a\", \"b\", \"c\"], None, [], [\"b\", None]]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;      │\n├────────────────────┤\n│ ['a', 'b', ... +1] │\n│ NULL               │\n│ []                 │\n│ ['b', None]        │\n└────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.join(\"|\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayStringJoin('|', arr) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                    │\n├───────────────────────────┤\n│ a|b|c                     │\n│ NULL                      │\n│ NULL                      │\n│ b                         │\n└───────────────────────────┘\n\n\n\n\n\nSee Also\nStringValue.join\n\n\n\nlength\nlength()\nCompute the length of an array.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe integer length of each element of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7, 42], [3], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ a            ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [7, 42]      │\n│ [3]          │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.length()\n\n┏━━━━━━━━━━━━━━━━┓\n┃ ArrayLength(a) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int64          │\n├────────────────┤\n│              2 │\n│              1 │\n│           NULL │\n└────────────────┘\n\n\n\n\n\n\nmap\nmap(func)\nApply a callable func to each element of this array expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfunc\nCallable[[ir.Value], ir.Value]\nFunction to apply to each element of this array\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nfunc applied to every element of this array expression.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[1, None, 2], [4], []]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ a                 ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;      │\n├───────────────────┤\n│ [1, None, ... +1] │\n│ [4]               │\n│ []                │\n└───────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.map(lambda x: (x + 100).cast(\"float\"))\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayMap(a, Cast(Add(x, 100), float64)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;float64&gt;                          │\n├─────────────────────────────────────────┤\n│ [101.0, None, ... +1]                   │\n│ [104.0]                                 │\n│ []                                      │\n└─────────────────────────────────────────┘\n\n\n\n.map() also supports more complex callables like functools.partial and lambdas with closures\n\n&gt;&gt;&gt; from functools import partial\n&gt;&gt;&gt; def add(x, y):\n...     return x + y\n...\n&gt;&gt;&gt; add2 = partial(add, y=2)\n&gt;&gt;&gt; t.a.map(add2)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayMap(a, Add(x, 2)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;           │\n├────────────────────────┤\n│ [3, None, ... +1]      │\n│ [6]                    │\n│ []                     │\n└────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; y = 2\n&gt;&gt;&gt; t.a.map(lambda x: x + y)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayMap(a, Add(x, 2)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;           │\n├────────────────────────┤\n│ [3, None, ... +1]      │\n│ [6]                    │\n│ []                     │\n└────────────────────────┘\n\n\n\n\n\n\nremove\nremove(other)\nRemove other from self.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nir.Value\nElement to remove from self.\nrequired\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[3, 2], [], [42, 2], [2, 2], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ arr          ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [3, 2]       │\n│ []           │\n│ [42, 2]      │\n│ [2, 2]       │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.remove(2)\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayRemove(arr, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;        │\n├─────────────────────┤\n│ [3]                 │\n│ []                  │\n│ [42]                │\n│ []                  │\n│ NULL                │\n└─────────────────────┘\n\n\n\n\n\n\nrepeat\nrepeat(n)\nRepeat this array n times.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ir.IntegerValue\nNumber of times to repeat self.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nself repeated n times\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7], [3], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ a            ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [7]          │\n│ [3]          │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.repeat(2)\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayRepeat(a, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;      │\n├───────────────────┤\n│ [7, 7]            │\n│ [3, 3]            │\n│ []                │\n└───────────────────┘\n\n\n\nrepeat is also available using the * operator\n\n&gt;&gt;&gt; 2 * t.a\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayRepeat(a, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;      │\n├───────────────────┤\n│ [7, 7]            │\n│ [3, 3]            │\n│ []                │\n└───────────────────┘\n\n\n\n\n\n\nsort\nsort()\nSort the elements in an array.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nSorted values in an array\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[3, 2], [], [42, 42], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ arr          ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [3, 2]       │\n│ []           │\n│ [42, 42]     │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.sort()\n\n┏━━━━━━━━━━━━━━━━┓\n┃ ArraySort(arr) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;   │\n├────────────────┤\n│ [2, 3]         │\n│ []             │\n│ [42, 42]       │\n│ NULL           │\n└────────────────┘\n\n\n\n\n\n\nunion\nunion(other)\nUnion two arrays.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nir.ArrayValue\nAnother array to union with self\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nUnioned arrays\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"arr1\": [[3, 2], [], None], \"arr2\": [[1, 3], [None], [5]]}\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ arr1         ┃ arr2         ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │ array&lt;int64&gt; │\n├──────────────┼──────────────┤\n│ [3, 2]       │ [1, 3]       │\n│ []           │ [None]       │\n│ NULL         │ [5]          │\n└──────────────┴──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr1.union(t.arr2)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayUnion(arr1, arr2) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;           │\n├────────────────────────┤\n│ [1, 2, ... +1]         │\n│ [None]                 │\n│ [5]                    │\n└────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr1.union(t.arr2).contains(3)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayContains(ArrayUnion(arr1, arr2), 3) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                                  │\n├──────────────────────────────────────────┤\n│ True                                     │\n│ False                                    │\n│ False                                    │\n└──────────────────────────────────────────┘\n\n\n\n\n\n\nunique\nunique()\nReturn the unique values in an array.\n\n\n\n\n\n\nElement ordering in array may not be retained.\n\n\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nUnique values in an array\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[1, 3, 3], [], [42, 42], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━┓\n┃ arr            ┃\n┡━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;   │\n├────────────────┤\n│ [1, 3, ... +1] │\n│ []             │\n│ [42, 42]       │\n│ NULL           │\n└────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.arr.unique()\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayDistinct(arr) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;       │\n├────────────────────┤\n│ [3, 1]             │\n│ []                 │\n│ [42]               │\n│ NULL               │\n└────────────────────┘\n\n\n\n\n\n\nunnest\nunnest()\nFlatten an array into a column.\n\n\n\n\n\n\nThis operation changes the cardinality of the result\n\n\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [[7, 42], [3, 3], None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┓\n┃ a            ┃\n┡━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │\n├──────────────┤\n│ [7, 42]      │\n│ [3, 3]       │\n│ NULL         │\n└──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.a.unnest()\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     7 │\n│    42 │\n│     3 │\n│     3 │\n└───────┘\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nir.Value\nUnnested array\n\n\n\n\n\n\nzip\nzip(other, *others)\nZip two or more arrays together.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nArrayValue\nAnother array to zip with self\nrequired\n\n\nothers\nArrayValue\nAdditional arrays to zip with self\n()\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nArray\nArray of structs where each struct field is an element of each input array.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"numbers\": [[3, 2], [], None], \"strings\": [[\"a\", \"c\"], None, [\"e\"]]}\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ numbers      ┃ strings       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │ array&lt;string&gt; │\n├──────────────┼───────────────┤\n│ [3, 2]       │ ['a', 'c']    │\n│ []           │ NULL          │\n│ NULL         │ ['e']         │\n└──────────────┴───────────────┘\n\n\n\n\n&gt;&gt;&gt; expr = t.numbers.zip(t.strings)\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;struct&lt;f1: int64, f2: string&gt;&gt; │\n├──────────────────────────────────────┤\n│ [{...}, {...}]                       │\n│ []                                   │\n│ [{...}]                              │\n└──────────────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; expr.unnest()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;f1: int64, f2: string&gt; │\n├───────────────────────────────┤\n│ {'f1': 3, 'f2': 'a'}          │\n│ {'f1': 2, 'f2': 'c'}          │\n│ {'f1': None, 'f2': 'e'}       │\n└───────────────────────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#examples-15",
    "href": "reference/expression-collections.html#examples-15",
    "title": "Collection expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; tab = pa.table(\n...     {\n...         \"m\": pa.array(\n...             [[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...             type=pa.map_(pa.utf8(), pa.int64()),\n...         )\n...     }\n... )\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt; │\n├─────────────────────┤\n│ {'a': 1, 'b': 2}    │\n│ {'a': 1}            │\n│ NULL                │\n└─────────────────────┘\n\n\n\nCan use [] to access values:\n\n&gt;&gt;&gt; t.m[\"a\"]\n\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'a', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    1 │\n│                    1 │\n│                 NULL │\n└──────────────────────┘\n\n\n\nTo provide default values, use get:\n\n&gt;&gt;&gt; t.m.get(\"b\", 0)\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ int64             │\n├───────────────────┤\n│                 2 │\n│                 0 │\n│                 0 │\n└───────────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#methods-1",
    "href": "reference/expression-collections.html#methods-1",
    "title": "Collection expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncontains\nReturn whether the map contains key.\n\n\nget\nReturn the value for key from expr.\n\n\nkeys\nExtract the keys of a map.\n\n\nlength\nReturn the number of key-value pairs in the map.\n\n\nvalues\nExtract the values of a map.\n\n\n\n\ncontains\ncontains(key)\nReturn whether the map contains key.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nint | str | ir.IntegerValue | ir.StringValue\nMapping key for which to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating the presence of key in the map expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table(\n...     {\n...         \"m\": pa.array(\n...             [[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...             type=pa.map_(pa.utf8(), pa.int64()),\n...         )\n...     }\n... )\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt; │\n├─────────────────────┤\n│ {'a': 1, 'b': 2}    │\n│ {'a': 1}            │\n│ NULL                │\n└─────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.contains(\"b\")\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapContains(m, 'b') ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean             │\n├─────────────────────┤\n│ True                │\n│ False               │\n│ False               │\n└─────────────────────┘\n\n\n\n\n\n\nget\nget(key, default=None)\nReturn the value for key from expr.\nReturn default if key is not in the map.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nir.Value\nExpression to use for key\nrequired\n\n\ndefault\nir.Value | None\nExpression to return if key is not a key in expr\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nThe element type of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table(\n...     {\n...         \"m\": pa.array(\n...             [[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...             type=pa.map_(pa.utf8(), pa.int64()),\n...         )\n...     }\n... )\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt; │\n├─────────────────────┤\n│ {'a': 1, 'b': 2}    │\n│ {'a': 1}            │\n│ NULL                │\n└─────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.get(\"a\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'a', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    1 │\n│                    1 │\n│                 NULL │\n└──────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.get(\"b\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', None) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                │\n├──────────────────────┤\n│                    2 │\n│                 NULL │\n│                 NULL │\n└──────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.get(\"b\", 0)\n\n┏━━━━━━━━━━━━━━━━━━━┓\n┃ MapGet(m, 'b', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━┩\n│ int64             │\n├───────────────────┤\n│                 2 │\n│                 0 │\n│                 0 │\n└───────────────────┘\n\n\n\n\n\n\nkeys\nkeys()\nExtract the keys of a map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe keys of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table(\n...     {\n...         \"m\": pa.array(\n...             [[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...             type=pa.map_(pa.utf8(), pa.int64()),\n...         )\n...     }\n... )\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt; │\n├─────────────────────┤\n│ {'a': 1, 'b': 2}    │\n│ {'a': 1}            │\n│ NULL                │\n└─────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.keys()\n\n┏━━━━━━━━━━━━━━━━┓\n┃ MapKeys(m)     ┃\n┡━━━━━━━━━━━━━━━━┩\n│ array&lt;!string&gt; │\n├────────────────┤\n│ ['a', 'b']     │\n│ ['a']          │\n│ NULL           │\n└────────────────┘\n\n\n\n\n\n\nlength\nlength()\nReturn the number of key-value pairs in the map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe number of elements in self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; tab = pa.table(\n...     {\n...         \"m\": pa.array(\n...             [[(\"a\", 1), (\"b\", 2)], [(\"a\", 1)], None],\n...             type=pa.map_(pa.utf8(), pa.int64()),\n...         )\n...     }\n... )\n&gt;&gt;&gt; t = ibis.memtable(tab)\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ m                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;!string, int64&gt; │\n├─────────────────────┤\n│ {'a': 1, 'b': 2}    │\n│ {'a': 1}            │\n│ NULL                │\n└─────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.m.length()\n\n┏━━━━━━━━━━━━━━┓\n┃ MapLength(m) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│            2 │\n│            1 │\n│         NULL │\n└──────────────┘\n\n\n\n\n\n\nvalues\nvalues()\nExtract the values of a map.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe values of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; m = ibis.map({\"a\": 1, \"b\": 2})\n&gt;&gt;&gt; m.values()\n\n\n\n\n\n[1, 2]"
  },
  {
    "objectID": "reference/expression-collections.html#examples-21",
    "href": "reference/expression-collections.html#examples-21",
    "title": "Collection expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [{\"a\": 1, \"b\": \"foo\"}, {\"a\": 3, \"b\": None}, None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ s                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 3, 'b': None}         │\n│ NULL                        │\n└─────────────────────────────┘\n\n\n\nCan use either . or [] to access fields:\n\n&gt;&gt;&gt; t.s.a\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; t.s[\"a\"]\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘"
  },
  {
    "objectID": "reference/expression-collections.html#attributes",
    "href": "reference/expression-collections.html#attributes",
    "title": "Collection expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nfields\nReturn a mapping from field name to field type of the struct.\n\n\nnames\nReturn the field names of the struct.\n\n\ntypes\nReturn the field types of the struct."
  },
  {
    "objectID": "reference/expression-collections.html#methods-2",
    "href": "reference/expression-collections.html#methods-2",
    "title": "Collection expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ndestructure\nDestructure a StructValue into the corresponding struct fields.\n\n\nlift\nProject the fields of self into a table.\n\n\n\n\ndestructure\ndestructure()\nDestructure a StructValue into the corresponding struct fields.\nWhen assigned, a destruct value will be destructured and assigned to multiple columns.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nlist[AnyValue]\nValue expressions corresponding to the struct fields.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [{\"a\": 1, \"b\": \"foo\"}, {\"a\": 3, \"b\": None}, None]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ s                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 3, 'b': None}         │\n│ NULL                        │\n└─────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; a, b = t.s.destructure()\n&gt;&gt;&gt; a\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int64 │\n├───────┤\n│     1 │\n│     3 │\n│  NULL │\n└───────┘\n\n\n\n\n&gt;&gt;&gt; b\n\n┏━━━━━━━━┓\n┃ b      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ foo    │\n│ NULL   │\n│ NULL   │\n└────────┘\n\n\n\n\n\n\nlift\nlift()\nProject the fields of self into a table.\nThis method is useful when analyzing data that has deeply nested structs or arrays of structs. lift can be chained to avoid repeating column names and table references.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA projection with this struct expression’s fields.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"pos\": [\n...             {\"lat\": 10.1, \"lon\": 30.3},\n...             {\"lat\": 10.2, \"lon\": 30.2},\n...             {\"lat\": 10.3, \"lon\": 30.1},\n...         ]\n...     }\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ pos                                ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;lat: float64, lon: float64&gt; │\n├────────────────────────────────────┤\n│ {'lat': 10.1, 'lon': 30.3}         │\n│ {'lat': 10.2, 'lon': 30.2}         │\n│ {'lat': 10.3, 'lon': 30.1}         │\n└────────────────────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.pos.lift()\n\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ lat     ┃ lon     ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ float64 │ float64 │\n├─────────┼─────────┤\n│    10.1 │    30.3 │\n│    10.2 │    30.2 │\n│    10.3 │    30.1 │\n└─────────┴─────────┘\n\n\n\n\n\nSee Also\nTable.unpack"
  },
  {
    "objectID": "reference/expression-collections.html#parameters-13",
    "href": "reference/expression-collections.html#parameters-13",
    "title": "Collection expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nIterable[V]\nAn iterable of Ibis expressions or a list of Python literals\nrequired\n\n\ntype\nstr | dt.DataType | None\nAn instance of ibis.expr.datatypes.DataType or a string indicating the ibis type of value.\nNone"
  },
  {
    "objectID": "reference/expression-collections.html#returns-21",
    "href": "reference/expression-collections.html#returns-21",
    "title": "Collection expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nAn array column (if the inputs are column expressions), or an array scalar (if the inputs are Python literals)"
  },
  {
    "objectID": "reference/expression-collections.html#examples-24",
    "href": "reference/expression-collections.html#examples-24",
    "title": "Collection expressions",
    "section": "Examples",
    "text": "Examples\nCreate an array column from column expressions\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n&gt;&gt;&gt; ibis.array([t.a, t.b])\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayColumn() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [1, 4]        │\n│ [2, 5]        │\n│ [3, 6]        │\n└───────────────┘\n\n\n\nCreate an array scalar from Python literals\n\n&gt;&gt;&gt; ibis.array([1.0, 2.0, 3.0])\n\n\n\n\n\n[1.0, 2.0, ... +1]\n\n\n\nMixing scalar and column expressions is allowed\n\n&gt;&gt;&gt; ibis.array([t.a, 42])\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayColumn() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [1, 42]       │\n│ [2, 42]       │\n│ [3, 42]       │\n└───────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#parameters-14",
    "href": "reference/expression-collections.html#parameters-14",
    "title": "Collection expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkeys\nIterable[Any] | Mapping[Any, Any] | ArrayColumn\nKeys of the map or Mapping. If keys is a Mapping, values must be None.\nrequired\n\n\nvalues\nIterable[Any] | ArrayColumn | None\nValues of the map or None. If None, the keys argument must be a Mapping.\nNone"
  },
  {
    "objectID": "reference/expression-collections.html#returns-22",
    "href": "reference/expression-collections.html#returns-22",
    "title": "Collection expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nMapValue\nAn expression representing either a map column or literal (associative array with key/value pairs of fixed types)"
  },
  {
    "objectID": "reference/expression-collections.html#examples-25",
    "href": "reference/expression-collections.html#examples-25",
    "title": "Collection expressions",
    "section": "Examples",
    "text": "Examples\nCreate a map literal from a dict with the type inferred\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; ibis.map(dict(a=1, b=2))\n\n\n\n\n\n{'a': 1, 'b': 2}\n\n\n\nCreate a new map column from columns with keys and values\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"keys\": [[\"a\", \"b\"], [\"b\"]], \"values\": [[1, 2], [3]]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ keys          ┃ values       ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ array&lt;string&gt; │ array&lt;int64&gt; │\n├───────────────┼──────────────┤\n│ ['a', 'b']    │ [1, 2]       │\n│ ['b']         │ [3]          │\n└───────────────┴──────────────┘\n\n\n\n\n&gt;&gt;&gt; ibis.map(t.keys, t.values)\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Map(keys, values)  ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ map&lt;string, int64&gt; │\n├────────────────────┤\n│ {'a': 1, 'b': 2}   │\n│ {'b': 3}           │\n└────────────────────┘"
  },
  {
    "objectID": "reference/expression-collections.html#parameters-15",
    "href": "reference/expression-collections.html#parameters-15",
    "title": "Collection expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nIterable[tuple[str, V]] | Mapping[str, V]\nThe underlying data for literal struct value or a pairs of field names and column expressions.\nrequired\n\n\ntype\nstr | dt.DataType | None\nAn instance of ibis.expr.datatypes.DataType or a string indicating the ibis type of value. This is only used if all of the input values are literals.\nNone"
  },
  {
    "objectID": "reference/expression-collections.html#returns-23",
    "href": "reference/expression-collections.html#returns-23",
    "title": "Collection expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nStructValue\nAn expression representing a literal or column struct (compound type with fields of fixed types)"
  },
  {
    "objectID": "reference/expression-collections.html#examples-26",
    "href": "reference/expression-collections.html#examples-26",
    "title": "Collection expressions",
    "section": "Examples",
    "text": "Examples\nCreate a struct literal from a dict with the type inferred\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.struct(dict(a=1, b=\"foo\"))\n\nCreate a struct literal from a dict with a specified type\n\n&gt;&gt;&gt; t = ibis.struct(dict(a=1, b=\"foo\"), type=\"struct&lt;a: float, b: string&gt;\")\n\nSpecify a specific type for the struct literal\n\n&gt;&gt;&gt; t = ibis.struct(dict(a=1, b=40), type=\"struct&lt;a: float, b: int32&gt;\")\n\nCreate a struct array from multiple arrays\n\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 2, 3], \"b\": [\"foo\", \"bar\", \"baz\"]})\n&gt;&gt;&gt; ibis.struct([(\"a\", t.a), (\"b\", t.b)])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StructColumn()              ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 2, 'b': 'bar'}        │\n│ {'a': 3, 'b': 'baz'}        │\n└─────────────────────────────┘\n\n\n\nCreate a struct array from columns and literals\n\n&gt;&gt;&gt; ibis.struct([(\"a\", t.a), (\"b\", \"foo\")])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StructColumn()              ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;a: int64, b: string&gt; │\n├─────────────────────────────┤\n│ {'a': 1, 'b': 'foo'}        │\n│ {'a': 2, 'b': 'foo'}        │\n│ {'a': 3, 'b': 'foo'}        │\n└─────────────────────────────┘"
  },
  {
    "objectID": "reference/selectors.html",
    "href": "reference/selectors.html",
    "title": "Column selectors",
    "section": "",
    "text": "Choose Table columns based on dtype, regex, and other criteria"
  },
  {
    "objectID": "reference/selectors.html#parameters",
    "href": "reference/selectors.html#parameters",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicate\nCallable[[ir.Value], bool]\nA callable that accepts an ibis value expression and returns a bool\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples",
    "href": "reference/selectors.html#examples",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"float32\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.where(lambda col: col.get_name() == \"a\"))\n&gt;&gt;&gt; expr.columns\n\n['a']"
  },
  {
    "objectID": "reference/selectors.html#examples-1",
    "href": "reference/selectors.html#examples-1",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\", b=\"string\", c=\"array&lt;string&gt;\"), name=\"t\")\n&gt;&gt;&gt; t\n\nUnboundTable: t\n  a int64\n  b string\n  c array&lt;string&gt;\n\n\n\n\n&gt;&gt;&gt; expr = t.select(s.numeric())  # `a` has integer type, so it's numeric\n&gt;&gt;&gt; expr.columns\n\n['a']"
  },
  {
    "objectID": "reference/selectors.html#see-also",
    "href": "reference/selectors.html#see-also",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\nof_type"
  },
  {
    "objectID": "reference/selectors.html#parameters-1",
    "href": "reference/selectors.html#parameters-1",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndtype\ndt.DataType | str | type[dt.DataType]\nDataType instance, str or DataType class\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples-2",
    "href": "reference/selectors.html#examples-2",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\nSelect according to a specific DataType instance\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.datatypes as dt\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(\n...     dict(name=\"string\", siblings=\"array&lt;string&gt;\", parents=\"array&lt;int64&gt;\")\n... )\n&gt;&gt;&gt; expr = t.select(s.of_type(dt.Array(dt.string)))\n&gt;&gt;&gt; expr.columns\n\n['siblings']\n\n\nStrings are also accepted\n\n&gt;&gt;&gt; expr = t.select(s.of_type(\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr.columns\n\n['siblings']\n\n\nAbstract/unparametrized types may also be specified by their string name (e.g. “integer” for any integer type), or by passing in a DataType class instead. The following options are equivalent.\n\n&gt;&gt;&gt; expr1 = t.select(s.of_type(\"array\"))\n&gt;&gt;&gt; expr2 = t.select(s.of_type(dt.Array))\n&gt;&gt;&gt; expr1.equals(expr2)\n\nTrue\n\n\n\n&gt;&gt;&gt; expr2.columns\n\n['siblings', 'parents']"
  },
  {
    "objectID": "reference/selectors.html#see-also-1",
    "href": "reference/selectors.html#see-also-1",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\nnumeric"
  },
  {
    "objectID": "reference/selectors.html#parameters-2",
    "href": "reference/selectors.html#parameters-2",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nprefixes\nstr | tuple[str, …]\nPrefixes to compare column names against\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples-3",
    "href": "reference/selectors.html#examples-3",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(apples=\"int\", oranges=\"float\", bananas=\"bool\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(s.startswith((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n\n['apples', 'bananas']"
  },
  {
    "objectID": "reference/selectors.html#see-also-2",
    "href": "reference/selectors.html#see-also-2",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\nendswith"
  },
  {
    "objectID": "reference/selectors.html#parameters-3",
    "href": "reference/selectors.html#parameters-3",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsuffixes\nstr | tuple[str, …]\nSuffixes to compare column names against\nrequired"
  },
  {
    "objectID": "reference/selectors.html#see-also-3",
    "href": "reference/selectors.html#see-also-3",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\nstartswith"
  },
  {
    "objectID": "reference/selectors.html#parameters-4",
    "href": "reference/selectors.html#parameters-4",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nneedles\nstr | tuple[str, …]\nOne or more strings to search for in column names\nrequired\n\n\nhow\nCallable[[Iterable[bool]], bool]\nA boolean reduction to allow the configuration of how needles are summarized.\nany"
  },
  {
    "objectID": "reference/selectors.html#examples-4",
    "href": "reference/selectors.html#examples-4",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\nSelect columns that contain either \"a\" or \"b\"\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(\n...     dict(\n...         a=\"int64\", b=\"string\", c=\"float\", d=\"array&lt;int16&gt;\", ab=\"struct&lt;x: int&gt;\"\n...     )\n... )\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\")))\n&gt;&gt;&gt; expr.columns\n\n['a', 'b', 'ab']\n\n\nSelect columns that contain all of \"a\" and \"b\", that is, both \"a\" and \"b\" must be in each column’s name to match.\n\n&gt;&gt;&gt; expr = t.select(s.contains((\"a\", \"b\"), how=all))\n&gt;&gt;&gt; expr.columns\n\n['ab']"
  },
  {
    "objectID": "reference/selectors.html#see-also-4",
    "href": "reference/selectors.html#see-also-4",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\nmatches"
  },
  {
    "objectID": "reference/selectors.html#parameters-5",
    "href": "reference/selectors.html#parameters-5",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nregex\nstr | re.Pattern\nA string or re.Pattern object\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples-5",
    "href": "reference/selectors.html#examples-5",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.selectors as s\n&gt;&gt;&gt; t = ibis.table(dict(ab=\"string\", abd=\"int\", be=\"array&lt;string&gt;\"))\n&gt;&gt;&gt; expr = t.select(s.matches(r\"ab+\"))\n&gt;&gt;&gt; expr.columns\n\n['ab', 'abd']"
  },
  {
    "objectID": "reference/selectors.html#see-also-5",
    "href": "reference/selectors.html#see-also-5",
    "title": "Column selectors",
    "section": "See Also",
    "text": "See Also\ncontains"
  },
  {
    "objectID": "reference/selectors.html#parameters-6",
    "href": "reference/selectors.html#parameters-6",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector | Iterable[str] | str\nAn expression that selects columns on which the transformation function will be applied, an iterable of str column names or a single str column name.\nrequired\n\n\nfunc\nDeferred | Callable[[ir.Value], ir.Value] | Mapping[str | None, Deferred | Callable[[ir.Value], ir.Value]]\nA function (or dictionary of functions) to use to transform the data.\nrequired\n\n\nnames\nstr | Callable[[str, str | None], str] | None\nA lambda function or a format string to name the columns created by the transformation function.\nNone"
  },
  {
    "objectID": "reference/selectors.html#returns",
    "href": "reference/selectors.html#returns",
    "title": "Column selectors",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nAcross\nAn Across selector object"
  },
  {
    "objectID": "reference/selectors.html#examples-6",
    "href": "reference/selectors.html#examples-6",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from ibis import _, selectors as s\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.select(s.startswith(\"bill\")).mutate(\n...     s.across(s.numeric(), dict(centered=_ - _.mean()), names=\"{fn}_{col}\")\n... )\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ centered_bill_length_mm ┃ centered_bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                 │ float64                │\n├────────────────┼───────────────┼─────────────────────────┼────────────────────────┤\n│           39.1 │          18.7 │                -4.82193 │                1.54883 │\n│           39.5 │          17.4 │                -4.42193 │                0.24883 │\n│           40.3 │          18.0 │                -3.62193 │                0.84883 │\n│           NULL │          NULL │                    NULL │                   NULL │\n│           36.7 │          19.3 │                -7.22193 │                2.14883 │\n│           39.3 │          20.6 │                -4.62193 │                3.44883 │\n│           38.9 │          17.8 │                -5.02193 │                0.64883 │\n│           39.2 │          19.6 │                -4.72193 │                2.44883 │\n│           34.1 │          18.1 │                -9.82193 │                0.94883 │\n│           42.0 │          20.2 │                -1.92193 │                3.04883 │\n│              … │             … │                       … │                      … │\n└────────────────┴───────────────┴─────────────────────────┴────────────────────────┘"
  },
  {
    "objectID": "reference/selectors.html#parameters-7",
    "href": "reference/selectors.html#parameters-7",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nDeferred | Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples-7",
    "href": "reference/selectors.html#examples-7",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_any(s.endswith(\"_mm\"), _.abs() &gt; 2))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 2)\n...     | (_.bill_depth_mm.abs() &gt; 2)\n...     | (_.flipper_length_mm.abs() &gt; 2)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\n\nTrue\n\n\n\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ float64           │ int64       │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Biscoe │      -1.103002 │      0.733662 │         -2.056307 │        3150 │ female │  2007 │\n│ Gentoo  │ Biscoe │       1.113285 │     -0.431017 │          2.068368 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       2.871660 │     -0.076550 │          2.068368 │        6050 │ male   │  2007 │\n│ Gentoo  │ Biscoe │       1.900890 │     -0.734846 │          2.139483 │        5650 │ male   │  2008 │\n│ Gentoo  │ Biscoe │       1.076652 │     -0.177826 │          2.068368 │        5700 │ male   │  2008 │\n│ Gentoo  │ Biscoe │       0.856855 │     -0.582932 │          2.068368 │        5800 │ male   │  2008 │\n│ Gentoo  │ Biscoe │       1.497929 │     -0.076550 │          2.068368 │        5550 │ male   │  2009 │\n│ Gentoo  │ Biscoe │       1.388031 │     -0.431017 │          2.068368 │        5500 │ male   │  2009 │\n│ Gentoo  │ Biscoe │       2.047422 │     -0.582932 │          2.068368 │        5850 │ male   │  2009 │\n│ Adelie  │ Dream  │      -2.165354 │     -0.836123 │         -0.918466 │        3050 │ female │  2009 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "reference/selectors.html#parameters-8",
    "href": "reference/selectors.html#parameters-8",
    "title": "Column selectors",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nselector\nSelector\nA column selector\nrequired\n\n\npredicate\nDeferred | Callable\nA callable or deferred object defining a predicate to apply to each column from selector.\nrequired"
  },
  {
    "objectID": "reference/selectors.html#examples-8",
    "href": "reference/selectors.html#examples-8",
    "title": "Column selectors",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import selectors as s, _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; cols = s.across(s.endswith(\"_mm\"), (_ - _.mean()) / _.std())\n&gt;&gt;&gt; expr = penguins.mutate(cols).filter(s.if_all(s.endswith(\"_mm\"), _.abs() &gt; 1))\n&gt;&gt;&gt; expr_by_hand = penguins.mutate(cols).filter(\n...     (_.bill_length_mm.abs() &gt; 1)\n...     & (_.bill_depth_mm.abs() &gt; 1)\n...     & (_.flipper_length_mm.abs() &gt; 1)\n... )\n&gt;&gt;&gt; expr.equals(expr_by_hand)\n\nTrue\n\n\n\n&gt;&gt;&gt; expr\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Dream     │      -1.157951 │      1.088129 │         -1.416272 │        3300 │ female │  2007 │\n│ Adelie  │ Torgersen │      -1.231217 │      1.138768 │         -1.202926 │        3900 │ male   │  2008 │\n│ Gentoo  │ Biscoe    │       1.149917 │     -1.443781 │          1.214987 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe    │       1.040019 │     -1.089314 │          1.072757 │        4750 │ male   │  2008 │\n│ Gentoo  │ Biscoe    │       1.131601 │     -1.089314 │          1.712792 │        5000 │ male   │  2008 │\n│ Gentoo  │ Biscoe    │       1.241499 │     -1.089314 │          1.570562 │        5550 │ male   │  2008 │\n│ Gentoo  │ Biscoe    │       1.351398 │     -1.494420 │          1.214987 │        5300 │ male   │  2009 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "reference/SQL.html",
    "href": "reference/SQL.html",
    "title": "SQL",
    "section": "",
    "text": "SQL()\nSQL-related options.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndefault_limit\nint | None\nNumber of rows to be retrieved for a table expression without an explicit limit. None means no limit.\n\n\ndefault_dialect\nstr\nDialect to use for printing SQL when the backend cannot be determined."
  },
  {
    "objectID": "reference/SQL.html#attributes",
    "href": "reference/SQL.html#attributes",
    "title": "SQL",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndefault_limit\nint | None\nNumber of rows to be retrieved for a table expression without an explicit limit. None means no limit.\n\n\ndefault_dialect\nstr\nDialect to use for printing SQL when the backend cannot be determined."
  },
  {
    "objectID": "reference/Interactive.html",
    "href": "reference/Interactive.html",
    "title": "Interactive",
    "section": "",
    "text": "Interactive()\nOptions controlling the interactive repr.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nmax_rows\nint\nMaximum rows to pretty print.\n\n\nmax_columns\nint | None\nThe maximum number of columns to pretty print. If 0 (the default), the number of columns will be inferred from output console size. Set to None for no limit.\n\n\nmax_length\nint\nMaximum length for pretty-printed arrays and maps.\n\n\nmax_string\nint\nMaximum length for pretty-printed strings.\n\n\nmax_depth\nint\nMaximum depth for nested data types.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the interactive repr."
  },
  {
    "objectID": "reference/Interactive.html#attributes",
    "href": "reference/Interactive.html#attributes",
    "title": "Interactive",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\nmax_rows\nint\nMaximum rows to pretty print.\n\n\nmax_columns\nint | None\nThe maximum number of columns to pretty print. If 0 (the default), the number of columns will be inferred from output console size. Set to None for no limit.\n\n\nmax_length\nint\nMaximum length for pretty-printed arrays and maps.\n\n\nmax_string\nint\nMaximum length for pretty-printed strings.\n\n\nmax_depth\nint\nMaximum depth for nested data types.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the interactive repr."
  },
  {
    "objectID": "reference/expression-numeric.html",
    "href": "reference/expression-numeric.html",
    "title": "Numeric and Boolean expressions",
    "section": "",
    "text": "Integer, floating point, decimal, and boolean expressions."
  },
  {
    "objectID": "reference/expression-numeric.html#methods",
    "href": "reference/expression-numeric.html#methods",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nabs\nReturn the absolute value of self.\n\n\nacos\nCompute the arc cosine of self.\n\n\nasin\nCompute the arc sine of self.\n\n\natan\nCompute the arc tangent of self.\n\n\natan2\nCompute the two-argument version of arc tangent.\n\n\nceil\nReturn the ceiling of self.\n\n\nclip\nTrim values outside of lower and upper bounds.\n\n\ncos\nCompute the cosine of self.\n\n\ncot\nCompute the cotangent of self.\n\n\ndegrees\nCompute the degrees of self radians.\n\n\nexp\nCompute \\(e^\\texttt{self}\\).\n\n\nfloor\nReturn the floor of an expression.\n\n\nln\nCompute \\(\\ln\\left(\\texttt{self}\\right)\\).\n\n\nlog\nCompute \\(\\log_{\\texttt{base}}\\left(\\texttt{self}\\right)\\).\n\n\nlog10\nCompute \\(\\log_{10}\\left(\\texttt{self}\\right)\\).\n\n\nlog2\nCompute \\(\\log_{2}\\left(\\texttt{self}\\right)\\).\n\n\nnegate\nNegate a numeric expression.\n\n\nnullifzero\nDEPRECATED: Use nullif(0) instead.\n\n\npoint\nReturn a point constructed from the coordinate values.\n\n\nradians\nCompute radians from self degrees.\n\n\nround\nRound values to an indicated number of decimal places.\n\n\nsign\nReturn the sign of the input.\n\n\nsin\nCompute the sine of self.\n\n\nsqrt\nCompute the square root of self.\n\n\ntan\nCompute the tangent of self.\n\n\nzeroifnull\nDEPRECATED: Use fillna(0) instead.\n\n\n\n\nabs\nabs()\nReturn the absolute value of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 2, -3, 4]})\n&gt;&gt;&gt; t.values.abs()\n\n┏━━━━━━━━━━━━━┓\n┃ Abs(values) ┃\n┡━━━━━━━━━━━━━┩\n│ int64       │\n├─────────────┤\n│           1 │\n│           2 │\n│           3 │\n│           4 │\n└─────────────┘\n\n\n\n\n\n\nacos\nacos()\nCompute the arc cosine of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.acos()\n\n┏━━━━━━━━━━━━━━┓\n┃ Acos(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│     3.141593 │\n│     1.570796 │\n│     0.000000 │\n└──────────────┘\n\n\n\n\n\n\nasin\nasin()\nCompute the arc sine of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.asin()\n\n┏━━━━━━━━━━━━━━┓\n┃ Asin(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│    -1.570796 │\n│     0.000000 │\n│     1.570796 │\n└──────────────┘\n\n\n\n\n\n\natan\natan()\nCompute the arc tangent of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.atan()\n\n┏━━━━━━━━━━━━━━┓\n┃ Atan(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│    -0.785398 │\n│     0.000000 │\n│     0.785398 │\n└──────────────┘\n\n\n\n\n\n\natan2\natan2(other)\nCompute the two-argument version of arc tangent.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.atan2(0)\n\n┏━━━━━━━━━━━━━━━━━━┓\n┃ Atan2(values, 0) ┃\n┡━━━━━━━━━━━━━━━━━━┩\n│ float64          │\n├──────────────────┤\n│        -1.570796 │\n│         0.000000 │\n│         1.570796 │\n└──────────────────┘\n\n\n\n\n\n\nceil\nceil()\nReturn the ceiling of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 1.1, 2, 2.1, 3.3]})\n&gt;&gt;&gt; t.values.ceil()\n\n┏━━━━━━━━━━━━━━┓\n┃ Ceil(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│            1 │\n│            2 │\n│            2 │\n│            3 │\n│            4 │\n└──────────────┘\n\n\n\n\n\n\nclip\nclip(lower=None, upper=None)\nTrim values outside of lower and upper bounds.\nNULL values are preserved and are not replaced with bounds.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlower\nNumericValue | None\nLower bound\nNone\n\n\nupper\nNumericValue | None\nUpper bound\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nClipped input\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"values\": [None, 2, 3, None, 5, None, None, 8]},\n...     schema=dict(values=\"int\"),\n... )\n&gt;&gt;&gt; t.values.clip(lower=3, upper=6)\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Clip(values, 3, 6) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│               NULL │\n│                  3 │\n│                  3 │\n│               NULL │\n│                  5 │\n│               NULL │\n│               NULL │\n│                  6 │\n└────────────────────┘\n\n\n\n\n\n\ncos\ncos()\nCompute the cosine of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.cos()\n\n┏━━━━━━━━━━━━━┓\n┃ Cos(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│    0.540302 │\n│    1.000000 │\n│    0.540302 │\n└─────────────┘\n\n\n\n\n\n\ncot\ncot()\nCompute the cotangent of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.cot()\n\n┏━━━━━━━━━━━━━┓\n┃ Cot(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -0.642093 │\n│         inf │\n│    0.642093 │\n└─────────────┘\n\n\n\n\n\n\ndegrees\ndegrees()\nCompute the degrees of self radians.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from math import pi\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [0, pi / 2, pi, 3 * pi / 2, 2 * pi]})\n&gt;&gt;&gt; t.values.degrees()\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ Degrees(values) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│             0.0 │\n│            90.0 │\n│           180.0 │\n│           270.0 │\n│           360.0 │\n└─────────────────┘\n\n\n\n\n\n\nexp\nexp()\nCompute \\(e^\\texttt{self}\\).\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\n\\(e^\\texttt{self}\\)\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": range(4)})\n&gt;&gt;&gt; t.values.exp()\n\n┏━━━━━━━━━━━━━┓\n┃ Exp(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│    1.000000 │\n│    2.718282 │\n│    7.389056 │\n│   20.085537 │\n└─────────────┘\n\n\n\n\n\n\nfloor\nfloor()\nReturn the floor of an expression.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 1.1, 2, 2.1, 3.3]})\n&gt;&gt;&gt; t.values.floor()\n\n┏━━━━━━━━━━━━━━━┓\n┃ Floor(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ int64         │\n├───────────────┤\n│             1 │\n│             1 │\n│             2 │\n│             2 │\n│             3 │\n└───────────────┘\n\n\n\n\n\n\nln\nln()\nCompute \\(\\ln\\left(\\texttt{self}\\right)\\).\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2.718281828, 3]})\n&gt;&gt;&gt; t.values.ln()\n\n┏━━━━━━━━━━━━┓\n┃ Ln(values) ┃\n┡━━━━━━━━━━━━┩\n│ float64    │\n├────────────┤\n│   0.000000 │\n│   1.000000 │\n│   1.098612 │\n└────────────┘\n\n\n\n\n\n\nlog\nlog(base=None)\nCompute \\(\\log_{\\texttt{base}}\\left(\\texttt{self}\\right)\\).\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbase\nNumericValue | None\nThe base of the logarithm. If None, base e is used.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nLogarithm of arg with base base\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; from math import e\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [e, e**2, e**3]})\n&gt;&gt;&gt; t.values.log()\n\n┏━━━━━━━━━━━━━┓\n┃ Log(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│         1.0 │\n│         2.0 │\n│         3.0 │\n└─────────────┘\n\n\n\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [10, 100, 1000]})\n&gt;&gt;&gt; t.values.log(base=10)\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ Log(values, 10) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│             1.0 │\n│             2.0 │\n│             3.0 │\n└─────────────────┘\n\n\n\n\n\n\nlog10\nlog10()\nCompute \\(\\log_{10}\\left(\\texttt{self}\\right)\\).\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 10, 100]})\n&gt;&gt;&gt; t.values.log10()\n\n┏━━━━━━━━━━━━━━━┓\n┃ Log10(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│           0.0 │\n│           1.0 │\n│           2.0 │\n└───────────────┘\n\n\n\n\n\n\nlog2\nlog2()\nCompute \\(\\log_{2}\\left(\\texttt{self}\\right)\\).\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 4, 8]})\n&gt;&gt;&gt; t.values.log2()\n\n┏━━━━━━━━━━━━━━┓\n┃ Log2(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│          0.0 │\n│          1.0 │\n│          2.0 │\n│          3.0 │\n└──────────────┘\n\n\n\n\n\n\nnegate\nnegate()\nNegate a numeric expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nA numeric value expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.negate()\n\n┏━━━━━━━━━━━━━━━━┓\n┃ Negate(values) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int64          │\n├────────────────┤\n│              1 │\n│              0 │\n│             -1 │\n└────────────────┘\n\n\n\n\n\n\nnullifzero\nnullifzero()\nDEPRECATED: Use nullif(0) instead.\n\n\npoint\npoint(right)\nReturn a point constructed from the coordinate values.\nConstant coordinates result in construction of a POINT literal or column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nint | float | NumericValue\nY coordinate\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nPointValue\nPoints\n\n\n\n\n\n\nradians\nradians()\nCompute radians from self degrees.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [0, 90, 180, 270, 360]})\n&gt;&gt;&gt; t.values.radians()\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ Radians(values) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│        0.000000 │\n│        1.570796 │\n│        3.141593 │\n│        4.712389 │\n│        6.283185 │\n└─────────────────┘\n\n\n\n\n\n\nround\nround(digits=None)\nRound values to an indicated number of decimal places.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndigits\nint | IntegerValue | None\nThe number of digits to round to. Here’s how the digits parameter affects the expression output type: - digits is False-y; self.type() is decimal → decimal - digits is nonzero; self.type() is decimal → decimal - digits is False-y; self.type() is Floating → int64 - digits is nonzero; self.type() is Floating → float64\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nNumericValue\nThe rounded expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1.22, 1.64, 2.15, 2.54]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┓\n┃ values  ┃\n┡━━━━━━━━━┩\n│ float64 │\n├─────────┤\n│    1.22 │\n│    1.64 │\n│    2.15 │\n│    2.54 │\n└─────────┘\n\n\n\n\n&gt;&gt;&gt; t.values.round()\n\n┏━━━━━━━━━━━━━━━┓\n┃ Round(values) ┃\n┡━━━━━━━━━━━━━━━┩\n│ int64         │\n├───────────────┤\n│             1 │\n│             2 │\n│             2 │\n│             3 │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.values.round(digits=1)\n\n┏━━━━━━━━━━━━━━━━━━┓\n┃ Round(values, 1) ┃\n┡━━━━━━━━━━━━━━━━━━┩\n│ float64          │\n├──────────────────┤\n│              1.2 │\n│              1.6 │\n│              2.2 │\n│              2.5 │\n└──────────────────┘\n\n\n\n\n\n\nsign\nsign()\nReturn the sign of the input.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 2, -3, 4]})\n&gt;&gt;&gt; t.values.sign()\n\n┏━━━━━━━━━━━━━━┓\n┃ Sign(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│           -1 │\n│            1 │\n│           -1 │\n│            1 │\n└──────────────┘\n\n\n\n\n\n\nsin\nsin()\nCompute the sine of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.sin()\n\n┏━━━━━━━━━━━━━┓\n┃ Sin(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -0.841471 │\n│    0.000000 │\n│    0.841471 │\n└─────────────┘\n\n\n\n\n\n\nsqrt\nsqrt()\nCompute the square root of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 4, 9, 16]})\n&gt;&gt;&gt; t.values.sqrt()\n\n┏━━━━━━━━━━━━━━┓\n┃ Sqrt(values) ┃\n┡━━━━━━━━━━━━━━┩\n│ float64      │\n├──────────────┤\n│          1.0 │\n│          2.0 │\n│          3.0 │\n│          4.0 │\n└──────────────┘\n\n\n\n\n\n\ntan\ntan()\nCompute the tangent of self.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [-1, 0, 1]})\n&gt;&gt;&gt; t.values.tan()\n\n┏━━━━━━━━━━━━━┓\n┃ Tan(values) ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│   -1.557408 │\n│    0.000000 │\n│    1.557408 │\n└─────────────┘\n\n\n\n\n\n\nzeroifnull\nzeroifnull()\nDEPRECATED: Use fillna(0) instead."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-1",
    "href": "reference/expression-numeric.html#methods-1",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbucket\nCompute a discrete binning of a numeric array.\n\n\ncorr\nReturn the correlation of two numeric columns.\n\n\ncov\nReturn the covariance of two numeric columns.\n\n\ncummean\nReturn the cumulative mean of the input.\n\n\ncumsum\nReturn the cumulative sum of the input.\n\n\nhistogram\nCompute a histogram with fixed width bins.\n\n\nmean\nReturn the mean of a numeric column.\n\n\nmedian\nReturn the median of the column.\n\n\nquantile\nReturn value at the given quantile.\n\n\nstd\nReturn the standard deviation of a numeric column.\n\n\nsum\nReturn the sum of a numeric column.\n\n\nvar\nReturn the variance of a numeric column.\n\n\n\n\nbucket\nbucket(buckets, closed='left', close_extreme=True, include_under=False, include_over=False)\nCompute a discrete binning of a numeric array.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuckets\nSequence[int]\nList of buckets\nrequired\n\n\nclosed\nLiteral[‘left’, ‘right’]\nWhich side of each interval is closed. For example: python buckets = [0, 100, 200] closed = \"left\"  # 100 falls in 2nd bucket closed = \"right\"  # 100 falls in 1st bucket\n'left'\n\n\nclose_extreme\nbool\nWhether the extreme values fall in the last bucket\nTrue\n\n\ninclude_over\nbool\nInclude values greater than the last bucket in the last bucket\nFalse\n\n\ninclude_under\nbool\nInclude values less than the first bucket in the first bucket\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nA categorical column expression\n\n\n\n\n\n\ncorr\ncorr(right, where=None, how='sample')\nReturn the correlation of two numeric columns.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nNumericColumn\nNumeric column\nrequired\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\nhow\nLiteral[‘sample’, ‘pop’]\nPopulation or sample correlation\n'sample'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe correlation of left and right\n\n\n\n\n\n\ncov\ncov(right, where=None, how='sample')\nReturn the covariance of two numeric columns.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nright\nNumericColumn\nNumeric column\nrequired\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\nhow\nLiteral[‘sample’, ‘pop’]\nPopulation or sample covariance\n'sample'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe covariance of self and right\n\n\n\n\n\n\ncummean\ncummean(where=None, group_by=None, order_by=None)\nReturn the cumulative mean of the input.\n\n\ncumsum\ncumsum(where=None, group_by=None, order_by=None)\nReturn the cumulative sum of the input.\n\n\nhistogram\nhistogram(nbins=None, binwidth=None, base=None, eps=1e-13)\nCompute a histogram with fixed width bins.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnbins\nint | None\nIf supplied, will be used to compute the binwidth\nNone\n\n\nbinwidth\nfloat | None\nIf not supplied, computed from the data (actual max and min values)\nNone\n\n\nbase\nfloat | None\nThe value of the first histogram bin. Defaults to the minimum value of column.\nNone\n\n\neps\nfloat\nAllowed floating point epsilon for histogram base\n1e-13\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nColumn\nBucketed column\n\n\n\n\n\n\nmean\nmean(where=None)\nReturn the mean of a numeric column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe mean of the input expression\n\n\n\n\n\n\nmedian\nmedian(where=None)\nReturn the median of the column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nOptional boolean expression. If given, only the values where where evaluates to true will be considered for the median.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nMedian of the column\n\n\n\n\n\n\nquantile\nquantile(quantile, where=None)\nReturn value at the given quantile.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquantile\nSequence[NumericValue | float]\n0 &lt;= quantile &lt;= 1, the quantile(s) to compute\nrequired\n\n\nwhere\nir.BooleanValue | None\nBoolean filter for input values\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nQuantile of the input\n\n\n\n\n\n\nstd\nstd(where=None, how='sample')\nReturn the standard deviation of a numeric column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\nhow\nLiteral[‘sample’, ‘pop’]\nSample or population standard deviation\n'sample'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nStandard deviation of arg\n\n\n\n\n\n\nsum\nsum(where=None)\nReturn the sum of a numeric column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nThe sum of the input expression\n\n\n\n\n\n\nvar\nvar(where=None, how='sample')\nReturn the variance of a numeric column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter\nNone\n\n\nhow\nLiteral[‘sample’, ‘pop’]\nSample or population variance\n'sample'\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nNumericScalar\nStandard deviation of arg"
  },
  {
    "objectID": "reference/expression-numeric.html#methods-2",
    "href": "reference/expression-numeric.html#methods-2",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nconvert_base\nConvert an integer from one base to another.\n\n\nlabel\nLabel a set of integer values with strings.\n\n\nto_interval\nConvert an integer to an interval.\n\n\nto_timestamp\nConvert an integral UNIX timestamp to a timestamp expression.\n\n\n\n\nconvert_base\nconvert_base(from_base, to_base)\nConvert an integer from one base to another.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_base\nIntegerValue\nNumeric base of expression\nrequired\n\n\nto_base\nIntegerValue\nNew base\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nConverted expression\n\n\n\n\n\n\nlabel\nlabel(labels, nulls=None)\nLabel a set of integer values with strings.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlabels\nIterable[str]\nAn iterable of string labels. Each integer value in self will be mapped to a value in labels.\nrequired\n\n\nnulls\nstr | None\nString label to use for NULL values\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nself labeled with labels\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [0, 1, 0, 2]})\n&gt;&gt;&gt; t.select(t.a, labeled=t.a.label([\"a\", \"b\", \"c\"]))\n\n┏━━━━━━━┳━━━━━━━━━┓\n┃ a     ┃ labeled ┃\n┡━━━━━━━╇━━━━━━━━━┩\n│ int64 │ string  │\n├───────┼─────────┤\n│     0 │ a       │\n│     1 │ b       │\n│     0 │ a       │\n│     2 │ c       │\n└───────┴─────────┘\n\n\n\n\n\n\nto_interval\nto_interval(unit='s')\nConvert an integer to an interval.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\nLiteral[‘Y’, ‘M’, ‘W’, ‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, ‘ns’]\nUnit for the resulting interval\n's'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntervalValue\nAn interval in units of unit\n\n\n\n\n\n\nto_timestamp\nto_timestamp(unit='s')\nConvert an integral UNIX timestamp to a timestamp expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\nLiteral[‘s’, ‘ms’, ‘us’]\nThe resolution of arg\n's'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nself converted to a timestamp"
  },
  {
    "objectID": "reference/expression-numeric.html#methods-3",
    "href": "reference/expression-numeric.html#methods-3",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbit_and\nAggregate the column using the bitwise and operator.\n\n\nbit_or\nAggregate the column using the bitwise or operator.\n\n\nbit_xor\nAggregate the column using the bitwise exclusive or operator.\n\n\n\n\nbit_and\nbit_and(where=None)\nAggregate the column using the bitwise and operator.\n\n\nbit_or\nbit_or(where=None)\nAggregate the column using the bitwise or operator.\n\n\nbit_xor\nbit_xor(where=None)\nAggregate the column using the bitwise exclusive or operator."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-4",
    "href": "reference/expression-numeric.html#methods-4",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nisinf\nReturn whether the value is infinity.\n\n\nisnan\nReturn whether the value is NaN.\n\n\n\n\nisinf\nisinf()\nReturn whether the value is infinity.\n\n\nisnan\nisnan()\nReturn whether the value is NaN."
  },
  {
    "objectID": "reference/expression-numeric.html#methods-5",
    "href": "reference/expression-numeric.html#methods-5",
    "title": "Numeric and Boolean expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nifelse\nConstruct a ternary conditional expression.\n\n\n\n\nifelse\nifelse(true_expr, false_expr)\nConstruct a ternary conditional expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntrue_expr\nir.Value\nExpression to return if self evaluates to True\nrequired\n\n\nfalse_expr\nir.Value\nExpression to return if self evaluates to False or NULL\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nValue\nThe value of true_expr if arg is True else false_expr\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"is_person\": [True, False, True, None]})\n&gt;&gt;&gt; t.is_person.ifelse(\"yes\", \"no\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ IfElse(is_person, 'yes', 'no') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                         │\n├────────────────────────────────┤\n│ yes                            │\n│ no                             │\n│ yes                            │\n│ no                             │\n└────────────────────────────────┘"
  },
  {
    "objectID": "reference/expression-numeric.html#parameters-19",
    "href": "reference/expression-numeric.html#parameters-19",
    "title": "Numeric and Boolean expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicates\nir.BooleanValue\nBoolean value expressions\n()"
  },
  {
    "objectID": "reference/expression-numeric.html#returns-21",
    "href": "reference/expression-numeric.html#returns-21",
    "title": "Numeric and Boolean expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nA new predicate that evaluates to True if all composing predicates are True. If no predicates were provided, returns True."
  },
  {
    "objectID": "reference/expression-numeric.html#parameters-20",
    "href": "reference/expression-numeric.html#parameters-20",
    "title": "Numeric and Boolean expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npredicates\nir.BooleanValue\nBoolean value expressions\n()"
  },
  {
    "objectID": "reference/expression-numeric.html#returns-22",
    "href": "reference/expression-numeric.html#returns-22",
    "title": "Numeric and Boolean expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nA new predicate that evaluates to True if any composing predicates are True. If no predicates were provided, returns False."
  },
  {
    "objectID": "reference/expression-numeric.html#returns-23",
    "href": "reference/expression-numeric.html#returns-23",
    "title": "Numeric and Boolean expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nFloatingScalar\nRandom float value expression"
  },
  {
    "objectID": "reference/expression-generic.html",
    "href": "reference/expression-generic.html",
    "title": "Generic expressions",
    "section": "",
    "text": "Scalars and columns of any element type."
  },
  {
    "objectID": "reference/expression-generic.html#methods",
    "href": "reference/expression-generic.html#methods",
    "title": "Generic expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nas_table\nPromote the expression to a Table.\n\n\nasc\nSort an expression ascending.\n\n\nbetween\nCheck if this expression is between lower and upper, inclusive.\n\n\ncase\nCreate a SimpleCaseBuilder to chain multiple if-else statements.\n\n\ncases\nCreate a case expression in one shot.\n\n\ncast\nCast expression to indicated data type.\n\n\ncoalesce\nReturn the first non-null value from args.\n\n\ncollect\nAggregate this expression’s elements into an array.\n\n\ndesc\nSort an expression descending.\n\n\nfillna\nReplace any null values with the indicated fill value.\n\n\ngreatest\nCompute the largest value among the supplied arguments.\n\n\ngroup_concat\nConcatenate values using the indicated separator to produce a string.\n\n\nhash\nCompute an integer hash value.\n\n\nidentical_to\nReturn whether this expression is identical to other.\n\n\nisin\nCheck whether this expression’s values are in values.\n\n\nisnull\nReturn whether this expression is NULL.\n\n\nleast\nCompute the smallest value among the supplied arguments.\n\n\nname\nRename an expression to name.\n\n\nnotin\nCheck whether this expression’s values are not in values.\n\n\nnotnull\nReturn whether this expression is not NULL.\n\n\nnullif\nSet values to null if they equal the values null_if_expr.\n\n\nover\nConstruct a window expression.\n\n\nsubstitute\nReplace values given in values with replacement.\n\n\nto_pandas\nConvert a column expression to a pandas Series or scalar object.\n\n\ntry_cast\nTry cast expression to indicated data type.\n\n\ntype\nReturn the DataType of self.\n\n\ntypeof\nReturn the string name of the datatype of self.\n\n\n\n\nas_table\nas_table()\nPromote the expression to a Table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table(dict(a=\"str\"), name=\"t\")\n&gt;&gt;&gt; expr = t.a.length().name(\"len\").as_table()\n&gt;&gt;&gt; expected = t.select(len=t.a.length())\n&gt;&gt;&gt; expr.equals(expected)\n\nTrue\n\n\n\n\n\nasc\nasc()\nSort an expression ascending.\n\n\nbetween\nbetween(lower, upper)\nCheck if this expression is between lower and upper, inclusive.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlower\nValue\nLower bound, inclusive\nrequired\n\n\nupper\nValue\nUpper bound, inclusive\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nExpression indicating membership in the provided range\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_length_mm.between(35, 38)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Between(bill_length_mm, 35, 38) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                         │\n├─────────────────────────────────┤\n│ False                           │\n│ False                           │\n│ False                           │\n│ NULL                            │\n│ True                            │\n└─────────────────────────────────┘\n\n\n\n\n\n\ncase\ncase()\nCreate a SimpleCaseBuilder to chain multiple if-else statements.\nAdd new search expressions with the .when() method. These must be comparable with this column expression. Conclude by calling .end().\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSimpleCaseBuilder\nA case builder\n\n\n\n\n\nSee Also\nValue.substitute() ibis.cases() ibis.case()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; x = ibis.examples.penguins.fetch().head(5)[\"sex\"]\n&gt;&gt;&gt; x\n\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; x.case().when(\"male\", \"M\").when(\"female\", \"F\").else_(\"U\").end()\n\n┏━━━━━━━━━━━━━━━━━━━━━━┓\n┃ SimpleCase(sex, 'U') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━┩\n│ string               │\n├──────────────────────┤\n│ M                    │\n│ F                    │\n│ F                    │\n│ U                    │\n│ F                    │\n└──────────────────────┘\n\n\n\nCases not given result in the ELSE case\n\n&gt;&gt;&gt; x.case().when(\"male\", \"M\").else_(\"OTHER\").end()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ SimpleCase(sex, 'OTHER') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                   │\n├──────────────────────────┤\n│ M                        │\n│ OTHER                    │\n│ OTHER                    │\n│ OTHER                    │\n│ OTHER                    │\n└──────────────────────────┘\n\n\n\nIf you don’t supply an ELSE, then NULL is used\n\n&gt;&gt;&gt; x.case().when(\"male\", \"M\").end()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ SimpleCase(sex, Cast(None, string)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                              │\n├─────────────────────────────────────┤\n│ M                                   │\n│ NULL                                │\n│ NULL                                │\n│ NULL                                │\n│ NULL                                │\n└─────────────────────────────────────┘\n\n\n\n\n\n\ncases\ncases(case_result_pairs, default=None)\nCreate a case expression in one shot.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncase_result_pairs\nIterable[tuple[ir.BooleanValue, Value]]\nConditional-result pairs\nrequired\n\n\ndefault\nValue | None\nValue to return if none of the case conditions are true\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nValue expression\n\n\n\n\n\nSee Also\nValue.substitute() ibis.cases() ibis.case()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2, 4]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ values ┃\n┡━━━━━━━━┩\n│ int64  │\n├────────┤\n│      1 │\n│      2 │\n│      1 │\n│      2 │\n│      3 │\n│      2 │\n│      4 │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; number_letter_map = ((1, \"a\"), (2, \"b\"), (3, \"c\"))\n&gt;&gt;&gt; t.values.cases(number_letter_map, default=\"unk\").name(\"replace\")\n\n┏━━━━━━━━━┓\n┃ replace ┃\n┡━━━━━━━━━┩\n│ string  │\n├─────────┤\n│ a       │\n│ b       │\n│ a       │\n│ b       │\n│ c       │\n│ b       │\n│ unk     │\n└─────────┘\n\n\n\n\n\n\ncast\ncast(target_type)\nCast expression to indicated data type.\nSimilar to pandas.Series.astype.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_type\nAny\nType to cast to. Anything accepted by ibis.dtype()\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCasted expression\n\n\n\n\n\nSee Also\nValue.try_cast() ibis.dtype()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; x = ibis.examples.penguins.fetch()[\"bill_depth_mm\"]\n&gt;&gt;&gt; x\n\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│          NULL │\n│          19.3 │\n│          20.6 │\n│          17.8 │\n│          19.6 │\n│          18.1 │\n│          20.2 │\n│             … │\n└───────────────┘\n\n\n\npython’s built-in types can be used\n\n&gt;&gt;&gt; x.cast(int)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Cast(bill_depth_mm, int64) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                      │\n├────────────────────────────┤\n│                         19 │\n│                         17 │\n│                         18 │\n│                       NULL │\n│                         19 │\n│                         21 │\n│                         18 │\n│                         20 │\n│                         18 │\n│                         20 │\n│                          … │\n└────────────────────────────┘\n\n\n\nor string names\n\n&gt;&gt;&gt; x.cast(\"uint16\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Cast(bill_depth_mm, uint16) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ uint16                      │\n├─────────────────────────────┤\n│                          19 │\n│                          17 │\n│                          18 │\n│                        NULL │\n│                          19 │\n│                          21 │\n│                          18 │\n│                          20 │\n│                          18 │\n│                          20 │\n│                           … │\n└─────────────────────────────┘\n\n\n\nIf you make an illegal cast, you won’t know until the backend actually executes it. Consider .try_cast().\n\n&gt;&gt;&gt; ibis.literal(\"a string\").cast(\"int64\")\n\n\n\n\nConversionException: Conversion Error: Could not convert string 'a string' to INT64\n\n\n\n\n\ncoalesce\ncoalesce(*args)\nReturn the first non-null value from args.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nValue\nArguments from which to choose the first non-null value\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCoalesced expression\n\n\n\n\n\nSee Also\nibis.coalesce() Value.fillna()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.coalesce(None, 4, 5).name(\"x\")\n\n\n\n\n\n4\n\n\n\n\n\n\ncollect\ncollect(where=None)\nAggregate this expression’s elements into an array.\nThis function is called array_agg, list_agg, or list in other systems.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter to apply before aggregation\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayScalar\nCollected array\n\n\n\n\n\nExamples\nBasic collect usage\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"key\": list(\"aaabb\"), \"value\": [1, 2, 3, 4, 5]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ key    ┃ value ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ string │ int64 │\n├────────┼───────┤\n│ a      │     1 │\n│ a      │     2 │\n│ a      │     3 │\n│ b      │     4 │\n│ b      │     5 │\n└────────┴───────┘\n\n\n\n\n&gt;&gt;&gt; t.value.collect()\n\n\n\n\n\n[1, 2, ... +3]\n\n\n\n\n&gt;&gt;&gt; type(t.value.collect())\n\nibis.expr.types.arrays.ArrayScalar\n\n\nCollect elements per group\n\n&gt;&gt;&gt; t.group_by(\"key\").agg(v=lambda t: t.value.collect()).order_by(\"key\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ key    ┃ v              ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string │ array&lt;int64&gt;   │\n├────────┼────────────────┤\n│ a      │ [1, 2, ... +1] │\n│ b      │ [4, 5]         │\n└────────┴────────────────┘\n\n\n\nCollect elements per group using a filter\n\n&gt;&gt;&gt; t.group_by(\"key\").agg(\n...     v=lambda t: t.value.collect(where=t.value &gt; 1)\n... ).order_by(\"key\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ key    ┃ v            ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ string │ array&lt;int64&gt; │\n├────────┼──────────────┤\n│ a      │ [2, 3]       │\n│ b      │ [4, 5]       │\n└────────┴──────────────┘\n\n\n\n\n\n\ndesc\ndesc()\nSort an expression descending.\n\n\nfillna\nfillna(fill_value)\nReplace any null values with the indicated fill value.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfill_value\nScalar\nValue with which to replace NA values in self\nrequired\n\n\n\n\n\nSee Also\nValue.coalesce() ibis.coalesce()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.sex\n\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.sex.fillna(\"unrecorded\").name(\"sex\")\n\n┏━━━━━━━━━━━━┓\n┃ sex        ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ male       │\n│ female     │\n│ female     │\n│ unrecorded │\n│ female     │\n└────────────┘\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nself filled with fill_value where it is NA\n\n\n\n\n\n\ngreatest\ngreatest(*args)\nCompute the largest value among the supplied arguments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nir.Value\nArguments to choose from\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMaximum of the passed arguments\n\n\n\n\n\n\ngroup_concat\ngroup_concat(sep=',', where=None)\nConcatenate values using the indicated separator to produce a string.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsep\nstr\nSeparator will be used to join strings\n','\n\n\nwhere\nir.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringScalar\nConcatenated string expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t[[\"bill_length_mm\", \"bill_depth_mm\"]]\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │\n├────────────────┼───────────────┤\n│           39.1 │          18.7 │\n│           39.5 │          17.4 │\n│           40.3 │          18.0 │\n│           NULL │          NULL │\n│           36.7 │          19.3 │\n└────────────────┴───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.bill_length_mm.group_concat()\n\n\n\n\n\n'39.1,39.5,40.3,36.7'\n\n\n\n\n&gt;&gt;&gt; t.bill_length_mm.group_concat(sep=\": \")\n\n\n\n\n\n'39.1: 39.5: 40.3: 36.7'\n\n\n\n\n&gt;&gt;&gt; t.bill_length_mm.group_concat(sep=\": \", where=t.bill_depth_mm &gt; 18)\n\n\n\n\n\n'39.1: 36.7'\n\n\n\n\n\n\nhash\nhash()\nCompute an integer hash value.\n\n\n\n\n\n\nThe hashing function used is backend-dependent.\n\n\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe hash value of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; ibis.literal(\"hello\").hash()\n\n\n\n\n\n-4155090522938856779\n\n\n\n\n\n\nidentical_to\nidentical_to(other)\nReturn whether this expression is identical to other.\nCorresponds to IS NOT DISTINCT FROM in SQL.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nValue\nExpression to compare to\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether this expression is not distinct from other\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; one = ibis.literal(1)\n&gt;&gt;&gt; two = ibis.literal(2)\n&gt;&gt;&gt; two.identical_to(one + one)\n\n\n\n\n\nTrue\n\n\n\n\n\n\nisin\nisin(values)\nCheck whether this expression’s values are in values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nValue | Sequence[Value]\nValues or expression to check for membership\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nExpression indicating membership\n\n\n\n\n\nSee Also\nValue.notin()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"a\": [1, 2, 3], \"b\": [2, 3, 4]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━┳━━━━━━━┓\n┃ a     ┃ b     ┃\n┡━━━━━━━╇━━━━━━━┩\n│ int64 │ int64 │\n├───────┼───────┤\n│     1 │     2 │\n│     2 │     3 │\n│     3 │     4 │\n└───────┴───────┘\n\n\n\nCheck against a literal sequence of values\n\n&gt;&gt;&gt; t.a.isin([1, 2])\n\n┏━━━━━━━━━━━━━┓\n┃ InValues(a) ┃\n┡━━━━━━━━━━━━━┩\n│ boolean     │\n├─────────────┤\n│ True        │\n│ True        │\n│ False       │\n└─────────────┘\n\n\n\nCheck against a derived expression\n\n&gt;&gt;&gt; t.a.isin(t.b + 1)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ InColumn(a, Add(b, 1)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ False                  │\n│ False                  │\n│ True                   │\n└────────────────────────┘\n\n\n\nCheck against a column from a different table\n\n&gt;&gt;&gt; t2 = ibis.memtable({\"x\": [99, 2, 99]})\n&gt;&gt;&gt; t.a.isin(t2.x)\n\n┏━━━━━━━━━━━━━━━━┓\n┃ InColumn(a, x) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ boolean        │\n├────────────────┤\n│ False          │\n│ True           │\n│ False          │\n└────────────────┘\n\n\n\n\n\n\nisnull\nisnull()\nReturn whether this expression is NULL.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│          NULL │\n│          19.3 │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.bill_depth_mm.isnull()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ IsNull(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean               │\n├───────────────────────┤\n│ False                 │\n│ False                 │\n│ False                 │\n│ True                  │\n│ False                 │\n└───────────────────────┘\n\n\n\n\n\n\nleast\nleast(*args)\nCompute the smallest value among the supplied arguments.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nir.Value\nArguments to choose from\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMinimum of the passed arguments\n\n\n\n\n\n\nname\nname(name)\nRename an expression to name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nThe new name of the expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nself with name name\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int64\"), name=\"t\")\n&gt;&gt;&gt; t.a.name(\"b\")\n\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.&lt;backend&gt;.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n\n\n\n\n\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.&lt;backend&gt;.execute(expr) or assign a backend instance to `ibis.options.default_backend`.\n\n\n\n\n\nnotin\nnotin(values)\nCheck whether this expression’s values are not in values.\nOpposite of Value.isin().\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nValue | Sequence[Value]\nValues or expression to check for lack of membership\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nWhether self’s values are not contained in values\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│          NULL │\n│          19.3 │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.bill_depth_mm.notin([18.7, 18.1])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Not(InValues(bill_depth_mm)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                      │\n├──────────────────────────────┤\n│ False                        │\n│ True                         │\n│ True                         │\n│ NULL                         │\n│ True                         │\n└──────────────────────────────┘\n\n\n\n\n\n\nnotnull\nnotnull()\nReturn whether this expression is not NULL.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.bill_depth_mm\n\n┏━━━━━━━━━━━━━━━┓\n┃ bill_depth_mm ┃\n┡━━━━━━━━━━━━━━━┩\n│ float64       │\n├───────────────┤\n│          18.7 │\n│          17.4 │\n│          18.0 │\n│          NULL │\n│          19.3 │\n└───────────────┘\n\n\n\n\n&gt;&gt;&gt; t.bill_depth_mm.notnull()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ NotNull(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ True                   │\n│ True                   │\n│ False                  │\n│ True                   │\n└────────────────────────┘\n\n\n\n\n\n\nnullif\nnullif(null_if_expr)\nSet values to null if they equal the values null_if_expr.\nCommonly used to avoid divide-by-zero problems by replacing zero with NULL in the divisor.\nEquivalent to (self == null_if_expr).ifelse(ibis.null(), self).\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnull_if_expr\nValue\nExpression indicating what values should be NULL\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nValue expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; vals = ibis.examples.penguins.fetch().head(5).sex\n&gt;&gt;&gt; vals\n\n┏━━━━━━━━┓\n┃ sex    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ male   │\n│ female │\n│ female │\n│ NULL   │\n│ female │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; vals.nullif(\"male\")\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ NullIf(sex, 'male') ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ string              │\n├─────────────────────┤\n│ NULL                │\n│ female              │\n│ female              │\n│ NULL                │\n│ female              │\n└─────────────────────┘\n\n\n\n\n\n\nover\nover(window=None, *, rows=None, range=None, group_by=None, order_by=None)\nConstruct a window expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwindow\n\nWindow specification\nNone\n\n\nrows\n\nWhether to use the ROWS window clause\nNone\n\n\nrange\n\nWhether to use the RANGE window clause\nNone\n\n\ngroup_by\n\nGrouping key\nNone\n\n\norder_by\n\nOrdering key\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nA window function expression\n\n\n\n\n\n\nsubstitute\nsubstitute(value, replacement=None, else_=None)\nReplace values given in values with replacement.\nThis is similar to the pandas replace method.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nValue | dict\nExpression or dict.\nrequired\n\n\nreplacement\nValue | None\nIf an expression is passed to value, this must be passed.\nNone\n\n\nelse_\nValue | None\nIf an original value does not match value, then else_ is used. The default of None means leave the original value unchanged.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nReplaced values\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.island.value_counts().order_by(\"island\")\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ island    ┃ island_count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ string    │ int64        │\n├───────────┼──────────────┤\n│ Biscoe    │          168 │\n│ Dream     │          124 │\n│ Torgersen │           52 │\n└───────────┴──────────────┘\n\n\n\n\n&gt;&gt;&gt; t.island.substitute({\"Torgersen\": \"torg\", \"Biscoe\": \"bisc\"}).name(\n...     \"island\"\n... ).value_counts().order_by(\"island\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ island ┃ island_count ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ string │ int64        │\n├────────┼──────────────┤\n│ Dream  │          124 │\n│ bisc   │          168 │\n│ torg   │           52 │\n└────────┴──────────────┘\n\n\n\n\n\n\nto_pandas\nto_pandas(**kwargs)\nConvert a column expression to a pandas Series or scalar object.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkwargs\n\nSame as keyword arguments to execute\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch().limit(5)\n&gt;&gt;&gt; t.to_pandas()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNone\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\n\n\n\ntry_cast\ntry_cast(target_type)\nTry cast expression to indicated data type. If the cast fails for a row, the value is returned as null or NaN depending on target_type and backend behavior.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntarget_type\nAny\nType to try cast to. Anything accepted by ibis.dtype()\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCasted expression\n\n\n\n\n\nSee Also\nValue.cast() ibis.dtype()\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"numbers\": [1, 2, 3, 4], \"strings\": [\"1.0\", \"2\", \"hello\", \"world\"]}\n... )\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ numbers ┃ strings ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ int64   │ string  │\n├─────────┼─────────┤\n│       1 │ 1.0     │\n│       2 │ 2       │\n│       3 │ hello   │\n│       4 │ world   │\n└─────────┴─────────┘\n\n\n\n\n&gt;&gt;&gt; t = t.mutate(numbers_to_strings=_.numbers.try_cast(\"string\"))\n&gt;&gt;&gt; t = t.mutate(strings_to_numbers=_.strings.try_cast(\"int\"))\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ numbers ┃ strings ┃ numbers_to_strings ┃ strings_to_numbers ┃\n┡━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ int64   │ string  │ string             │ int64              │\n├─────────┼─────────┼────────────────────┼────────────────────┤\n│       1 │ 1.0     │ 1                  │                  1 │\n│       2 │ 2       │ 2                  │                  2 │\n│       3 │ hello   │ 3                  │               NULL │\n│       4 │ world   │ 4                  │               NULL │\n└─────────┴─────────┴────────────────────┴────────────────────┘\n\n\n\n\n\n\ntype\ntype()\nReturn the DataType of self.\n\n\ntypeof\ntypeof()\nReturn the string name of the datatype of self.\nThe values of the returned strings are necessarily backend dependent. e.g. duckdb may say “DOUBLE”, while sqlite may say “real”.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nA string indicating the type of the value\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; vals = ibis.examples.penguins.fetch().head(5).bill_length_mm\n&gt;&gt;&gt; vals\n\n┏━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃\n┡━━━━━━━━━━━━━━━━┩\n│ float64        │\n├────────────────┤\n│           39.1 │\n│           39.5 │\n│           40.3 │\n│           NULL │\n│           36.7 │\n└────────────────┘\n\n\n\n\n&gt;&gt;&gt; vals.typeof()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ TypeOf(bill_length_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                 │\n├────────────────────────┤\n│ DOUBLE                 │\n│ DOUBLE                 │\n│ DOUBLE                 │\n│ DOUBLE                 │\n│ DOUBLE                 │\n└────────────────────────┘\n\n\n\nDifferent backends have different names for their native types\n\n&gt;&gt;&gt; ibis.duckdb.connect().execute(ibis.literal(5.4).typeof())\n\n'DOUBLE'\n\n\n\n&gt;&gt;&gt; ibis.sqlite.connect().execute(ibis.literal(5.4).typeof())\n\n'real'"
  },
  {
    "objectID": "reference/expression-generic.html#methods-1",
    "href": "reference/expression-generic.html#methods-1",
    "title": "Generic expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\napprox_median\nReturn an approximate of the median of self.\n\n\napprox_nunique\nReturn the approximate number of distinct elements in self.\n\n\narbitrary\nSelect an arbitrary value in a column.\n\n\nargmax\nReturn the value of self that maximizes key.\n\n\nargmin\nReturn the value of self that minimizes key.\n\n\ncount\nCompute the number of rows in an expression.\n\n\ncume_dist\nReturn the cumulative distribution over a window.\n\n\ncummax\nReturn the cumulative max over a window.\n\n\ncummin\nReturn the cumulative min over a window.\n\n\ndense_rank\nPosition of first element within each group of equal values.\n\n\nfirst\nReturn the first value of a column.\n\n\nlag\nReturn the row located at offset rows before the current row.\n\n\nlast\nReturn the last value of a column.\n\n\nlead\nReturn the row located at offset rows after the current row.\n\n\nmax\nReturn the maximum of a column.\n\n\nmin\nReturn the minimum of a column.\n\n\nmode\nReturn the mode of a column.\n\n\nnth\nReturn the nth value (0-indexed) over a window.\n\n\nntile\nReturn the integer number of a partitioning of the column values.\n\n\nnunique\nCompute the number of distinct rows in an expression.\n\n\npercent_rank\nReturn the relative rank of the values in the column.\n\n\nrank\nCompute position of first element within each equal-value group in sorted order.\n\n\ntopk\nReturn a “top k” expression.\n\n\nvalue_counts\nCompute a frequency table.\n\n\n\n\napprox_median\napprox_median(where=None)\nReturn an approximate of the median of self.\n\n\n\n\n\n\nThe result may or may not be exact\n\n\n\nWhether the result is an approximation depends on the backend.\n\n\n\n\n\n\nDo not depend on the results being exact\n\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn approximation of the median of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.approx_median()\n\n\n\n\n\n4030\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.approx_median(where=t.species == \"Chinstrap\")\n\n\n\n\n\n3700\n\n\n\n\n\n\napprox_nunique\napprox_nunique(where=None)\nReturn the approximate number of distinct elements in self.\n\n\n\n\n\n\nThe result may or may not be exact\n\n\n\nWhether the result is an approximation depends on the backend.\n\n\n\n\n\n\nDo not depend on the results being exact\n\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn approximate count of the distinct elements of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.approx_nunique()\n\n\n\n\n\n94\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.approx_nunique(where=t.species == \"Adelie\")\n\n\n\n\n\n55\n\n\n\n\n\n\narbitrary\narbitrary(where=None, how='first')\nSelect an arbitrary value in a column.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nA filter expression\nNone\n\n\nhow\nLiteral[‘first’, ‘last’, ‘heavy’]\nThe method to use for selecting the element. * \"first\": Select the first non-NULL element * \"last\": Select the last non-NULL element * \"heavy\": Select a frequently occurring value using the heavy hitters algorithm. \"heavy\" is only supported by Clickhouse backend.\n'first'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn expression\n\n\n\n\n\n\nargmax\nargmax(key, where=None)\nReturn the value of self that maximizes key.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe value of self that maximizes key\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.species.argmax(t.body_mass_g)\n\n\n\n\n\n'Gentoo'\n\n\n\n\n&gt;&gt;&gt; t.species.argmax(t.body_mass_g, where=t.island == \"Dream\")\n\n\n\n\n\n'Chinstrap'\n\n\n\n\n\n\nargmin\nargmin(key, where=None)\nReturn the value of self that minimizes key.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe value of self that minimizes key\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.species.argmin(t.body_mass_g)\n\n\n\n\n\n'Chinstrap'\n\n\n\n\n&gt;&gt;&gt; t.species.argmin(t.body_mass_g, where=t.island == \"Biscoe\")\n\n\n\n\n\n'Adelie'\n\n\n\n\n\n\ncount\ncount(where=None)\nCompute the number of rows in an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of elements in an expression\n\n\n\n\n\n\ncume_dist\ncume_dist()\nReturn the cumulative distribution over a window.\n\n\ncummax\ncummax(where=None, group_by=None, order_by=None)\nReturn the cumulative max over a window.\n\n\ncummin\ncummin(where=None, group_by=None, order_by=None)\nReturn the cumulative min over a window.\n\n\ndense_rank\ndense_rank()\nPosition of first element within each group of equal values.\nValues are returned in sorted order and duplicate values are ignored.\nEquivalent to SQL’s DENSE_RANK().\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerColumn\nThe rank\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=t.values.dense_rank())\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     1 │\n│      2 │     1 │\n│      2 │     1 │\n│      3 │     2 │\n└────────┴───────┘\n\n\n\n\n\n\nfirst\nfirst(where=None)\nReturn the first value of a column.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": [\"a\", \"b\", \"c\", \"d\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.chars.first()\n\n\n\n\n\n'a'\n\n\n\n\n&gt;&gt;&gt; t.chars.first(where=t.chars != \"a\")\n\n\n\n\n\n'b'\n\n\n\n\n\n\nlag\nlag(offset=None, default=None)\nReturn the row located at offset rows before the current row.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noffset\nint | ir.IntegerValue | None\nIndex of row to select\nNone\n\n\ndefault\nValue | None\nValue used if no row exists at offset\nNone\n\n\n\n\n\n\nlast\nlast(where=None)\nReturn the last value of a column.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": [\"a\", \"b\", \"c\", \"d\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.chars.last()\n\n\n\n\n\n'd'\n\n\n\n\n&gt;&gt;&gt; t.chars.last(where=t.chars != \"d\")\n\n\n\n\n\n'c'\n\n\n\n\n\n\nlead\nlead(offset=None, default=None)\nReturn the row located at offset rows after the current row.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noffset\nint | ir.IntegerValue | None\nIndex of row to select\nNone\n\n\ndefault\nValue | None\nValue used if no row exists at offset\nNone\n\n\n\n\n\n\nmax\nmax(where=None)\nReturn the maximum of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe maximum value in self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.max()\n\n\n\n\n\n6300\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.max(where=t.species == \"Chinstrap\")\n\n\n\n\n\n4800\n\n\n\n\n\n\nmin\nmin(where=None)\nReturn the minimum of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe minimum value in self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.min()\n\n\n\n\n\n2700\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.min(where=t.species == \"Adelie\")\n\n\n\n\n\n2850\n\n\n\n\n\n\nmode\nmode(where=None)\nReturn the mode of a column.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter in values when where is True\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nThe mode of self\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.mode()\n\n\n\n\n\n3800\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.mode(where=(t.species == \"Gentoo\") & (t.sex == \"male\"))\n\n\n\n\n\n5550\n\n\n\n\n\n\nnth\nnth(n)\nReturn the nth value (0-indexed) over a window.\n.nth(0) is equivalent to .first(). Negative will result in NULL. If the value of n is greater than the number of rows in the window, NULL will be returned.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ir.IntegerValue\nDesired rank value\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nColumn\nThe nth value over a window\n\n\n\n\n\n\nntile\nntile(buckets)\nReturn the integer number of a partitioning of the column values.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbuckets\nint | ir.IntegerValue\nNumber of buckets to partition into\nrequired\n\n\n\n\n\n\nnunique\nnunique(where=None)\nCompute the number of distinct rows in an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwhere\nir.BooleanValue | None\nFilter expression\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerScalar\nNumber of distinct elements in an expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t.body_mass_g.nunique()\n\n\n\n\n\n94\n\n\n\n\n&gt;&gt;&gt; t.body_mass_g.nunique(where=t.species == \"Adelie\")\n\n\n\n\n\n55\n\n\n\n\n\n\npercent_rank\npercent_rank()\nReturn the relative rank of the values in the column.\n\n\nrank\nrank()\nCompute position of first element within each equal-value group in sorted order.\nEquivalent to SQL’s RANK() window function.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nInt64Column\nThe min rank\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"values\": [1, 2, 1, 2, 3, 2]})\n&gt;&gt;&gt; t.mutate(rank=t.values.rank())\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ values ┃ rank  ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ int64  │ int64 │\n├────────┼───────┤\n│      1 │     0 │\n│      1 │     0 │\n│      2 │     2 │\n│      2 │     2 │\n│      2 │     2 │\n│      3 │     5 │\n└────────┴───────┘\n\n\n\n\n\n\ntopk\ntopk(k, by=None)\nReturn a “top k” expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nk\nint\nReturn this number of rows\nrequired\n\n\nby\nir.Value | None\nAn expression. Defaults to count.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTableExpr\nA top-k expression\n\n\n\n\n\n\nvalue_counts\nvalue_counts()\nCompute a frequency table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nFrequency table expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"chars\": char} for char in \"aabcddd\")\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ chars  ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ a      │\n│ b      │\n│ c      │\n│ d      │\n│ d      │\n│ d      │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.chars.value_counts().order_by(\"chars\")\n\n┏━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ chars  ┃ chars_count ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string │ int64       │\n├────────┼─────────────┤\n│ a      │           2 │\n│ b      │           1 │\n│ c      │           1 │\n│ d      │           3 │\n└────────┴─────────────┘"
  },
  {
    "objectID": "reference/expression-generic.html#methods-2",
    "href": "reference/expression-generic.html#methods-2",
    "title": "Generic expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nas_table\nPromote the scalar expression to a table.\n\n\n\n\nas_table\nas_table()\nPromote the scalar expression to a table.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\nExamples\nPromote an aggregation to a table\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.types as ir\n&gt;&gt;&gt; t = ibis.table(dict(a=\"str\"), name=\"t\")\n&gt;&gt;&gt; expr = t.a.length().sum().name(\"len\").as_table()\n&gt;&gt;&gt; isinstance(expr, ir.Table)\n\nTrue\n\n\nPromote a literal value to a table\n\n&gt;&gt;&gt; import ibis.expr.types as ir\n&gt;&gt;&gt; lit = ibis.literal(1).name(\"a\").as_table()\n&gt;&gt;&gt; isinstance(lit, ir.Table)\n\nTrue"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-33",
    "href": "reference/expression-generic.html#parameters-33",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nAny\nA Python value\nrequired\n\n\ntype\ndt.DataType | str | None\nAn instance of DataType or a string indicating the ibis type of value. This parameter can be used in cases where ibis’s type inference isn’t sufficient for discovering the type of value.\nNone"
  },
  {
    "objectID": "reference/expression-generic.html#returns-37",
    "href": "reference/expression-generic.html#returns-37",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nAn expression representing a literal value"
  },
  {
    "objectID": "reference/expression-generic.html#examples-35",
    "href": "reference/expression-generic.html#examples-35",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\nConstruct an integer literal\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; x = ibis.literal(42)\n&gt;&gt;&gt; x.type()\n\nInt8(nullable=True)\n\n\nConstruct a float64 literal from an int\n\n&gt;&gt;&gt; y = ibis.literal(42, type=\"double\")\n&gt;&gt;&gt; y.type()\n\nFloat64(nullable=True)\n\n\nIbis checks for invalid types\n\n&gt;&gt;&gt; ibis.literal(\"foobar\", type=\"int64\")  \n\nTypeError: Unable to normalize 'foobar' to Int64(nullable=True)"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-34",
    "href": "reference/expression-generic.html#parameters-34",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntype\ndt.DataType\nThe type of the unbound parameter, e.g., double, int64, date, etc.\nrequired"
  },
  {
    "objectID": "reference/expression-generic.html#returns-38",
    "href": "reference/expression-generic.html#returns-38",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nScalar\nA scalar expression backend by a parameter"
  },
  {
    "objectID": "reference/expression-generic.html#examples-36",
    "href": "reference/expression-generic.html#examples-36",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; start = ibis.param(\"date\")\n&gt;&gt;&gt; end = ibis.param(\"date\")\n&gt;&gt;&gt; schema = dict(timestamp_col=\"timestamp\", value=\"double\")\n&gt;&gt;&gt; t = ibis.table(schema, name=\"t\")\n&gt;&gt;&gt; predicates = [t.timestamp_col &gt;= start, t.timestamp_col &lt;= end]\n&gt;&gt;&gt; t.filter(predicates).value.sum()\n\n\n\n\nIbisError: Expression contains unbound tables and therefore cannot be executed. Use ibis.&lt;backend&gt;.execute(expr) or assign a backend instance to `ibis.options.default_backend`."
  },
  {
    "objectID": "reference/expression-generic.html#examples-37",
    "href": "reference/expression-generic.html#examples-37",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; my_null = ibis.NA\n&gt;&gt;&gt; my_null.isnull()\n\n\n\n\n\nTrue"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-35",
    "href": "reference/expression-generic.html#parameters-35",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nAny\nArguments from which to choose the first non-null value\n()"
  },
  {
    "objectID": "reference/expression-generic.html#returns-39",
    "href": "reference/expression-generic.html#returns-39",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nValue\nCoalesced expression"
  },
  {
    "objectID": "reference/expression-generic.html#see-also-7",
    "href": "reference/expression-generic.html#see-also-7",
    "title": "Generic expressions",
    "section": "See Also",
    "text": "See Also\nValue.coalesce() Value.fillna()"
  },
  {
    "objectID": "reference/expression-generic.html#examples-38",
    "href": "reference/expression-generic.html#examples-38",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; ibis.coalesce(None, 4, 5)\n\n\n\n\n\n4"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-36",
    "href": "reference/expression-generic.html#parameters-36",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nAny\nArguments to choose from\n()"
  },
  {
    "objectID": "reference/expression-generic.html#returns-40",
    "href": "reference/expression-generic.html#returns-40",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMinimum of the passed arguments"
  },
  {
    "objectID": "reference/expression-generic.html#examples-39",
    "href": "reference/expression-generic.html#examples-39",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; ibis.least(None, 4, 5)\n\n\n\n\n\n4"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-37",
    "href": "reference/expression-generic.html#parameters-37",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nargs\nAny\nArguments to choose from\n()"
  },
  {
    "objectID": "reference/expression-generic.html#returns-41",
    "href": "reference/expression-generic.html#returns-41",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nValue\nMaximum of the passed arguments"
  },
  {
    "objectID": "reference/expression-generic.html#examples-40",
    "href": "reference/expression-generic.html#examples-40",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; ibis.greatest(None, 4, 5)\n\n\n\n\n\n5"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-38",
    "href": "reference/expression-generic.html#parameters-38",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Column | str\nThe expression or column name to use for sorting\nrequired"
  },
  {
    "objectID": "reference/expression-generic.html#see-also-8",
    "href": "reference/expression-generic.html#see-also-8",
    "title": "Generic expressions",
    "section": "See Also",
    "text": "See Also\nValue.asc()"
  },
  {
    "objectID": "reference/expression-generic.html#examples-41",
    "href": "reference/expression-generic.html#examples-41",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t[[\"species\", \"year\"]].order_by(ibis.asc(\"year\")).head()\n\n┏━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━┩\n│ string  │ int64 │\n├─────────┼───────┤\n│ Adelie  │  2007 │\n│ Adelie  │  2007 │\n│ Adelie  │  2007 │\n│ Adelie  │  2007 │\n│ Adelie  │  2007 │\n└─────────┴───────┘"
  },
  {
    "objectID": "reference/expression-generic.html#returns-42",
    "href": "reference/expression-generic.html#returns-42",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nir.ValueExpr\nAn expression"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-39",
    "href": "reference/expression-generic.html#parameters-39",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Column | str\nThe expression or column name to use for sorting\nrequired"
  },
  {
    "objectID": "reference/expression-generic.html#see-also-9",
    "href": "reference/expression-generic.html#see-also-9",
    "title": "Generic expressions",
    "section": "See Also",
    "text": "See Also\nValue.desc()"
  },
  {
    "objectID": "reference/expression-generic.html#examples-42",
    "href": "reference/expression-generic.html#examples-42",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; t[[\"species\", \"year\"]].order_by(ibis.desc(\"year\")).head()\n\n┏━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━┩\n│ string  │ int64 │\n├─────────┼───────┤\n│ Adelie  │  2009 │\n│ Adelie  │  2009 │\n│ Adelie  │  2009 │\n│ Adelie  │  2009 │\n│ Adelie  │  2009 │\n└─────────┴───────┘"
  },
  {
    "objectID": "reference/expression-generic.html#returns-43",
    "href": "reference/expression-generic.html#returns-43",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nir.ValueExpr\nAn expression"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-40",
    "href": "reference/expression-generic.html#parameters-40",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncondition\nAny\nA boolean expression\nrequired\n\n\ntrue_expr\nAny\nExpression to return if condition evaluates to True\nrequired\n\n\nfalse_expr\nAny\nExpression to return if condition evaluates to False or NULL\nrequired"
  },
  {
    "objectID": "reference/expression-generic.html#returns-44",
    "href": "reference/expression-generic.html#returns-44",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nir.Value\nThe value of true_expr if condition is True else false_expr"
  },
  {
    "objectID": "reference/expression-generic.html#see-also-10",
    "href": "reference/expression-generic.html#see-also-10",
    "title": "Generic expressions",
    "section": "See Also",
    "text": "See Also\nBooleanValue.ifelse()"
  },
  {
    "objectID": "reference/expression-generic.html#examples-43",
    "href": "reference/expression-generic.html#examples-43",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"condition\": [True, False, True, None]})\n&gt;&gt;&gt; ibis.ifelse(t.condition, \"yes\", \"no\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ IfElse(condition, 'yes', 'no') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                         │\n├────────────────────────────────┤\n│ yes                            │\n│ no                             │\n│ yes                            │\n│ no                             │\n└────────────────────────────────┘"
  },
  {
    "objectID": "reference/expression-generic.html#returns-45",
    "href": "reference/expression-generic.html#returns-45",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nSearchedCaseBuilder\nA builder object to use for constructing a case expression."
  },
  {
    "objectID": "reference/expression-generic.html#see-also-11",
    "href": "reference/expression-generic.html#see-also-11",
    "title": "Generic expressions",
    "section": "See Also",
    "text": "See Also\nValue.case()"
  },
  {
    "objectID": "reference/expression-generic.html#examples-44",
    "href": "reference/expression-generic.html#examples-44",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\n...         \"left\": [1, 2, 3, 4],\n...         \"symbol\": [\"+\", \"-\", \"*\", \"/\"],\n...         \"right\": [5, 6, 7, 8],\n...     }\n... )\n&gt;&gt;&gt; t.mutate(\n...     result=(\n...         ibis.case()\n...         .when(_.symbol == \"+\", _.left + _.right)\n...         .when(_.symbol == \"-\", _.left - _.right)\n...         .when(_.symbol == \"*\", _.left * _.right)\n...         .when(_.symbol == \"/\", _.left / _.right)\n...         .end()\n...     )\n... )\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n┃ left  ┃ symbol ┃ right ┃ result  ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n│ int64 │ string │ int64 │ float64 │\n├───────┼────────┼───────┼─────────┤\n│     1 │ +      │     5 │     6.0 │\n│     2 │ -      │     6 │    -4.0 │\n│     3 │ *      │     7 │    21.0 │\n│     4 │ /      │     8 │     0.5 │\n└───────┴────────┴───────┴─────────┘"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-41",
    "href": "reference/expression-generic.html#parameters-41",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Expr\nIbis expression whose SQL will be printed\nrequired\n\n\ndialect\nstr | None\nString dialect. This is typically not required, but can be useful if ibis cannot infer the backend dialect.\nNone\n\n\nfile\nIO[str] | None\nFile to write output to\nNone"
  },
  {
    "objectID": "reference/expression-generic.html#examples-45",
    "href": "reference/expression-generic.html#examples-45",
    "title": "Generic expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; t = ibis.table(dict(a=\"int\"), name=\"t\")\n&gt;&gt;&gt; expr = t.select(c=_.a * 2)\n&gt;&gt;&gt; ibis.show_sql(expr)  # duckdb dialect by default\n\nSELECT\n  t0.a * CAST(2 AS TINYINT) AS c\nFROM t AS t0\n\n\n\n&gt;&gt;&gt; ibis.show_sql(expr, dialect=\"mysql\")\n\nSELECT\n  t0.a * 2 AS c\nFROM t AS t0"
  },
  {
    "objectID": "reference/expression-generic.html#parameters-42",
    "href": "reference/expression-generic.html#parameters-42",
    "title": "Generic expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Expr\nIbis expression.\nrequired\n\n\ndialect\nstr | None\nSQL dialect to use for compilation.\nNone\n\n\nkwargs\n\nScalar parameters\n{}"
  },
  {
    "objectID": "reference/expression-generic.html#returns-46",
    "href": "reference/expression-generic.html#returns-46",
    "title": "Generic expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nstr\nFormatted SQL string"
  },
  {
    "objectID": "reference/datatypes.html",
    "href": "reference/datatypes.html",
    "title": "Data types",
    "section": "",
    "text": "Scalar and column data types"
  },
  {
    "objectID": "reference/datatypes.html#parameters",
    "href": "reference/datatypes.html#parameters",
    "title": "Data types",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nAny\nThe object to coerce to an Ibis DataType. Supported inputs include strings, python type annotations, numpy dtypes, pandas dtypes, and pyarrow types.\nrequired\n\n\nnullable\nbool\nWhether the type should be nullable. Defaults to True.\nTrue"
  },
  {
    "objectID": "reference/datatypes.html#examples",
    "href": "reference/datatypes.html#examples",
    "title": "Data types",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.dtype(\"int32\")\n\nInt32(nullable=True)\n\n\n\n&gt;&gt;&gt; ibis.dtype(\"array&lt;float&gt;\")\n\nArray(value_type=Float64(nullable=True), nullable=True)\n\n\nDataType objects may also be created from Python types:\n\n&gt;&gt;&gt; ibis.dtype(int)\n\nInt64(nullable=True)\n\n\n\n&gt;&gt;&gt; ibis.dtype(list[float])\n\nArray(value_type=Float64(nullable=True), nullable=True)\n\n\nOr other type systems, like numpy/pandas/pyarrow types:\n\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; ibis.dtype(pa.int32())\n\nInt32(nullable=True)"
  },
  {
    "objectID": "reference/datatypes.html#notes",
    "href": "reference/datatypes.html#notes",
    "title": "Data types",
    "section": "Notes",
    "text": "Notes\nSome databases treat strings and blobs of equally, and some do not.\nFor example, Impala doesn’t make a distinction between string and binary types but PostgreSQL has a TEXT type and a BYTEA type which are distinct types that have different behavior."
  },
  {
    "objectID": "reference/datatypes.html#attributes",
    "href": "reference/datatypes.html#attributes",
    "title": "Data types",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nname\nReturn the name of the data type."
  },
  {
    "objectID": "reference/datatypes.html#methods",
    "href": "reference/datatypes.html#methods",
    "title": "Data types",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfrom_dask\nReturn the equivalent ibis datatype.\n\n\nfrom_numpy\nReturn the equivalent ibis datatype.\n\n\nfrom_pandas\nReturn the equivalent ibis datatype.\n\n\nfrom_pyarrow\nReturn the equivalent ibis datatype.\n\n\nis_array\nReturn True if an instance of an Array type.\n\n\nis_binary\nReturn True if an instance of a Binary type.\n\n\nis_boolean\nReturn True if an instance of a Boolean type.\n\n\nis_date\nReturn True if an instance of a Date type.\n\n\nis_decimal\nReturn True if an instance of a Decimal type.\n\n\nis_enum\nReturn True if an instance of an Enum type.\n\n\nis_float16\nReturn True if an instance of a Float16 type.\n\n\nis_float32\nReturn True if an instance of a Float32 type.\n\n\nis_float64\nReturn True if an instance of a Float64 type.\n\n\nis_floating\nReturn True if an instance of any Floating type.\n\n\nis_geospatial\nReturn True if an instance of a Geospatial type.\n\n\nis_inet\nReturn True if an instance of an Inet type.\n\n\nis_int16\nReturn True if an instance of an Int16 type.\n\n\nis_int32\nReturn True if an instance of an Int32 type.\n\n\nis_int64\nReturn True if an instance of an Int64 type.\n\n\nis_int8\nReturn True if an instance of an Int8 type.\n\n\nis_integer\nReturn True if an instance of any Integer type.\n\n\nis_interval\nReturn True if an instance of an Interval type.\n\n\nis_json\nReturn True if an instance of a JSON type.\n\n\nis_linestring\nReturn True if an instance of a LineString type.\n\n\nis_macaddr\nReturn True if an instance of a MACADDR type.\n\n\nis_map\nReturn True if an instance of a Map type.\n\n\nis_multilinestring\nReturn True if an instance of a MultiLineString type.\n\n\nis_multipoint\nReturn True if an instance of a MultiPoint type.\n\n\nis_multipolygon\nReturn True if an instance of a MultiPolygon type.\n\n\nis_nested\nReturn true if an instance of any nested (Array/Map/Struct) type.\n\n\nis_null\nReturn true if an instance of a Null type.\n\n\nis_numeric\nReturn true if an instance of a Numeric type.\n\n\nis_point\nReturn true if an instance of a Point type.\n\n\nis_polygon\nReturn true if an instance of a Polygon type.\n\n\nis_primitive\nReturn true if an instance of a Primitive type.\n\n\nis_signed_integer\nReturn true if an instance of a SignedInteger type.\n\n\nis_string\nReturn true if an instance of a String type.\n\n\nis_struct\nReturn true if an instance of a Struct type.\n\n\nis_temporal\nReturn true if an instance of a Temporal type.\n\n\nis_time\nReturn true if an instance of a Time type.\n\n\nis_timestamp\nReturn true if an instance of a Timestamp type.\n\n\nis_uint16\nReturn true if an instance of a UInt16 type.\n\n\nis_uint32\nReturn true if an instance of a UInt32 type.\n\n\nis_uint64\nReturn true if an instance of a UInt64 type.\n\n\nis_uint8\nReturn true if an instance of a UInt8 type.\n\n\nis_unknown\nReturn true if an instance of an Unknown type.\n\n\nis_unsigned_integer\nReturn true if an instance of an UnsignedInteger type.\n\n\nis_uuid\nReturn true if an instance of a UUID type.\n\n\nis_variadic\nReturn true if an instance of a Variadic type.\n\n\nto_dask\nReturn the equivalent dask datatype.\n\n\nto_numpy\nReturn the equivalent numpy datatype.\n\n\nto_pandas\nReturn the equivalent pandas datatype.\n\n\nto_pyarrow\nReturn the equivalent pyarrow datatype.\n\n\n\n\nfrom_dask\nfrom_dask(dask_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\nfrom_numpy\nfrom_numpy(numpy_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\nfrom_pandas\nfrom_pandas(pandas_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\nfrom_pyarrow\nfrom_pyarrow(arrow_type, nullable=True)\nReturn the equivalent ibis datatype.\n\n\nis_array\nis_array()\nReturn True if an instance of an Array type.\n\n\nis_binary\nis_binary()\nReturn True if an instance of a Binary type.\n\n\nis_boolean\nis_boolean()\nReturn True if an instance of a Boolean type.\n\n\nis_date\nis_date()\nReturn True if an instance of a Date type.\n\n\nis_decimal\nis_decimal()\nReturn True if an instance of a Decimal type.\n\n\nis_enum\nis_enum()\nReturn True if an instance of an Enum type.\n\n\nis_float16\nis_float16()\nReturn True if an instance of a Float16 type.\n\n\nis_float32\nis_float32()\nReturn True if an instance of a Float32 type.\n\n\nis_float64\nis_float64()\nReturn True if an instance of a Float64 type.\n\n\nis_floating\nis_floating()\nReturn True if an instance of any Floating type.\n\n\nis_geospatial\nis_geospatial()\nReturn True if an instance of a Geospatial type.\n\n\nis_inet\nis_inet()\nReturn True if an instance of an Inet type.\n\n\nis_int16\nis_int16()\nReturn True if an instance of an Int16 type.\n\n\nis_int32\nis_int32()\nReturn True if an instance of an Int32 type.\n\n\nis_int64\nis_int64()\nReturn True if an instance of an Int64 type.\n\n\nis_int8\nis_int8()\nReturn True if an instance of an Int8 type.\n\n\nis_integer\nis_integer()\nReturn True if an instance of any Integer type.\n\n\nis_interval\nis_interval()\nReturn True if an instance of an Interval type.\n\n\nis_json\nis_json()\nReturn True if an instance of a JSON type.\n\n\nis_linestring\nis_linestring()\nReturn True if an instance of a LineString type.\n\n\nis_macaddr\nis_macaddr()\nReturn True if an instance of a MACADDR type.\n\n\nis_map\nis_map()\nReturn True if an instance of a Map type.\n\n\nis_multilinestring\nis_multilinestring()\nReturn True if an instance of a MultiLineString type.\n\n\nis_multipoint\nis_multipoint()\nReturn True if an instance of a MultiPoint type.\n\n\nis_multipolygon\nis_multipolygon()\nReturn True if an instance of a MultiPolygon type.\n\n\nis_nested\nis_nested()\nReturn true if an instance of any nested (Array/Map/Struct) type.\n\n\nis_null\nis_null()\nReturn true if an instance of a Null type.\n\n\nis_numeric\nis_numeric()\nReturn true if an instance of a Numeric type.\n\n\nis_point\nis_point()\nReturn true if an instance of a Point type.\n\n\nis_polygon\nis_polygon()\nReturn true if an instance of a Polygon type.\n\n\nis_primitive\nis_primitive()\nReturn true if an instance of a Primitive type.\n\n\nis_signed_integer\nis_signed_integer()\nReturn true if an instance of a SignedInteger type.\n\n\nis_string\nis_string()\nReturn true if an instance of a String type.\n\n\nis_struct\nis_struct()\nReturn true if an instance of a Struct type.\n\n\nis_temporal\nis_temporal()\nReturn true if an instance of a Temporal type.\n\n\nis_time\nis_time()\nReturn true if an instance of a Time type.\n\n\nis_timestamp\nis_timestamp()\nReturn true if an instance of a Timestamp type.\n\n\nis_uint16\nis_uint16()\nReturn true if an instance of a UInt16 type.\n\n\nis_uint32\nis_uint32()\nReturn true if an instance of a UInt32 type.\n\n\nis_uint64\nis_uint64()\nReturn true if an instance of a UInt64 type.\n\n\nis_uint8\nis_uint8()\nReturn true if an instance of a UInt8 type.\n\n\nis_unknown\nis_unknown()\nReturn true if an instance of an Unknown type.\n\n\nis_unsigned_integer\nis_unsigned_integer()\nReturn true if an instance of an UnsignedInteger type.\n\n\nis_uuid\nis_uuid()\nReturn true if an instance of a UUID type.\n\n\nis_variadic\nis_variadic()\nReturn true if an instance of a Variadic type.\n\n\nto_dask\nto_dask()\nReturn the equivalent dask datatype.\n\n\nto_numpy\nto_numpy()\nReturn the equivalent numpy datatype.\n\n\nto_pandas\nto_pandas()\nReturn the equivalent pandas datatype.\n\n\nto_pyarrow\nto_pyarrow()\nReturn the equivalent pyarrow datatype."
  },
  {
    "objectID": "reference/datatypes.html#attributes-1",
    "href": "reference/datatypes.html#attributes-1",
    "title": "Data types",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nprecision\nThe number of decimal places values of this type can hold.\n\n\nscale\nThe number of values after the decimal point."
  },
  {
    "objectID": "reference/datatypes.html#attributes-2",
    "href": "reference/datatypes.html#attributes-2",
    "title": "Data types",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nresolution\nThe interval unit’s name.\n\n\nunit\nThe time unit of the interval."
  },
  {
    "objectID": "reference/datatypes.html#notes-1",
    "href": "reference/datatypes.html#notes-1",
    "title": "Data types",
    "section": "Notes",
    "text": "Notes\nBecause of differences in the way different backends handle strings, we cannot assume that strings are UTF-8 encoded."
  },
  {
    "objectID": "reference/datatypes.html#methods-1",
    "href": "reference/datatypes.html#methods-1",
    "title": "Data types",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfrom_tuples\nConstruct a Struct type from pairs.\n\n\nnames\nReturn the names of the struct’s fields.\n\n\ntypes\nReturn the types of the struct’s fields.\n\n\n\n\nfrom_tuples\nfrom_tuples(pairs, nullable=True)\nConstruct a Struct type from pairs.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npairs\nIterable[tuple[str, str | DataType]]\nAn iterable of pairs of field name and type\nrequired\n\n\nnullable\nbool\nWhether the type is nullable\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStruct\nStruct data type instance\n\n\n\n\n\n\nnames\nnames()\nReturn the names of the struct’s fields.\n\n\ntypes\ntypes()\nReturn the types of the struct’s fields."
  },
  {
    "objectID": "reference/datatypes.html#attributes-3",
    "href": "reference/datatypes.html#attributes-3",
    "title": "Data types",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nscale\nThe scale of the timestamp if known.\n\n\ntimezone\nThe timezone of values of this type.\n\n\nunit\nReturn the unit of the timestamp."
  },
  {
    "objectID": "reference/datatypes.html#methods-2",
    "href": "reference/datatypes.html#methods-2",
    "title": "Data types",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfrom_unit\nReturn a timestamp type with the given unit and timezone.\n\n\n\n\nfrom_unit\nfrom_unit(unit, timezone=None, nullable=True)\nReturn a timestamp type with the given unit and timezone."
  },
  {
    "objectID": "reference/Repr.html",
    "href": "reference/Repr.html",
    "title": "Repr",
    "section": "",
    "text": "Repr()\nExpression printing options.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ndepth\nint\nThe maximum number of expression nodes to print when repring.\n\n\ntable_columns\nint\nThe number of columns to show in leaf table expressions.\n\n\ntable_rows\nint\nThe number of rows to show for in memory tables.\n\n\nquery_text_length\nint\nThe maximum number of characters to show in the query field repr of SQLQueryResult operations.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the repr.\n\n\ninteractive\nbool\nOptions controlling the interactive repr."
  },
  {
    "objectID": "reference/Repr.html#attributes",
    "href": "reference/Repr.html#attributes",
    "title": "Repr",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ndepth\nint\nThe maximum number of expression nodes to print when repring.\n\n\ntable_columns\nint\nThe number of columns to show in leaf table expressions.\n\n\ntable_rows\nint\nThe number of rows to show for in memory tables.\n\n\nquery_text_length\nint\nThe maximum number of characters to show in the query field repr of SQLQueryResult operations.\n\n\nshow_types\nbool\nShow the inferred type of value expressions in the repr.\n\n\ninteractive\nbool\nOptions controlling the interactive repr."
  },
  {
    "objectID": "install.html",
    "href": "install.html",
    "title": "Installation",
    "section": "",
    "text": "You can install Ibis and a supported backend with pip, conda, mamba, or pixi.\n\npipcondamambapixi\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall with the bigquery extra:\npip install 'ibis-framework[bigquery]'\nConnect using ibis.bigquery.connect.\n\n\nInstall with the clickhouse extra:\npip install 'ibis-framework[clickhouse]'\nConnect using ibis.clickhouse.connect.\n\n\nInstall with the dask extra:\npip install 'ibis-framework[dask]'\nConnect using ibis.dask.connect.\n\n\nInstall with the datafusion extra:\npip install 'ibis-framework[datafusion]'\nConnect using ibis.datafusion.connect.\n\n\nInstall with the druid extra:\npip install 'ibis-framework[druid]'\nConnect using ibis.druid.connect.\n\n\nInstall with the duckdb extra:\npip install 'ibis-framework[duckdb]'\nConnect using ibis.duckdb.connect.\n\n\nInstall with the impala extra:\npip install 'ibis-framework[impala]'\nConnect using ibis.impala.connect.\n\n\nInstall with the mssql extra:\npip install 'ibis-framework[mssql]'\nConnect using ibis.mssql.connect.\n\n\nInstall with the mysql extra:\npip install 'ibis-framework[mysql]'\nConnect using ibis.mysql.connect.\n\n\nInstall with the oracle extra:\npip install 'ibis-framework[oracle]'\nConnect using ibis.oracle.connect.\n\n\nInstall with the pandas extra:\npip install 'ibis-framework[pandas]'\nConnect using ibis.pandas.connect.\n\n\nInstall with the polars extra:\npip install 'ibis-framework[polars]'\nConnect using ibis.polars.connect.\n\n\nInstall with the postgres extra:\npip install 'ibis-framework[postgres]'\nConnect using ibis.postgres.connect.\n\n\nInstall with the pyspark extra:\npip install 'ibis-framework[pyspark]'\nConnect using ibis.pyspark.connect.\n\n\nInstall with the snowflake extra:\npip install 'ibis-framework[snowflake]'\nConnect using ibis.snowflake.connect.\n\n\nInstall with the sqlite extra:\npip install 'ibis-framework[sqlite]'\nConnect using ibis.sqlite.connect.\n\n\nInstall with the trino extra:\npip install 'ibis-framework[trino]'\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nconda install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nconda install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nconda install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nconda install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nconda install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nconda install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nconda install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nconda install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nconda install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nconda install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nconda install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nconda install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nconda install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nconda install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nconda install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nconda install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nconda install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nmamba install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nmamba install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nmamba install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nmamba install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nmamba install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nmamba install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nmamba install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nmamba install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nmamba install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nmamba install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nmamba install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nmamba install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nmamba install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nmamba install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nmamba install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nmamba install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nmamba install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nAdd the ibis-bigquery package:\npixi add ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nAdd the ibis-clickhouse package:\npixi add ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nAdd the ibis-dask package:\npixi add ibis-dask\nConnect using ibis.dask.connect.\n\n\nAdd the ibis-datafusion package:\npixi add ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nAdd the ibis-druid package:\npixi add ibis-druid\nConnect using ibis.druid.connect.\n\n\nAdd the ibis-duckdb package:\npixi add ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nAdd the ibis-impala package:\npixi add ibis-impala\nConnect using ibis.impala.connect.\n\n\nAdd the ibis-mssql package:\npixi add ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nAdd the ibis-mysql package:\npixi add ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nAdd the ibis-oracle package:\npixi add ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nAdd the ibis-pandas package:\npixi add ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nAdd the ibis-polars package:\npixi add ibis-polars\nConnect using ibis.polars.connect.\n\n\nAdd the ibis-postgres package:\npixi add ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nAdd the ibis-pyspark package:\npixi add ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nAdd the ibis-snowflake package:\npixi add ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nAdd the ibis-sqlite package:\npixi add ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nAdd the ibis-trino package:\npixi add ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "backends/bigquery.html",
    "href": "backends/bigquery.html",
    "title": "BigQuery",
    "section": "",
    "text": "https://cloud.google.com/bigquery"
  },
  {
    "objectID": "backends/bigquery.html#install",
    "href": "backends/bigquery.html#install",
    "title": "BigQuery",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the BigQuery backend:\n\npipcondamamba\n\n\nInstall with the bigquery extra:\npip install 'ibis-framework[bigquery]'\nAnd connect:\nimport ibis\n\n1con = ibis.bigquery.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for BigQuery:\nconda install -c conda-forge ibis-bigquery\nAnd connect:\nimport ibis\n\n1con = ibis.bigquery.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for BigQuery:\nmamba install -c conda-forge ibis-bigquery\nAnd connect:\nimport ibis\n\n1con = ibis.bigquery.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/bigquery.html#connect",
    "href": "backends/bigquery.html#connect",
    "title": "BigQuery",
    "section": "Connect",
    "text": "Connect\n\nibis.bigquery.connect\ncon = ibis.bigquery.connect(\n    project_id=\"ibis-bq-project\",\n    dataset_id=\"testing\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.bigquery.connect is a thin wrapper around ibis.backends.bigquery.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, project_id=None, dataset_id='', credentials=None, application_name=None, auth_local_webserver=True, auth_external_data=False, auth_cache='default', partition_column='PARTITIONTIME', client=None, storage_client=None, location=None)\nCreate a Backend for use with Ibis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nproject_id\nstr | None\nA BigQuery project id.\nNone\n\n\ndataset_id\nstr\nA dataset id that lives inside of the project indicated by project_id.\n''\n\n\ncredentials\ngoogle.google.auth.google.auth.credentials.google.auth.credentials.Credentials | None\nOptional credentials.\nNone\n\n\napplication_name\nstr | None\nA string identifying your application to Google API endpoints.\nNone\n\n\nauth_local_webserver\nbool\nUse a local webserver for the user authentication. Binds a webserver to an open port on localhost between 8080 and 8089, inclusive, to receive authentication token. If not set, defaults to False, which requests a token via the console.\nTrue\n\n\nauth_external_data\nbool\nAuthenticate using additional scopes required to query external data sources &lt;https://cloud.google.com/bigquery/external-data-sources&gt;_, such as Google Sheets, files in Google Cloud Storage, or files in Google Drive. If not set, defaults to False, which requests the default BigQuery scopes.\nFalse\n\n\nauth_cache\nstr\nSelects the behavior of the credentials cache. 'default' Reads credentials from disk if available, otherwise authenticates and caches credentials to disk. 'reauth' Authenticates and caches credentials to disk. 'none' Authenticates and does not cache credentials. Defaults to 'default'.\n'default'\n\n\npartition_column\nstr | None\nIdentifier to use instead of default _PARTITIONTIME partition column. Defaults to 'PARTITIONTIME'.\n'PARTITIONTIME'\n\n\nclient\ngoogle.google.Client | None\nA Client from the google.cloud.bigquery package. If not set, one is created using the project_id and credentials.\nNone\n\n\nstorage_client\ngoogle.google.BigQueryReadClient | None\nA BigQueryReadClient from the google.cloud.bigquery_storage_v1 package. If not set, one is created using the project_id and credentials.\nNone\n\n\nlocation\nstr | None\nDefault location for BigQuery objects.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.bigquery.Backend\nAn instance of the BigQuery backend.\n\n\n\n\n\n\n\nibis.connect URL format\nIn addition to ibis.bigquery.connect, you can also connect to BigQuery by passing a properly formatted BigQuery connection URL to ibis.connect\ncon = ibis.connect(f\"bigquery://{project_id}/{dataset_id}\")\n\n\n\n\n\n\nNote\n\n\n\nThis assumes you have already authenticated via the gcloud CLI.\n\n\n\n\nFinding your project_id and dataset_id\nLog in to the Google Cloud Console to see which project_ids and dataset_ids are available to use.\n\n\n\nbigquery_ids\n\n\n\n\nBigQuery Authentication\nThe simplest way to authenticate with the BigQuery backend is to use Google’s gcloud CLI tool.\nOnce you have gcloud installed, you can authenticate to BigQuery (and other Google Cloud services) by running\ngcloud auth login\nFor any authentication problems, or information on other ways of authenticating, see the gcloud CLI authorization guide."
  },
  {
    "objectID": "backends/bigquery.html#ibis.backends.bigquery.Backend",
    "href": "backends/bigquery.html#ibis.backends.bigquery.Backend",
    "title": "BigQuery",
    "section": "bigquery.Backend",
    "text": "bigquery.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\ncompile\ncompile(self, expr, limit=None, params=None, **_)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False, collate=None, **options)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False, default_collate=None, partition_by=None, cluster_by=None, options=None)\nCreate a table in BigQuery.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to create\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nThe data with which to populate the table; optional, but one of obj or schema must be specified\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema of the table to create; optional, but one of obj or schema must be specified\nNone\n\n\ndatabase\nstr | None\nThe BigQuery dataset in which to create the table; optional\nNone\n\n\ntemp\nbool\nWhether the table is temporary\nFalse\n\n\noverwrite\nbool\nIf True, replace the table if it already exists, otherwise fail if the table exists\nFalse\n\n\ndefault_collate\nstr | None\nDefault collation for string columns. See BigQuery’s documentation for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/collation-concepts\nNone\n\n\npartition_by\nstr | None\nPartition the table by the given expression. See BigQuery’s documentation for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#partition_expression\nNone\n\n\ncluster_by\ncollections.abc.Iterable[str] | None\nList of columns to cluster the table by. See BigQuery’s documentation for more details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#clustering_column_list\nNone\n\n\noptions\ncollections.abc.Mapping[str, typing.Any] | None\nBigQuery-specific table options; see the BigQuery documentation for details: https://cloud.google.com/bigquery/docs/reference/standard-sql/data-definition-language#table_option_list\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was just created\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, schema=None, database=None, overwrite=False)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nibis.ibis.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created.\n\n\n\n\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False, cascade=False)\nDrop a BigQuery dataset.\n\n\ndrop_table\ndrop_table(self, name, *, schema=None, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the table exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the table does not exist.\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, schema=None, database=None, force=False)\nDrop a view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the view to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the view does not exist.\nFalse\n\n\n\n\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute the given Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\n\nIbis expression to execute\nrequired\n\n\nlimit\n\nRetrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\n\nQuery parameters\nNone\n\n\nkwargs\n\nExtra arguments specific to the backend\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\npandas.pandas.DataFrame | pandas.pandas.Series | scalar\nOutput from execution\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\nFetch data from cursor.\n\n\nget_schema\nget_schema(self, name, schema=None, database=None)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\nlist_databases\nlist_databases(self, like=None)\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\nList existing schemas in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned schema names.\nNone\n\n\ndatabase\nstr | None\nThe database to list schemas from. If None, the current database is searched.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe schema names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList the tables in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern to use for listing tables.\nNone\n\n\ndatabase\nstr | None\nThe database (project) to perform the list against.\nNone\n\n\nschema\nstr | None\nThe schema (dataset) inside database to perform the list against. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query, results=False, params=None)\nExecute a query string and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL query string\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRead CSV data into a BigQuery table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nPath to a CSV file on GCS or the local filesystem. Globs are supported.\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to google.cloud.bigquery.LoadJobConfig.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn Ibis table expression\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRead newline-delimited JSON data into a BigQuery table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nPath to a newline-delimited JSON file on GCS or the local filesystem. Globs are supported.\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to google.cloud.bigquery.LoadJobConfig.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn Ibis table expression\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRead Parquet data into a BigQuery table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nPath to a Parquet file on GCS or the local filesystem. Globs are supported.\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to google.cloud.bigquery.LoadJobConfig.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn Ibis table expression\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nset_database\nset_database(self, name)\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name."
  },
  {
    "objectID": "backends/postgresql.html",
    "href": "backends/postgresql.html",
    "title": "PostgreSQL",
    "section": "",
    "text": "https://www.postgresql.org"
  },
  {
    "objectID": "backends/postgresql.html#install",
    "href": "backends/postgresql.html#install",
    "title": "PostgreSQL",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Postgres backend:\n\npipcondamamba\n\n\nInstall with the postgres extra:\npip install 'ibis-framework[postgres]'\nAnd connect:\nimport ibis\n\n1con = ibis.postgres.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Postgres:\nconda install -c conda-forge ibis-postgres\nAnd connect:\nimport ibis\n\n1con = ibis.postgres.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Postgres:\nmamba install -c conda-forge ibis-postgres\nAnd connect:\nimport ibis\n\n1con = ibis.postgres.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/postgresql.html#connect",
    "href": "backends/postgresql.html#connect",
    "title": "PostgreSQL",
    "section": "Connect",
    "text": "Connect\n\nibis.postgres.connect\ncon = ibis.postgres.connect(\n    user=\"username\",\n    password=\"password\",\n    host=\"hostname\",\n    port=5432,\n    database=\"database\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.postgres.connect is a thin wrapper around ibis.backends.postgres.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, host=None, user=None, password=None, port=5432, database=None, schema=None, url=None, driver='psycopg2')\nCreate an Ibis client connected to PostgreSQL database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhost\nstr | None\nHostname\nNone\n\n\nuser\nstr | None\nUsername\nNone\n\n\npassword\nstr | None\nPassword\nNone\n\n\nport\nint\nPort number\n5432\n\n\ndatabase\nstr | None\nDatabase to connect to\nNone\n\n\nschema\nstr | None\nPostgreSQL schema to use. If None, use the default search_path.\nNone\n\n\nurl\nstr | None\nSQLAlchemy connection string. If passed, the other connection arguments are ignored.\nNone\n\n\ndriver\ntyping.Literal[‘psycopg2’]\nDatabase driver\n'psycopg2'\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; import getpass\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; host = os.environ.get(\"IBIS_TEST_POSTGRES_HOST\", \"localhost\")\n&gt;&gt;&gt; user = os.environ.get(\"IBIS_TEST_POSTGRES_USER\", getpass.getuser())\n&gt;&gt;&gt; password = os.environ.get(\"IBIS_TEST_POSTGRES_PASSWORD\")\n&gt;&gt;&gt; database = os.environ.get(\"IBIS_TEST_POSTGRES_DATABASE\", \"ibis_testing\")\n&gt;&gt;&gt; con = connect(database=database, host=host, user=user, password=password)\n&gt;&gt;&gt; con.list_tables()\n[...]\n&gt;&gt;&gt; t = con.table(\"functional_alltypes\")\n&gt;&gt;&gt; t\nPostgreSQLTable[table]\n  name: functional_alltypes\n  schema:\n    id : int32\n    bool_col : boolean\n    tinyint_col : int16\n    smallint_col : int16\n    int_col : int32\n    bigint_col : int64\n    float_col : float32\n    double_col : float64\n    date_string_col : string\n    string_col : string\n    timestamp_col : timestamp\n    year : int32\n    month : int32\n\n\n\n\nibis.connect URL format\nIn addition to ibis.postgres.connect, you can also connect to Postgres by passing a properly formatted Postgres connection URL to ibis.connect\ncon = ibis.connect(f\"postgres://{user}:{password}@{host}:{port}/{database}\")"
  },
  {
    "objectID": "backends/postgresql.html#ibis.backends.postgres.Backend",
    "href": "backends/postgresql.html#ibis.backends.postgres.Backend",
    "title": "PostgreSQL",
    "section": "postgres.Backend",
    "text": "postgres.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nfunction\nfunction(self, name, *, schema=None)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList the tables in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\n\nA pattern to use for listing tables.\nNone\n\n\ndatabase\n\n(deprecated) The database to perform the list against.\nNone\n\n\nschema\n\nThe schema to perform the list against. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/mysql.html",
    "href": "backends/mysql.html",
    "title": "MySQL",
    "section": "",
    "text": "https://www.mysql.com"
  },
  {
    "objectID": "backends/mysql.html#install",
    "href": "backends/mysql.html#install",
    "title": "MySQL",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the MySQL backend:\n\npipcondamamba\n\n\nInstall with the mysql extra:\npip install 'ibis-framework[mysql]'\nAnd connect:\nimport ibis\n\n1con = ibis.mysql.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for MySQL:\nconda install -c conda-forge ibis-mysql\nAnd connect:\nimport ibis\n\n1con = ibis.mysql.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for MySQL:\nmamba install -c conda-forge ibis-mysql\nAnd connect:\nimport ibis\n\n1con = ibis.mysql.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/mysql.html#connect",
    "href": "backends/mysql.html#connect",
    "title": "MySQL",
    "section": "Connect",
    "text": "Connect\n\nibis.mysql.connect\ncon = ibis.mysql.connect(\n    user=\"username\",\n    password=\"password\",\n    host=\"hostname\",\n    port=3306,\n    database=\"database\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.mysql.connect is a thin wrapper around ibis.backends.mysql.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, host='localhost', user=None, password=None, port=3306, database=None, url=None, driver='pymysql', **kwargs)\nCreate an Ibis client using the passed connection parameters.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhost\nstr\nHostname\n'localhost'\n\n\nuser\nstr | None\nUsername\nNone\n\n\npassword\nstr | None\nPassword\nNone\n\n\nport\nint\nPort\n3306\n\n\ndatabase\nstr | None\nDatabase to connect to\nNone\n\n\nurl\nstr | None\nComplete SQLAlchemy connection string. If passed, the other connection arguments are ignored.\nNone\n\n\ndriver\ntyping.Literal[‘pymysql’]\nPython MySQL database driver\n'pymysql'\n\n\nkwargs\n\nAdditional keyword arguments passed to connect_args in sqlalchemy.create_engine. Use these to pass dialect specific arguments.\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import os\n&gt;&gt;&gt; import getpass\n&gt;&gt;&gt; host = os.environ.get(\"IBIS_TEST_MYSQL_HOST\", \"localhost\")\n&gt;&gt;&gt; user = os.environ.get(\"IBIS_TEST_MYSQL_USER\", getpass.getuser())\n&gt;&gt;&gt; password = os.environ.get(\"IBIS_TEST_MYSQL_PASSWORD\")\n&gt;&gt;&gt; database = os.environ.get(\"IBIS_TEST_MYSQL_DATABASE\", \"ibis_testing\")\n&gt;&gt;&gt; con = connect(database=database, host=host, user=user, password=password)\n&gt;&gt;&gt; con.list_tables()\n[...]\n&gt;&gt;&gt; t = con.table(\"functional_alltypes\")\n&gt;&gt;&gt; t\nMySQLTable[table]\n  name: functional_alltypes\n  schema:\n    id : int32\n    bool_col : int8\n    tinyint_col : int8\n    smallint_col : int16\n    int_col : int32\n    bigint_col : int64\n    float_col : float32\n    double_col : float64\n    date_string_col : string\n    string_col : string\n    timestamp_col : timestamp\n    year : int32\n    month : int32\n\n\n\n\nibis.connect URL format\nIn addition to ibis.mysql.connect, you can also connect to MySQL by passing a properly formatted MySQL connection URL to ibis.connect\ncon = ibis.connect(f\"mysql://{user}:{password}@{host}:{port}/{database}\")"
  },
  {
    "objectID": "backends/mysql.html#ibis.backends.mysql.Backend",
    "href": "backends/mysql.html#ibis.backends.mysql.Backend",
    "title": "MySQL",
    "section": "mysql.Backend",
    "text": "mysql.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_database\ncreate_database(self, name, force=False)\nCreate a new database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new database.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database already exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop a database with name name.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase to drop.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/impala.html",
    "href": "backends/impala.html",
    "title": "Impala",
    "section": "",
    "text": "https://impala.apache.org"
  },
  {
    "objectID": "backends/impala.html#install",
    "href": "backends/impala.html#install",
    "title": "Impala",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Impala backend:\n\npipcondamamba\n\n\nInstall with the impala extra:\npip install 'ibis-framework[impala]'\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Impala:\nconda install -c conda-forge ibis-impala\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Impala:\nmamba install -c conda-forge ibis-impala\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/impala.html#database-methods",
    "href": "backends/impala.html#database-methods",
    "title": "Impala",
    "section": "Database methods",
    "text": "Database methods\n\ncreate_database\ncreate_database(self, name, path=None, force=False)\nCreate a new Impala database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nDatabase name\nrequired\n\n\npath\n\nHDFS path where to store the database data; otherwise uses Impala default\nNone\n\n\nforce\n\nForcibly create the database\nFalse\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop an Impala database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nDatabase name\nrequired\n\n\nforce\n\nIf False and there are any tables in this database, raises an IntegrityError\nFalse\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)"
  },
  {
    "objectID": "backends/impala.html#table-methods",
    "href": "backends/impala.html#table-methods",
    "title": "Impala",
    "section": "Table methods",
    "text": "Table methods\nThe Backend object itself has many helper utility methods. You’ll find the most methods on ImpalaTable.\n\ntable\ntable(self, name, database=None, **kwargs)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query string and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL query string\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\nReturn the list of table names in the current database.\nFor some backends, the tables may be files in a directory, or other equivalent entities in a SQL database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format.\nNone\n\n\ndatabase\nstr | None\nThe database from which to list tables. If not provided, the current database is used.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe list of the table names that match the pattern like.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop an Impala table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\nforce\nbool\nDatabase may throw exception if table does not exist\nFalse\n\n\n\n\n\nExamples\n&gt;&gt;&gt; table = \"my_table\"\n&gt;&gt;&gt; db = \"operations\"\n&gt;&gt;&gt; con.drop_table(table, database=db, force=True)  # quartodoc: +SKIP\n\n\n\ninsert\ninsert(self, table_name, obj=None, database=None, overwrite=False, partition=None, values=None, validate=True)\nInsert data into an existing table.\nSee ImpalaTable.insert for parameters.\n\nExamples\n&gt;&gt;&gt; table = \"my_table\"\n&gt;&gt;&gt; con.insert(table, table_expr)  # quartodoc: +SKIP\nCompletely overwrite contents\n&gt;&gt;&gt; con.insert(table, table_expr, overwrite=True)  # quartodoc: +SKIP\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)\nDelete all rows from an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\n\nget_schema\nget_schema(self, table_name, database=None)\nReturn a Schema object for the indicated table and database.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nIbis schema\n\n\n\n\n\n\ncache_table\ncache_table(self, table_name, *, database=None, pool='default')\nCaches a table in cluster memory in the given pool.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\n\nTable name\nrequired\n\n\ndatabase\n\nDatabase name\nNone\n\n\npool\n\nThe name of the pool in which to cache the table\n'default'\n\n\n\n\n\nExamples\n&gt;&gt;&gt; table = \"my_table\"\n&gt;&gt;&gt; db = \"operations\"\n&gt;&gt;&gt; pool = \"op_4GB_pool\"\n&gt;&gt;&gt; con.cache_table(\"my_table\", database=db, pool=pool)  # quartodoc: +SKIP\nThe best way to interact with a single table is through the ImpalaTable object you get back from Backend.table.\n\n\n\ndrop\ndrop(self)\nDrop the table from the database.\n\n\ndrop_partition\ndrop_partition(self, spec)\nDrop an existing table partition.\n\n\nfiles\nfiles(self)\nReturn results of SHOW FILES statement.\n\n\ninsert\ninsert(self, obj=None, overwrite=False, partition=None, values=None, validate=True)\nInsert into an Impala table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\n\nTable expression or DataFrame\nNone\n\n\noverwrite\n\nIf True, will replace existing contents of table\nFalse\n\n\npartition\n\nFor partitioned tables, indicate the partition that’s being inserted into, either with an ordered list of partition keys or a dict of partition field name to value. For example for the partition (year=2007, month=7), this can be either (2007, 7) or {‘year’: 2007, ‘month’: 7}.\nNone\n\n\nvalues\n\nUnsupported and unused\nNone\n\n\nvalidate\n\nIf True, do more rigorous validation that schema of table being inserted is compatible with the existing table\nTrue\n\n\n\n\n\nExamples\nAppend to an existing table\n&gt;&gt;&gt; t.insert(table_expr)  # quartodoc: +SKIP\nCompletely overwrite contents\n&gt;&gt;&gt; t.insert(table_expr, overwrite=True)  # quartodoc: +SKIP\n\n\n\nis_partitioned\nTrue if the table is partitioned.\n\n\npartition_schema\npartition_schema(self)\nReturn the schema for the partition columns.\n\n\npartitions\npartitions(self)\nReturn information about the table’s partitions.\nRaises an exception if the table is not partitioned.\n\n\nrefresh\nrefresh(self)\n\n\ndescribe_formatted"
  },
  {
    "objectID": "backends/impala.html#creating-views",
    "href": "backends/impala.html#creating-views",
    "title": "Impala",
    "section": "Creating views",
    "text": "Creating views\n\ndrop_table_or_view\ndrop_table_or_view(self, name, *, database=None, force=False)\nDrop view or table.\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nibis.ibis.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created."
  },
  {
    "objectID": "backends/impala.html#accessing-data-formats-in-hdfs",
    "href": "backends/impala.html#accessing-data-formats-in-hdfs",
    "title": "Impala",
    "section": "Accessing data formats in HDFS",
    "text": "Accessing data formats in HDFS\n\ndelimited_file\ndelimited_file(self, hdfs_dir, schema, name=None, database=None, delimiter=',', na_rep=None, escapechar=None, lineterminator=None, external=True, persist=False)\nInterpret delimited text files as an Ibis table expression.\nSee the parquet_file method for more details on what happens under the hood.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhdfs_dir\n\nHDFS directory containing delimited text files\nrequired\n\n\nschema\n\nIbis schema\nrequired\n\n\nname\n\nName for temporary or persistent table; otherwise random names are generated\nNone\n\n\ndatabase\n\nDatabase to create the table in\nNone\n\n\ndelimiter\n\nCharacter used to delimit columns\n','\n\n\nna_rep\n\nCharacter used to represent NULL values\nNone\n\n\nescapechar\n\nCharacter used to escape special characters\nNone\n\n\nlineterminator\n\nCharacter used to delimit lines\nNone\n\n\nexternal\n\nCreate table as EXTERNAL (data will not be deleted on drop). Not that if persist=False and external=False, whatever data you reference will be deleted\nTrue\n\n\npersist\n\nIf True, do not delete the table upon garbage collection of ibis table object\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.impala.client.ImpalaTable\nImpala table expression\n\n\n\n\n\n\nparquet_file\nparquet_file(self, hdfs_dir, schema=None, name=None, database=None, external=True, like_file=None, like_table=None, persist=False)\nMake indicated parquet file in HDFS available as an Ibis table.\nThe table created can be optionally named and persisted, otherwise a unique name will be generated. Temporarily, for any non-persistent external table created by Ibis we will attempt to drop it when the underlying object is garbage collected (or the Python interpreter shuts down normally).\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhdfs_dir\n\nPath in HDFS\nrequired\n\n\nschema\n\nIf no schema provided, and neither of the like_* argument is passed, one will be inferred from one of the parquet files in the directory.\nNone\n\n\nlike_file\n\nAbsolute path to Parquet file in HDFS to use for schema definitions. An alternative to having to supply an explicit schema\nNone\n\n\nlike_table\n\nFully scoped and escaped string to an Impala table whose schema we will use for the newly created table.\nNone\n\n\nname\n\nRandom unique name generated otherwise\nNone\n\n\ndatabase\n\nDatabase to create the (possibly temporary) table in\nNone\n\n\nexternal\n\nIf a table is external, the referenced data will not be deleted when the table is dropped in Impala. Otherwise (external=False) Impala takes ownership of the Parquet file.\nTrue\n\n\npersist\n\nDo not drop the table during garbage collection\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.impala.client.ImpalaTable\nImpala table expression\n\n\n\n\n\n\navro_file\navro_file(self, hdfs_dir, avro_schema, name=None, database=None, external=True, persist=False)\nCreate a table to read a collection of Avro data.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhdfs_dir\n\nAbsolute HDFS path to directory containing avro files\nrequired\n\n\navro_schema\n\nThe Avro schema for the data as a Python dict\nrequired\n\n\nname\n\nTable name\nNone\n\n\ndatabase\n\nDatabase name\nNone\n\n\nexternal\n\nWhether the table is external\nTrue\n\n\npersist\n\nPersist the table\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.impala.client.ImpalaTable\nImpala table expression"
  },
  {
    "objectID": "backends/impala.html#hdfs-interaction",
    "href": "backends/impala.html#hdfs-interaction",
    "title": "Impala",
    "section": "HDFS Interaction",
    "text": "HDFS Interaction\nIbis delegates all HDFS interaction to the fsspec library."
  },
  {
    "objectID": "backends/impala.html#the-impala-client-object",
    "href": "backends/impala.html#the-impala-client-object",
    "title": "Impala",
    "section": "The Impala client object",
    "text": "The Impala client object\nTo use Ibis with Impala, you first must connect to a cluster using the ibis.impala.connect function, optionally supplying an HDFS connection:\nimport ibis\n\nhdfs = ibis.impala.hdfs_connect(host=webhdfs_host, port=webhdfs_port)\nclient = ibis.impala.connect(host=impala_host, port=impala_port, hdfs_client=hdfs)\nBy default binary transport mode is used, however it is also possible to use HTTP. Depending on your configuration, additional connection arguments may need to be provided. For the full list of possible connection arguments please refer to the impyla documentation.\nimport ibis\n\nclient = ibis.impala.connect(\n    host=impala_host,\n    port=impala_port,\n    username=username,\n    password=password,\n    use_ssl=True,\n    auth_mechanism='LDAP',\n    use_http_transport=True,\n    http_path='cliservice',\n)\nAll examples here use the following block of code to connect to impala using docker:\nimport ibis\n\nhdfs = ibis.impala.hdfs_connect(host=\"localhost\", port=50070)\nclient = ibis.impala.connect(host=host, hdfs_client=hdfs)\nYou can accomplish many tasks directly through the client object, but we additionally provide APIs to streamline tasks involving a single Impala table or database."
  },
  {
    "objectID": "backends/impala.html#table-objects",
    "href": "backends/impala.html#table-objects",
    "title": "Impala",
    "section": "Table objects",
    "text": "Table objects\n\ntable\ntable(self, name, database=None)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\nThe client’s table method allows you to create an Ibis table expression referencing a physical Impala table:\ntable = client.table('functional_alltypes', database='ibis_testing')\nImpalaTable is a Python subclass of the more general Ibis Table that has additional Impala-specific methods. So you can use it interchangeably with any code expecting a Table.\nWhile the client has a drop_table method you can use to drop tables, the table itself has a method drop that you can use:\ntable.drop()"
  },
  {
    "objectID": "backends/impala.html#expression-execution",
    "href": "backends/impala.html#expression-execution",
    "title": "Impala",
    "section": "Expression execution",
    "text": "Expression execution\nIbis expressions have execution methods like to_pandas that compile and run the expressions on Impala or whichever backend is being referenced.\nFor example:\n&gt;&gt;&gt; fa = db.functional_alltypes\n&gt;&gt;&gt; expr = fa.double_col.sum()\n&gt;&gt;&gt; expr.to_pandas()\n331785.00000000006\nFor longer-running queries, Ibis will attempt to cancel the query in progress if an interrupt is received."
  },
  {
    "objectID": "backends/impala.html#creating-tables",
    "href": "backends/impala.html#creating-tables",
    "title": "Impala",
    "section": "Creating tables",
    "text": "Creating tables\nThere are several ways to create new Impala tables:\n\nFrom an Ibis table expression\nEmpty, from a declared schema\nEmpty and partitioned\n\nIn all cases, you should use the create_table method either on the top-level client connection or a database object.\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False, external=False, format='parquet', location=None, partition=None, like_parquet=None)\nCreate a new table in Impala using an Ibis table expression.\nThis is currently designed for tables whose data is stored in HDFS.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\nobj\nibis.ibis.Table | None\nIf passed, creates table from select statement results\nNone\n\n\nschema\n\nMutually exclusive with obj, creates an empty table with a particular schema\nNone\n\n\ndatabase\n\nDatabase name\nNone\n\n\ntemp\nbool | None\nWhether a table is temporary\nNone\n\n\noverwrite\nbool\nDo not create table if table with indicated name already exists\nFalse\n\n\nexternal\nbool\nCreate an external table; Impala will not delete the underlying data when the table is dropped\nFalse\n\n\nformat\n\nFile format\n'parquet'\n\n\nlocation\n\nSpecify the directory location where Impala reads and writes files for the table\nNone\n\n\npartition\n\nMust pass a schema to use this. Cannot partition from an expression.\nNone\n\n\nlike_parquet\n\nCan specify instead of a schema\nNone\n\n\n\n\n\n\nCreating tables from a table expression\nIf you pass an Ibis expression to create_table, Ibis issues a CREATE TABLE ... AS SELECT (CTAS) statement:\n&gt;&gt;&gt; table = db.table('functional_alltypes')\n&gt;&gt;&gt; expr = table.group_by('string_col').size()\n&gt;&gt;&gt; db.create_table('string_freqs', expr, format='parquet')\n\n&gt;&gt;&gt; freqs = db.table('string_freqs')\n&gt;&gt;&gt; freqs.to_pandas()\n  string_col  count\n0          9    730\n1          3    730\n2          6    730\n3          4    730\n4          1    730\n5          8    730\n6          2    730\n7          7    730\n8          5    730\n9          0    730\n\n&gt;&gt;&gt; files = freqs.files()\n&gt;&gt;&gt; files\n                                                Path  Size Partition\n0  hdfs://impala:8020/user/hive/warehouse/ibis_te...  584B\n\n&gt;&gt;&gt; freqs.drop()\nYou can also choose to create an empty table and use insert (see below).\n\n\nCreating an empty table\nTo create an empty table, you must declare an Ibis schema that will be translated to the appropriate Impala schema and data types.\nAs Ibis types are simplified compared with Impala types, this may expand in the future to include a more fine-grained schema declaration.\nYou can use the create_table method either on a database or client object.\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema)\nBy default, this stores the data files in the database default location. You can force a particular path with the location option.\nfrom getpass import getuser\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\nlocation = '/home/{}/new-table-data'.format(getuser())\ndb.create_table(name, schema=schema, location=location)\nIf the schema matches a known table schema, you can always use the schema method to get a schema object:\n&gt;&gt;&gt; t = db.table('functional_alltypes')\n&gt;&gt;&gt; t.schema()\nibis.Schema {\n  id               int32\n  bool_col         boolean\n  tinyint_col      int8\n  smallint_col     int16\n  int_col          int32\n  bigint_col       int64\n  float_col        float32\n  double_col       float64\n  date_string_col  string\n  string_col       string\n  timestamp_col    timestamp\n  year             int32\n  month            int32\n}\n\n\nCreating a partitioned table\nTo create an empty partitioned table, include a list of columns to be used as the partition keys.\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema, partition=['year', 'month'])"
  },
  {
    "objectID": "backends/impala.html#partitioned-tables",
    "href": "backends/impala.html#partitioned-tables",
    "title": "Impala",
    "section": "Partitioned tables",
    "text": "Partitioned tables\nIbis enables you to manage partitioned tables in various ways. Since each partition behaves as its own \"subtable\" sharing a common schema, each partition can have its own file format, directory path, serialization properties, and so forth.\nThere are a handful of table methods for adding and removing partitions and getting information about the partition schema and any existing partition data:\n\nadd_partition\nadd_partition(self, spec, location=None)\nAdd a new table partition.\nThis API creates any necessary new directories in HDFS.\nPartition parameters can be set in a single DDL statement or you can use alter_partition to set them after the fact.\n\n\ndrop_partition\ndrop_partition(self, spec)\nDrop an existing table partition.\n\n\nis_partitioned\nTrue if the table is partitioned.\n\n\npartition_schema\npartition_schema(self)\nReturn the schema for the partition columns.\n\n\npartitions\npartitions(self)\nReturn information about the table’s partitions.\nRaises an exception if the table is not partitioned.\nTo address a specific partition in any method that is partition specific, you can either use a dict with the partition key names and values, or pass a list of the partition values:\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema, partition=['year', 'month'])\n\ntable = db.table(name)\n\ntable.add_partition({'year': 2007, 'month', 4})\ntable.add_partition([2007, 5])\ntable.add_partition([2007, 6])\n\ntable.drop_partition([2007, 6])\nWe’ll cover partition metadata management and data loading below."
  },
  {
    "objectID": "backends/impala.html#inserting-data-into-tables",
    "href": "backends/impala.html#inserting-data-into-tables",
    "title": "Impala",
    "section": "Inserting data into tables",
    "text": "Inserting data into tables\nIf the schemas are compatible, you can insert into a table directly from an Ibis table expression:\n&gt;&gt;&gt; t = db.functional_alltypes\n&gt;&gt;&gt; db.create_table('insert_test', schema=t.schema())\n&gt;&gt;&gt; target = db.table('insert_test')\n\n&gt;&gt;&gt; target.insert(t[:3])\n&gt;&gt;&gt; target.insert(t[:3])\n&gt;&gt;&gt; target.insert(t[:3])\n\n&gt;&gt;&gt; target.to_pandas()\n     id  bool_col  tinyint_col  ...           timestamp_col  year  month\n0  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n1  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n2  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n3  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n4  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n5  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n6  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n7  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n8  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n\n[9 rows x 13 columns]\n\n&gt;&gt;&gt; target.drop()\nIf the table is partitioned, you must indicate the partition you are inserting into:\npart = {'year': 2007, 'month': 4}\ntable.insert(expr, partition=part)"
  },
  {
    "objectID": "backends/impala.html#managing-table-metadata",
    "href": "backends/impala.html#managing-table-metadata",
    "title": "Impala",
    "section": "Managing table metadata",
    "text": "Managing table metadata\nIbis has functions that wrap many of the DDL commands for Impala table metadata.\n\nDetailed table metadata: DESCRIBE FORMATTED\nTo get a handy wrangled version of DESCRIBE FORMATTED use the metadata method.\n\nmetadata\nmetadata(self)\nReturn results of DESCRIBE FORMATTED statement.\n&gt;&gt;&gt; t = client.table('ibis_testing.functional_alltypes')\n&gt;&gt;&gt; meta = t.metadata()\n&gt;&gt;&gt; meta\n&lt;class 'ibis.backends.impala.metadata.TableMetadata'&gt;\n{'info': {'CreateTime': datetime.datetime(2021, 1, 14, 21, 23, 8),\n          'Database': 'ibis_testing',\n          'LastAccessTime': 'UNKNOWN',\n          'Location': 'hdfs://impala:8020/__ibis/ibis-testing-data/parquet/functional_alltypes',\n          'Owner': 'root',\n          'Protect Mode': 'None',\n          'Retention': 0,\n          'Table Parameters': {'COLUMN_STATS_ACCURATE': False,\n                               'EXTERNAL': True,\n                               'STATS_GENERATED_VIA_STATS_TASK': True,\n                               'numFiles': 3,\n                               'numRows': 7300,\n                               'rawDataSize': '-1',\n                               'totalSize': 106278,\n                               'transient_lastDdlTime': datetime.datetime(2021, 1, 14, 21, 23, 17)},\n          'Table Type': 'EXTERNAL_TABLE'},\n 'schema': [('id', 'int'),\n            ('bool_col', 'boolean'),\n            ('tinyint_col', 'tinyint'),\n            ('smallint_col', 'smallint'),\n            ('int_col', 'int'),\n            ('bigint_col', 'bigint'),\n            ('float_col', 'float'),\n            ('double_col', 'double'),\n            ('date_string_col', 'string'),\n            ('string_col', 'string'),\n            ('timestamp_col', 'timestamp'),\n            ('year', 'int'),\n            ('month', 'int')],\n 'storage info': {'Bucket Columns': '[]',\n                  'Compressed': False,\n                  'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n                  'Num Buckets': 0,\n                  'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n                  'SerDe Library': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n                  'Sort Columns': '[]'}}\n\n&gt;&gt;&gt; meta.location\n'hdfs://impala:8020/__ibis/ibis-testing-data/parquet/functional_alltypes'\n\n&gt;&gt;&gt; meta.create_time\ndatetime.datetime(2021, 1, 14, 21, 23, 8)\nThe files function is also available to see all of the physical HDFS data files backing a table:\n\n\nfiles\nfiles(self)\nReturn results of SHOW FILES statement.\n&gt;&gt;&gt; ss = c.table('tpcds_parquet.store_sales')\n\n&gt;&gt;&gt; ss.files()[:5]\n                                                path      size  \\\n0  hdfs://localhost:20500/test-warehouse/tpcds.st...  160.61KB\n1  hdfs://localhost:20500/test-warehouse/tpcds.st...  123.88KB\n2  hdfs://localhost:20500/test-warehouse/tpcds.st...  139.28KB\n3  hdfs://localhost:20500/test-warehouse/tpcds.st...  139.60KB\n4  hdfs://localhost:20500/test-warehouse/tpcds.st...   62.84KB\n\n                 partition\n0  ss_sold_date_sk=2451803\n1  ss_sold_date_sk=2451819\n2  ss_sold_date_sk=2451772\n3  ss_sold_date_sk=2451789\n4  ss_sold_date_sk=2451741\n\n\n\nModifying table metadata\nFor unpartitioned tables, you can use the alter method to change its location, file format, and other properties. For partitioned tables, to change partition-specific metadata use alter_partition.\n\nalter\nalter(self, location=None, format=None, tbl_properties=None, serde_properties=None)\nChange settings and parameters of the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlocation\n\nFor partitioned tables, you may want the alter_partition function\nNone\n\n\nformat\n\nTable format\nNone\n\n\ntbl_properties\n\nTable properties\nNone\n\n\nserde_properties\n\nSerialization/deserialization properties\nNone\n\n\n\n\n\n\nalter_partition\nalter_partition(self, spec, location=None, format=None, tbl_properties=None, serde_properties=None)\nChange settings and parameters of an existing partition.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspec\n\nThe partition keys for the partition being modified\nrequired\n\n\nlocation\n\nLocation of the partition\nNone\n\n\nformat\n\nTable format\nNone\n\n\ntbl_properties\n\nTable properties\nNone\n\n\nserde_properties\n\nSerialization/deserialization properties\nNone\n\n\n\nFor example, if you wanted to \"point\" an existing table at a directory of CSV files, you could run the following command:\nfrom getpass import getuser\n\ncsv_props = {\n    'serialization.format': ',',\n    'field.delim': ',',\n}\ndata_dir = '/home/{}/my-csv-files'.format(getuser())\n\ntable.alter(location=data_dir, format='text', serde_properties=csv_props)\nIf the table is partitioned, you can modify only the properties of a particular partition:\ntable.alter_partition(\n    {'year': 2007, 'month': 5},\n    location=data_dir,\n    format='text',\n    serde_properties=csv_props\n)"
  },
  {
    "objectID": "backends/impala.html#table-statistics",
    "href": "backends/impala.html#table-statistics",
    "title": "Impala",
    "section": "Table statistics",
    "text": "Table statistics\n\nComputing table and partition statistics\n\ncompute_stats\ncompute_stats(self, incremental=False)\nInvoke Impala COMPUTE STATS command on the table.\nImpala-backed physical tables have a method compute_stats that computes table, column, and partition-level statistics to assist with query planning and optimization. It is standard practice to invoke this after creating a table or loading new data:\ntable.compute_stats()\nIf you are using a recent version of Impala, you can also access the COMPUTE INCREMENTAL STATS DDL command:\ntable.compute_stats(incremental=True)\n\n\n\nSeeing table and column statistics\n\ncolumn_stats\ncolumn_stats(self)\nReturn results of SHOW COLUMN STATS.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\nColumn statistics\n\n\n\n\n\n\nstats\nstats(self)\nReturn results of SHOW TABLE STATS.\nIf not partitioned, contains only one row.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\nTable statistics\n\n\n\nThe compute_stats and stats functions return the results of SHOW COLUMN STATS and SHOW TABLE STATS, respectively, and their output will depend, of course, on the last COMPUTE STATS call.\n&gt;&gt;&gt; ss = c.table('tpcds_parquet.store_sales')\n&gt;&gt;&gt; ss.compute_stats(incremental=True)\n&gt;&gt;&gt; stats = ss.stats()\n&gt;&gt;&gt; stats[:5]\n  ss_sold_date_sk  #Rows  #Files     Size Bytes Cached Cache Replication  \\\n0         2450829   1071       1  78.34KB   NOT CACHED        NOT CACHED\n1         2450846    839       1  61.83KB   NOT CACHED        NOT CACHED\n2         2450860    747       1  54.86KB   NOT CACHED        NOT CACHED\n3         2450874    922       1  66.74KB   NOT CACHED        NOT CACHED\n4         2450888    856       1  63.33KB   NOT CACHED        NOT CACHED\n\n    Format Incremental stats  \\\n0  PARQUET              true\n1  PARQUET              true\n2  PARQUET              true\n3  PARQUET              true\n4  PARQUET              true\n\n                                            Location\n0  hdfs://localhost:20500/test-warehouse/tpcds.st...\n1  hdfs://localhost:20500/test-warehouse/tpcds.st...\n2  hdfs://localhost:20500/test-warehouse/tpcds.st...\n3  hdfs://localhost:20500/test-warehouse/tpcds.st...\n4  hdfs://localhost:20500/test-warehouse/tpcds.st...\n\n&gt;&gt;&gt; cstats = ss.column_stats()\n&gt;&gt;&gt; cstats\n                   Column          Type  #Distinct Values  #Nulls  Max Size  Avg Size\n0         ss_sold_time_sk        BIGINT             13879      -1       NaN         8\n1              ss_item_sk        BIGINT             17925      -1       NaN         8\n2          ss_customer_sk        BIGINT             15207      -1       NaN         8\n3             ss_cdemo_sk        BIGINT             16968      -1       NaN         8\n4             ss_hdemo_sk        BIGINT              6220      -1       NaN         8\n5              ss_addr_sk        BIGINT             14077      -1       NaN         8\n6             ss_store_sk        BIGINT                 6      -1       NaN         8\n7             ss_promo_sk        BIGINT               298      -1       NaN         8\n8        ss_ticket_number           INT             15006      -1       NaN         4\n9             ss_quantity           INT                99      -1       NaN         4\n10      ss_wholesale_cost  DECIMAL(7,2)             10196      -1       NaN         4\n11          ss_list_price  DECIMAL(7,2)             19393      -1       NaN         4\n12         ss_sales_price  DECIMAL(7,2)             15594      -1       NaN         4\n13    ss_ext_discount_amt  DECIMAL(7,2)             29772      -1       NaN         4\n14     ss_ext_sales_price  DECIMAL(7,2)            102758      -1       NaN         4\n15  ss_ext_wholesale_cost  DECIMAL(7,2)            125448      -1       NaN         4\n16      ss_ext_list_price  DECIMAL(7,2)            141419      -1       NaN         4\n17             ss_ext_tax  DECIMAL(7,2)             33837      -1       NaN         4\n18          ss_coupon_amt  DECIMAL(7,2)             29772      -1       NaN         4\n19            ss_net_paid  DECIMAL(7,2)            109981      -1       NaN         4\n20    ss_net_paid_inc_tax  DECIMAL(7,2)            132286      -1       NaN         4\n21          ss_net_profit  DECIMAL(7,2)            122436      -1       NaN         4\n22        ss_sold_date_sk        BIGINT               120       0       NaN         8\n\n\n\n\nREFRESH and INVALIDATE METADATA\nThese DDL commands are available as table-level and client-level methods:\n\ninvalidate_metadata\ninvalidate_metadata(self, name=None, database=None)\nIssue an INVALIDATE METADATA command.\nOptionally this applies to a specific table. See Impala documentation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nTable name. Can be fully qualified (with database)\nNone\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\n\ninvalidate_metadata\ninvalidate_metadata(self)\n\n\nrefresh\nrefresh(self)\nYou can invalidate the cached metadata for a single table or for all tables using invalidate_metadata, and similarly invoke REFRESH db_name.table_name using the refresh method.\nclient.invalidate_metadata()\n\ntable = db.table(table_name)\ntable.invalidate_metadata()\n\ntable.refresh()\nThese methods are often used in conjunction with the LOAD DATA commands and COMPUTE STATS. See the Impala documentation for full details."
  },
  {
    "objectID": "backends/impala.html#parquet-and-other-session-options",
    "href": "backends/impala.html#parquet-and-other-session-options",
    "title": "Impala",
    "section": "Parquet and other session options",
    "text": "Parquet and other session options\nIbis gives you access to Impala session-level variables that affect query execution:\n\ndisable_codegen\ndisable_codegen(self, disabled=True)\nTurn off or on LLVM codegen in Impala query execution.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndisabled\n\nTo disable codegen, pass with no argument or True. To enable codegen, pass False.\nTrue\n\n\n\n\n\n\nget_options\nget_options(self)\nReturn current query options for the Impala session.\n\n\nset_options\nset_options(self, options)\n\n\nset_compression_codec\nset_compression_codec(self, codec)\nFor example:\n&gt;&gt;&gt; client.get_options()\n{'ABORT_ON_ERROR': '0',\n 'APPX_COUNT_DISTINCT': '0',\n 'BUFFER_POOL_LIMIT': '',\n 'COMPRESSION_CODEC': '',\n 'COMPUTE_STATS_MIN_SAMPLE_SIZE': '1073741824',\n 'DEFAULT_JOIN_DISTRIBUTION_MODE': '0',\n 'DEFAULT_SPILLABLE_BUFFER_SIZE': '2097152',\n 'DISABLE_CODEGEN': '0',\n 'DISABLE_CODEGEN_ROWS_THRESHOLD': '50000',\n 'DISABLE_ROW_RUNTIME_FILTERING': '0',\n 'DISABLE_STREAMING_PREAGGREGATIONS': '0',\n 'DISABLE_UNSAFE_SPILLS': '0',\n 'ENABLE_EXPR_REWRITES': '1',\n 'EXEC_SINGLE_NODE_ROWS_THRESHOLD': '100',\n 'EXEC_TIME_LIMIT_S': '0',\n 'EXPLAIN_LEVEL': '1',\n 'HBASE_CACHE_BLOCKS': '0',\n 'HBASE_CACHING': '0',\n 'IDLE_SESSION_TIMEOUT': '0',\n 'MAX_ERRORS': '100',\n 'MAX_NUM_RUNTIME_FILTERS': '10',\n 'MAX_ROW_SIZE': '524288',\n 'MEM_LIMIT': '0',\n 'MIN_SPILLABLE_BUFFER_SIZE': '65536',\n 'MT_DOP': '',\n 'NUM_SCANNER_THREADS': '0',\n 'OPTIMIZE_PARTITION_KEY_SCANS': '0',\n 'PARQUET_ANNOTATE_STRINGS_UTF8': '0',\n 'PARQUET_ARRAY_RESOLUTION': '2',\n 'PARQUET_DICTIONARY_FILTERING': '1',\n 'PARQUET_FALLBACK_SCHEMA_RESOLUTION': '0',\n 'PARQUET_FILE_SIZE': '0',\n 'PARQUET_READ_STATISTICS': '1',\n 'PREFETCH_MODE': '1',\n 'QUERY_TIMEOUT_S': '0',\n 'REPLICA_PREFERENCE': '0',\n 'REQUEST_POOL': '',\n 'RUNTIME_BLOOM_FILTER_SIZE': '1048576',\n 'RUNTIME_FILTER_MAX_SIZE': '16777216',\n 'RUNTIME_FILTER_MIN_SIZE': '1048576',\n 'RUNTIME_FILTER_MODE': '2',\n 'RUNTIME_FILTER_WAIT_TIME_MS': '0',\n 'S3_SKIP_INSERT_STAGING': '1',\n 'SCHEDULE_RANDOM_REPLICA': '0',\n 'SCRATCH_LIMIT': '-1',\n 'SEQ_COMPRESSION_MODE': '',\n 'SYNC_DDL': '0'}\nTo enable Snappy compression for Parquet files, you could do either of:\n&gt;&gt;&gt; client.set_options({'COMPRESSION_CODEC': 'snappy'})\n&gt;&gt;&gt; client.set_compression_codec('snappy')\n\n&gt;&gt;&gt; client.get_options()['COMPRESSION_CODEC']\n'SNAPPY'"
  },
  {
    "objectID": "backends/impala.html#ingesting-data-from-pandas",
    "href": "backends/impala.html#ingesting-data-from-pandas",
    "title": "Impala",
    "section": "Ingesting data from pandas",
    "text": "Ingesting data from pandas\nOverall interoperability between the Hadoop / Spark ecosystems and pandas / the PyData stack is poor, but it will improve in time (this is a major part of the Ibis roadmap).\nIbis’s Impala tools currently interoperate with pandas in these ways:\n\nIbis expressions return pandas objects (i.e. DataFrame or Series) for non-scalar expressions when calling their to_pandas method\nThe create_table and insert methods can accept pandas objects. This includes inserting into partitioned tables. It currently uses CSV as the ingest route.\n\nFor example:\n&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; data = pd.DataFrame({'foo': [1, 2, 3, 4], 'bar': ['a', 'b', 'c', 'd']})\n\n&gt;&gt;&gt; db.create_table('pandas_table', data)\n&gt;&gt;&gt; t = db.pandas_table\n&gt;&gt;&gt; t.to_pandas()\n  bar  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\n&gt;&gt;&gt; t.drop()\n\n&gt;&gt;&gt; db.create_table('empty_for_insert', schema=t.schema())\n\n&gt;&gt;&gt; to_insert = db.empty_for_insert\n&gt;&gt;&gt; to_insert.insert(data)\n&gt;&gt;&gt; to_insert.to_pandas()\n  bar  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\n&gt;&gt;&gt; to_insert.drop()\n&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; data = pd.DataFrame({'foo': [1, 2, 3, 4], 'bar': ['a', 'b', 'c', 'd']})\n\n&gt;&gt;&gt; db.create_table('pandas_table', data)\n&gt;&gt;&gt; t = db.pandas_table\n&gt;&gt;&gt; t.to_pandas()\n   foo bar\n0    1   a\n1    2   b\n2    3   c\n3    4   d\n\n&gt;&gt;&gt; t.drop()\n&gt;&gt;&gt; db.create_table('empty_for_insert', schema=t.schema())\n&gt;&gt;&gt; to_insert = db.empty_for_insert\n&gt;&gt;&gt; to_insert.insert(data)\n&gt;&gt;&gt; to_insert.to_pandas()\n   foo bar\n0    1   a\n1    2   b\n2    3   c\n3    4   d\n\n&gt;&gt;&gt; to_insert.drop()"
  },
  {
    "objectID": "backends/impala.html#uploading-downloading-data-from-hdfs",
    "href": "backends/impala.html#uploading-downloading-data-from-hdfs",
    "title": "Impala",
    "section": "Uploading / downloading data from HDFS",
    "text": "Uploading / downloading data from HDFS\nIf you’ve set up an HDFS connection, you can use the Ibis HDFS interface to look through your data and read and write files to and from HDFS:\n&gt;&gt;&gt; hdfs = con.hdfs\n&gt;&gt;&gt; hdfs.ls('/__ibis/ibis-testing-data')\n['README.md',\n 'avro',\n 'awards_players.csv',\n 'batting.csv',\n 'csv',\n 'diamonds.csv',\n 'functional_alltypes.csv',\n 'functional_alltypes.parquet',\n 'geo.csv',\n 'ibis_testing.db',\n 'parquet',\n 'struct_table.avro',\n 'udf']\n&gt;&gt;&gt; hdfs.ls('/__ibis/ibis-testing-data/parquet')\n['functional_alltypes',\n 'tpch_customer',\n 'tpch_lineitem',\n 'tpch_nation',\n 'tpch_orders',\n 'tpch_part',\n 'tpch_partsupp',\n 'tpch_region',\n 'tpch_supplier']\nSuppose we wanted to download /__ibis/ibis-testing-data/parquet/functional_alltypes, which is a directory. We need only do:\n$ rm -rf parquet_dir/\n&gt;&gt;&gt; hdfs.get('/__ibis/ibis-testing-data/parquet/functional_alltypes',\n...          'parquet_dir',\n...           recursive=True)\n'/ibis/docs/source/tutorial/parquet_dir'\nNow we have that directory locally:\n$ ls parquet_dir/\n9a41de519352ab07-4e76bc4d9fb5a789_1624886651_data.0.parq\n9a41de519352ab07-4e76bc4d9fb5a78a_778826485_data.0.parq\n9a41de519352ab07-4e76bc4d9fb5a78b_1277612014_data.0.parq\nFiles and directories can be written to HDFS just as easily using put:\n&gt;&gt;&gt; path = '/__ibis/dir-write-example'\n&gt;&gt;&gt; hdfs.rm(path, recursive=True)\n&gt;&gt;&gt; hdfs.put(path, 'parquet_dir', recursive=True)\n&gt;&gt;&gt; hdfs.ls('/__ibis/dir-write-example')\n['9a41de519352ab07-4e76bc4d9fb5a789_1624886651_data.0.parq',\n '9a41de519352ab07-4e76bc4d9fb5a78a_778826485_data.0.parq',\n '9a41de519352ab07-4e76bc4d9fb5a78b_1277612014_data.0.parq']\nDelete files and directories with rm:\n&gt;&gt;&gt; hdfs.rm('/__ibis/dir-write-example', recursive=True)\nrm -rf parquet_dir/"
  },
  {
    "objectID": "backends/impala.html#queries-on-parquet-avro-and-delimited-files-in-hdfs",
    "href": "backends/impala.html#queries-on-parquet-avro-and-delimited-files-in-hdfs",
    "title": "Impala",
    "section": "Queries on Parquet, Avro, and Delimited files in HDFS",
    "text": "Queries on Parquet, Avro, and Delimited files in HDFS\nIbis can easily create temporary or persistent Impala tables that reference data in the following formats:\n\nParquet (parquet_file)\nAvro (avro_file)\nDelimited text formats (CSV, TSV, etc.) (delimited_file)\n\nParquet is the easiest because the schema can be read from the data files:\n&gt;&gt;&gt; path = '/__ibis/ibis-testing-data/parquet/tpch_lineitem'\n&gt;&gt;&gt; lineitem = con.parquet_file(path)\n&gt;&gt;&gt; lineitem.limit(2)\n   l_orderkey  l_partkey  l_suppkey  l_linenumber l_quantity l_extendedprice  \\\n0           1     155190       7706             1      17.00        21168.23\n1           1      67310       7311             2      36.00        45983.16\n\n  l_discount l_tax l_returnflag l_linestatus  l_shipdate l_commitdate  \\\n0       0.04  0.02            N            O  1996-03-13   1996-02-12\n1       0.09  0.06            N            O  1996-04-12   1996-02-28\n\n  l_receiptdate     l_shipinstruct l_shipmode  \\\n0    1996-03-22  DELIVER IN PERSON      TRUCK\n1    1996-04-20   TAKE BACK RETURN       MAIL\n\n                            l_comment\n0             egular courts above the\n1  ly final dependencies: slyly bold\n&gt;&gt;&gt; lineitem.l_extendedprice.sum()\nDecimal('229577310901.20')\nIf you want to query a Parquet file and also create a table in Impala that remains after your session, you can pass more information to parquet_file:\n&gt;&gt;&gt; table = con.parquet_file(path, name='my_parquet_table',\n...                          database='ibis_testing',\n...                          persist=True)\n&gt;&gt;&gt; table.l_extendedprice.sum()\nDecimal('229577310901.20')\n&gt;&gt;&gt; con.table('my_parquet_table').l_extendedprice.sum()\nDecimal('229577310901.20')\n&gt;&gt;&gt; con.drop_table('my_parquet_table')\nTo query delimited files, you need to write down an Ibis schema. At some point we’d like to build some helper tools that will infer the schema for you, all in good time.\nThere’s some CSV files in the test folder, so let’s use those:\n&gt;&gt;&gt; hdfs.get('/__ibis/ibis-testing-data/csv', 'csv-files', recursive=True)\n'/ibis/docs/source/tutorial/csv-files'\n$ cat csv-files/0.csv\n63IEbRheTh,0.679388707915,6\nmG4hlqnjeG,2.80710565922,15\nJTPdX9SZH5,-0.155126406372,55\n2jcl6FypOl,1.03787834032,21\nk3TbJLaadQ,-1.40190801103,23\nrP5J4xvinM,-0.442092712869,22\nWniUylixYt,-0.863748033806,27\nznsDuKOB1n,-0.566029637098,47\n4SRP9jlo1M,0.331460412318,88\nKsfjPyDf5e,-0.578930506363,70\n$ rm -rf csv-files/\nThe schema here is pretty simple (see ibis.schema for more):\n&gt;&gt;&gt; schema = ibis.schema(dict(foo='string', bar='double', baz='int32'))\n&gt;&gt;&gt; table = con.delimited_file('/__ibis/ibis-testing-data/csv', schema)\n&gt;&gt;&gt; table.limit(10)\n          foo       bar  baz\n0  63IEbRheTh  0.679389    6\n1  mG4hlqnjeG  2.807106   15\n2  JTPdX9SZH5 -0.155126   55\n3  2jcl6FypOl  1.037878   21\n4  k3TbJLaadQ -1.401908   23\n5  rP5J4xvinM -0.442093   22\n6  WniUylixYt -0.863748   27\n7  znsDuKOB1n -0.566030   47\n8  4SRP9jlo1M  0.331460   88\n9  KsfjPyDf5e -0.578931   70\n&gt;&gt;&gt; table.bar.summary()\n   count  nulls       min       max       sum    mean  approx_nunique\n0    100      0 -1.401908  2.807106  8.479978  0.0848              10\nFor functions like parquet_file and delimited_file, an HDFS directory must be passed and the directory must contain files all having the same schema.\nIf you have Avro data, you can query it too if you have the full avro schema:\n&gt;&gt;&gt; avro_schema = {\n...     \"fields\": [\n...         {\"type\": [\"int\", \"null\"], \"name\": \"R_REGIONKEY\"},\n...         {\"type\": [\"string\", \"null\"], \"name\": \"R_NAME\"},\n...         {\"type\": [\"string\", \"null\"], \"name\": \"R_COMMENT\"}],\n...     \"type\": \"record\",\n...     \"name\": \"a\"\n... }\n\n&gt;&gt;&gt; path = '/__ibis/ibis-testing-data/avro/tpch.region'\n\n&gt;&gt;&gt; hdfs.mkdir(path, create_parents=True)\n&gt;&gt;&gt; table = con.avro_file(path, avro_schema)\n&gt;&gt;&gt; table\nEmpty DataFrame\nColumns: [r_regionkey, r_name, r_comment]\nIndex: []"
  },
  {
    "objectID": "backends/impala.html#other-helper-functions-for-interacting-with-the-database",
    "href": "backends/impala.html#other-helper-functions-for-interacting-with-the-database",
    "title": "Impala",
    "section": "Other helper functions for interacting with the database",
    "text": "Other helper functions for interacting with the database\nWe’re adding a growing list of useful utility functions for interacting with an Impala cluster on the client object. The idea is that you should be able to do any database-admin-type work with Ibis and not have to switch over to the Impala SQL shell. Any ways we can make this more pleasant, please let us know.\nHere’s some of the features, which we’ll give examples for:\n\nListing and searching for available databases and tables\nCreating and dropping databases\nGetting table schemas\n\n&gt;&gt;&gt; con.list_databases(like='ibis*')\n['ibis_testing', 'ibis_testing_tmp_db']\n&gt;&gt;&gt; con.list_tables(database='ibis_testing', like='tpch*')\n['tpch_customer',\n 'tpch_lineitem',\n 'tpch_nation',\n 'tpch_orders',\n 'tpch_part',\n 'tpch_partsupp',\n 'tpch_region',\n 'tpch_region_avro',\n 'tpch_supplier']\n&gt;&gt;&gt; schema = con.get_schema('functional_alltypes')\n&gt;&gt;&gt; schema\nibis.Schema {\n  id               int32\n  bool_col         boolean\n  tinyint_col      int8\n  smallint_col     int16\n  int_col          int32\n  bigint_col       int64\n  float_col        float32\n  double_col       float64\n  date_string_col  string\n  string_col       string\n  timestamp_col    timestamp\n  year             int32\n  month            int32\n}\nDatabases can be created, too, and you can set the storage path in HDFS you want for the data files\n&gt;&gt;&gt; db = 'ibis_testing2'\n&gt;&gt;&gt; con.create_database(db, path='/__ibis/my-test-database', force=True)\n\n&gt;&gt;&gt; # you may or may not have to give the impala user write and execute permissions to '/__ibis/my-test-database'\n&gt;&gt;&gt; hdfs.chmod('/__ibis/my-test-database', 0o777)\n&gt;&gt;&gt; con.create_table('example_table', con.table('functional_alltypes'),\n...                  database=db, force=True)\nHopefully, there will be data files in the indicated spot in HDFS:\n&gt;&gt;&gt; hdfs.ls('/__ibis/my-test-database')\n['example_table']\nTo drop a database, including all tables in it, you can use drop_database with force=True:\n&gt;&gt;&gt; con.drop_database(db, force=True)"
  },
  {
    "objectID": "backends/impala.html#faster-queries-on-small-data-in-impala",
    "href": "backends/impala.html#faster-queries-on-small-data-in-impala",
    "title": "Impala",
    "section": "Faster queries on small data in Impala",
    "text": "Faster queries on small data in Impala\nSince Impala internally uses LLVM to compile parts of queries (aka “codegen”) to make them faster on large data sets there is a certain amount of overhead with running many kinds of queries, even on small datasets. You can disable LLVM code generation when using Ibis, which may significantly speed up queries on smaller datasets:\n&gt;&gt;&gt; from numpy.random import rand\n&gt;&gt;&gt; con.disable_codegen()\n&gt;&gt;&gt; t = con.table('ibis_testing.functional_alltypes')\n$ time python -c \"(t.double_col + rand()).sum().to_pandas()\"\n27.7 ms ± 996 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n# Turn codegen back on\ncon.disable_codegen(False)\n$ time python -c \"(t.double_col + rand()).sum().to_pandas()\"\n27 ms ± 1.62 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nIt’s important to remember that codegen is a fixed overhead and will significantly speed up queries on big data"
  },
  {
    "objectID": "backends/impala.html#user-defined-functions-udf",
    "href": "backends/impala.html#user-defined-functions-udf",
    "title": "Impala",
    "section": "User Defined functions (UDF)",
    "text": "User Defined functions (UDF)\nImpala currently supports user-defined scalar functions (known henceforth as UDFs) and aggregate functions (respectively UDAs) via a C++ extension API.\nInitial support for using C++ UDFs in Ibis came in version 0.4.0.\n\nUsing scalar functions (UDFs)\nLet’s take an example to illustrate how to make a C++ UDF available to Ibis. Here is a function that computes an approximate equality between floating point values:\n#include \"impala_udf/udf.h\"\n\n#include &lt;cctype&gt;\n#include &lt;cmath&gt;\n\nBooleanVal FuzzyEquals(FunctionContext* ctx, const DoubleVal& x, const DoubleVal& y) {\n  const double EPSILON = 0.000001f;\n  if (x.is_null || y.is_null) return BooleanVal::null();\n  double delta = fabs(x.val - y.val);\n  return BooleanVal(delta &lt; EPSILON);\n}\nYou can compile this to either a shared library (a .so file) or to LLVM bitcode with clang (a .ll file). Skipping that step for now (will add some more detailed instructions here later, promise).\nTo make this function callable, we use ibis.impala.wrap_udf:\nlibrary = '/ibis/udfs/udftest.ll'\ninputs = ['double', 'double']\noutput = 'boolean'\nsymbol = 'FuzzyEquals'\nudf_db = 'ibis_testing'\nudf_name = 'fuzzy_equals'\n\nfuzzy_equals = ibis.impala.wrap_udf(\n    library, inputs, output, symbol, name=udf_name\n)\nIn typical workflows, you will set up a UDF in Impala once then use it thenceforth. So the first time you do this, you need to create the UDF in Impala:\nclient.create_function(fuzzy_equals, database=udf_db)\nNow, we must register this function as a new Impala operation in Ibis. This must take place each time you load your Ibis session.\nfunc.register(fuzzy_equals.name, udf_db)\nThe object fuzzy_equals is callable and works with Ibis expressions:\n&gt;&gt;&gt; db = c.database('ibis_testing')\n\n&gt;&gt;&gt; t = db.functional_alltypes\n\n&gt;&gt;&gt; expr = fuzzy_equals(t.float_col, t.double_col / 10)\n\n&gt;&gt;&gt; expr.to_pandas()[:10]\n0     True\n1    False\n2    False\n3    False\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\nName: tmp, dtype: bool\nNote that the call to register on the UDF object must happen each time you use Ibis. If you have a lot of UDFs, I suggest you create a file with all of your wrapper declarations and user APIs that you load with your Ibis session to plug in all your own functions."
  },
  {
    "objectID": "backends/impala.html#working-with-secure-clusters-kerberos",
    "href": "backends/impala.html#working-with-secure-clusters-kerberos",
    "title": "Impala",
    "section": "Working with secure clusters (Kerberos)",
    "text": "Working with secure clusters (Kerberos)\nIbis is compatible with Hadoop clusters that are secured with Kerberos (as well as SSL and LDAP). Note that to enable this support, you’ll also need to install the kerberos package.\n$ pip install kerberos\nJust like the Impala shell and ODBC/JDBC connectors, Ibis connects to Impala through the HiveServer2 interface (using the impyla client). Therefore, the connection semantics are similar to the other access methods for working with secure clusters.\nSpecifically, after authenticating yourself against Kerberos (e.g., by issuing the appropriate kinit command), pass auth_mechanism='GSSAPI' or auth_mechanism='LDAP' (and set kerberos_service_name if necessary along with user and password if necessary) to the ibis.impala_connect(...) method when instantiating an ImpalaConnection. This method also takes arguments to configure SSL (use_ssl, ca_cert). See the documentation for the Impala shell for more details.\nIbis also includes functionality that communicates directly with HDFS, using the WebHDFS REST API. When calling ibis.impala.hdfs_connect(...), also pass auth_mechanism='GSSAPI' or auth_mechanism='LDAP', and ensure that you are connecting to the correct port, which may likely be an SSL-secured WebHDFS port. Also note that you can pass verify=False to avoid verifying SSL certificates (which may be helpful in testing). Ibis will assume https when connecting to a Kerberized cluster. Because some Ibis commands create HDFS directories as well as new Impala databases and/or tables, your user will require the necessary privileges."
  },
  {
    "objectID": "backends/impala.html#default-configuration-values-for-cdh-components",
    "href": "backends/impala.html#default-configuration-values-for-cdh-components",
    "title": "Impala",
    "section": "Default Configuration Values for CDH Components",
    "text": "Default Configuration Values for CDH Components\nCloudera CDH ships with HDFS, Impala, Hive and many other components. Sometimes it’s not obvious what default configuration values these tools are using or should be using.\nCheck out this link to see the default configuration values for every component of CDH."
  },
  {
    "objectID": "backends/oracle.html",
    "href": "backends/oracle.html",
    "title": "Oracle",
    "section": "",
    "text": "https://docs.oracle.com/database/oracle/oracle-database"
  },
  {
    "objectID": "backends/oracle.html#install",
    "href": "backends/oracle.html#install",
    "title": "Oracle",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Oracle backend:\n\npipcondamamba\n\n\nInstall with the oracle extra:\npip install 'ibis-framework[oracle]'\nAnd connect:\nimport ibis\n\n1con = ibis.oracle.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Oracle:\nconda install -c conda-forge ibis-oracle\nAnd connect:\nimport ibis\n\n1con = ibis.oracle.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Oracle:\nmamba install -c conda-forge ibis-oracle\nAnd connect:\nimport ibis\n\n1con = ibis.oracle.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/oracle.html#connect",
    "href": "backends/oracle.html#connect",
    "title": "Oracle",
    "section": "Connect",
    "text": "Connect\n\nibis.oracle.connect\ncon = ibis.oracle.connect(\n    user=\"username\",\n    password=\"password\",\n    host=\"hostname\",\n    port=1521,\n    database=\"database\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.oracle.connect is a thin wrapper around ibis.backends.oracle.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, *, user, password, host='localhost', port=1521, database=None, sid=None, service_name=None, dsn=None, **_)\nCreate an Ibis client using the passed connection parameters.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUsername\nrequired\n\n\npassword\nstr\nPassword\nrequired\n\n\nhost\nstr\nHostname\n'localhost'\n\n\nport\nint\nPort\n1521\n\n\ndatabase\nstr | None\nUsed as an Oracle service name if provided.\nNone\n\n\nsid\nstr | None\nUnique name of an Oracle Instance, used to construct a DSN if provided.\nNone\n\n\nservice_name\nstr | None\nOracle service name, used to construct a DSN if provided. Only one of database and service_name should be provided.\nNone\n\n\ndsn\nstr | None\nAn Oracle Data Source Name. If provided, overrides all other connection arguments except username and password.\nNone\n\n\n\n\n\n\n\nibis.connect URL format\nIn addition to ibis.oracle.connect, you can also connect to Oracle by passing a properly formatted Oracle connection URL to ibis.connect\ncon = ibis.connect(f\"oracle://{user}:{password}@{host}:{port}/{database}\")"
  },
  {
    "objectID": "backends/oracle.html#connecting-to-older-oracle-databases",
    "href": "backends/oracle.html#connecting-to-older-oracle-databases",
    "title": "Oracle",
    "section": "Connecting to older Oracle databases",
    "text": "Connecting to older Oracle databases\nibis uses the python-oracledb “thin client” to connect to Oracle databases. Because early versions of Oracle did not perform case-sensitive checks in passwords, some DBAs disable case sensitivity to avoid requiring users to update their passwords. If case-sensitive passwords are disabled, then Ibis will not be able to connect to the database.\nTo check if case-sensitivity is enforced you can run\nshow parameter sec_case_sensitive_logon;\nIf the returned value is FALSE then Ibis will not connect.\nFor more information, see this issue."
  },
  {
    "objectID": "backends/oracle.html#ibis.backends.oracle.Backend",
    "href": "backends/oracle.html#ibis.backends.oracle.Backend",
    "title": "Oracle",
    "section": "oracle.Backend",
    "text": "oracle.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList the tables in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\n\nA pattern to use for listing tables.\nNone\n\n\ndatabase\n\n(deprecated) The database to perform the list against.\nNone\n\n\nschema\n\nThe schema to perform the list against. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/clickhouse.html",
    "href": "backends/clickhouse.html",
    "title": "ClickHouse",
    "section": "",
    "text": "https://clickhouse.com"
  },
  {
    "objectID": "backends/clickhouse.html#install",
    "href": "backends/clickhouse.html#install",
    "title": "ClickHouse",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the ClickHouse backend:\n\npipcondamamba\n\n\nInstall with the clickhouse extra:\npip install 'ibis-framework[clickhouse]'\nAnd connect:\nimport ibis\n\n1con = ibis.clickhouse.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for ClickHouse:\nconda install -c conda-forge ibis-clickhouse\nAnd connect:\nimport ibis\n\n1con = ibis.clickhouse.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for ClickHouse:\nmamba install -c conda-forge ibis-clickhouse\nAnd connect:\nimport ibis\n\n1con = ibis.clickhouse.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/clickhouse.html#connect",
    "href": "backends/clickhouse.html#connect",
    "title": "ClickHouse",
    "section": "Connect",
    "text": "Connect\n\nibis.clickhouse.connect\ncon = ibis.clickhouse.connect(\n    user=\"username\",\n    password=\"password\",\n    host=\"hostname\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.clickhouse.connect is a thin wrapper around ibis.backends.clickhouse.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, host='localhost', port=None, database='default', user='default', password='', client_name='ibis', secure=None, compression=True, **kwargs)\nCreate a ClickHouse client for use with Ibis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhost\nstr\nHost name of the clickhouse server\n'localhost'\n\n\nport\nint | None\nClickHouse HTTP server’s port. If not passed, the value depends on whether secure is True or False.\nNone\n\n\ndatabase\nstr\nDefault database when executing queries\n'default'\n\n\nuser\nstr\nUser to authenticate with\n'default'\n\n\npassword\nstr\nPassword to authenticate with\n''\n\n\nclient_name\nstr\nName of client that will appear in clickhouse server logs\n'ibis'\n\n\nsecure\nbool | None\nWhether or not to use an authenticated endpoint\nNone\n\n\ncompression\nstr | bool\nThe kind of compression to use for requests. See https://clickhouse.com/docs/en/integrations/python#compression for more information.\nTrue\n\n\nkwargs\nAny\nClient specific keyword arguments\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; client = ibis.clickhouse.connect()\n&gt;&gt;&gt; client\n&lt;ibis.clickhouse.client.ClickhouseClient object at 0x...&gt;\n\n\n\n\nibis.connect URL format\nIn addition to ibis.clickhouse.connect, you can also connect to ClickHouse by passing a properly formatted ClickHouse connection URL to ibis.connect\ncon = ibis.connect(f\"clickhouse://{user}:{password}@{host}:{port}?secure={secure}\")"
  },
  {
    "objectID": "backends/clickhouse.html#clickhouse-playground",
    "href": "backends/clickhouse.html#clickhouse-playground",
    "title": "ClickHouse",
    "section": "ClickHouse playground",
    "text": "ClickHouse playground\nClickHouse provides a free playground with several datasets that you can connect to using Ibis:\n\nfrom ibis.interactive import *\n\ncon = ibis.connect(\"clickhouse://play:clickhouse@play.clickhouse.com:443?secure=True\")\nactors = con.table(\"actors\")\nactors\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ login            ┃ type    ┃ site_admin ┃ name         ┃ company             ┃ blog                ┃ location          ┃ email                     ┃ hireable ┃ bio                                       ┃ twitter_username ┃ public_repos ┃ public_gists ┃ followers ┃ following ┃ created_at          ┃ updated_at          ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ !string          │ !string │ !boolean   │ !string      │ !string             │ !string             │ !string           │ !string                   │ !boolean │ !string                                   │ !string          │ !int64       │ !int64       │ !int64    │ !int64    │ !timestamp(0)       │ !timestamp(0)       │\n├──────────────────┼─────────┼────────────┼──────────────┼─────────────────────┼─────────────────────┼───────────────────┼───────────────────────────┼──────────┼───────────────────────────────────────────┼──────────────────┼──────────────┼──────────────┼───────────┼───────────┼─────────────────────┼─────────────────────┤\n│ 0000Blaze        │ User    │ False      │ RohanChhetry │ Pulchowk Campus,IOE │ rohanchhetry.com.np │ Sanepa , Lalitpur │ ~                         │ True     │ ~                                         │ rohanchhetry9    │           56 │            0 │        57 │        83 │ 2019-02-24 02:31:21 │ 2023-07-30 11:30:14 │\n│ 007developforfun │ User    │ False      │ ~            │ ~                   │ ~                   │ ~                 │ ~                         │ False    │ ~                                         │ ~                │            0 │            0 │         0 │         0 │ 2015-08-07 11:28:01 │ 2022-08-12 08:45:30 │\n│ 00arthur00       │ User    │ False      │ Arthur       │ ~                   │ ~                   │ Beijing, China    │ yang_yapo@126.com         │ False    │ ~                                         │ ~                │           72 │            1 │         5 │        61 │ 2017-04-01 13:37:01 │ 2023-06-15 14:50:12 │\n│ 010001           │ User    │ False      │ ~            │ ~                   │ ~                   │ ~                 │ 1025394547@qq.com         │ False    │ ~                                         │ ~                │           15 │            0 │         1 │         1 │ 2015-02-05 03:11:59 │ 2023-03-17 06:07:01 │\n│ 01001101ilad     │ User    │ False      │ Milad        │ ~                   │ ~                   │ ~                 │ ~                         │ False    │ Programmer, Writer and Full-Time Learner. │ ~                │           10 │            0 │         0 │         0 │ 2016-10-31 19:12:55 │ 2023-07-24 11:43:03 │\n│ 010227leo        │ User    │ False      │ zucker       │ trip.com            │ ~                   │ Shanghai, China   │ ~                         │ False    │ ~                                         │ ~                │            2 │            1 │         7 │        11 │ 2012-01-11 06:23:15 │ 2023-07-24 03:35:26 │\n│ 010ric           │ User    │ False      │ Mario Turic  │ ~                   │ ~                   │ Munich            │ ~                         │ True     │ Enthusiast and Maker                      │ ~                │           19 │            0 │        23 │        68 │ 2017-10-27 14:00:07 │ 2023-08-04 18:44:35 │\n│ 01egen           │ User    │ False      │ ~            │ ~                   │ ~                   │ ~                 │ ~                         │ False    │ ~                                         │ ~                │            1 │            0 │         0 │         2 │ 2019-02-27 08:59:00 │ 2023-06-20 04:02:51 │\n│ 0400H            │ User    │ False      │ 0400H        │ ~                   │ ~                   │ Shanghai          │ git@0400h.cn              │ True     │ HPC & MLSys & PPML                        │ ~                │           17 │            0 │         3 │        42 │ 2015-12-20 17:38:00 │ 2023-07-21 11:28:22 │\n│ 0442A403         │ User    │ False      │ Damir Petrov │ ~                   │ ~                   │ Moscow            │ petrovdamir2235@gmail.com │ False    │ HSE student                               │ ~                │           19 │            0 │        15 │        30 │ 2016-11-05 18:59:38 │ 2023-06-22 06:08:50 │\n│ …                │ …       │ …          │ …            │ …                   │ …                   │ …                 │ …                         │ …        │ …                                         │ …                │            … │            … │         … │         … │ …                   │ …                   │\n└──────────────────┴─────────┴────────────┴──────────────┴─────────────────────┴─────────────────────┴───────────────────┴───────────────────────────┴──────────┴───────────────────────────────────────────┴──────────────────┴──────────────┴──────────────┴───────────┴───────────┴─────────────────────┴─────────────────────┘"
  },
  {
    "objectID": "backends/clickhouse.html#ibis.backends.clickhouse.Backend",
    "href": "backends/clickhouse.html#ibis.backends.clickhouse.Backend",
    "title": "ClickHouse",
    "section": "clickhouse.Backend",
    "text": "clickhouse.Backend\n\nclose\nclose(self)\nClose ClickHouse connection.\n\n\ncompile\ncompile(self, expr, limit=None, params=None, **kwargs)\nCompile an Ibis expression to a ClickHouse SQL string.\n\n\ncreate_database\ncreate_database(self, name, *, force=False, engine='Atomic')\nCreate a new database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new database.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database already exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False, engine='MergeTree', order_by=None, partition_by=None, sample_by=None, settings=None)\nCreate a table in a ClickHouse database.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to create\nrequired\n\n\nobj\npd.DataFrame | pa.Table | ir.Table | None\nOptional data to create the table with\nNone\n\n\nschema\nibis.Schema | None\nOptional names and types of the table\nNone\n\n\ndatabase\nstr | None\nDatabase to create the table in\nNone\n\n\ntemp\nbool\nCreate a temporary table. This is not yet supported, and exists for API compatibility.\nFalse\n\n\noverwrite\nbool\nWhether to overwrite the table\nFalse\n\n\nengine\nstr\nThe table engine to use. See ClickHouse’s CREATE TABLE documentation for specifics. Defaults to MergeTree with ORDER BY tuple() because MergeTree is the most feature-complete engine.\n'MergeTree'\n\n\norder_by\nIterable[str] | None\nString column names to order by. Required for some table engines like MergeTree.\nNone\n\n\npartition_by\nIterable[str] | None\nString column names to partition by\nNone\n\n\nsample_by\nstr | None\nString column names to sample by\nNone\n\n\nsettings\nMapping[str, Any] | None\nKey-value pairs of settings for table creation\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe new table\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nir.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, *, force=False)\nDrop a database with name name.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase to drop.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the table exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the table does not exist.\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\nDrop a view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the view to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the view does not exist.\nFalse\n\n\n\n\n\n\nexecute\nexecute(self, expr, limit='default', external_tables=None, **kwargs)\nExecute an expression.\n\n\nget_schema\nget_schema(self, table_name, database=None)\nReturn a Schema object for the indicated table and database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nMay not be fully qualified. Use database if you want to qualify the identifier.\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nsch.Schema\nIbis schema\n\n\n\n\n\n\nhas_operation\nhas_operation(cls, operation)\nReturn whether the backend implements support for operation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noperation\ntype[ops.Value]\nA class corresponding to an operation.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the backend implements the operation.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.operations as ops\n&gt;&gt;&gt; ibis.sqlite.has_operation(ops.ArrayIndex)\nFalse\n&gt;&gt;&gt; ibis.postgres.has_operation(ops.ArrayIndex)\nTrue\n\n\n\ninsert\ninsert(self, name, obj, settings=None, **kwargs)\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\nReturn the list of table names in the current database.\nFor some backends, the tables may be files in a directory, or other equivalent entities in a SQL database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format.\nNone\n\n\ndatabase\nstr | None\nThe database from which to list tables. If not provided, the current database is used.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe list of the table names that match the pattern like.\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query, external_tables=None, **kwargs)\nExecute a SQL string query against the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sg.exp.Expression\nRaw SQL string\nrequired\n\n\nexternal_tables\nMapping[str, pd.DataFrame] | None\nMapping of table name to pandas DataFrames providing external datasources for the query\nNone\n\n\nkwargs\n\nBackend specific query arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nCursor\nClickhouse cursor\n\n\n\n\n\n\nread_csv\nread_csv(self, path, table_name=None, engine='MergeTree', **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, engine='MergeTree', **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\nAny\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nir.Table\nThe just-registered table\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\n\n\ntable\ntable(self, name, database=None)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, external_tables=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\nMapping[ir.Scalar, Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\nAny\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, limit=None, params=None, external_tables=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nir.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\nMapping[ir.Scalar, Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nexternal_tables\nMapping[str, Any] | None\nExternal data\nNone\n\n\nchunk_size\nint\nMaximum number of row to return in a single chunk\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nresults\nRecordBatchReader\n\n\n\n\n\nNotes\nThere are a variety of ways to implement clickhouse -&gt; record batches.\n\nFORMAT ArrowStream -&gt; record batches via raw_query This has the same type conversion problem(s) as to_pyarrow. It’s harder to address due to lack of cast on RecordBatch. However, this is a ClickHouse problem: we should be able to get string data out without a bunch of settings/permissions rigmarole.\nNative -&gt; Python objects -&gt; pyarrow batches This is what is implemented, using query_column_block_stream.\nNative -&gt; Python objects -&gt; DataFrame chunks -&gt; pyarrow batches This is not implemented because it adds an unnecessary pandas step in between Python object -&gt; arrow. We can go directly to record batches without pandas in the middle.\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/snowflake.html",
    "href": "backends/snowflake.html",
    "title": "Snowflake",
    "section": "",
    "text": "https://www.snowflake.com"
  },
  {
    "objectID": "backends/snowflake.html#install",
    "href": "backends/snowflake.html#install",
    "title": "Snowflake",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Snowflake backend:\n\npipcondamamba\n\n\nInstall with the snowflake extra:\npip install 'ibis-framework[snowflake]'\nAnd connect:\nimport ibis\n\n1con = ibis.snowflake.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Snowflake:\nconda install -c conda-forge ibis-snowflake\nAnd connect:\nimport ibis\n\n1con = ibis.snowflake.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Snowflake:\nmamba install -c conda-forge ibis-snowflake\nAnd connect:\nimport ibis\n\n1con = ibis.snowflake.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/snowflake.html#connect",
    "href": "backends/snowflake.html#connect",
    "title": "Snowflake",
    "section": "Connect",
    "text": "Connect\n\nibis.snowflake.connect\ncon = ibis.snowflake.connect(\n    user=\"user\",\n    password=\"password\",\n    account=\"safpqpq-sq55555\",\n    database=\"IBIS_TESTING/IBIS_TESTING\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.snowflake.connect is a thin wrapper around ibis.backends.snowflake.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, user, account, database, password=None, authenticator=None, connect_args=None, create_object_udfs=True, **kwargs)\nConnect to Snowflake.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUsername\nrequired\n\n\naccount\nstr\nA Snowflake organization ID and a Snowflake user ID, separated by a hyphen. Note that a Snowflake user ID is a separate identifier from a username. See https://ibis-project.org/backends/Snowflake/ for details\nrequired\n\n\ndatabase\nstr\nA Snowflake database and a Snowflake schema, separated by a /. See https://ibis-project.org/backends/Snowflake/ for details\nrequired\n\n\npassword\nstr | None\nPassword. If empty or None then authenticator must be passed.\nNone\n\n\nauthenticator\nstr | None\nString indicating authentication method. See https://docs.snowflake.com/en/developer-guide/python-connector/python-connector-example#connecting-with-oauth for details. Note that the authentication flow will not take place until a database connection is made. This means that ibis.snowflake.connect(...) can succeed, while subsequent API calls fail if the authentication fails for any reason.\nNone\n\n\ncreate_object_udfs\nbool\nEnable object UDF extensions defined by ibis on the first connection to the database.\nTrue\n\n\nconnect_args\ncollections.abc.Mapping[str, typing.Any] | None\nAdditional arguments passed to the SQLAlchemy engine creation call.\nNone\n\n\nkwargs\ntyping.Any\nAdditional arguments passed to the SQLAlchemy URL constructor. See https://docs.snowflake.com/en/developer-guide/python-connector/sqlalchemy#additional-connection-parameters for more details\n{}\n\n\n\n\n\n\n\nibis.connect URL format\nIn addition to ibis.snowflake.connect, you can also connect to Snowflake by passing a properly formatted Snowflake connection URL to ibis.connect\ncon = ibis.connect(f\"snowflake://{user}:{password}@{account}/{database}\")\n\n\nAuthenticating with SSO\nIbis supports connecting to SSO-enabled Snowflake warehouses using the authenticator parameter.\nYou can use it in the explicit-parameters-style or in the URL-style connection APIs. All values of authenticator are supported.\n\nExplicit\ncon = ibis.snowflake.connect(\n    user=\"user\",\n    account=\"safpqpq-sq55555\",\n    database=\"my_database/my_schema\",\n    warehouse=\"my_warehouse\",\n    authenticator=\"externalbrowser\",\n)\n\n\nURL\ncon = ibis.connect(\n    f\"snowflake://{user}@{account}/{database}?warehouse={warehouse}\",\n    authenticator=\"externalbrowser\",\n)\n\n\n\nLooking up your Snowflake organization ID and user ID\nA Snowflake account identifier consists of an organization ID and a user ID, separated by a hyphen.\n\n\n\n\n\n\nNote\n\n\n\nThis user ID is not the same as the username you log in with.\n\n\nTo find your organization ID and user ID, log in to the Snowflake web app, then click on the text just to the right of the Snowflake logo (in the lower-left-hand corner of the screen).\nThe bold text at the top of the little pop-up window is your organization ID. The bold blue text with a checkmark next to it is your user ID.\n\n\n\nSnowflake Organization and User ID\n\n\n\n\nChoosing a value for database\nSnowflake refers to a collection of tables as a schema, and a collection of schema as a database.\nYou must choose a database and a schema to connect to. You can refer to the available databases and schema in the “Data” sidebar item in the Snowflake web app.\n\n\n\nSnowflake Database"
  },
  {
    "objectID": "backends/snowflake.html#ibis.backends.snowflake.Backend",
    "href": "backends/snowflake.html#ibis.backends.snowflake.Backend",
    "title": "Snowflake",
    "section": "snowflake.Backend",
    "text": "snowflake.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_database\ncreate_database(self, name, force=False)\nCreate a new database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new database.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database already exists.\nFalse\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False, comment=None)\nCreate a table in Snowflake.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to create\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nThe data with which to populate the table; optional, but at least one of obj or schema must be specified\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema of the table to create; optional, but at least one of obj or schema must be specified\nNone\n\n\ndatabase\nstr | None\nThe name of the database in which to create the table; if not passed, the current database is used.\nNone\n\n\ntemp\nbool\nCreate a temporary table\nFalse\n\n\noverwrite\nbool\nIf True, replace the table if it already exists, otherwise fail if the table exists\nFalse\n\n\ncomment\nstr | None\nAdd a comment to the table\nNone\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop a database with name name.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase to drop.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database does not exist.\nFalse\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, database=None, schema=None, force=False)\nDrop a table from Snowflake.\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList the tables in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern to use for listing tables.\nNone\n\n\ndatabase\nstr | None\nThe database (catalog) to perform the list against.\nNone\n\n\nschema\nstr | None\nThe schema inside database to perform the list against. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the Snowflake backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nA string or Path to a CSV file; globs are supported\nrequired\n\n\ntable_name\nstr | None\nOptional name for the table; if not passed, a random name will be generated\nNone\n\n\nkwargs\ntyping.Any\nSnowflake-specific file format configuration arguments. See the documentation for the full list of options: https://docs.snowflake.com/en/sql-reference/sql/create-file-format#type-csv\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was read from the CSV file\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRead newline-delimited JSON into an ibis table, using Snowflake.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nA string or Path to a JSON file; globs are supported\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments. See https://docs.snowflake.com/en/sql-reference/sql/create-file-format#type-json for the full list of options.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn ibis table expression\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRead a Parquet file into an ibis table, using Snowflake.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nA string or Path to a Parquet file; globs are supported\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments. See https://docs.snowflake.com/en/sql-reference/sql/create-file-format#type-parquet for the full list of options.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn ibis table expression\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, **_)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **_)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/pandas.html",
    "href": "backends/pandas.html",
    "title": "pandas",
    "section": "",
    "text": "https://pandas.pydata.org/"
  },
  {
    "objectID": "backends/pandas.html#install",
    "href": "backends/pandas.html#install",
    "title": "pandas",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the pandas backend:\n\npipcondamamba\n\n\nInstall with the pandas extra:\npip install 'ibis-framework[pandas]'\nAnd connect:\nimport ibis\n\n1con = ibis.pandas.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for pandas:\nconda install -c conda-forge ibis-pandas\nAnd connect:\nimport ibis\n\n1con = ibis.pandas.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for pandas:\nmamba install -c conda-forge ibis-pandas\nAnd connect:\nimport ibis\n\n1con = ibis.pandas.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/pandas.html#user-defined-functions-udf",
    "href": "backends/pandas.html#user-defined-functions-udf",
    "title": "pandas",
    "section": "User Defined functions (UDF)",
    "text": "User Defined functions (UDF)\nIbis supports defining three kinds of user-defined functions for operations on expressions targeting the pandas backend: element-wise, reduction, and analytic.\n\nElementwise Functions\nAn element-wise function is a function that takes N rows as input and produces N rows of output. log, exp, and floor are examples of element-wise functions.\nHere’s how to define an element-wise function:\nimport ibis.expr.datatypes as dt\nfrom ibis.backends.pandas.udf import udf\n\n@udf.elementwise(input_type=[dt.int64], output_type=dt.double)\ndef add_one(x):\n    return x + 1.0\n\n\nReduction Functions\nA reduction is a function that takes N rows as input and produces 1 row as output. sum, mean and count are examples of reductions. In the context of a GROUP BY, reductions produce 1 row of output per group.\nHere’s how to define a reduction function:\nimport ibis.expr.datatypes as dt\nfrom ibis.backends.pandas.udf import udf\n\n@udf.reduction(input_type=[dt.double], output_type=dt.double)\ndef double_mean(series):\n    return 2 * series.mean()\n\n\nAnalytic Functions\nAn analytic function is like an element-wise function in that it takes N rows as input and produces N rows of output. The key difference is that analytic functions can be applied per group using window functions. Z-score is an example of an analytic function.\nHere’s how to define an analytic function:\nimport ibis.expr.datatypes as dt\nfrom ibis.backends.pandas.udf import udf\n\n@udf.analytic(input_type=[dt.double], output_type=dt.double)\ndef zscore(series):\n    return (series - series.mean()) / series.std()\n\n\nDetails of pandas UDFs\n\nElement-wise provide support for applying your UDF to any combination of scalar values and columns.\nReductions provide support for whole column aggregations, grouped aggregations, and application of your function over a window.\nAnalytic functions work in both grouped and non-grouped settings\nThe objects you receive as input arguments are either pandas.Series or Python/NumPy scalars.\n\n!!! warning “Keyword arguments must be given a default”\nAny keyword arguments must be given a default value or the function **will\nnot work**.\nA common Python convention is to set the default value to None and handle setting it to something not None in the body of the function.\nUsing add_one from above as an example, the following call will receive a pandas.Series for the x argument:\nimport ibis\nimport pandas as pd\ndf = pd.DataFrame({'a': [1, 2, 3]})\ncon = ibis.pandas.connect({'df': df})\nt = con.table('df')\nexpr = add_one(t.a)\nexpr\nAnd this will receive the int 1:\nexpr = add_one(1)\nexpr\nSince the pandas backend passes around **kwargs you can accept **kwargs in your function:\nimport ibis.expr.datatypes as dt\nfrom ibis.backends.pandas.udf import udf\n\n@udf.elementwise([dt.int64], dt.double)\ndef add_two(x, **kwargs): # do stuff with kwargs\n    return x + 2.0\nOr you can leave them out as we did in the example above. You can also optionally accept specific keyword arguments.\nFor example:\nimport ibis.expr.datatypes as dt\nfrom ibis.backends.pandas.udf import udf\n\n@udf.elementwise([dt.int64], dt.double)\ndef add_two_with_none(x, y=None):\n    if y is None:\n    y = 2.0\n    return x + y"
  },
  {
    "objectID": "backends/pandas.html#ibis.backends.pandas.BasePandasBackend",
    "href": "backends/pandas.html#ibis.backends.pandas.BasePandasBackend",
    "title": "pandas",
    "section": "pandas.Backend",
    "text": "pandas.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\ncompile\ncompile(self, expr, *args, **kwargs)\nCompile an expression.\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False)\nCreate a table.\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nibis.ibis.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created.\n\n\n\n\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the table exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the table does not exist.\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, force=False)\nDrop a view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the view to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the view does not exist.\nFalse\n\n\n\n\n\n\nexecute\nexecute(self, expr)\nExecute an expression.\n\n\nfrom_dataframe\nfrom_dataframe(self, df, name='df', client=None)\nConstruct an ibis table from a pandas DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nA pandas DataFrame\nrequired\n\n\nname\nstr\nThe name of the pandas DataFrame\n'df'\n\n\nclient\nibis.backends.pandas.BasePandasBackend | None\nClient dictionary will be mutated with the name of the DataFrame, if not provided a new client is created\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\n\nget_schema\nget_schema(self, table_name, database=None)\n\n\nhas_operation\nhas_operation(cls, operation)\nReturn whether the backend implements support for operation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noperation\ntype[ibis.ibis.Value]\nA class corresponding to an operation.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the backend implements the operation.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.operations as ops\n&gt;&gt;&gt; ibis.sqlite.has_operation(ops.ArrayIndex)\nFalse\n&gt;&gt;&gt; ibis.postgres.has_operation(ops.ArrayIndex)\nTrue\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\nReturn the list of table names in the current database.\nFor some backends, the tables may be files in a directory, or other equivalent entities in a SQL database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format.\nNone\n\n\ndatabase\nstr | None\nThe database from which to list tables. If not provided, the current database is used.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe list of the table names that match the pattern like.\n\n\n\n\n\n\nread_csv\nread_csv(self, source, table_name=None, **kwargs)\nRegister a CSV file as a table in the current session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.pathlib.Path\nThe data source. Can be a local or remote file, pathlike objects also accepted.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Pandas loading function. See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, source, table_name=None, **kwargs)\nRegister a parquet file as a table in the current session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.pathlib.Path\nThe data source(s). May be a path to a file, an iterable of files, or directory of parquet files.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Pandas loading function. See https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\ntable\ntable(self, name, schema=None)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute expression and return a RecordBatchReader.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nresults\nRecordBatchReader\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name."
  },
  {
    "objectID": "backends/sqlite.html",
    "href": "backends/sqlite.html",
    "title": "SQLite",
    "section": "",
    "text": "https://www.sqlite.org"
  },
  {
    "objectID": "backends/sqlite.html#install",
    "href": "backends/sqlite.html#install",
    "title": "SQLite",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the SQLite backend:\n\npipcondamamba\n\n\nInstall with the sqlite extra:\npip install 'ibis-framework[sqlite]'\nAnd connect:\nimport ibis\n\n1con = ibis.sqlite.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for SQLite:\nconda install -c conda-forge ibis-sqlite\nAnd connect:\nimport ibis\n\n1con = ibis.sqlite.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for SQLite:\nmamba install -c conda-forge ibis-sqlite\nAnd connect:\nimport ibis\n\n1con = ibis.sqlite.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/sqlite.html#connect",
    "href": "backends/sqlite.html#connect",
    "title": "SQLite",
    "section": "Connect",
    "text": "Connect\n\nibis.sqlite.connect\nUse an ephemeral, in-memory database.\ncon = ibis.sqlite.connect()\nConnect to, or create, a local SQLite file\ncon = ibis.sqlite.connect(\"mydb.sqlite\")\n\n\n\n\n\n\nNote\n\n\n\nibis.sqlite.connect is a thin wrapper around ibis.backends.sqlite.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, database=None, type_map=None)\nCreate an Ibis client connected to a SQLite database.\nMultiple database files can be accessed using the attach() method.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndatabase\nstr | pathlib.Path | None\nFile path to the SQLite database file. If None, creates an in-memory transient database and you can use attach() to add more files\nNone\n\n\ntype_map\ndict[str, str | ibis.ibis.DataType] | None\nAn optional mapping from a string name of a SQLite “type” to the corresponding ibis DataType that it represents. This can be used to override schema inference for a given SQLite database.\nNone\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.sqlite.connect(\"path/to/my/sqlite.db\")\n\n\n\n\nibis.connect URL format\nIn addition to ibis.sqlite.connect, you can also connect to SQLite by passing a properly formatted SQLite connection URL to ibis.connect:\ncon = ibis.connect(\"sqlite:///path/to/local/file\")\nThe URL can be sqlite:// which will connect to an ephemeral in-memory database:\ncon = ibis.connect(\"sqlite://\")"
  },
  {
    "objectID": "backends/sqlite.html#ibis.backends.sqlite.Backend",
    "href": "backends/sqlite.html#ibis.backends.sqlite.Backend",
    "title": "SQLite",
    "section": "sqlite.Backend",
    "text": "sqlite.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nattach\nattach(self, name, path)\nConnect another SQLite database file to the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase name within SQLite\nrequired\n\n\npath\nstr | pathlib.Path\nPath to sqlite3 database files\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con1 = ibis.sqlite.connect(\"original.db\")\n&gt;&gt;&gt; con2 = ibis.sqlite.connect(\"new.db\")\n&gt;&gt;&gt; con1.attach(\"new\", \"new.db\")\n&gt;&gt;&gt; con1.list_tables(database=\"new\")\n\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/polars.html",
    "href": "backends/polars.html",
    "title": "Polars",
    "section": "",
    "text": "https://www.pola.rs"
  },
  {
    "objectID": "backends/polars.html#install",
    "href": "backends/polars.html#install",
    "title": "Polars",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Polars backend:\n\npipcondamamba\n\n\nInstall with the polars extra:\npip install 'ibis-framework[polars]'\nAnd connect:\nimport ibis\n\n1con = ibis.polars.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Polars:\nconda install -c conda-forge ibis-polars\nAnd connect:\nimport ibis\n\n1con = ibis.polars.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Polars:\nmamba install -c conda-forge ibis-polars\nAnd connect:\nimport ibis\n\n1con = ibis.polars.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/polars.html#connect",
    "href": "backends/polars.html#connect",
    "title": "Polars",
    "section": "Connect",
    "text": "Connect\n\nibis.polars.connect\ncon = ibis.polars.connect()\n\n\n\n\n\n\nNote\n\n\n\nibis.polars.connect is a thin wrapper around ibis.backends.polars.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, tables=None)\nConstruct a client from a dictionary of polars LazyFrames and/or DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntables\ncollections.abc.MutableMapping[str, polars.polars.LazyFrame | polars.polars.DataFrame] | None\nAn optional mapping of string table names to polars LazyFrames.\nNone"
  },
  {
    "objectID": "backends/polars.html#ibis.backends.polars.Backend",
    "href": "backends/polars.html#ibis.backends.polars.Backend",
    "title": "Polars",
    "section": "polars.Backend",
    "text": "polars.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\ncompile\ncompile(self, expr, params=None, **_)\nCompile an expression.\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False)\nCreate a new table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nWhether a table is temporary or not\nFalse\n\n\noverwrite\nbool\nWhether to clobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, *_, **__)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nibis.ibis.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created.\n\n\n\n\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, *_, **__)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the table exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the table does not exist.\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, *_, **__)\nDrop a view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the view to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the view does not exist.\nFalse\n\n\n\n\n\n\nexecute\nexecute(self, expr, params=None, limit=None, streaming=False, **kwargs)\nExecute an expression.\n\n\nget_schema\nget_schema(self, table_name, database=None)\n\n\nhas_operation\nhas_operation(cls, operation)\nReturn whether the backend implements support for operation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noperation\ntype[ibis.ibis.Value]\nA class corresponding to an operation.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the backend implements the operation.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.operations as ops\n&gt;&gt;&gt; ibis.sqlite.has_operation(ops.ArrayIndex)\nFalse\n&gt;&gt;&gt; ibis.postgres.has_operation(ops.ArrayIndex)\nTrue\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\nReturn the list of table names in the current database.\nFor some backends, the tables may be files in a directory, or other equivalent entities in a SQL database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format.\nNone\n\n\ndatabase\nstr | None\nThe database from which to list tables. If not provided, the current database is used.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe list of the table names that match the pattern like.\n\n\n\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading function. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_csv.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, path, table_name=None, **kwargs)\nRegister a Delta Lake as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source(s). Path to a Delta Lake table directory.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading function. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_delta.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nA string or Path to a JSON file; globs are supported\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading function. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_ndjson.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_pandas\nread_pandas(self, source, table_name=None, **kwargs)\nRegister a Pandas DataFrame or pyarrow Table a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\npandas.pandas.DataFrame\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading function. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.from_pandas.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path | collections.abc.Iterable[str]\nThe data source(s). May be a path to a file, an iterable of files, or directory of parquet files.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading function. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_parquet.html for more information (if loading a single file or glob; when loading multiple files polars’ scan_pyarrow_dataset method is used instead).\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister\nregister(self, source, table_name=None, **kwargs)\nRegister a data source as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path | typing.Any\nThe data source(s). May be a path to a file, a parquet directory, or a pandas dataframe.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Polars loading functions for CSV or parquet. See https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_csv.html and https://pola-rs.github.io/polars/py-polars/html/reference/api/polars.scan_parquet.html for more information\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\n\n\ntable\ntable(self, name, _schema=None)\nConstruct a table expression.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute expression and return a RecordBatchReader.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nresults\nRecordBatchReader\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name."
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html",
    "href": "tutorials/ibis-for-sql-users.html",
    "title": "Tutorial: Ibis for SQL users",
    "section": "",
    "text": "We recommend starting with the default (DuckDB) backend for a performant, fully-featured local experience. You can install Ibis with pip, conda, mamba, or pixi.\n\nUsing pipUsing condaUsing mambaUsing pixi\n\n\npip install 'ibis-framework[duckdb]'\n\n\n\n\n\n\nWarning\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name.\n\n\n\n\nconda install -c conda-forge ibis-duckdb\n\n\nmamba install -c conda-forge ibis-duckdb\n\n\npixi add ibis-duckdb"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#prerequisites",
    "href": "tutorials/ibis-for-sql-users.html#prerequisites",
    "title": "Tutorial: Ibis for SQL users",
    "section": "",
    "text": "We recommend starting with the default (DuckDB) backend for a performant, fully-featured local experience. You can install Ibis with pip, conda, mamba, or pixi.\n\nUsing pipUsing condaUsing mambaUsing pixi\n\n\npip install 'ibis-framework[duckdb]'\n\n\n\n\n\n\nWarning\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name.\n\n\n\n\nconda install -c conda-forge ibis-duckdb\n\n\nmamba install -c conda-forge ibis-duckdb\n\n\npixi add ibis-duckdb"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#overview",
    "href": "tutorials/ibis-for-sql-users.html#overview",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Overview",
    "text": "Overview\nIbis provides a full-featured replacement for SQL SELECT queries, but expressed with Python code that is:\n\nType-checked and validated as you go. No more debugging cryptic database errors; Ibis catches your mistakes right away.\nEasier to write. Pythonic function calls with tab completion in IPython.\nMore composable. Break complex queries down into easier-to-digest pieces\nEasier to reuse. Mix and match Ibis snippets to create expressions tailored for your analysis.\n\nWe intend for all SELECT queries to be fully portable to Ibis. Coverage of other DDL statements (e.g. CREATE TABLE or INSERT) may vary from engine to engine.\n\n\n\n\n\n\nNote\n\n\n\nIf you find any SQL idioms or use cases in your work that are not represented here, please reach out so we can add more to this guide!"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#projections-selectaddremove-columns",
    "href": "tutorials/ibis-for-sql-users.html#projections-selectaddremove-columns",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Projections: select/add/remove columns",
    "text": "Projections: select/add/remove columns\nAll tables in Ibis are immutable. To select a subset of a table’s columns, or to add new columns, you must produce a new table by means of a projection.\n\nimport ibis\n\nt = ibis.table(dict(one=\"string\", two=\"float\", three=\"int32\"), name=\"my_data\")\nt\n\nUnboundTable: my_data\n  one   string\n  two   float64\n  three int32\n\n\n\nIn SQL, you might write something like:\nSELECT two, one\nFROM my_data\nIn Ibis, this is\n\nproj = t[\"two\", \"one\"]\n\nor\n\nproj = t.select(\"two\", \"one\")\n\nThis generates the expected SQL:\n\nibis.to_sql(proj)\n\nSELECT\n  t0.two,\n  t0.one\nFROM my_data AS t0\n\n\nWhat about adding new columns?\nSELECT two, one, three * 2 AS new_col\nFROM my_data\nThe last expression is written:\n\nnew_col = (t.three * 2).name(\"new_col\")\n\nNow, we have:\n\nproj = t[\"two\", \"one\", new_col]\nibis.to_sql(proj)\n\nSELECT\n  t0.two,\n  t0.one,\n  t0.three * CAST(2 AS TINYINT) AS new_col\nFROM my_data AS t0\n\n\n\nmutate: Add or modify columns easily\nSince adding new columns or modifying existing columns is so common, there is a convenience method mutate:\n\nmutated = t.mutate(new_col=t.three * 2)\n\nNotice that using the name was not necessary here because we’re using Python keyword arguments to provide the name:\n\nibis.to_sql(mutated)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.three * CAST(2 AS TINYINT) AS new_col\nFROM my_data AS t0\n\n\nIf you modify an existing column with mutate it will list out all the other columns:\n\nmutated = t.mutate(two=t.two * 2)\nibis.to_sql(mutated)\n\nSELECT\n  t0.one,\n  t0.two * CAST(2 AS TINYINT) AS two,\n  t0.three\nFROM my_data AS t0\n\n\n\n\nSELECT * equivalent\nEspecially in combination with relational joins, it’s convenient to be able to select all columns in a table using the SELECT * construct. To do this, use the table expression itself in a projection:\n\nproj = t[t]\nibis.to_sql(proj)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\n\n\nThis is how mutate is implemented. The example above t.mutate(new_col=t.three * 2) can be written as a normal projection:\n\nproj = t[t, new_col]\nibis.to_sql(proj)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.three * CAST(2 AS TINYINT) AS new_col\nFROM my_data AS t0\n\n\nLet’s consider a table we might wish to join with t:\n\nt2 = ibis.table(dict(key=\"string\", value=\"float\"), name=\"dim_table\")\n\nNow let’s take the SQL:\nSELECT t0.*, t0.two - t1.value AS diff\nFROM my_data t0\n  INNER JOIN dim_table t1\n    ON t0.one = t1.key\nTo write this with Ibis, it is:\n\ndiff = (t.two - t2.value).name(\"diff\")\njoined = t.join(t2, t.one == t2.key)[t, diff]\n\nAnd verify the generated SQL:\n\nibis.to_sql(joined)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.two - t1.value AS diff\nFROM my_data AS t0\nJOIN dim_table AS t1\n  ON t0.one = t1.key\n\n\n\n\nUsing functions in projections\nIf you pass a function instead of a string or Ibis expression in any projection context, it will be invoked with the “parent” table as its argument. This can help significantly when [composing complex operations. Consider this SQL:\nSELECT one, avg(abs(the_sum)) AS mad\nFROM (\n  SELECT one, three, sum(two) AS the_sum\n  FROM my_data\n  GROUP BY 1, 2\n) t0\nGROUP BY 1\nThis can be written as one chained expression:\n\nexpr = (\n    t.group_by([\"one\", \"three\"])\n    .aggregate(the_sum=t.two.sum())\n    .group_by(\"one\")\n    .aggregate(mad=lambda x: x.the_sum.abs().mean())\n)\n\nHere’s the SQL:\n\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  AVG(ABS(t0.the_sum)) AS mad\nFROM (\n  SELECT\n    t1.one AS one,\n    t1.three AS three,\n    SUM(t1.two) AS the_sum\n  FROM my_data AS t1\n  GROUP BY\n    1,\n    2\n) AS t0\nGROUP BY\n  1"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#filtering-where",
    "href": "tutorials/ibis-for-sql-users.html#filtering-where",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Filtering / WHERE",
    "text": "Filtering / WHERE\nYou can add filter clauses to a table expression either by indexing with [] (similar to pandas) or use the filter method:\n\nfiltered = t[t.two &gt; 0]\nibis.to_sql(filtered)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nWHERE\n  t0.two &gt; CAST(0 AS TINYINT)\n\n\nfilter can take a list of expressions, which must all be satisfied for a row to appear in the result:\n\nfiltered = t.filter([t.two &gt; 0, t.one.isin([\"A\", \"B\"])])\nibis.to_sql(filtered)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nWHERE\n  t0.two &gt; CAST(0 AS TINYINT) AND t0.one IN ('A', 'B')\n\n\nTo compose boolean expressions with AND or OR, use the respective & and | operators:\n\ncond = (t.two &lt; 0) | ((t.two &gt; 0) | t.one.isin([\"A\", \"B\"]))\nfiltered = t[cond]\nibis.to_sql(filtered)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nWHERE\n  t0.two &lt; CAST(0 AS TINYINT) OR t0.two &gt; CAST(0 AS TINYINT) OR t0.one IN ('A', 'B')\n\n\nNote the parentheses around the second expression. These are necessary due to operator precedence."
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#aggregation-group-by",
    "href": "tutorials/ibis-for-sql-users.html#aggregation-group-by",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Aggregation / GROUP BY",
    "text": "Aggregation / GROUP BY\nTo aggregate a table, you need:\n\nZero or more grouping expressions (these can be column names)\nOne or more aggregation expressions\n\nLet’s look at the aggregate method on tables:\n\nagged = t.aggregate(total_two=t.two.sum(), avg_three=t.three.mean())\n\nIf you don’t use any group expressions, the result will have a single row with your statistics of interest:\n\nagged.schema()\n\nibis.Schema {\n  total_two  float64\n  avg_three  float64\n}\n\n\n\nibis.to_sql(agged)\n\nSELECT\n  SUM(t0.two) AS total_two,\n  AVG(t0.three) AS avg_three\nFROM my_data AS t0\n\n\nTo add groupings, use either the by argument of aggregate or use the group_by construct:\n\nagged2 = t.aggregate(total_two=t.two.sum(), avg_three=t.three.mean())\nagged3 = t.group_by(\"one\").aggregate(total_two=t.two.sum(), avg_three=t.three.mean())\nibis.to_sql(agged3)\n\nSELECT\n  t0.one,\n  SUM(t0.two) AS total_two,\n  AVG(t0.three) AS avg_three\nFROM my_data AS t0\nGROUP BY\n  1\n\n\n\nNon-trivial grouping keys\nYou can use any expression (or function, like in projections) deriving from the table you are aggregating. The only constraint is that the expressions must be named. Let’s look at an example:\n\nevents = ibis.table(\n    dict(ts=\"timestamp\", event_type=\"int32\", session_id=\"int64\"),\n    name=\"web_events\",\n)\n\nSuppose we wanted to total up event types by year and month:\n\nstats = (\n    events.group_by(year=events.ts.year(), month=events.ts.month())\n    .aggregate(total=events.count(), sessions=events.session_id.nunique())\n)\n\nNow we have:\n\nibis.to_sql(stats)\n\nSELECT\n  CAST(EXTRACT(year FROM t0.ts) AS SMALLINT) AS year,\n  CAST(EXTRACT(month FROM t0.ts) AS SMALLINT) AS month,\n  COUNT(*) AS total,\n  COUNT(DISTINCT t0.session_id) AS sessions\nFROM web_events AS t0\nGROUP BY\n  1,\n  2\n\n\n\n\nAggregates considering table subsets\nIn analytics is it common to compare statistics from different subsets of a table. Let’s consider a dataset containing people’s name, age, gender, and nationality:\n\npop = ibis.table(\n    dict(name=\"string\", country=\"string\", gender=\"string\", age=\"int16\"),\n    name=\"population\",\n)\n\nNow, suppose you wanted to know for each country:\n\nAverage overall age\nAverage male age\nAverage female age\nTotal number of persons\n\nIn SQL, you may write:\nSELECT\n  country,\n  count(*) AS num_persons,\n  AVG(age) AS avg_age\n  AVG(\n    CASE WHEN gender = 'M' THEN age\n    ELSE NULL\n    END\n  ) AS avg_male,\n  AVG(\n    CASE WHEN gender = 'F' THEN age\n    ELSE NULL\n    END\n  ) AS avg_female,\nFROM population\nGROUP BY 1\nIbis makes this much simpler by giving you where option in aggregation functions:\n\nexpr = pop.group_by(\"country\").aggregate(\n    num_persons=pop.count(),\n    avg_age=pop.age.mean(),\n    avg_male=pop.age.mean(where=pop.gender == \"M\"),\n    avg_female=pop.age.mean(where=pop.gender == \"F\"),\n)\n\nThis indeed generates the correct SQL. Note that SQL engines handle NULL values differently in aggregation functions, but Ibis will write the SQL expression that is correct for your query engine.\n\nibis.to_sql(expr)\n\nSELECT\n  t0.country,\n  COUNT(*) AS num_persons,\n  AVG(t0.age) AS avg_age,\n  AVG(t0.age) FILTER(WHERE\n    t0.gender = 'M') AS avg_male,\n  AVG(t0.age) FILTER(WHERE\n    t0.gender = 'F') AS avg_female\nFROM population AS t0\nGROUP BY\n  1\n\n\n\n\nCounting rows with Table.count():\nComputing group frequencies can be done by calling count() after calling group_by():\n\nfreqs = events.group_by(year=events.ts.year(), month=events.ts.month()).count()\nibis.to_sql(freqs)\n\nSELECT\n  CAST(EXTRACT(year FROM t0.ts) AS SMALLINT) AS year,\n  CAST(EXTRACT(month FROM t0.ts) AS SMALLINT) AS month,\n  COUNT(*) AS \"CountStar(web_events)\"\nFROM web_events AS t0\nGROUP BY\n  1,\n  2\n\n\n\n\nFrequency table convenience: value_counts\nConsider the SQL idiom:\nSELECT some_column_expression, count(*)\nFROM table\nGROUP BY 1\nThis is so common that, like pandas, there is a generic column method value_counts which does this:\n\nexpr = events.ts.year().value_counts()\nibis.to_sql(expr)\n\nSELECT\n  t0.\"ExtractYear(ts)\",\n  COUNT(*) AS \"ExtractYear(ts)_count\"\nFROM (\n  SELECT\n    CAST(EXTRACT(year FROM t1.ts) AS SMALLINT) AS \"ExtractYear(ts)\"\n  FROM web_events AS t1\n) AS t0\nGROUP BY\n  1\n\n\n\n\nHAVING clause\nThe SQL HAVING clause enables you to filter the results of an aggregation based on some group-wise condition holding true. For example, suppose we wanted to limit our analysis to groups containing at least 1000 observations:\nSELECT one, sum(two) AS total\nFROM my_data\nGROUP BY 1\nHAVING count(*) &gt;= 1000\nWith Ibis, you can do:\n\nexpr = (\n    t.group_by(\"one\")\n    .having(t.count() &gt;= 1000)\n    .aggregate(total=t.two.sum())\n)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  SUM(t0.two) AS total\nFROM my_data AS t0\nGROUP BY\n  1\nHAVING\n  COUNT(*) &gt;= CAST(1000 AS SMALLINT)"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#sorting-order-by",
    "href": "tutorials/ibis-for-sql-users.html#sorting-order-by",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Sorting / ORDER BY",
    "text": "Sorting / ORDER BY\nTo sort a table, use the order_by method along with either column names or expressions that indicate the sorting keys:\n\nsorted = events.order_by([events.ts.year(), events.ts.month()])\n\nibis.to_sql(sorted)\n\nSELECT\n  t0.ts,\n  t0.event_type,\n  t0.session_id\nFROM web_events AS t0\nORDER BY\n  CAST(EXTRACT(year FROM t0.ts) AS SMALLINT) ASC,\n  CAST(EXTRACT(month FROM t0.ts) AS SMALLINT) ASC\n\n\nThe default for sorting is in ascending order. To reverse the sort direction of any key, wrap it in ibis.desc:\n\nsorted = (\n    events.order_by([ibis.desc(\"event_type\"), ibis.desc(events.ts.month())])\n    .limit(100)\n)\n\nibis.to_sql(sorted)\n\nSELECT\n  t0.ts,\n  t0.event_type,\n  t0.session_id\nFROM web_events AS t0\nORDER BY\n  t0.event_type DESC,\n  CAST(EXTRACT(month FROM t0.ts) AS SMALLINT) DESC\nLIMIT 100"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#limit-and-offset",
    "href": "tutorials/ibis-for-sql-users.html#limit-and-offset",
    "title": "Tutorial: Ibis for SQL users",
    "section": "LIMIT and OFFSET",
    "text": "LIMIT and OFFSET\nThe table limit function truncates a table to the indicates number of rows. So if you only want the first 1000 rows (which may not be deterministic depending on the SQL engine), you can do:\n\nlimited = t.limit(1000)\nibis.to_sql(limited)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nLIMIT 1000\n\n\nThe offset option in limit skips rows. So if you wanted rows 11 through 20, you could do:\n\nlimited = t.limit(10, offset=10)\nibis.to_sql(limited)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nLIMIT 10\nOFFSET 10"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#common-column-expressions",
    "href": "tutorials/ibis-for-sql-users.html#common-column-expressions",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Common column expressions",
    "text": "Common column expressions\nSee the full API documentation for all of the available value methods and tools for creating value expressions.\nWe mention a few common ones here as they relate to common SQL queries.\n\nType casts\nIbis’s type system is independent of any SQL system. You cast Ibis expressions from one Ibis type to another. For example:\n\nexpr = t.mutate(\n    date=t.one.cast(\"timestamp\"),\n    four=t.three.cast(\"float32\"),\n)\n\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CAST(t0.one AS TIMESTAMP) AS date,\n  CAST(t0.three AS REAL) AS four\nFROM my_data AS t0\n\n\n\n\nCASE statements\nSQL dialects typically support one or more kind of CASE statements. The first is the simple case that compares against exact values of an expression.\nCASE expr\n  WHEN value_1 THEN result_1\n  WHEN value_2 THEN result_2\n  ELSE default\nEND\nValue expressions in Ibis have a case method that allows us to emulate these semantics:\n\ncase = (\n    t.one.cast(\"timestamp\")\n    .year()\n    .case()\n    .when(2015, \"This year\")\n    .when(2014, \"Last year\")\n    .else_(\"Earlier\")\n    .end()\n)\n\nexpr = t.mutate(year_group=case)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CASE CAST(EXTRACT(year FROM CAST(t0.one AS TIMESTAMP)) AS SMALLINT)\n    WHEN CAST(2015 AS SMALLINT)\n    THEN 'This year'\n    WHEN CAST(2014 AS SMALLINT)\n    THEN 'Last year'\n    ELSE 'Earlier'\n  END AS year_group\nFROM my_data AS t0\n\n\nThe more general case is that of an arbitrary list of boolean expressions and result values:\nCASE\n  WHEN boolean_expr1 THEN result_1\n  WHEN boolean_expr2 THEN result_2\n  WHEN boolean_expr3 THEN result_3\n  ELSE default\nEND\nTo do this, use ibis.case:\n\ncase = (\n    ibis.case()\n    .when(t.two &lt; 0, t.three * 2)\n    .when(t.two &gt; 1, t.three)\n    .else_(t.two)\n    .end()\n)\n\nexpr = t.mutate(cond_value=case)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CASE\n    WHEN (\n      t0.two &lt; CAST(0 AS TINYINT)\n    )\n    THEN t0.three * CAST(2 AS TINYINT)\n    WHEN (\n      t0.two &gt; CAST(1 AS TINYINT)\n    )\n    THEN t0.three\n    ELSE t0.two\n  END AS cond_value\nFROM my_data AS t0\n\n\nThere are several places where Ibis builds cases for you in a simplified way. One example is the ifelse function:\n\nswitch = (t.two &lt; 0).ifelse(\"Negative\", \"Non-Negative\")\nexpr = t.mutate(group=switch)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CASE WHEN (\n    t0.two &lt; CAST(0 AS TINYINT)\n  ) THEN 'Negative' ELSE 'Non-Negative' END AS \"group\"\nFROM my_data AS t0\n\n\n\n\nUsing NULL in expressions\nTo use NULL in an expression, either use the special ibis.NA value:\n\npos_two = (t.two &gt; 0).ifelse(t.two, ibis.NA)\nexpr = t.mutate(two_positive=pos_two)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CASE WHEN (\n    t0.two &gt; CAST(0 AS TINYINT)\n  ) THEN t0.two ELSE NULL END AS two_positive\nFROM my_data AS t0\n\n\n\n\nSet membership: IN / NOT IN\nLet’s look again at the population dataset. Suppose you wanted to combine the United States and Canada data into a “North America” category. Here would be some SQL to do it:\nCASE\n  WHEN UPPER(country) IN ('UNITED STATES', 'CANADA') THEN 'North America'\n  ELSE country\nEND AS refined_group\nThe Ibis equivalent of IN is the isin method. So we can write:\n\nrefined = (\n    pop.country.upper()\n    .isin([\"UNITED STATES\", \"CANADA\"])\n    .ifelse(\"North America\", pop.country)\n)\n\nexpr = pop.mutate(refined_group=refined)\nibis.to_sql(expr)\n\nSELECT\n  t0.name,\n  t0.country,\n  t0.gender,\n  t0.age,\n  CASE\n    WHEN (\n      UPPER(t0.country) IN ('UNITED STATES', 'CANADA')\n    )\n    THEN 'North America'\n    ELSE t0.country\n  END AS refined_group\nFROM population AS t0\n\n\nThe opposite of isin is notin.\n\n\nConstant and literal expressions\nConsider a SQL expression like:\n'foo' IN (column1, column2)\nwhich is equivalent to\ncolumn1 = 'foo' OR column2 = 'foo'\nTo build expressions off constant values, you must first convert the value (whether a Python string or number) to an Ibis expression using ibis.literal:\n\nt3 = ibis.table(\n    dict(column1=\"string\", column2=\"string\", column3=\"float\"),\n    name=\"data\",\n)\n\nvalue = ibis.literal(\"foo\")\n\nOnce you’ve done this, you can use the literal expression like any other array or scalar expression:\n\nhas_foo = value.isin([t3.column1, t3.column2])\n\nexpr = t3.mutate(has_foo=has_foo)\nibis.to_sql(expr)\n\nSELECT\n  t0.column1,\n  t0.column2,\n  t0.column3,\n  'foo' IN (t0.column1, t0.column2) AS has_foo\nFROM data AS t0\n\n\nIn many other situations, you can use constants without having to use ibis.literal. For example, we could add a column containing nothing but the number 5 like so:\n\nexpr = t3.mutate(number5=5)\nibis.to_sql(expr)\n\nSELECT\n  t0.column1,\n  t0.column2,\n  t0.column3,\n  CAST(5 AS TINYINT) AS number5\nFROM data AS t0\n\n\n\n\nIS NULL and IS NOT NULL\nThese are simple: use the isnull and notnull functions respectively, which yield boolean arrays:\n\nindic = t.two.isnull().ifelse(\"valid\", \"invalid\")\nexpr = t.mutate(is_valid=indic)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  CASE WHEN (\n    t0.two IS NULL\n  ) THEN 'valid' ELSE 'invalid' END AS is_valid\nFROM my_data AS t0\n\n\n\nagged = (\n    expr[expr.one.notnull()]\n    .group_by(\"is_valid\")\n    .aggregate(three_count=lambda t: t.three.notnull().sum())\n)\n\nibis.to_sql(agged)\n\nSELECT\n  t0.is_valid,\n  SUM(CAST(NOT t0.three IS NULL AS INT)) AS three_count\nFROM (\n  SELECT\n    t1.one AS one,\n    t1.two AS two,\n    t1.three AS three,\n    CASE WHEN (\n      t1.two IS NULL\n    ) THEN 'valid' ELSE 'invalid' END AS is_valid\n  FROM my_data AS t1\n  WHERE\n    NOT t1.one IS NULL\n) AS t0\nGROUP BY\n  1\n\n\n\n\nBETWEEN\nThe between method on arrays and scalars compiles to the SQL BETWEEN keyword. The result of between is boolean and can be used with any other boolean expression:\n\nexpr = t[t.two.between(10, 50) & t.one.notnull()]\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three\nFROM my_data AS t0\nWHERE\n  t0.two BETWEEN CAST(10 AS TINYINT) AND CAST(50 AS TINYINT) AND NOT t0.one IS NULL"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#joins",
    "href": "tutorials/ibis-for-sql-users.html#joins",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Joins",
    "text": "Joins\nIbis supports several kinds of joins between table expressions:\n\ninner_join: maps to SQL INNER JOIN\ncross_join: a cartesian product join with no keys. Equivalent to inner_join with no join predicates\nleft_join: maps to SQL LEFT OUTER JOIN\nouter_join: maps to SQL FULL OUTER JOIN\nsemi_join: maps to SQL LEFT SEMI JOIN. May or may not be an explicit join type in your query engine.\nanti_join: maps to SQL LEFT ANTI JOIN. May or may not be an explicit join type in your query engine.\n\nThe join table method is by default the same as inner_join.\nLet’s look at a couple example tables to see how joins work in Ibis:\n\nt1 = ibis.table(dict(value1=\"float\", key1=\"string\", key2=\"string\"), name=\"table1\")\nt2 = ibis.table(dict(value2=\"float\", key3=\"string\", key4=\"string\"), name=\"table2\")\n\nLet’s join on one key:\n\njoined = t1.left_join(t2, t1.key1 == t2.key3)\n\nThe immediate result of a join does not yet have a set schema. That is determined by the next action that you take. There’s several ways forward from here that address a variety of SQL use cases.\n\nJoin + projection\nConsider the SQL:\nSELECT t0.*, t1.value2\nFROM table1 t0\n  LEFT OUTER JOIN table2 t1\n    ON t0.key1 = t1.key3\nAfter one or more joins, you can reference any of the joined tables in a projection immediately after:\n\nexpr = joined[t1, t2.value2]\nibis.to_sql(expr)\n\nSELECT\n  t0.value1,\n  t0.key1,\n  t0.key2,\n  t1.value2\nFROM table1 AS t0\nLEFT OUTER JOIN table2 AS t1\n  ON t0.key1 = t1.key3\n\n\nIf you need to compute an expression that involves both tables, you can do that also:\n\nexpr = joined[t1.key1, (t1.value1 - t2.value2).name(\"diff\")]\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  t0.value1 - t1.value2 AS diff\nFROM table1 AS t0\nLEFT OUTER JOIN table2 AS t1\n  ON t0.key1 = t1.key3\n\n\n\n\nJoin + aggregation\nYou can directly aggregate a join without need for projection, which also allows you to form statistics that reference any of the joined tables.\nConsider this SQL:\nSELECT t0.key1, AVG(t0.value1 - t1.value2) AS avg_diff\nFROM table1 t0\n  LEFT OUTER JOIN table2 t1\n    ON t0.key1 = t1.key3\nGROUP BY 1\nAs you would hope, the code is as follows:\n\navg_diff = (t1.value1 - t2.value2).mean()\nexpr = (\n    t1.left_join(t2, t1.key1 == t2.key3)\n    .group_by(t1.key1)\n    .aggregate(avg_diff=avg_diff)\n)\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  AVG(t0.value1 - t1.value2) AS avg_diff\nFROM table1 AS t0\nLEFT OUTER JOIN table2 AS t1\n  ON t0.key1 = t1.key3\nGROUP BY\n  1\n\n\n\n\nJoin with SELECT *\nIf you try to compile or execute a join that has not been projected or aggregated, it will be fully materialized:\n\njoined = t1.left_join(t2, t1.key1 == t2.key3)\nibis.to_sql(joined)\n\nSELECT\n  t0.value1,\n  t0.key1,\n  t0.key2,\n  t1.value2,\n  t1.key3,\n  t1.key4\nFROM table1 AS t0\nLEFT OUTER JOIN table2 AS t1\n  ON t0.key1 = t1.key3\n\n\n\n\nMultiple joins\nYou can join multiple tables together in succession without needing to address any of the above concerns.\n\nt3 = ibis.table(dict(value3=\"float\", key5=\"string\"), name=\"table3\")\n\ntotal = (t1.value1 + t2.value2 + t3.value3).sum()\nexpr = (\n    t1.join(t2, [t1.key1 == t2.key3, t1.key2 == t2.key4])\n    .join(t3, t1.key1 == t3.key5)\n    .group_by([t2.key4, t3.key5])\n    .aggregate(total=total)\n)\nibis.to_sql(expr)\n\nSELECT\n  t1.key4,\n  t2.key5,\n  SUM(t0.value1 + t1.value2 + t2.value3) AS total\nFROM table1 AS t0\nJOIN table2 AS t1\n  ON t0.key1 = t1.key3 AND t0.key2 = t1.key4\nJOIN table3 AS t2\n  ON t0.key1 = t2.key5\nGROUP BY\n  1,\n  2\n\n\n\n\nSelf joins\nWhat about when you need to join a table on itself? For example:\nSELECT t0.one, avg(t0.two - t1.three) AS metric\nFROM my_data t0\n  INNER JOIN my_data t1\n    ON t0.one = t1.one\nGROUP BY 1\nThe table view method enables you to form a self-reference that is referentially distinct in expressions. Now you can proceed normally:\n\nt_view = t.view()\n\nstat = (t.two - t_view.three).mean()\nexpr = (\n    t.join(t_view, t.three.cast(\"string\") == t_view.one)\n    .group_by(t.one)\n    .aggregate(metric=stat)\n)\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  AVG(t1.two - t1.three) AS metric\nFROM (\n  SELECT\n    t1.one AS one,\n    t1.two AS two,\n    t1.three AS three,\n    t2.one AS one_right,\n    t2.two AS two_right,\n    t2.three AS three_right\n  FROM my_data AS t1\n  JOIN my_data AS t2\n    ON CAST(t1.three AS TEXT) = t2.one\n) AS t0, my_data AS t1, my_data AS t1\nGROUP BY\n  1\n\n\n\n\nOverlapping join keys\nIn many cases the columns being joined between two tables or table expressions have the same name. Consider this example:\n\nt4 = ibis.table(\n    dict(key1=\"string\", key2=\"string\", key3=\"string\", value1=\"float\"),\n    name=\"table4\",\n)\n\nt5 = ibis.table(\n    dict(key1=\"string\", key2=\"string\", key3=\"string\", value2=\"float\"),\n    name=\"table5\",\n)\n\nIn these case, we can specify a list of common join keys:\n\njoined = t4.join(t5, [\"key1\", \"key2\", \"key3\"])\nexpr = joined[t4, t5.value2]\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  t0.key2,\n  t0.key3,\n  t0.value1,\n  t1.value2\nFROM table4 AS t0\nJOIN table5 AS t1\n  ON t0.key1 = t1.key1 AND t0.key2 = t1.key2 AND t0.key3 = t1.key3\n\n\nYou can mix the overlapping key names with other expressions:\n\njoined = t4.join(t5, [\"key1\", \"key2\", t4.key3.left(4) == t4.key3.left(4)])\nexpr = joined[t4, t5.value2]\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  t0.key2,\n  t0.key3,\n  t0.value1,\n  t1.value2\nFROM table4 AS t0\nJOIN table5 AS t1\n  ON t0.key1 = t1.key1\n  AND t0.key2 = t1.key2\n  AND CASE\n    WHEN (\n      CAST(0 AS TINYINT) + 1 &gt;= 1\n    )\n    THEN SUBSTR(t0.key3, CAST(0 AS TINYINT) + 1, CAST(4 AS TINYINT))\n    ELSE SUBSTR(t0.key3, CAST(0 AS TINYINT) + 1 + LENGTH(t0.key3), CAST(4 AS TINYINT))\n  END = CASE\n    WHEN (\n      CAST(0 AS TINYINT) + 1 &gt;= 1\n    )\n    THEN SUBSTR(t0.key3, CAST(0 AS TINYINT) + 1, CAST(4 AS TINYINT))\n    ELSE SUBSTR(t0.key3, CAST(0 AS TINYINT) + 1 + LENGTH(t0.key3), CAST(4 AS TINYINT))\n  END\n\n\n\n\nNon-equality join predicates\nYou can join tables with boolean clauses that are not equality. Some query engines support these efficiently, some inefficiently, or some not at all. In the latter case, these conditions get moved by Ibis into the WHERE part of the SELECT query.\n\nexpr = t1.join(t2, t1.value1 &lt; t2.value2).group_by(t1.key1).size()\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  COUNT(*) AS \"CountStar()\"\nFROM table1 AS t0\nJOIN table2 AS t1\n  ON t0.value1 &lt; t1.value2\nGROUP BY\n  1\n\n\n\n\nOther ways to specify join keys\nYou can also pass a list of column names instead of forming boolean expressions:\n\njoined = t1.join(t2, [(\"key1\", \"key3\"), (\"key2\", \"key4\")])"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#subqueries",
    "href": "tutorials/ibis-for-sql-users.html#subqueries",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Subqueries",
    "text": "Subqueries\nIbis creates inline views and nested subqueries automatically. This section concerns more complex subqueries involving foreign references and other advanced relational algebra.\n\nCorrelated EXISTS / NOT EXISTS filters\nThe SQL EXISTS and NOT EXISTS constructs are typically used for efficient filtering in large many-to-many relationships.\nLet’s consider a web dataset involving website session / usage data and purchases:\n\nevents = ibis.table(\n    dict(session_id=\"int64\", user_id=\"int64\", event_type=\"int32\", ts=\"timestamp\"),\n    name=\"events\",\n)\n\npurchases = ibis.table(\n    dict(item_id=\"int64\", user_id=\"int64\", price=\"float\", ts=\"timestamp\"),\n    name=\"purchases\",\n)\n\nNow, the key user_id appears with high frequency in both tables. But let’s say you want to limit your analysis of the events table to only sessions by users who have made a purchase.\nIn SQL, you can do this using the somewhat esoteric EXISTS construct:\nSELECT t0.*\nFROM events t0\nWHERE EXISTS (\n  SELECT 1\n  FROM purchases t1\n  WHERE t0.user_id = t1.user_id\n)\nTo describe this operation in Ibis, you compare the user_id columns and use the any reduction:\n\ncond = (events.user_id == purchases.user_id).any()\n\nThis can now be used to filter events:\n\nexpr = events[cond]\nibis.to_sql(expr)\n\nSELECT\n  t0.session_id,\n  t0.user_id,\n  t0.event_type,\n  t0.ts\nFROM events AS t0\nWHERE\n  EXISTS(\n    SELECT\n      CAST(1 AS TINYINT) AS anon_1\n    FROM purchases AS t1\n    WHERE\n      t0.user_id = t1.user_id\n  )\n\n\nIf you negate the condition, it will instead give you only event data from user that have not made a purchase:\n\nexpr = events[-cond]\nibis.to_sql(expr)\n\nSELECT\n  t0.session_id,\n  t0.user_id,\n  t0.event_type,\n  t0.ts\nFROM events AS t0\nWHERE\n  NOT (\n    EXISTS(\n      SELECT\n        CAST(1 AS TINYINT) AS anon_1\n      FROM purchases AS t1\n      WHERE\n        t0.user_id = t1.user_id\n    )\n  )\n\n\n\n\nSubqueries with IN / NOT IN\nSubquery filters with IN (and NOT IN) are functionally similar to EXISTS subqueries. Let’s look at some SQL:\nSELECT *\nFROM events\nWHERE user_id IN (\n  SELECT user_id\n  FROM purchases\n)\nThis is almost semantically the same as the EXISTS example. Indeed, you can write with Ibis:\n\ncond = events.user_id.isin(purchases.user_id)\nexpr = events[cond]\nibis.to_sql(expr)\n\nSELECT\n  t0.session_id,\n  t0.user_id,\n  t0.event_type,\n  t0.ts\nFROM events AS t0\nWHERE\n  t0.user_id IN (\n    SELECT\n      t1.user_id\n    FROM purchases AS t1\n  )\n\n\nDepending on the query engine, the query planner/optimizer will often rewrite IN or EXISTS subqueries into the same set of relational algebra operations.\n\n\nComparison with scalar aggregates\nSometime you want to compare a value with an unconditional aggregate value from a different table. Take the SQL:\nSELECT *\nFROM table1\nWHERE value1 &gt; (\n  SELECT max(value2)\n  FROM table2\n)\nWith Ibis, the code is simpler and more pandas-like:\n\nexpr = t1[t1.value1 &gt; t2.value2.max()]\nibis.to_sql(expr)\n\nSELECT\n  t0.value1,\n  t0.key1,\n  t0.key2\nFROM table1 AS t0\nWHERE\n  t0.value1 &gt; (\n    SELECT\n      MAX(t1.value2) AS \"Max(value2)\"\n    FROM table2 AS t1\n  )\n\n\n\n\nConditional aggregates\nSuppose you want to compare a value with the aggregate value for some common group values between two tables. Here’s some SQL:\nSELECT *\nFROM table1 t0\nWHERE value1 &gt; (\n  SELECT avg(value2)\n  FROM table2 t1\n  WHERE t0.key1 = t1.key3\n)\nThis query computes the average for each distinct value of key3 and uses the corresponding average for the comparison, rather than the whole-table average as above.\nWith Ibis, the code is similar, but you add the correlated filter to the average statistic:\n\nstat = t2[t1.key1 == t2.key3].value2.mean()\nexpr = t1[t1.value1 &gt; stat]\nibis.to_sql(expr)\n\nSELECT\n  t0.value1,\n  t0.key1,\n  t0.key2\nFROM table1 AS t0\nWHERE\n  t0.value1 &gt; (\n    SELECT\n      AVG(t1.value2) AS \"Mean(value2)\"\n    FROM table2 AS t1\n    WHERE\n      t0.key1 = t1.key3\n  )"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#distinct-expressions",
    "href": "tutorials/ibis-for-sql-users.html#distinct-expressions",
    "title": "Tutorial: Ibis for SQL users",
    "section": "DISTINCT expressions",
    "text": "DISTINCT expressions\nIn SQL, the DISTINCT keyword is used in a couple of ways:\n\nDeduplicating identical rows in some SELECT statement\nAggregating on the distinct values of some column expression\n\nIbis supports both use cases. So let’s have a look. The first case is the simplest: call distinct on a table expression. First, here’s the SQL:\nSELECT DISTINCT *\nFROM table1\nAnd the Ibis Python code:\n\nexpr = t1.distinct()\nibis.to_sql(expr)\n\nSELECT DISTINCT\n  t0.value1,\n  t0.key1,\n  t0.key2\nFROM table1 AS t0\n\n\nFor distinct aggregates, the most common case is COUNT(DISTINCT ...), which computes the number of unique values in an expression. So if we’re looking at the events table, let’s compute the number of distinct event_type values for each user_id. First, the SQL:\nSELECT user_id, COUNT(DISTINCT event_type) AS unique_events\nFROM events\nGROUP BY 1\nIn Ibis this is:\n\nmetric = events.event_type.nunique()\nexpr = events.group_by(\"user_id\").aggregate(unique_events=metric)\nibis.to_sql(expr)\n\nSELECT\n  t0.user_id,\n  COUNT(DISTINCT t0.event_type) AS unique_events\nFROM events AS t0\nGROUP BY\n  1"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#window-functions",
    "href": "tutorials/ibis-for-sql-users.html#window-functions",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Window functions",
    "text": "Window functions\nWindow functions in SQL allow you to write expressions that involve possibly-ordered groups of a table. Each window function involves one of the following:\n\nAn analytic function. Most aggregate functions are valid analytic functions, and there are additional ones such as LEAD, LAG, NTILE, and others.\nA PARTITION BY clause. This may be omitted.\nAn ORDER BY clause. This may be omitted for many functions.\nA window frame clause. The default is to use the entire partition.\n\nSo you may see SQL like:\nAVG(value) OVER (PARTITION BY key1)\nOr simply\nAVG(value) OVER ()\nIbis will automatically write window clauses when you use aggregate functions in a non-aggregate context. Suppose you wanted to subtract the mean of a column from itself:\n\nexpr = t.mutate(two_demean=t.two - t.two.mean())\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.two - AVG(t0.two) OVER (ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS two_demean\nFROM my_data AS t0\n\n\nIf you use mutate in conjunction with group_by, it will add a PARTITION BY to the OVER specification:\n\nexpr = t.group_by(\"one\").mutate(two_demean=t.two - t.two.mean())\n\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.two - AVG(t0.two) OVER (PARTITION BY t0.one ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS two_demean\nFROM my_data AS t0\n\n\nFor functions like LAG that require an ordering, we can add an order_by call:\n\nexpr = t.group_by(\"one\").order_by(t.two).mutate(two_first_diff=t.two - t.two.lag())\n\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.two - LAG(t0.two, 1) OVER (PARTITION BY t0.one ORDER BY t0.two ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS two_first_diff\nFROM my_data AS t0\n\n\nFor more precision, you can create a Window object that also includes a window frame clause:\n\nw = ibis.window(group_by=\"one\", preceding=5, following=5)\nexpr = t.mutate(group_demeaned=t.two - t.two.mean().over(w))\nibis.to_sql(expr)\n\nSELECT\n  t0.one,\n  t0.two,\n  t0.three,\n  t0.two - AVG(t0.two) OVER (PARTITION BY t0.one ROWS BETWEEN CAST(5 AS TINYINT) PRECEDING AND CAST(5 AS TINYINT) FOLLOWING) AS group_demeaned\nFROM my_data AS t0"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#top-k-operations",
    "href": "tutorials/ibis-for-sql-users.html#top-k-operations",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Top-K operations",
    "text": "Top-K operations\nA common SQL idiom is the “top-K” or “top-N” operation: subsetting a dimension by aggregate statistics:\nSELECT key1, count(*) AS `count`\nFROM table1\nGROUP BY 1\nORDER BY `count` DESC\nLIMIT 10\nIbis has a special analytic expression topk:\n\nexpr = t1.key1.topk(10)\n\nThis can be evaluated directly, yielding the above query:\n\nibis.to_sql(expr)\n\nSELECT\n  t0.key1,\n  t0.\"Count(key1)\"\nFROM (\n  SELECT\n    t1.key1 AS key1,\n    COUNT(t1.key1) AS \"Count(key1)\"\n  FROM table1 AS t1\n  GROUP BY\n    1\n) AS t0\nORDER BY\n  t0.\"Count(key1)\" DESC\nLIMIT 10"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#date-time-data",
    "href": "tutorials/ibis-for-sql-users.html#date-time-data",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Date / time data",
    "text": "Date / time data\nSee Timestamp methods &lt;api.timestamp&gt; for a table of available date/time methods.\nFor example, we can do:\n\nexpr = events.mutate(year=events.ts.year(), month=events.ts.month())\n\nibis.to_sql(expr)\n\nSELECT\n  t0.session_id,\n  t0.user_id,\n  t0.event_type,\n  t0.ts,\n  CAST(EXTRACT(year FROM t0.ts) AS SMALLINT) AS year,\n  CAST(EXTRACT(month FROM t0.ts) AS SMALLINT) AS month\nFROM events AS t0\n\n\n\nCasting to date / time types\nIn many cases, you can convert string values to datetime / timestamp with strings.cast('timestamp'), but you may have to do some more reconnaissance into the data if this does not work.\n\n\nIntervals\nIbis has a set of interval APIs that allow you to do date/time arithmetic. For example:\n\nexpr = events[events.ts &gt; (ibis.now() - ibis.interval(years=1))]\nibis.to_sql(expr)\n\nSELECT\n  t0.session_id,\n  t0.user_id,\n  t0.event_type,\n  t0.ts\nFROM events AS t0\nWHERE\n  t0.ts &gt; CAST(NOW() AS TIMESTAMP) - TO_YEARS(1)\n\n\nThe implementation of each timedelta offset will depend on the query engine."
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#buckets-and-histograms",
    "href": "tutorials/ibis-for-sql-users.html#buckets-and-histograms",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Buckets and histograms",
    "text": "Buckets and histograms\nTo appear."
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#unions",
    "href": "tutorials/ibis-for-sql-users.html#unions",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Unions",
    "text": "Unions\nSQL dialects often support two kinds of UNION operations:\n\nUNION: the combination of distinct rows from each table.\nUNION ALL: the combination of all rows from each table, whether or not they are distinct.\n\nThe Ibis union function by distinct is a UNION ALL, and you can set distinct=True to get the normal UNION behavior:\n\nexpr1 = t1.limit(10)\nexpr2 = t1.limit(10, offset=10)\n\nexpr = expr1.union(expr2)\nibis.to_sql(expr)\n\nWITH anon_1 AS (\n  SELECT\n    t1.value1 AS value1,\n    t1.key1 AS key1,\n    t1.key2 AS key2\n  FROM table1 AS t1\n  LIMIT 10\n), anon_2 AS (\n  SELECT\n    t1.value1 AS value1,\n    t1.key1 AS key1,\n    t1.key2 AS key2\n  FROM table1 AS t1\n  LIMIT 10\n  OFFSET 10\n)\nSELECT\n  t0.value1,\n  t0.key1,\n  t0.key2\nFROM (\n  SELECT\n    anon_1.value1 AS value1,\n    anon_1.key1 AS key1,\n    anon_1.key2 AS key2\n  FROM anon_1\n  UNION ALL\n  SELECT\n    anon_2.value1 AS value1,\n    anon_2.key1 AS key1,\n    anon_2.key2 AS key2\n  FROM anon_2\n) AS t0"
  },
  {
    "objectID": "tutorials/ibis-for-sql-users.html#esoterica",
    "href": "tutorials/ibis-for-sql-users.html#esoterica",
    "title": "Tutorial: Ibis for SQL users",
    "section": "Esoterica",
    "text": "Esoterica\nThis area will be the spillover for miscellaneous SQL concepts and how queries featuring them can be ported to Ibis.\n\nCommon table expressions (CTEs)\nThe simplest SQL CTE is a SQL statement that is used multiple times in a SELECT query, which can be “factored” out using the WITH keyword:\nWITH t0 AS (\n   SELECT region, kind, sum(amount) AS total\n   FROM purchases\n   GROUP BY 1, 2\n)\nSELECT t0.region, t0.total - t1.total\nFROM t0\n  INNER JOIN t0 t1\n    ON t0.region = t1.region\nWHERE t0.kind = 'foo' AND t1.kind = 'bar'\nExplicit CTEs are not necessary with Ibis. Let’s look at an example involving joining an aggregated table on itself after filtering:\n\npurchases = ibis.table(\n    dict(region=\"string\", kind=\"string\", user=\"int64\", amount=\"float\"),\n    name=\"purchases\",\n)\n\nmetric = purchases.amount.sum().name(\"total\")\nagged = purchases.group_by([\"region\", \"kind\"]).aggregate(metric)\n\nleft = agged[agged.kind == \"foo\"]\nright = agged[agged.kind == \"bar\"]\n\nresult = left.join(right, left.region == right.region)[\n    left.region, (left.total - right.total).name(\"diff\")\n]\n\nIbis automatically creates a CTE for agged:\n\nibis.to_sql(result)\n\nWITH t1 AS (\n  SELECT\n    t2.region AS region,\n    t2.kind AS kind,\n    SUM(t2.amount) AS total\n  FROM purchases AS t2\n  WHERE\n    t2.kind = 'foo'\n  GROUP BY\n    1,\n    2\n), t0 AS (\n  SELECT\n    t2.region AS region,\n    t2.kind AS kind,\n    SUM(t2.amount) AS total\n  FROM purchases AS t2\n  WHERE\n    t2.kind = 'bar'\n  GROUP BY\n    1,\n    2\n)\nSELECT\n  t1.region,\n  t1.total - t0.total AS diff\nFROM t1\nJOIN t0\n  ON t1.region = t0.region"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html",
    "href": "tutorials/ibis-for-pandas-users.html",
    "title": "Tutorial: Ibis for pandas users",
    "section": "",
    "text": "We recommend starting with the default (DuckDB) backend for a performant, fully-featured local experience. You can install Ibis with pip, conda, mamba, or pixi.\n\nUsing pipUsing condaUsing mambaUsing pixi\n\n\npip install 'ibis-framework[duckdb]'\n\n\n\n\n\n\nWarning\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name.\n\n\n\n\nconda install -c conda-forge ibis-duckdb\n\n\nmamba install -c conda-forge ibis-duckdb\n\n\npixi add ibis-duckdb"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#prerequisites",
    "href": "tutorials/ibis-for-pandas-users.html#prerequisites",
    "title": "Tutorial: Ibis for pandas users",
    "section": "",
    "text": "We recommend starting with the default (DuckDB) backend for a performant, fully-featured local experience. You can install Ibis with pip, conda, mamba, or pixi.\n\nUsing pipUsing condaUsing mambaUsing pixi\n\n\npip install 'ibis-framework[duckdb]'\n\n\n\n\n\n\nWarning\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name.\n\n\n\n\nconda install -c conda-forge ibis-duckdb\n\n\nmamba install -c conda-forge ibis-duckdb\n\n\npixi add ibis-duckdb"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#overview",
    "href": "tutorials/ibis-for-pandas-users.html#overview",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Overview",
    "text": "Overview\nMuch of the syntax and many of the operations in Ibis are inspired by the pandas DataFrame but the primary domain of Ibis is SQL so there are some differences in how they operate.\nFor one thing, SQL (and therefore Ibis) makes no guarantees about row order, which is a key assumption that numpy/pandas users are used to. So two columns can’t be deterministically “lined up” unless they are actually part of the same Table. An outcome of this is that you can’t index into Columns by position (column.head(5) or column[3:5]). You can only index into Tables (table.head(5) or table[3:5]). So if you want the first 5 rows of a column, you have to do table.head(5).my_column, table.my_column.head(5) will not work.\nAnother difference between Ibis tables and pandas DataFrames are that many of the pandas DataFrame operations do in-place operations (they are “mutable”), whereas Ibis table operations always return a new table expression (“immutable”).\nFinally, Ibis table expressions are lazy, meaning that as you build up a table expression, no computation is actually performed until you call an action method such as to_pandas. Only then does Ibis compile the table expression into SQL and send it to the backend. (Note that we’ll be using Ibis’ interactive mode to automatically execute queries at the end of each cell in this notebook. If you are using similar code in a program, you will have to add .to_pandas() to each operation that you want to evaluate.)\n\nimport ibis\nimport pandas as pd\n\nibis.options.interactive = True\n\nWe’ll be using the DuckDB backend in Ibis in the examples below. First we’ll create a simple DataFrame.\n\ndf = pd.DataFrame(\n    [[\"a\", 1, 2], [\"b\", 3, 4]],\n    columns=[\"one\", \"two\", \"three\"],\n    index=[5, 6],\n)\ndf\n\n\n\n\n\n\n\n\none\ntwo\nthree\n\n\n\n\n5\na\n1\n2\n\n\n6\nb\n3\n4\n\n\n\n\n\n\n\nNow we can create an Ibis table from the above DataFrame.\nNote that the index from the pandas DataFrame is dropped. Ibis has no notion of an index: If you want to use the index, you will need to turn it into a column.\n\nt = ibis.memtable(df, name=\"t\")\nt\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃ three ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     1 │     2 │\n│ b      │     3 │     4 │\n└────────┴───────┴───────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#data-types",
    "href": "tutorials/ibis-for-pandas-users.html#data-types",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Data types",
    "text": "Data types\nThe data types of columns in pandas are accessed using the dtypes attribute. This returns a Series object.\n\ndf.dtypes\n\none      object\ntwo       int64\nthree     int64\ndtype: object\n\n\nIn Ibis, you use the schema method which returns an ibis.Schema object.\n\nt.schema()\n\nibis.Schema {\n  one    string\n  two    int64\n  three  int64\n}\n\n\nIt is possible to convert the schema information to pandas data types using the to_pandas method, if needed.\n\nt.schema().to_pandas()\n\n[('one', dtype('O')), ('two', dtype('int64')), ('three', dtype('int64'))]"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#table-layout",
    "href": "tutorials/ibis-for-pandas-users.html#table-layout",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Table layout",
    "text": "Table layout\nIn pandas, the layout of the table is contained in the shape attribute which contains the number of rows and number of columns in a tuple. The number of columns in an Ibis table can be gotten from the length of the schema.\n\nlen(t.schema())\n\n3\n\n\nTo get the number of rows of a table, you use the count method.\n\nt.count()\n\n\n\n\n\n2\n\n\n\nTo mimic pandas’ behavior, you would use the following code. Note that you need to use the to_pandas method after count to evaluate the expression returned by count.\n\n(t.count().to_pandas(), len(t.schema()))\n\n(2, 3)\n\n\n\ndf.shape\n\n(2, 3)"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#subsetting-columns",
    "href": "tutorials/ibis-for-pandas-users.html#subsetting-columns",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Subsetting columns",
    "text": "Subsetting columns\nSelecting columns is very similar to in pandas. In fact, you can use the same syntax.\n\nt[[\"one\", \"two\"]]\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ string │ int64 │\n├────────┼───────┤\n│ a      │     1 │\n│ b      │     3 │\n└────────┴───────┘\n\n\n\nHowever, since row-level indexing is not supported in Ibis, the inner list is not necessary.\n\nt[\"one\", \"two\"]\n\n┏━━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃\n┡━━━━━━━━╇━━━━━━━┩\n│ string │ int64 │\n├────────┼───────┤\n│ a      │     1 │\n│ b      │     3 │\n└────────┴───────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#selecting-columns",
    "href": "tutorials/ibis-for-pandas-users.html#selecting-columns",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Selecting columns",
    "text": "Selecting columns\nSelecting columns is done using the same syntax as in pandas DataFrames. You can use either the indexing syntax or attribute syntax.\n\nt[\"one\"]\n\n┏━━━━━━━━┓\n┃ one    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n└────────┘\n\n\n\nor:\n\nt.one\n\n┏━━━━━━━━┓\n┃ one    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a      │\n│ b      │\n└────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#adding-removing-and-modifying-columns",
    "href": "tutorials/ibis-for-pandas-users.html#adding-removing-and-modifying-columns",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Adding, removing, and modifying columns",
    "text": "Adding, removing, and modifying columns\nModifying the columns of an Ibis table is a bit different than doing the same operations in a pandas DataFrame. This is primarily due to the fact that in-place operations are not supported on Ibis tables. Each time you do a column modification to a table, a new table expression is returned.\n\nAdding columns\nAdding columns is done through the mutate method.\n\nmutated = t.mutate(new_col=t.three * 2)\nmutated\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n┃ one    ┃ two   ┃ three ┃ new_col ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n│ string │ int64 │ int64 │ int64   │\n├────────┼───────┼───────┼─────────┤\n│ a      │     1 │     2 │       4 │\n│ b      │     3 │     4 │       8 │\n└────────┴───────┴───────┴─────────┘\n\n\n\nNotice that the original table object remains unchanged. Only the mutated object that was returned contains the new column.\n\nt\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃ three ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     1 │     2 │\n│ b      │     3 │     4 │\n└────────┴───────┴───────┘\n\n\n\nIt is also possible to create a column in isolation. This is similar to a Series in pandas. Note that the name of the column by default is a representation of the expression:\n\nunnamed = t.three * 2\nunnamed\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Multiply(three, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                  4 │\n│                  8 │\n└────────────────────┘\n\n\n\nTo get a version with a specific name, you can use the name method:\n\nnew_col = unnamed.name(\"new_col\")\nnew_col\n\n┏━━━━━━━━━┓\n┃ new_col ┃\n┡━━━━━━━━━┩\n│ int64   │\n├─────────┤\n│       4 │\n│       8 │\n└─────────┘\n\n\n\nYou can then add this column to the table using a projection.\n\nproj = t[\"one\", \"two\", new_col]\nproj\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n┃ one    ┃ two   ┃ new_col ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n│ string │ int64 │ int64   │\n├────────┼───────┼─────────┤\n│ a      │     1 │       4 │\n│ b      │     3 │       8 │\n└────────┴───────┴─────────┘\n\n\n\n\n\nRemoving columns\nRemoving a column is done using the drop method.\n\nt.columns\n\n['one', 'two', 'three']\n\n\n\nsubset = t.drop(\"one\", \"two\")\nsubset.columns\n\n['three']\n\n\nIt is also possible to drop columns by selecting the columns you want to remain.\n\nsubset = t[\"two\", \"three\"]\nsubset.columns\n\n['two', 'three']\n\n\n\n\nModifying columns\nReplacing existing columns is done using the mutate method just like adding columns. You add a column of the same name to replace it.\n\nt\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃ three ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     1 │     2 │\n│ b      │     3 │     4 │\n└────────┴───────┴───────┘\n\n\n\n\nmutated = t.mutate(two=t.two * 2)\nmutated\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ one    ┃ two   ┃ three ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     2 │     2 │\n│ b      │     6 │     4 │\n└────────┴───────┴───────┘\n\n\n\n\n\nRenaming columns\nIn addition to replacing columns, you can rename them as well. This is done with the rename method which takes a dictionary containing the name mappings.\n\nrenamed = t.rename(\n    dict(\n        a=\"one\",\n        b=\"two\",\n    )\n)\nrenamed\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ a      ┃ b     ┃ three ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     1 │     2 │\n│ b      │     3 │     4 │\n└────────┴───────┴───────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#selecting-rows",
    "href": "tutorials/ibis-for-pandas-users.html#selecting-rows",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Selecting rows",
    "text": "Selecting rows\nThere are several methods that can be used to select rows of data in various ways. These are described in the sections below. We’ll use the Palmer Penguins\\(^1\\) dataset to investigate! Ibis has several built-in example datasets that you can access using the ibis.examples module.\n\\(^1\\): Horst AM, Hill AP, Gorman KB (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. R package version 0.1.0. https://allisonhorst.github.io/palmerpenguins/. doi: 10.5281/zenodo.3960218.\n\npenguins = ibis.examples.penguins.fetch()\n\n\npenguins\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\nHead, tail and limit\nThe head method works the same ways as in pandas. Note that some Ibis backends may not have an inherent ordering of their rows and using head may not return deterministic results. In those cases, you can use sorting before calling head to ensure a stable result.\n\npenguins.head(5)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nHowever, the tail method is not implemented since it is not supported in all databases. It is possible to emulate the tail method if you use sorting in your table to do a reverse sort then use the head method to retrieve the “top” rows.\nAnother way to limit the number of retrieved rows is using the limit method. The following will return the same result as head(5). This is often used in conjunction with other filtering techniques that we will cover later.\n\npenguins.limit(5)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nFiltering rows\nIn addition to limiting the number of rows that are returned, it is possible to filter the rows using expressions. Expressions are constructed very similarly to the way they are in pandas. Ibis expressions are constructed from operations on columns in a table which return a boolean result. This result is then used to filter the table.\n\nexpr = penguins.bill_length_mm &gt; 37.0\nexpr\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Greater(bill_length_mm, 37.0) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                       │\n├───────────────────────────────┤\n│ True                          │\n│ True                          │\n│ True                          │\n│ NULL                          │\n│ False                         │\n│ True                          │\n│ True                          │\n│ True                          │\n│ False                         │\n│ True                          │\n│ …                             │\n└───────────────────────────────┘\n\n\n\nWe can evaluate the value counts to see how many rows we will expect to get back after filtering.\n\nexpr.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Greater(bill_length_mm, 37.0) ┃ Greater(bill_length_mm, 37.0)_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                       │ int64                               │\n├───────────────────────────────┼─────────────────────────────────────┤\n│ False                         │                                  42 │\n│ NULL                          │                                   2 │\n│ True                          │                                 300 │\n└───────────────────────────────┴─────────────────────────────────────┘\n\n\n\nNow we apply the filter to the table. Since there are 6 True values in the expression, we should get 6 rows back.\n\nfiltered = penguins[expr]\nfiltered\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           37.8 │          17.1 │               186 │        3300 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           37.8 │          17.3 │               180 │        3700 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │               182 │        3200 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nOf course, the filtering expression can be applied inline as well.\n\nfiltered = penguins[penguins.bill_length_mm &gt; 37.0]\nfiltered\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           37.8 │          17.1 │               186 │        3300 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           37.8 │          17.3 │               180 │        3700 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │               182 │        3200 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nMultiple filtering expressions can be combined into a single expression or chained onto existing table expressions.\n\nfiltered = penguins[(penguins.bill_length_mm &gt; 37.0) & (penguins.bill_depth_mm &gt; 18.0)]\nfiltered\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.7 │          19.0 │               195 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           42.5 │          20.7 │               197 │        4500 │ male   │  2007 │\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │  2007 │\n│ Adelie  │ Biscoe    │           37.8 │          18.3 │               174 │        3400 │ female │  2007 │\n│ Adelie  │ Biscoe    │           37.7 │          18.7 │               180 │        3600 │ male   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nThe code above will return the same rows as the code below.\n\nfiltered = penguins[penguins.bill_length_mm &gt; 37.0][penguins.bill_depth_mm &gt; 18.0]\nfiltered\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.7 │          19.0 │               195 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           42.5 │          20.7 │               197 │        4500 │ male   │  2007 │\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │  2007 │\n│ Adelie  │ Biscoe    │           37.8 │          18.3 │               174 │        3400 │ female │  2007 │\n│ Adelie  │ Biscoe    │           37.7 │          18.7 │               180 │        3600 │ male   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nAggregation has not been discussed yet, but aggregate values can be used in expressions to return things such as all of the rows in a data set where the value in a column is greater than the mean.\n\nfiltered = penguins[penguins.bill_length_mm &gt; penguins.bill_length_mm.mean()]\nfiltered\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │  2007 │\n│ Adelie  │ Dream     │           44.1 │          19.7 │               196 │        4400 │ male   │  2007 │\n│ Adelie  │ Torgersen │           45.8 │          18.9 │               197 │        4150 │ male   │  2008 │\n│ Adelie  │ Biscoe    │           45.6 │          20.3 │               191 │        4600 │ male   │  2009 │\n│ Adelie  │ Torgersen │           44.1 │          18.0 │               210 │        4000 │ male   │  2009 │\n│ Gentoo  │ Biscoe    │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n│ Gentoo  │ Biscoe    │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe    │           48.7 │          14.1 │               210 │        4450 │ female │  2007 │\n│ Gentoo  │ Biscoe    │           50.0 │          15.2 │               218 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe    │           47.6 │          14.5 │               215 │        5400 │ male   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nModifying rows\nSometimes you want to modify the values in a column based on some condition. In pandas, you would do something like df.loc[condition] = new_value. In Ibis though, remember that all expressions are immutable, so you need to create a new table expression with the modified values. You do this using the ifelse method on boolean columns:\n\nlong_billed_penguins = penguins.bill_length_mm &gt; 37.0\nspecies_modified = long_billed_penguins.ifelse(\"wide\", penguins.species)\npenguins.mutate(species_modified=species_modified)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ species_modified ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ string           │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼──────────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │ wide             │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │ wide             │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │ wide             │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │ Adelie           │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │ Adelie           │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │ wide             │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │ wide             │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │ wide             │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │ Adelie           │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │ wide             │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │ …                │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴──────────────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#sorting-rows",
    "href": "tutorials/ibis-for-pandas-users.html#sorting-rows",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Sorting rows",
    "text": "Sorting rows\nSorting rows in Ibis uses a somewhat different API than in pandas. In pandas, you would use the sort_values method to order rows by values in specified columns. Ibis uses a method called order_by. To specify ascending or descending orders, pandas uses an ascending= argument to sort_values that indicates the order for each sorting column. Ibis allows you to tag the column name in the order_by list as ascending or descending by wrapping it with ibis.asc or ibis.desc.\nFirst, let’s ask Ibis for a pandas DataFrame version of the penguin data:\n\ndf = penguins.to_pandas()\n\nHere is an example of sorting a DataFrame using two sort keys. One key is sorting in ascending order and the other is in descending order.\n\ndf.sort_values(\n    [\"bill_length_mm\", \"bill_depth_mm\"], ascending=[True, False], na_position=\"first\"\n).head(5)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNone\n2007\n\n\n271\nGentoo\nBiscoe\nNaN\nNaN\nNaN\nNaN\nNone\n2009\n\n\n142\nAdelie\nDream\n32.1\n15.5\n188.0\n3050.0\nfemale\n2009\n\n\n98\nAdelie\nDream\n33.1\n16.1\n178.0\n2900.0\nfemale\n2008\n\n\n70\nAdelie\nTorgersen\n33.5\n19.0\n190.0\n3600.0\nfemale\n2008\n\n\n\n\n\n\n\nThe same operation in Ibis would look like the following. Note that the index values of the resulting DataFrame start from zero and count up, whereas in the example above, they retain their original index value. This is because rows in tables don’t necessarily have a stable index in database backends, so the index is generated on the result.\n\nsorted = penguins.order_by([\"bill_length_mm\", ibis.desc(\"bill_depth_mm\")]).head(5)\nsorted\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Dream     │           32.1 │          15.5 │               188 │        3050 │ female │  2009 │\n│ Adelie  │ Dream     │           33.1 │          16.1 │               178 │        2900 │ female │  2008 │\n│ Adelie  │ Torgersen │           33.5 │          19.0 │               190 │        3600 │ female │  2008 │\n│ Adelie  │ Dream     │           34.0 │          17.1 │               185 │        3400 │ female │  2008 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#aggregation",
    "href": "tutorials/ibis-for-pandas-users.html#aggregation",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Aggregation",
    "text": "Aggregation\nAggregation in pandas is typically done by computing columns based on an aggregate function.\n\nstats = [df.bill_depth_mm.sum(), df.bill_length_mm.mean()]\npd.DataFrame([stats], columns=[\"total_bill_depth\", \"avg.bill_length\"])\n\n\n\n\n\n\n\n\ntotal_bill_depth\navg.bill_length\n\n\n\n\n0\n5865.7\n43.92193\n\n\n\n\n\n\n\nIn Ibis, you construct aggregate expressions then apply them to the table using the aggregate method.\n\nstats = [\n    penguins.bill_depth_mm.sum().name(\"total_bill_width\"),\n    penguins.bill_length_mm.mean().name(\"avg_bill_length\"),\n]\nagged = penguins.aggregate(stats)\nagged\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ total_bill_width ┃ avg_bill_length ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ float64          │ float64         │\n├──────────────────┼─────────────────┤\n│           5865.7 │        43.92193 │\n└──────────────────┴─────────────────┘\n\n\n\nYou can also combine both operations into one and pass the aggregate expressions using keyword parameters.\n\nagged = penguins.aggregate(\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ total_bill_depth ┃ avg_bill_length ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ float64          │ float64         │\n├──────────────────┼─────────────────┤\n│           5865.7 │        43.92193 │\n└──────────────────┴─────────────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#group-by",
    "href": "tutorials/ibis-for-pandas-users.html#group-by",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Group by",
    "text": "Group by\nUsing a similar API as above, aggregations can also be done across groupings using the by= parameter.\n\nagged = penguins.aggregate(\n    by=\"species\",\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ total_bill_depth ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64          │ float64         │\n├───────────┼──────────────────┼─────────────────┤\n│ Adelie    │           2770.3 │       38.791391 │\n│ Chinstrap │           1252.6 │       48.833824 │\n│ Gentoo    │           1842.8 │       47.504878 │\n└───────────┴──────────────────┴─────────────────┘\n\n\n\nAlternatively, by groups can be computed using a grouped table.\n\nagged = penguins.group_by(\"species\").aggregate(\n    total_bill_depth=penguins.bill_depth_mm.sum(),\n    avg_bill_length=penguins.bill_length_mm.mean(),\n)\nagged\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ total_bill_depth ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ float64          │ float64         │\n├───────────┼──────────────────┼─────────────────┤\n│ Adelie    │           2770.3 │       38.791391 │\n│ Chinstrap │           1252.6 │       48.833824 │\n│ Gentoo    │           1842.8 │       47.504878 │\n└───────────┴──────────────────┴─────────────────┘\n\n\n\nYou can group over multiple columns too, and rename them if you want.\nIf you only need to aggregate over a single column, then you don’t need to use the .aggregate() method.\n\npenguins.group_by([\"species\", \"sex\"], location=\"island\").body_mass_g.approx_median()\n\n┏━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ sex    ┃ location  ┃ approx_median(body_mass_g) ┃\n┡━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string │ string    │ int64                      │\n├───────────┼────────┼───────────┼────────────────────────────┤\n│ Adelie    │ female │ Dream     │                       3400 │\n│ Gentoo    │ female │ Biscoe    │                       4700 │\n│ Gentoo    │ NULL   │ Biscoe    │                       4688 │\n│ Adelie    │ NULL   │ Torgersen │                       3588 │\n│ Adelie    │ NULL   │ Dream     │                       2975 │\n│ Chinstrap │ male   │ Dream     │                       3950 │\n│ Adelie    │ female │ Torgersen │                       3400 │\n│ Adelie    │ female │ Biscoe    │                       3375 │\n│ Adelie    │ male   │ Torgersen │                       4000 │\n│ Adelie    │ male   │ Biscoe    │                       4000 │\n│ …         │ …      │ …         │                          … │\n└───────────┴────────┴───────────┴────────────────────────────┘\n\n\n\nInstead of aggregating after a group by, you can also transform the table so that the output table has the same number of rows as the input table. This is analogous to the groupby().transform() pattern in pandas. You can pass complex expressions to compute per-group:\n\n# Calculate how much the mass of each penguin deviates from the mean\npenguins.group_by([\"species\", \"sex\"]).mutate(\n    # This column isn't needed, but it makes it easier to see what's going on\n    mass_mean=penguins.body_mass_g.mean(),\n    mass_deviation=penguins.body_mass_g - penguins.body_mass_g.mean(),\n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ mass_mean   ┃ mass_deviation ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ float64     │ float64        │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼─────────────┼────────────────┤\n│ Gentoo  │ Biscoe │           44.5 │          14.3 │               216 │        4100 │ NULL   │  2007 │ 4587.500000 │    -487.500000 │\n│ Gentoo  │ Biscoe │           46.2 │          14.4 │               214 │        4650 │ NULL   │  2008 │ 4587.500000 │      62.500000 │\n│ Gentoo  │ Biscoe │           47.3 │          13.8 │               216 │        4725 │ NULL   │  2009 │ 4587.500000 │     137.500000 │\n│ Gentoo  │ Biscoe │           44.5 │          15.7 │               217 │        4875 │ NULL   │  2009 │ 4587.500000 │     287.500000 │\n│ Gentoo  │ Biscoe │           NULL │          NULL │              NULL │        NULL │ NULL   │  2009 │ 4587.500000 │           NULL │\n│ Gentoo  │ Biscoe │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │ 5484.836066 │     215.163934 │\n│ Gentoo  │ Biscoe │           49.9 │          16.1 │               213 │        5400 │ male   │  2009 │ 5484.836066 │     -84.836066 │\n│ Gentoo  │ Biscoe │           47.6 │          14.5 │               215 │        5400 │ male   │  2007 │ 5484.836066 │     -84.836066 │\n│ Gentoo  │ Biscoe │           46.7 │          15.3 │               219 │        5200 │ male   │  2007 │ 5484.836066 │    -284.836066 │\n│ Gentoo  │ Biscoe │           46.8 │          15.4 │               215 │        5150 │ male   │  2007 │ 5484.836066 │    -334.836066 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │           … │              … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴─────────────┴────────────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#null-values",
    "href": "tutorials/ibis-for-pandas-users.html#null-values",
    "title": "Tutorial: Ibis for pandas users",
    "section": "NULL values",
    "text": "NULL values\nIbis has first-class support for NULL values. In pandas and numpy, NULL values are represented by NaN. This can be confusing when working with numeric data, since NaN is also a valid floating point value (along with +/-inf).\nIn Ibis, we try to be more precise: All data types are nullable, and we use ibis.NA to represent NULL values, and all datatypes have a .isnull() method. For floating point values, we use different values for NaN and +/-inf, and there are the additional methods .isnan() and .isinf().\n\nDropping rows with NULLs\nBoth pandas and Ibis allow you to drop rows from a table based on whether a set of columns contains a NULL value. This method is called dropna in both packages. The common set of parameters in the two are subset= and how=. The subset= parameter indicates which columns to inspect for NULL values. The how= parameter specifies whether ‘any’ or ‘all’ of the specified columns must be NULL in order for the row to be dropped.\n\nno_null_peng = penguins.dropna([\"bill_depth_mm\", \"bill_length_mm\"], how=\"any\")\n\n\n\nFilling NULL values\nBoth pandas and Ibis allow you to fill NULL values in a table. In Ibis, the replacement value can only be a scalar value of a dictionary of values. If it is a dictionary, the keys of the dictionary specify the column name for the value to apply to.\n\nno_null_peng = penguins.fillna(dict(bill_depth_mm=0, bill_length_mm=0))\n\n\n\nReplacing NULLs\nBoth pandas and Ibis have fillna methods which allow you to specify a replacement value for NULL values.\n\nbill_length_no_nulls = penguins.bill_length_mm.fillna(0)"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#type-casts",
    "href": "tutorials/ibis-for-pandas-users.html#type-casts",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Type casts",
    "text": "Type casts\nType casting in pandas is done using the astype method on columns.\n\ndf.bill_depth_mm.astype(str)\n\n0      18.7\n1      17.4\n2      18.0\n3       nan\n4      19.3\n       ... \n339    19.8\n340    18.1\n341    18.2\n342    19.0\n343    18.7\nName: bill_depth_mm, Length: 344, dtype: object\n\n\nIn Ibis, you cast the column type using the cast method.\n\npenguins.bill_depth_mm.cast(\"int\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Cast(bill_depth_mm, int64) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                      │\n├────────────────────────────┤\n│                         19 │\n│                         17 │\n│                         18 │\n│                       NULL │\n│                         19 │\n│                         21 │\n│                         18 │\n│                         20 │\n│                         18 │\n│                         20 │\n│                          … │\n└────────────────────────────┘\n\n\n\nCasted columns can be assigned back to the table using the mutate method described earlier.\n\ncasted = penguins.mutate(\n    bill_depth_mm=penguins.bill_depth_mm.cast(\"int\"),\n    bill_length_mm=penguins.bill_length_mm.cast(\"int\"),\n)\ncasted.schema()\n\nibis.Schema {\n  species            string\n  island             string\n  bill_length_mm     int64\n  bill_depth_mm      int64\n  flipper_length_mm  int64\n  body_mass_g        int64\n  sex                string\n  year               int64\n}"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#set-membership",
    "href": "tutorials/ibis-for-pandas-users.html#set-membership",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Set membership",
    "text": "Set membership\npandas set membership uses the in and not in operators such as 'a' in df.species. Ibis uses isin and notin methods. In addition to testing membership in a set, these methods allow you to specify an else case to assign a value when the value isn’t in the set.\n\npenguins.species.value_counts()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ species   ┃ species_count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string    │ int64         │\n├───────────┼───────────────┤\n│ Chinstrap │            68 │\n│ Adelie    │           152 │\n│ Gentoo    │           124 │\n└───────────┴───────────────┘\n\n\n\n\nrefined = penguins.species.isin([\"Adelie\", \"Chinstrap\"])\nrefined.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ InValues(species) ┃ InValues(species)_count ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean           │ int64                   │\n├───────────────────┼─────────────────────────┤\n│ False             │                     124 │\n│ True              │                     220 │\n└───────────────────┴─────────────────────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#merging-tables",
    "href": "tutorials/ibis-for-pandas-users.html#merging-tables",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Merging tables",
    "text": "Merging tables\nWhile pandas uses the merge method to combine data from multiple DataFrames, Ibis uses the join method. They both have similar capabilities.\nThe biggest difference between Ibis’ join method and pandas’ merge method is that pandas only accepts column names or index levels to join on, whereas Ibis can merge on expressions.\nHere are some examples of merging using pandas.\n\ndf_left = pd.DataFrame(\n    [\n        [\"a\", 1, 2],\n        [\"b\", 3, 4],\n        [\"c\", 4, 6],\n    ],\n    columns=[\"name\", \"x\", \"y\"],\n)\n\ndf_right = pd.DataFrame(\n    [\n        [\"a\", 100, 200],\n        [\"m\", 300, 400],\n        [\"n\", 400, 600],\n    ],\n    columns=[\"name\", \"x_100\", \"y_100\"],\n)\n\n\ndf_left.merge(df_right, on=\"name\")\n\n\n\n\n\n\n\n\nname\nx\ny\nx_100\ny_100\n\n\n\n\n0\na\n1\n2\n100\n200\n\n\n\n\n\n\n\n\ndf_left.merge(df_right, on=\"name\", how=\"outer\")\n\n\n\n\n\n\n\n\nname\nx\ny\nx_100\ny_100\n\n\n\n\n0\na\n1.0\n2.0\n100.0\n200.0\n\n\n1\nb\n3.0\n4.0\nNaN\nNaN\n\n\n2\nc\n4.0\n6.0\nNaN\nNaN\n\n\n3\nm\nNaN\nNaN\n300.0\n400.0\n\n\n4\nn\nNaN\nNaN\n400.0\n600.0\n\n\n\n\n\n\n\nWe can now convert DataFrames to Ibis tables to do joins.\n\nt_left = ibis.memtable(df_left, name=\"t_left\")\nt_right = ibis.memtable(df_right, name=\"t_right\")\n\n\nt_left.join(t_right, t_left.name == t_right.name)\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ name   ┃ x     ┃ y     ┃ x_100 ┃ y_100 ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │ int64 │ int64 │\n├────────┼───────┼───────┼───────┼───────┤\n│ a      │     1 │     2 │   100 │   200 │\n└────────┴───────┴───────┴───────┴───────┘\n\n\n\nBelow is an outer join where missing values are filled with NaN.\n\nt_left.join(t_right, t_left.name == t_right.name, how=\"outer\")\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ name   ┃ x     ┃ y     ┃ name_right ┃ x_100 ┃ y_100 ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │ string     │ int64 │ int64 │\n├────────┼───────┼───────┼────────────┼───────┼───────┤\n│ a      │     1 │     2 │ a          │   100 │   200 │\n│ b      │     3 │     4 │ NULL       │  NULL │  NULL │\n│ c      │     4 │     6 │ NULL       │  NULL │  NULL │\n│ NULL   │  NULL │  NULL │ m          │   300 │   400 │\n│ NULL   │  NULL │  NULL │ n          │   400 │   600 │\n└────────┴───────┴───────┴────────────┴───────┴───────┘"
  },
  {
    "objectID": "tutorials/ibis-for-pandas-users.html#concatenating-tables",
    "href": "tutorials/ibis-for-pandas-users.html#concatenating-tables",
    "title": "Tutorial: Ibis for pandas users",
    "section": "Concatenating tables",
    "text": "Concatenating tables\nConcatenating DataFrames in pandas is done with the concat top-level function. It takes multiple DataFrames and concatenates the rows of one DataFrame to the next. If the columns are mis-matched, it extends the list of columns to include the full set of columns and inserts NaNs and Nones into the missing values.\nConcatenating tables in Ibis can only be done on tables with matching schemas. The concatenation is done using the top-level union function or the union method on a table.\nWe’ll demonstrate a pandas concat first.\n\ndf_1 = pd.DataFrame(\n    [\n        [\"a\", 1, 2],\n        [\"b\", 3, 4],\n        [\"c\", 4, 6],\n    ],\n    columns=[\"name\", \"x\", \"y\"],\n)\n\ndf_2 = pd.DataFrame(\n    [\n        [\"a\", 100, 200],\n        [\"m\", 300, 400],\n        [\"n\", 400, 600],\n    ],\n    columns=[\"name\", \"x\", \"y\"],\n)\n\n\npd.concat([df_1, df_2])\n\n\n\n\n\n\n\n\nname\nx\ny\n\n\n\n\n0\na\n1\n2\n\n\n1\nb\n3\n4\n\n\n2\nc\n4\n6\n\n\n0\na\n100\n200\n\n\n1\nm\n300\n400\n\n\n2\nn\n400\n600\n\n\n\n\n\n\n\nNow we can convert the DataFrames to Ibis tables and combine the tables using a union.\n\nt_1 = ibis.memtable(df_1, name=\"t_1\")\nt_2 = ibis.memtable(df_2, name=\"t_2\")\n\n\nunioned = ibis.union(t_1, t_2)\nunioned\n\n┏━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n┃ name   ┃ x     ┃ y     ┃\n┡━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n│ string │ int64 │ int64 │\n├────────┼───────┼───────┤\n│ a      │     1 │     2 │\n│ b      │     3 │     4 │\n│ c      │     4 │     6 │\n│ a      │   100 │   200 │\n│ m      │   300 │   400 │\n│ n      │   400 │   600 │\n└────────┴───────┴───────┘"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/0_setup.html",
    "href": "tutorials/data-platforms/starburst-galaxy/0_setup.html",
    "title": "Requirements and setup",
    "section": "",
    "text": "In this tutorial, we will connect to Starburst Galaxy and verify our connection. Following tutorials will go through the basics of Ibis on Starburst Galaxy’s demo data."
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/0_setup.html#prerequisites",
    "href": "tutorials/data-platforms/starburst-galaxy/0_setup.html#prerequisites",
    "title": "Requirements and setup",
    "section": "Prerequisites",
    "text": "Prerequisites\nYou need a Python environment with Ibis installed and a Starburst Galaxy account."
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/0_setup.html#connect-to-starburst-galaxy",
    "href": "tutorials/data-platforms/starburst-galaxy/0_setup.html#connect-to-starburst-galaxy",
    "title": "Requirements and setup",
    "section": "Connect to Starburst Galaxy",
    "text": "Connect to Starburst Galaxy\nFirst, connect to Starburst Galaxy. We’ll use a .env in this example for secrets that are loaded as environment variables. This requires installing the python-dotenv package – alternatively, you can set the environment variables for your system.\n\n\n\n\n\n\nTip\n\n\n\nHover over (or click on mobile) the numbers in the code blocks to see tips and explanations.\n\n\n\n1import os\nimport ibis\nfrom dotenv import load_dotenv\n\n2ibis.options.interactive = True\n\n3load_dotenv()\n\n4user = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nhost = os.getenv(\"HOSTNAME\")\nport = os.getenv(\"PORTNUMBER\")\n5catalog = \"sample\"\nschema = \"demo\"\n\n6con = ibis.trino.connect(\n    user=user, password=password, host=host, port=port, database=catalog, schema=schema\n)\n7con\n\n\n1\n\nImport necessary libraries.\n\n2\n\nUse Ibis in interactive mode.\n\n3\n\nLoad environment variables.\n\n4\n\nLoad secrets from environment variables.\n\n5\n\nUse the sample demo data.\n\n6\n\nConnect to Starburst Galaxy.\n\n7\n\nDisplay the connection object.\n\n\n\n\n&lt;ibis.backends.trino.Backend at 0x13f48e850&gt;"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/0_setup.html#verify-connection",
    "href": "tutorials/data-platforms/starburst-galaxy/0_setup.html#verify-connection",
    "title": "Requirements and setup",
    "section": "Verify connection",
    "text": "Verify connection\nList the tables your connection has:\n\ncon.list_tables()\n\n['astronauts', 'missions']\n\n\nRun a SQL query:\n\ncon.sql(\"select 1 as a\")\n\n┏━━━━━━━┓\n┃ a     ┃\n┡━━━━━━━┩\n│ int32 │\n├───────┤\n│     1 │\n└───────┘\n\n\n\nIf you have any issues, check your connection details above. If you are still having issues, open an issue on Ibis and we’ll do our best to help you!"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/0_setup.html#next-steps",
    "href": "tutorials/data-platforms/starburst-galaxy/0_setup.html#next-steps",
    "title": "Requirements and setup",
    "section": "Next steps",
    "text": "Next steps\nNow that you’re connected to Starburst Galaxy, you can continue this tutorial to learn the basics of Ibis or query your own data. See the rest of the Ibis documentation or Starburst Galaxy documentation. You can open an issue if you run into one!"
  },
  {
    "objectID": "contribute/05_reference.html",
    "href": "contribute/05_reference.html",
    "title": "Test Class Reference",
    "section": "",
    "text": "This page provides a partial reference to the attributes, methods, properties and class-level variables that are used to help configure a backend for the Ibis test suite.\nContributors are encouraged to look over the methods and class-level variables in ibis/backends/tests/base.py.\nTo add a new backend test configuration import one of BackendTest or ServiceBackendTest into a conftest.py file with the path ibis/backends/{backend_name}/tests/conftest.py. Then update / override the relevant class-level variables and methods."
  },
  {
    "objectID": "contribute/05_reference.html#attributes",
    "href": "contribute/05_reference.html#attributes",
    "title": "Test Class Reference",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ncheck_dtype\nCheck that dtypes match when comparing Pandas Series\n\n\ncheck_names\nCheck that column name matches when comparing Pandas Series\n\n\ndeps\nA list of dependencies that must be present to run tests.\n\n\ndriver_supports_multiple_statements\nWhether the driver supports executing multiple statements in a single call.\n\n\nforce_sort\nSort results before comparing against reference computation.\n\n\nnative_bool\nWhether backend has native boolean types\n\n\nreduction_tolerance\nUsed for a single test in test_aggregation.py. You should not need to touch this.\n\n\nrounding_method\nName of round method to use for rounding test comparisons.\n\n\nstateful\nWhether special handling is needed for running a multi-process pytest run.\n\n\nsupports_arrays\nWhether backend supports Arrays / Lists\n\n\nsupports_json\nWhether backend supports operating on JSON\n\n\nsupports_map\nWhether backend supports mappings (currently DuckDB, Snowflake, and Trino)\n\n\nsupports_structs\nWhether backend supports Structs\n\n\nsupports_tpcds\nChild class defines a load_tpcds method that loads the required TPC-DS tables into a connection.\n\n\nsupports_tpch\nChild class defines a load_tpch method that loads the required TPC-H tables into a connection.\n\n\ntpc_absolute_tolerance\nAbsolute tolerance for floating point comparisons with pytest.approx in TPC correctness tests."
  },
  {
    "objectID": "contribute/05_reference.html#methods",
    "href": "contribute/05_reference.html#methods",
    "title": "Test Class Reference",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nassert_frame_equal\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\n\n\nassert_series_equal\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\n\n\nconnect\nReturn a connection with data loaded from data_dir.\n\n\nload_data\nLoad testdata from data_dir.\n\n\nload_tpcds\nLoad TPC-DS data.\n\n\nload_tpch\nLoad TPC-H data.\n\n\npostload\nCode to execute after loading data.\n\n\npreload\nCode to execute before loading data.\n\n\nskip_if_missing_deps\nAdd an importorskip for any missing dependencies.\n\n\n\n\nassert_frame_equal\nassert_frame_equal(left, right, *args, **kwargs)\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\nforce_sort, and check_dtype are set as class-level variables.\n\n\nassert_series_equal\nassert_series_equal(left, right, *args, **kwargs)\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\nforce_sort, check_dtype, and check_names are set as class-level variables.\n\n\nconnect\nconnect(tmpdir, worker_id, **kw)\nReturn a connection with data loaded from data_dir.\n\n\nload_data\nload_data(data_dir, tmpdir, worker_id, **kw)\nLoad testdata from data_dir.\n\n\nload_tpcds\nload_tpcds()\nLoad TPC-DS data.\n\n\nload_tpch\nload_tpch()\nLoad TPC-H data.\n\n\npostload\npostload(**_)\nCode to execute after loading data.\n\n\npreload\npreload()\nCode to execute before loading data.\n\n\nskip_if_missing_deps\nskip_if_missing_deps()\nAdd an importorskip for any missing dependencies. # ServiceBackendTest { #ibis.backends.tests.base.ServiceBackendTest }\nServiceBackendTest(self, *, data_dir, tmpdir, worker_id, **kw)\nParent class to use for backend test configuration if backend requires a Docker container(s) in order to run locally."
  },
  {
    "objectID": "contribute/05_reference.html#attributes-1",
    "href": "contribute/05_reference.html#attributes-1",
    "title": "Test Class Reference",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndata_volume\nData volume defined in compose.yaml corresponding to backend.\n\n\nservice_name\nName of service defined in compose.yaml corresponding to backend.\n\n\ntest_files\nReturns an iterable of test files to load into a Docker container before testing."
  },
  {
    "objectID": "contribute/05_reference.html#methods-1",
    "href": "contribute/05_reference.html#methods-1",
    "title": "Test Class Reference",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\npreload\nUse docker compose cp to copy all files from test_files into a container.\n\n\n\n\npreload\npreload()\nUse docker compose cp to copy all files from test_files into a container.\nservice_name and data_volume are set as class-level variables."
  },
  {
    "objectID": "contribute/02_workflow.html",
    "href": "contribute/02_workflow.html",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "First, set up a development environment.\n\n\n\nIf you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you.\n\n\n\nTo run tests that do not require a backend:\npytest -m core\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou may be able to skip this section\n\n\nIf you haven't made changes to the core of ibis (e.g., `ibis/expr`)\nor any specific backends (`ibis/backends`) this material isn't necessary to\nfollow to make a pull request.\nFirst, we need to download example data to run the tests successfully:\njust download-data\nTo run the tests for a specific backend (e.g. sqlite):\npytest -m sqlite\n\n\n\n\nThese client-server backends need to be started before testing them. They can be started with docker compose directly, or using the just tool.\n\nClickHouse: just up clickhouse\nPostgreSQL: just up postgres\nMySQL: just up mysql\nimpala: just up impala\n\n\n\nIf anything seems amiss with a backend, you can of course test it locally:\nexport PGPASSWORD=postgres\npsql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\"\n\n\n\n\nIbis follows the Conventional Commits structure. In brief, the commit summary should look like:\nfix(types): make all floats doubles\nThe type (e.g. fix) can be:\n\nfix: A bug fix. Correlates with PATCH in SemVer\nfeat: A new feature. Correlates with MINOR in SemVer\ndocs: Documentation only changes\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) ` If the commit fixes a Github issue, add something like this to the bottom of the description:\nfixes #4242\n\n\n\n\nIbis follows the standard GitHub pull request process. The team will review the PR and merge when it’s ready."
  },
  {
    "objectID": "contribute/02_workflow.html#getting-started",
    "href": "contribute/02_workflow.html#getting-started",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "First, set up a development environment."
  },
  {
    "objectID": "contribute/02_workflow.html#taking-issues",
    "href": "contribute/02_workflow.html#taking-issues",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "If you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you."
  },
  {
    "objectID": "contribute/02_workflow.html#running-the-test-suite",
    "href": "contribute/02_workflow.html#running-the-test-suite",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "To run tests that do not require a backend:\npytest -m core\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou may be able to skip this section\n\n\nIf you haven't made changes to the core of ibis (e.g., `ibis/expr`)\nor any specific backends (`ibis/backends`) this material isn't necessary to\nfollow to make a pull request.\nFirst, we need to download example data to run the tests successfully:\njust download-data\nTo run the tests for a specific backend (e.g. sqlite):\npytest -m sqlite"
  },
  {
    "objectID": "contribute/02_workflow.html#setting-up-non-trivial-backends",
    "href": "contribute/02_workflow.html#setting-up-non-trivial-backends",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "These client-server backends need to be started before testing them. They can be started with docker compose directly, or using the just tool.\n\nClickHouse: just up clickhouse\nPostgreSQL: just up postgres\nMySQL: just up mysql\nimpala: just up impala\n\n\n\nIf anything seems amiss with a backend, you can of course test it locally:\nexport PGPASSWORD=postgres\npsql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\""
  },
  {
    "objectID": "contribute/02_workflow.html#writing-the-commit",
    "href": "contribute/02_workflow.html#writing-the-commit",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "Ibis follows the Conventional Commits structure. In brief, the commit summary should look like:\nfix(types): make all floats doubles\nThe type (e.g. fix) can be:\n\nfix: A bug fix. Correlates with PATCH in SemVer\nfeat: A new feature. Correlates with MINOR in SemVer\ndocs: Documentation only changes\nstyle: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) ` If the commit fixes a Github issue, add something like this to the bottom of the description:\nfixes #4242"
  },
  {
    "objectID": "contribute/02_workflow.html#submit-a-pull-request",
    "href": "contribute/02_workflow.html#submit-a-pull-request",
    "title": "Contribute to the Ibis codebase",
    "section": "",
    "text": "Ibis follows the standard GitHub pull request process. The team will review the PR and merge when it’s ready."
  },
  {
    "objectID": "contribute/index.html",
    "href": "contribute/index.html",
    "title": "Contribute",
    "section": "",
    "text": "Contribute\nCheck out our contributing guide for details! Guides for setting up an environment and getting started are here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "",
    "text": "Seth Larson wrote a great blog post on querying a PyPI dataset to look for trends in the use of memory-safe languages in Python.\nCheck out Seth’s article for more information on the dataset (and it’s a good read!). It caught our eye because it makes use of DuckDB to clean the data for analysis.\nThat’s right up our alley here in Ibis land, so let’s see if we can duplicate Seth’s results (and then continue on to plot them!)"
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html#grab-the-data-locations",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html#grab-the-data-locations",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "Grab the data (locations)",
    "text": "Grab the data (locations)\nSeth showed (and then safely decomposed) a nested curl statement and that’s always viable – we’re in Python land so why not grab the filenames using urllib3?\n\nimport urllib3\n\nurl = \"https://raw.githubusercontent.com/pypi-data/data/main/links/dataset.txt\"\n\nwith urllib3.PoolManager() as http:\n    resp = http.request(\"GET\", url)\n\nparquet_files = resp.data.decode().split()\nparquet_files\n\n['https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-0.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-1.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-10.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-11.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-12.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-13.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-14.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-2.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-3.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-4.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-5.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-6.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-7.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-8.parquet',\n 'https://github.com/pypi-data/data/releases/download/2023-11-27-03-06/index-9.parquet']"
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html#grab-the-data",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html#grab-the-data",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "Grab the data",
    "text": "Grab the data\nNow we’re ready to get started with Ibis!\nDuckDB is clever enough to grab only the parquet metadata. This means we can use read_parquet to create a lazy view of the parquet files and then build up our expression without downloading everything beforehand!\n\nimport ibis\n1from ibis import _\n\nibis.options.interactive = True\n\n\n1\n\nSee https://ibis-project.org/how-to/analytics/chain_expressions.html for docs on the deferred operator!\n\n\n\n\nCreate a DuckDB connection:\n\ncon = ibis.duckdb.connect()\n\nAnd load up one of the files (we can run the full query after)!\n\npypi = con.read_parquet(parquet_files[0], table_name=\"pypi\")\n\n\npypi.schema()\n\nibis.Schema {\n  project_name     string\n  project_version  string\n  project_release  string\n  uploaded_on      timestamp\n  path             string\n  archive_path     string\n  size             uint64\n  hash             binary\n  skip_reason      string\n  lines            uint64\n  repository       uint32\n}"
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html#query-crafting",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html#query-crafting",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "Query crafting",
    "text": "Query crafting\nLet’s break down what we’re looking for. As a high-level view of the use of compiled languages, Seth is using file extensions as an indicator that a given filetype is used in a Python project.\nThe dataset we’re using has every file in every project – what criteria should we use?\nWe can follow Seth’s lead and look for things:\n\nA file extension that is one of: asm, cc, cpp, cxx, h, hpp, rs, go, and variants of F90, f90, etc… That is, C, C++, Assembly, Rust, Go, and Fortran.\nWe exclude matches where the file path is within the site-packages/ directory.\nWe exclude matches that are in directories used for testing.\n\n\nexpr = pypi.filter(\n    [\n        _.path.re_search(r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"),\n        ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n        ~_.path.contains(\"/site-packages/\"),\n    ]\n)\nexpr\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┓\n┃ project_name    ┃ project_version ┃ project_release              ┃ uploaded_on             ┃ path                                                                             ┃ archive_path                                                                     ┃ size   ┃ hash                                                                 ┃ skip_reason ┃ lines  ┃ repository ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━┩\n│ string          │ string          │ string                       │ timestamp               │ string                                                                           │ string                                                                           │ uint64 │ binary                                                               │ string      │ uint64 │ uint32     │\n├─────────────────┼─────────────────┼──────────────────────────────┼─────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┼────────┼──────────────────────────────────────────────────────────────────────┼─────────────┼────────┼────────────┤\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/support.c                                  │   1607 │ b'\\xca\\x0c\\xf2\\\\R\\x83\\xefS\\x0c\\xe4\\x0c\\x15`\\x1fM\\x16\"\\x93\\x88\\x08'   │ ~           │     66 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/stemmer.c                      │   5054 │ b'y\\xc3A\\x12\\x17\\xd4\\xeb\\xbb\\xcfan\\xfd\\x80\\xbac\\x18\\xcf\\xc0W\\x9a'    │ ~           │    230 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │    313 │ b'\\x81s\\xa1t\\x86}\\xf9\\xe5\\xb5Zt\\xcb\\xd3\\xae\\nHfe\\x8c\\x9d'            │ ~           │     16 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │  80922 │ b'\\xae&lt;\\xc7f\\x02\\xc5{\\xc50\\xf4\\xdc\\x8fa\\x1a\\t..k\\xd5\\x9d'            │ ~           │   2205 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │    313 │ b'\\x14D\\xeb\\xb4\\x9ac\\xab\\x14:b\\xa4\\xba\\xa5\\x9f\\x1f\\x06\\xce\\x0bj\\xf2' │ ~           │     16 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │  10684 │ b\"!\\xa259'\\x94\\xc7.\\x16\\x0b\\x08\\x95J\\x0e\\xef\\x86{\\x0e\\xd6\\x8f\"       │ ~           │    309 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │    313 │ b'\\x10W.\\xcc7\\x08=WV\\xde\\x1bP9\\x03w\\x03\\xa2\\x8c\\xe7\\xec'             │ ~           │     16 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │  41620 │ b'\\x95P\\xd6|\\x85\\x97\\xb2H\\x14\\xa0d&lt;q-iu\\xc1\\x98h\\xbb'                │ ~           │   1097 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │    313 │ b'N\\xf7t\\xdd\\xcc\\xbb8Y\\x0b\\xbc\\xd5No_\\x8d\\xc7\\xf2\\x80\\x10\\xd0'       │ ~           │     16 │          1 │\n│ zopyx.txng3.ext │ 3.3.2           │ zopyx.txng3.ext-3.3.2.tar.gz │ 2010-03-06 16:09:43.735 │ packages/zopyx.txng3.ext/zopyx.txng3.ext-3.3.2.tar.gz/zopyx.txng3.ext-3.3.2/zop… │ zopyx.txng3.ext-3.3.2/zopyx/txng3/ext/stemmer_src/libstemmer_c/src_c/stem_UTF_8… │  25440 │ b'o\\n\\x96M+\\xb0\\xfbV\\xaa6&lt;*\\xc8\\xb0B\\x03\\x8a\\xa9\\xc3\\x10'            │ ~           │    694 │          1 │\n│ …               │ …               │ …                            │ …                       │ …                                                                                │ …                                                                                │      … │ …                                                                    │ …           │      … │          … │\n└─────────────────┴─────────────────┴──────────────────────────────┴─────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴────────┴──────────────────────────────────────────────────────────────────────┴─────────────┴────────┴────────────┘\n\n\n\nThat could be right – we can peak at the filename at the end of the path column to do a quick check:\n\nexpr.path.split(\"/\")[-1]\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayIndex(StringSplit(path, '/'), -1) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                                 │\n├────────────────────────────────────────┤\n│ support.c                              │\n│ stemmer.c                              │\n│ stem_UTF_8_turkish.h                   │\n│ stem_UTF_8_turkish.c                   │\n│ stem_UTF_8_swedish.h                   │\n│ stem_UTF_8_swedish.c                   │\n│ stem_UTF_8_spanish.h                   │\n│ stem_UTF_8_spanish.c                   │\n│ stem_UTF_8_russian.h                   │\n│ stem_UTF_8_russian.c                   │\n│ …                                      │\n└────────────────────────────────────────┘\n\n\n\nOk! Next up, we want to group the matches by:\n\nThe month that the package / file was published For this, we can use the truncate method and ask for month as our truncation window.\nThe file extension of the file used\n\n\nexpr.group_by(\n    month=_.uploaded_on.truncate(\"M\"),\n    ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n).aggregate()\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┓\n┃ month               ┃ ext    ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━┩\n│ timestamp           │ string │\n├─────────────────────┼────────┤\n│ 2015-12-01 00:00:00 │ cpp    │\n│ 2015-11-01 00:00:00 │ h      │\n│ 2015-12-01 00:00:00 │ f90    │\n│ 2015-11-01 00:00:00 │ hpp    │\n│ 2010-11-01 00:00:00 │ c      │\n│ 2010-07-01 00:00:00 │ h      │\n│ 2010-12-01 00:00:00 │ cpp    │\n│ 2010-03-01 00:00:00 │ h      │\n│ 2011-08-01 00:00:00 │ cpp    │\n│ 2010-05-01 00:00:00 │ h      │\n│ …                   │ …      │\n└─────────────────────┴────────┘\n\n\n\nThat looks promising. Now we need to grab the package names that correspond to a given file extension in a given month and deduplicate it. And to match Seth’s results, we’ll also sort by the month in descending order:\n\nexpr = (\n    expr.group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .order_by(_.month.desc())\n)\n\nexpr\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ month               ┃ ext    ┃ projects                                            ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp           │ string │ array&lt;string&gt;                                       │\n├─────────────────────┼────────┼─────────────────────────────────────────────────────┤\n│ 2017-07-01 00:00:00 │ c      │ ['newrelic', 'nuclitrack', ... +262]                │\n│ 2017-07-01 00:00:00 │ asm    │ ['pwntools', 'fibers', ... +6]                      │\n│ 2017-07-01 00:00:00 │ rs     │ ['rust-pypi-example', 'tokio', ... +2]              │\n│ 2017-07-01 00:00:00 │ f      │ ['okada-wrapper', 'numpy', ... +6]                  │\n│ 2017-07-01 00:00:00 │ cpp    │ ['pipcudemo', 'pyDEM', ... +108]                    │\n│ 2017-07-01 00:00:00 │ f90    │ ['pySpecData', 'numpy', ... +8]                     │\n│ 2017-07-01 00:00:00 │ cxx    │ ['pytetgen', 'python-libsbml-experimental', ... +8] │\n│ 2017-07-01 00:00:00 │ go     │ ['pre-commit', 'django-instant', ... +5]            │\n│ 2017-07-01 00:00:00 │ cc     │ ['nixio', 'pogeo', ... +14]                         │\n│ 2017-07-01 00:00:00 │ h      │ ['numba', 'p4d', ... +222]                          │\n│ …                   │ …      │ …                                                   │\n└─────────────────────┴────────┴─────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html#massage-and-plot",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html#massage-and-plot",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "Massage and plot",
    "text": "Massage and plot\nLet’s continue and see what our results look like.\nWe’ll do a few things:\n\nCombine all of the C and C++ extensions into a single group by renaming them all.\nCount the number of distinct entries in each group\nPlot the results!\n\n\ncollapse_names = expr.mutate(\n    ext=_.ext.re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n    .re_replace(\"^f.*$\", \"Fortran\")\n    .replace(\"rs\", \"Rust\")\n    .replace(\"go\", \"Go\")\n    .replace(\"asm\", \"Assembly\")\n    .nullif(\"\"),\n).dropna(\"ext\")\n\ncollapse_names\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ month               ┃ ext      ┃ projects                                            ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp           │ string   │ array&lt;string&gt;                                       │\n├─────────────────────┼──────────┼─────────────────────────────────────────────────────┤\n│ 2017-07-01 00:00:00 │ C/C++    │ ['pipcudemo', 'pyDEM', ... +108]                    │\n│ 2017-07-01 00:00:00 │ Fortran  │ ['numpy', 'pySpecData', ... +8]                     │\n│ 2017-07-01 00:00:00 │ Go       │ ['pre-commit', 'ronin', ... +5]                     │\n│ 2017-07-01 00:00:00 │ C/C++    │ ['pytetgen', 'python-libsbml-experimental', ... +8] │\n│ 2017-07-01 00:00:00 │ C/C++    │ ['newrelic', 'nuclitrack', ... +262]                │\n│ 2017-07-01 00:00:00 │ Fortran  │ ['okada-wrapper', 'numpy', ... +6]                  │\n│ 2017-07-01 00:00:00 │ Assembly │ ['pwntools', 'xmldirector.plonecore', ... +6]       │\n│ 2017-07-01 00:00:00 │ Rust     │ ['rust-pypi-example', 'tokio', ... +2]              │\n│ 2017-07-01 00:00:00 │ C/C++    │ ['numba', 'numpythia', ... +222]                    │\n│ 2017-07-01 00:00:00 │ C/C++    │ ['pyemd', 'pogeo', ... +19]                         │\n│ …                   │ …        │ …                                                   │\n└─────────────────────┴──────────┴─────────────────────────────────────────────────────┘\n\n\n\nNote that now we need to de-duplicate again, since we might’ve had separate unique entries for both an h and c file extension, and we don’t want to double-count!\nWe could rewrite our original query and include the renames in the original group_by (this would be the smart thing to do), but let’s push on and see if we can make this work.\nThe projects column is now a column of string arrays, so we want to collect all of the arrays in each group, this will give us a “list of lists”, then we’ll flatten that list and call unique().length() as before.\nDuckDB has a flatten function, but it isn’t exposed in Ibis (yet!).\nWe’ll use a handy bit of Ibis magic to define a builtin UDF that will map directly onto the underlying DuckDB function (what!? See here for more info):\n\n@ibis.udf.scalar.builtin\ndef flatten(x: list[list[str]]) -&gt; list[str]:\n    ...\n\n\ncollapse_names = collapse_names.group_by([\"month\", \"ext\"]).aggregate(\n    projects=flatten(_.projects.collect())\n)\n\ncollapse_names\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ month               ┃ ext      ┃ projects                                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp           │ string   │ array&lt;string&gt;                               │\n├─────────────────────┼──────────┼─────────────────────────────────────────────┤\n│ 2009-07-01 00:00:00 │ C/C++    │ ['gevent', 'hashlib', ... +52]              │\n│ 2009-07-01 00:00:00 │ Assembly │ ['pycryptopp']                              │\n│ 2008-10-01 00:00:00 │ Fortran  │ ['numscons']                                │\n│ 2008-08-01 00:00:00 │ Fortran  │ ['numscons']                                │\n│ 2008-06-01 00:00:00 │ C/C++    │ ['dm.incrementalsearch', 'Cython', ... +45] │\n│ 2008-05-01 00:00:00 │ Fortran  │ ['numscons']                                │\n│ 2007-03-01 00:00:00 │ Fortran  │ ['Model-Builder']                           │\n│ 2005-05-01 00:00:00 │ C/C++    │ ['ll-xist', 'll-xist']                      │\n│ 2005-03-01 00:00:00 │ C/C++    │ ['pygenx', 'pygenx']                        │\n│ 2011-08-01 00:00:00 │ Fortran  │ ['pysces', 'ffnet']                         │\n│ …                   │ …        │ …                                           │\n└─────────────────────┴──────────┴─────────────────────────────────────────────┘\n\n\n\nWe could have included the unique().length() in the aggregate call, but sometimes it’s good to check that your slightly off-kilter idea has worked (and it has!).\n\ncollapse_names = collapse_names.select(\n    _.month, _.ext, project_count=_.projects.unique().length()\n)\n\ncollapse_names\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ month               ┃ ext     ┃ project_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ timestamp           │ string  │ int64         │\n├─────────────────────┼─────────┼───────────────┤\n│ 2007-03-01 00:00:00 │ C/C++   │             6 │\n│ 2006-01-01 00:00:00 │ C/C++   │             5 │\n│ 2005-10-01 00:00:00 │ C/C++   │             2 │\n│ 2011-08-01 00:00:00 │ C/C++   │            57 │\n│ 2011-03-01 00:00:00 │ C/C++   │            63 │\n│ 2011-01-01 00:00:00 │ Fortran │             2 │\n│ 2010-12-01 00:00:00 │ C/C++   │            48 │\n│ 2010-08-01 00:00:00 │ Fortran │             1 │\n│ 2010-07-01 00:00:00 │ Fortran │             3 │\n│ 2010-03-01 00:00:00 │ Fortran │             1 │\n│ …                   │ …       │             … │\n└─────────────────────┴─────────┴───────────────┘\n\n\n\nNow that the data are tidied, we can pass our expression directly to Altair and see what it looks like!\n\nimport altair as alt\n\nchart = (\n    alt.Chart(collapse_names.to_pandas())\n    .mark_line()\n    .encode(x=\"month\", y=\"project_count\", color=\"ext\")\n    .properties(width=600, height=300)\n)\nchart\n\n\n\n\n\n\n\nThat looks good, but it definitely doesn’t match the plot from Seth’s post:\n\n\n\nupstream plot\n\n\nOur current plot is only showing the results from a subset of the available data. Now that our expression is complete, we can re-run on the full dataset and compare."
  },
  {
    "objectID": "posts/querying-pypi-metadata-compiled-languages/index.html#the-full-run",
    "href": "posts/querying-pypi-metadata-compiled-languages/index.html#the-full-run",
    "title": "Querying every file in every release on the Python Package Index (redux)",
    "section": "The full run",
    "text": "The full run\nTo recap – we pulled a lazy view of a single parquet file from the pypi-data repo, filtered for all the files that contain file extensions we care about, then grouped them all together to get counts of the various filetypes used across projects by month.\nHere’s the entire query chained together into a single command, now running on all of the parquet files we have access to:\n\npypi = con.read_parquet(parquet_files, table_name=\"pypi\")\n\nfull_query = (\n    pypi.filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .order_by(_.month.desc())\n    .mutate(\n        ext=_.ext.re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .dropna(\"ext\")\n    .group_by([\"month\", \"ext\"])\n    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n)\nchart = (\n    alt.Chart(full_query.to_pandas())\n    .mark_line()\n    .encode(x=\"month\", y=\"project_count\", color=\"ext\")\n    .properties(width=600, height=300)\n)\nchart"
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html",
    "href": "posts/Ibis-version-3.1.0-release/index.html",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "Ibis 3.1 has officially been released as the latest version of the package. With this release comes new convenience features, increased backend operation coverage and a plethora of bug fixes. As usual, a full list of the changes can be found in the project release notes here Let’s talk about some of the new changes 3.1 brings for Ibis users."
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html#introduction",
    "href": "posts/Ibis-version-3.1.0-release/index.html#introduction",
    "title": "Ibis v3.1.0",
    "section": "",
    "text": "Ibis 3.1 has officially been released as the latest version of the package. With this release comes new convenience features, increased backend operation coverage and a plethora of bug fixes. As usual, a full list of the changes can be found in the project release notes here Let’s talk about some of the new changes 3.1 brings for Ibis users."
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html#ibis.connect",
    "href": "posts/Ibis-version-3.1.0-release/index.html#ibis.connect",
    "title": "Ibis v3.1.0",
    "section": "ibis.connect",
    "text": "ibis.connect\nThe first significant change to note is that, Ibis now provides a more convenient way to connect to a backend using the ibis.connect method. You can now use this function to connect to an appropriate backend using a connection string.\nHere are some examples:\n\nDuckDBPostgres\n\n\nInitialize a DuckDB instance using 'duckdb://:memory:'\nconn = ibis.connect('duckdb://:memory:')\nAnd begin registering your tables:\nconn.register('csv://farm_data/dates.csv', 'dates')\nconn.register('csv://farm_data/farmer_groups.csv', 'farmer_groups')\nconn.register('csv://farm_data/crops.csv', 'crops')\nconn.register('csv://farm_data/farms.csv', 'farms')\nconn.register('csv://farm_data/harvest.csv', 'harvest')\nconn.register('csv://farm_data/farmers.csv', 'farmers')\nconn.register('csv://farm_data/tracts.csv', 'tracts')\nconn.register('csv://farm_data/fields.csv', 'fields')\nYou can also do this programmatically:\nfiles = glob.glob('farm_data/*.csv')\n\nfor file in files:\n    fname = 'csv://' + file\n    tname = file.replace('farm_data/', '').replace('.csv', '')\n    conn.register(fname, tname)\nThis method isn’t limited to csv://. It works with parquet:// and csv.gz:// as well. Give it a try!\n\n\nconn = ibis.connect('postgres://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')\nOr, using a .pgpass file:\nconn = ibis.connect('postgres://&lt;username&gt;@&lt;host&gt;:&lt;port&gt;/&lt;database&gt;')"
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html#unnest-support",
    "href": "posts/Ibis-version-3.1.0-release/index.html#unnest-support",
    "title": "Ibis v3.1.0",
    "section": "Unnest Support",
    "text": "Unnest Support\nOne of the trickier parts about working with data is that it doesn’t usually come organized in neat, predictable rows and columns. Instead data often consists of rows that could contain a single bit of data or arrays of it. When data is organized in layers, as with arrays, it can sometimes be difficult to work with. Ibis 3.1 introduces the unnest function as a way to flatten arrays of data.\nUnnest takes a column containing an array of values and separates the individual values into rows as shown:\nBefore Unnest:\n\n\n\ncol\n\n\n\n\n[1, 2]\n\n\n\nAfter Unnest:\n\n\n\ncol\n\n\n\n\n1\n\n\n2\n\n\n\nHere is a self-contained example of creating a dataset with an array and then unnesting it:\n\nDuckDBPostgres\n\n\nimport ibis\nimport pandas as pd\n\n# Parquet save path\nfname = 'array_data.parquet'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Save as parquet\npd.DataFrame(data[1:], columns=data[0]).to_parquet(fname)\n\n# Connect to the file using a DuckDB backend\nconn = ibis.connect(f\"duckdb://{fname}\")\n\n# Create a table expression for your loaded data\narray_data = conn.table(\"array_data\")\n\n# Optionally execute the array data to preview\narray_data.execute()\n\n# select the unnested values with their corresponding IDs\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()\n\n\nimport ibis\nimport pandas as pd\n\n# Postgres connection string for user 'ibistutorials' with a valid .pgpass file in `/\n# See https://www.postgresql.org/docs/9.3/libpq-pgpass.html for details on `/.pgpass\ncstring = 'postgres://ibistutorials@localhost:5432/pg-ibis'\n\n# Mock Data\ndata = [\n    ['array_id', 'array_value']\n    ,[1, [1, 3, 4]]\n    ,[2, [2, 4, 5]]\n    ,[3, [6, 8]]\n    ,[4, [1, 6]]\n]\n\n# Create a dataframe for easy loading\ndf = pd.DataFrame(data[1:], columns=data[0])\n\n# Postgres backend connection\nconn = ibis.connect(cstring)\n\n# SQLAlchemy Types\n# Integer type\nint_type = ibis.backends.postgres.sa.types.INT()\n# Array type function\narr_f = ibis.backends.postgres.sa.types.ARRAY\n\n# Load data to table using pd.DataFrame.to_sql\ndf.to_sql(\n    name='array_data'\n    ,con=conn.con.connect()\n    ,if_exists='replace'\n    ,index=False\n    ,dtype={\n        'array_id': int_type\n        ,'array_value': arr_f(int_type)\n    }\n)\n\n# Array Data Table Expression\narray_data = conn.table(\"array_data\")\n\n# Optionally execute to preview entire table\n# array_data.execute()\n\n# Unnest\narray_data.select(['array_id', array_data['array_value'].unnest()]).execute()"
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html#api",
    "href": "posts/Ibis-version-3.1.0-release/index.html#api",
    "title": "Ibis v3.1.0",
    "section": "_ API",
    "text": "_ API\nThere is now a shorthand for lambda functions using underscore (_). This is useful for chaining expressions to one another and helps reduce total line characters and appearances of lambdas.\nFor example, let’s use array_data from above. We will unnest array_value, find the weighted average, and then sum in one expression:\nfrom ibis import _\n\n(\n    array_data\n    .select([\n        'array_id'\n        # array_data returns a TableExpr, `_` here is shorthand\n        # for that returned expression\n        ,_['array_value'].unnest().name('arval')\n        # we can use it instead of saying `array_data`\n        ,(_['array_value'].length().cast('float')\n          / _['array_value'].length().sum().cast('float')).name('wgt')\n    ])\n    # Since the above `select` statement returns a TableExpr, we can use\n    # `_` to reference that one as well:\n    .mutate(wgt_prod=_.arval * _.wgt)\n    # And again:\n    .aggregate(vsum=_.wgt_prod.sum(), vcount=_.wgt_prod.count())\n    # And again:\n    .mutate(wgt_mean=_.vsum / _.vcount)\n).execute()\nNote that if you import _ directly from ibis (from ibis import _), the default _ object will lose its functionality, so be mindful if you have a habit of using it outside of Ibis."
  },
  {
    "objectID": "posts/Ibis-version-3.1.0-release/index.html#additional-changes",
    "href": "posts/Ibis-version-3.1.0-release/index.html#additional-changes",
    "title": "Ibis v3.1.0",
    "section": "Additional Changes",
    "text": "Additional Changes\nAlong with these changes, the operation matrix has had a few more holes filled. Contributors should note that backend test data is now loaded dynamically. Most users won’t be exposed to this update, but it should make contribution a bit more streamlined.\nTo see the full patch notes, go to the patch notes page\nAs always, Ibis is free and open source. Contributions are welcome and encouraged–drop into the discussions, raise an issue, or put in a pull request.\nDownload ibis 3.1 today!"
  },
  {
    "objectID": "posts/ibis-examples/index.html",
    "href": "posts/ibis-examples/index.html",
    "title": "Ibis sneak peek: examples",
    "section": "",
    "text": "Ibis has been moving quickly to provide a powerful but easy-to-use interface for interacting with analytical engines. However, as we’re approaching the 5.0 release of Ibis, we’ve realized that moving from not knowing Ibis to writing a first expression is not trivial.\nAs is, in our tutorial structure, work must be done on the user’s part — though we do provide the commands — to download a SQLite database onto disk, which can only be used with said backend. We feel that this put too much emphasis on a single backend, and added too much effort into picking the right backend for the first tutorial. We want minimal steps between users and learning the Ibis API.\nThis is why we’ve added the ibis.examples module."
  },
  {
    "objectID": "posts/ibis-examples/index.html#getting-started-with-examples",
    "href": "posts/ibis-examples/index.html#getting-started-with-examples",
    "title": "Ibis sneak peek: examples",
    "section": "Getting Started with Examples",
    "text": "Getting Started with Examples\nThis module offers in-Ibis access to multiple small tables (the largest is around only 30k rows), which are downloaded when requested and immediately read into the backend upon completion. We worked to keep pulling in examples simple, so it looks like this:\n\nimport ibis\nimport ibis.examples as ex\n\nibis.options.interactive = True\n\nt = ex.penguins.fetch()\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nAnother advantage of this new method is that we were able to register all of them so you can tab-complete, as you can see here:\n\n\n\nTab Complete\n\n\nOnce you’ve retrieved an example table, you can get straight to learning and experimenting, instead of struggling with just getting the data itself.\nIn the future, our tutorials will use the examples module to to help speed up learning of the Ibis framework.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html",
    "href": "posts/ibis-version-4.0.0-release/index.html",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "Ibis 4.0 has officially been released as the latest version of the package. This release includes several new backends, improved functionality, and some major internal refactors. A full list of the changes can be found in the project release notes. Let’s talk about some of the new changes 4.0 brings for Ibis users."
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html#introduction",
    "href": "posts/ibis-version-4.0.0-release/index.html#introduction",
    "title": "Ibis v4.0.0",
    "section": "",
    "text": "Ibis 4.0 has officially been released as the latest version of the package. This release includes several new backends, improved functionality, and some major internal refactors. A full list of the changes can be found in the project release notes. Let’s talk about some of the new changes 4.0 brings for Ibis users."
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html#backends",
    "href": "posts/ibis-version-4.0.0-release/index.html#backends",
    "title": "Ibis v4.0.0",
    "section": "Backends",
    "text": "Backends\nIbis 4.0 brings Polars, Snowflake, and Trino into an already-impressive stock of supported backends. The Polars backend adds another way for users to work locally with DataFrames. The Snowflake and Trino backends add a free and familiar python API to popular data warehouses.\nAlongside these new backends, Google BigQuery and Microsoft SQL have been moved to the main repo, so their release cycle will follow the Ibis core."
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html#functionality",
    "href": "posts/ibis-version-4.0.0-release/index.html#functionality",
    "title": "Ibis v4.0.0",
    "section": "Functionality",
    "text": "Functionality\nThere are a lot of improvements incoming, but some notable changes include:\n\nread API: allows users to read various file formats directly into their configured default_backend (default DuckDB) through read_* functions, which makes working with local files easier than ever.\nto_pyarrow and to_pyarrow_batches: users can now return PyArrow objects (Tables, Arrays, Scalars, RecordBatchReader) and therefore grants all of the functionality that PyArrow provides\nJSON getitem: users can now run getitem on a JSON field using Ibis expressions with some backends\nPlotting support through __array__: allows users to plot Ibis expressions out of the box"
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html#refactors",
    "href": "posts/ibis-version-4.0.0-release/index.html#refactors",
    "title": "Ibis v4.0.0",
    "section": "Refactors",
    "text": "Refactors\nThis won’t be visible to most users, but the project underwent a series of refactors that spans multiple PRs. Notable changes include removing intermediate expressions, improving the testing framework, and UX updates."
  },
  {
    "objectID": "posts/ibis-version-4.0.0-release/index.html#additional-changes",
    "href": "posts/ibis-version-4.0.0-release/index.html#additional-changes",
    "title": "Ibis v4.0.0",
    "section": "Additional Changes",
    "text": "Additional Changes\nAs mentioned previously, additional functionality, bugfixes, and more have been included in the latest 4.0 release. To stay up to date and learn more about recent changes: check out the project’s homepage at ibis-project.org, follow @IbisData on Twitter, find the source code and community on GitHub, and join the discussion on Zulip.\nAs always, try Ibis by installing it today."
  },
  {
    "objectID": "posts/dbt-ibis/index.html",
    "href": "posts/dbt-ibis/index.html",
    "title": "dbt-ibis: Write your dbt models using Ibis",
    "section": "",
    "text": "Introduction to dbt\ndbt has revolutionized how transformations are orchestrated and managed within modern data warehouses. Initially released in 2016, dbt quickly gained traction within the data analytics community due to its focus on bringing software engineering best practices to analytics code like modularity, portability, CI/CD, and documentation.\nAt the heart of dbt are so called “models” which are just simple SQL SELECT statements (see further below for an example). dbt removes the need to write any DDL/DML, allowing users to focus on writing SELECT statements. Depending on how you configure it, the queries are materialized as tables, views, or custom materializations. dbt also infers dependencies between models and runs them in order. The following is a dbt model which selects from two other models called stg_orders and stg_customers:\nWITH customer_orders as (\n    SELECT\n        customer_id AS customer_id,\n        MIN(order_date) AS first_order,\n        MAX(order_date) AS most_recent_order,\n        COUNT(*) AS number_of_orders\n    FROM {{ ref('stg_orders') }} AS orders\n    GROUP BY\n        customer_id\n), customer_orders_info as (\n    SELECT\n        customers.customer_id AS customer_id,\n        customers.first_name AS first_name,\n        customers.last_name AS last_name,\n        customer_orders.customer_id AS customer_id_right,\n        customer_orders.first_order AS first_order,\n        customer_orders.most_recent_order AS most_recent_order,\n        customer_orders.number_of_orders AS number_of_orders\n    FROM {{ ref('stg_customers') }} AS customers\n    LEFT OUTER JOIN customer_orders\n        ON customers.customer_id = customer_orders.customer_id\n)\nSELECT\n    customer_id,\n    first_name,\n    last_name,\n    first_order,\n    most_recent_order,\n    number_of_orders\nFROM customer_orders_info\ndbt will make sure that the resulting table will be created after stg_orders and stg_customers. This model is inspired by the jaffle shop demo project by dbt Labs where you can find more example queries.\nAt the end of 2022, dbt added support for Python models on specific platforms (Snowflake, Databricks, Google Cloud Platform). This can be useful for complex transformations such as using a machine learning model and storing the results. However, it also requires that your Python code is run in a cloud data warehouse and often, that data is moved into a Python process which can be slower than leveraging the power of modern SQL engines.\n\n\nWhy dbt and Ibis go great together\ndbt-ibis offers a lightweight and compatible alternative, which allows you to write dbt models using Ibis. dbt-ibis transparently converts your Ibis statements into SQL and then hands it over to dbt. Your database does not need to have Python support for this as everything is executed in the same process as dbt. Hence, this allows for working in Python for all dbt adapters with supported Ibis backends. Rewriting the above SQL model in Ibis we get:\nfrom dbt_ibis import depends_on, ref\n\n\n@depends_on(ref(\"stg_customers\"), ref(\"stg_orders\"))\ndef model(customers, orders):\n    customer_orders = orders.group_by(\"customer_id\").aggregate(\n        first_order=orders[\"order_date\"].min(),\n        most_recent_order=orders[\"order_date\"].max(),\n        number_of_orders=orders.count(),\n    )\n    # Add first_name and last_name\n    customer_orders = customers.join(customer_orders, \"customer_id\", how=\"left\")\n    return customer_orders.select(\n            \"customer_id\",\n            \"first_name\",\n            \"last_name\",\n            \"first_order\",\n            \"most_recent_order\",\n            \"number_of_orders\",\n        )\nUsing Ibis instead of SQL for dbt models brings you many advantages:\n\nType checks and validation before your code is executed in a database.\nMore composable as you can break down complex queries into smaller pieces.\nBetter reusability of code. Although dbt allows you to use Jinja and macros, which is an improvement over plain SQL, this gets you only so far. String manipulation is inherently fragile. With dbt-ibis, you can easily share common code between models.\nYour dbt models become backend agnostic which reduces lock-in to a specific database. Furthermore, you get the possibility of building a multi-engine data stack. For example, you could use DuckDB for small to medium workloads and Snowflake for heavy workloads and as an end-user and BI layer leveraging its governance features. Depending on the size of your warehouse, this can result in significant cost savings.\nUnit test your code with your favorite Python testing frameworks such as pytest.\n\nIn addition, you can stick to the tool (Ibis) you like, no matter if you’re writing an ingestion pipeline, a dbt model to transform the data in your data warehouse, or conduct an ad-hoc analysis in a Jupyter notebook.\nBe aware that a current limitation of dbt-ibis is that you cannot connect to the database from within your dbt models, i.e. you purely use Ibis to construct a SELECT statement. You cannot execute statements and act based on the results.\n\n\nFurther readings\nIf you want to give dbt-ibis a try, head over to the GitHub repo for more information on how to get up and running in no time!\nFor more details on the future of the integration of Ibis within dbt, you can check out this PR and this GitHub issue on adding an official plugin system to dbt which could be used to provide first-class support for modeling languages in general and which might allow dbt-ibis to provide an even better user experience and more features. See also this discussion on Ibis as a dataframe API in the dbt GitHub repo.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/campaign-finance/index.html",
    "href": "posts/campaign-finance/index.html",
    "title": "Exploring campaign finance data",
    "section": "",
    "text": "Hi! My name is Nick Crews, and I’m a data engineer that looks at public campaign finance data.\nIn this post, I’ll walk through how I use Ibis to explore public campaign contribution data from the Federal Election Commission (FEC). We’ll do some loading, cleaning, featurizing, and visualization. There will be filtering, sorting, grouping, and aggregation."
  },
  {
    "objectID": "posts/campaign-finance/index.html#downloading-the-data",
    "href": "posts/campaign-finance/index.html#downloading-the-data",
    "title": "Exploring campaign finance data",
    "section": "Downloading The Data",
    "text": "Downloading The Data\n\nfrom pathlib import Path\nfrom zipfile import ZipFile\nfrom urllib.request import urlretrieve\n\n# Download and unzip the 2018 individual contributions data\nurl = \"https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/2018/indiv18.zip\"\nzip_path = Path(\"indiv18.zip\")\ncsv_path = Path(\"indiv18.csv\")\n\nif not zip_path.exists():\n    urlretrieve(url, zip_path)\n\nif not csv_path.exists():\n    with ZipFile(zip_path) as zip_file, csv_path.open(\"w\") as csv_file:\n        for line in zip_file.open(\"itcont.txt\"):\n            csv_file.write(line.decode())"
  },
  {
    "objectID": "posts/campaign-finance/index.html#loading-the-data",
    "href": "posts/campaign-finance/index.html#loading-the-data",
    "title": "Exploring campaign finance data",
    "section": "Loading the data",
    "text": "Loading the data\nNow that we have our raw data in a .csv format, let’s load it into Ibis, using the duckdb backend.\nNote that a 4.3 GB .csv would be near the limit of what pandas could handle on my laptop with 16GB of RAM. In pandas, typically every time you perform a transformation on the data, a copy of the data is made. I could only do a few transformations before I ran out of memory.\nWith Ibis, this problem is solved in two different ways.\nFirst, because they are designed to work with very large datasets, many (all?) SQL backends support out of core operations. The data lives on disk, and are only loaded in a streaming fashion when needed, and then written back to disk as the operation is performed.\nSecond, unless you explicitly ask for it, Ibis makes use of lazy evaluation. This means that when you ask for a result, the result is not persisted in memory. Only the original source data is persisted. Everything else is derived from this on the fly.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True\n\n# The raw .csv file doesn't have column names, so we will add them in the next step.\nraw = ibis.read_csv(csv_path)\nraw\n\n┏━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ C00401224 ┃ A      ┃ M6     ┃ P      ┃ 201804059101866001 ┃ 24T    ┃ IND    ┃ STOUFFER, LEIGH   ┃ AMSTELVEEN   ┃ ZZ     ┃ 1187RC    ┃ MYSELF            ┃ SELF EMPLOYED           ┃ 05172017 ┃ 10    ┃ C00458000 ┃ SA11AI_81445687 ┃ 1217152 ┃ column18 ┃ EARMARKED FOR PROGRESSIVE CHANGE CAMPAIGN COMMITTEE (C00458000) ┃ 4050820181544765358 ┃\n┡━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string │ string │ string │ int64              │ string │ string │ string            │ string       │ string │ string    │ string            │ string                  │ string   │ int64 │ string    │ string          │ int64   │ string   │ string                                                          │ int64               │\n├───────────┼────────┼────────┼────────┼────────────────────┼────────┼────────┼───────────────────┼──────────────┼────────┼───────────┼───────────────────┼─────────────────────────┼──────────┼───────┼───────────┼─────────────────┼─────────┼──────────┼─────────────────────────────────────────────────────────────────┼─────────────────────┤\n│ C00401224 │ A      │ M6     │ P      │ 201804059101867748 │ 24T    │ IND    │ STRAWS, JOYCE     │ OCOEE        │ FL     │ 34761     │ SILVERSEA CRUISES │ RESERVATIONS SUPERVISOR │ 05182017 │    10 │ C00000935 │ SA11AI_81592336 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544770597 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101867748 │ 24T    │ IND    │ STRAWS, JOYCE     │ OCOEE        │ FL     │ 34761     │ SILVERSEA CRUISES │ RESERVATIONS SUPERVISOR │ 05192017 │    15 │ C00000935 │ SA11AI_81627562 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544770598 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865942 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 039020760 │ NONE              │ NONE                    │ 05132017 │    35 │ C00000935 │ SA11AI_81047921 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544765179 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865942 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 039020760 │ NONE              │ NONE                    │ 05152017 │    35 │ C00000935 │ SA11AI_81209209 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544765180 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865942 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 039020760 │ NONE              │ NONE                    │ 05192017 │     5 │ C00000935 │ SA11AI_81605223 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544765181 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865943 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 039020760 │ NONE              │ NONE                    │ 05242017 │    15 │ C00000935 │ SA11AI_82200022 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544765182 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865943 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 03902     │ NOT EMPLOYED      │ NOT EMPLOYED            │ 05292017 │   100 │ C00213512 │ SA11AI_82589834 │ 1217152 │ NULL     │ EARMARKED FOR NANCY PELOSI FOR CONGRESS (C00213512)             │ 4050820181544765184 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101865944 │ 24T    │ IND    │ STOTT, JIM        │ CAPE NEDDICK │ ME     │ 039020760 │ NONE              │ NONE                    │ 05302017 │    35 │ C00000935 │ SA11AI_82643727 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544765185 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101867050 │ 24T    │ IND    │ STRANGE, WINIFRED │ ANNA MSRIA   │ FL     │ 34216     │ NOT EMPLOYED      │ NOT EMPLOYED            │ 05162017 │    25 │ C00000935 │ SA11AI_81325918 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544768505 │\n│ C00401224 │ A      │ M6     │ P      │ 201804059101867051 │ 24T    │ IND    │ STRANGE, WINIFRED │ ANNA MSRIA   │ FL     │ 34216     │ NOT EMPLOYED      │ NOT EMPLOYED            │ 05232017 │    25 │ C00000935 │ SA11AI_81991189 │ 1217152 │ NULL     │ EARMARKED FOR DCCC (C00000935)                                  │ 4050820181544768506 │\n│ …         │ …      │ …      │ …      │                  … │ …      │ …      │ …                 │ …            │ …      │ …         │ …                 │ …                       │ …        │     … │ …         │ …               │       … │ …        │ …                                                               │                   … │\n└───────────┴────────┴────────┴────────┴────────────────────┴────────┴────────┴───────────────────┴──────────────┴────────┴───────────┴───────────────────┴─────────────────────────┴──────────┴───────┴───────────┴─────────────────┴─────────┴──────────┴─────────────────────────────────────────────────────────────────┴─────────────────────┘\n\n\n\n\n# For a more comprehesive description of the columns and their meaning, see\n# https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description/\ncolumns = {\n    \"CMTE_ID\": \"keep\",  # Committee ID\n    \"AMNDT_IND\": \"drop\",  # Amendment indicator. A = amendment, N = new, T = termination\n    \"RPT_TP\": \"drop\",  # Report type (monthly, quarterly, etc)\n    \"TRANSACTION_PGI\": \"keep\",  # Primary/general indicator\n    \"IMAGE_NUM\": \"drop\",  # Image number\n    \"TRANSACTION_TP\": \"drop\",  # Transaction type\n    \"ENTITY_TP\": \"keep\",  # Entity type\n    \"NAME\": \"drop\",  # Contributor name\n    \"CITY\": \"keep\",  # Contributor city\n    \"STATE\": \"keep\",  # Contributor state\n    \"ZIP_CODE\": \"drop\",  # Contributor zip code\n    \"EMPLOYER\": \"drop\",  # Contributor employer\n    \"OCCUPATION\": \"drop\",  # Contributor occupation\n    \"TRANSACTION_DT\": \"keep\",  # Transaction date\n    \"TRANSACTION_AMT\": \"keep\",  # Transaction amount\n    # Other ID. For individual contributions will be null. For contributions from\n    # other FEC committees, will be the committee ID of the other committee.\n    \"OTHER_ID\": \"drop\",\n    \"TRAN_ID\": \"drop\",  # Transaction ID\n    \"FILE_NUM\": \"drop\",  # File number, unique number assigned to each report filed with the FEC\n    \"MEMO_CD\": \"drop\",  # Memo code\n    \"MEMO_TEXT\": \"drop\",  # Memo text\n    \"SUB_ID\": \"drop\",  # Submission ID. Unique number assigned to each transaction.\n}\n\nrenaming = {old: new for old, new in zip(raw.columns, columns.keys())}\nto_keep = [k for k, v in columns.items() if v == \"keep\"]\nkept = raw.relabel(renaming)[to_keep]\nkept\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY         ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string          │ string    │ string       │ string │ string         │ int64           │\n├───────────┼─────────────────┼───────────┼──────────────┼────────┼────────────────┼─────────────────┤\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05182017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05192017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05132017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05152017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05192017       │               5 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05242017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05292017       │             100 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05302017       │              35 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05162017       │              25 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05232017       │              25 │\n│ …         │ …               │ …         │ …            │ …      │ …              │               … │\n└───────────┴─────────────────┴───────────┴──────────────┴────────┴────────────────┴─────────────────┘\n\n\n\n\n# 21 million rows\nkept.count()\n\n\n\n\n\n21730730\n\n\n\nHuh, what’s up with those timings? Previewing the head only took a fraction of a second, but finding the number of rows took 10 seconds.\nThat’s because duckdb is scanning the .csv file on the fly every time we access it. So we only have to read the first few lines to get that preview, but we have to read the whole file to get the number of rows.\nNote that this isn’t a feature of Ibis, but a feature of Duckdb. This what I think is one of the strengths of Ibis: Ibis itself doesn’t have to implement any of the optimimizations or features of the backends. Those backends can focus on what they do best, and Ibis can get those things for free.\nSo, let’s tell duckdb to actually read in the file to its native format so later accesses will be faster. This will be a ~20 seconds that we’ll only have to pay once.\n\nkept = kept.cache()\nkept\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY         ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string          │ string    │ string       │ string │ string         │ int64           │\n├───────────┼─────────────────┼───────────┼──────────────┼────────┼────────────────┼─────────────────┤\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05182017       │              10 │\n│ C00401224 │ P               │ IND       │ OCOEE        │ FL     │ 05192017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05132017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05152017       │              35 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05192017       │               5 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05242017       │              15 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05292017       │             100 │\n│ C00401224 │ P               │ IND       │ CAPE NEDDICK │ ME     │ 05302017       │              35 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05162017       │              25 │\n│ C00401224 │ P               │ IND       │ ANNA MSRIA   │ FL     │ 05232017       │              25 │\n│ …         │ …               │ …         │ …            │ …      │ …              │               … │\n└───────────┴─────────────────┴───────────┴──────────────┴────────┴────────────────┴─────────────────┘\n\n\n\nLook, now accessing it only takes a fraction of a second!\n\nkept.count()\n\n\n\n\n\n21730730\n\n\n\n\nCommittees Data\nThe contributions only list an opaque CMTE_ID column. We want to know which actual committee this is. Let’s load the committees table so we can lookup from committee ID to committee name.\n\ndef read_committees():\n    committees_url = \"https://cg-519a459a-0ea3-42c2-b7bc-fa1143481f74.s3-us-gov-west-1.amazonaws.com/bulk-downloads/2018/committee_summary_2018.csv\"\n    # This just creates a view, it doesn't actually fetch the data yet\n    tmp = ibis.read_csv(committees_url)\n    tmp = tmp[\"CMTE_ID\", \"CMTE_NM\"]\n    # The raw table contains multiple rows for each committee id, so lets pick\n    # an arbitrary row for each committee id as the representative name.\n    deduped = tmp.group_by(\"CMTE_ID\").agg(CMTE_NM=_.CMTE_NM.arbitrary())\n    return deduped\n\n\ncomms = read_committees().cache()\ncomms\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ CMTE_ID   ┃ CMTE_NM                                                        ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string                                                         │\n├───────────┼────────────────────────────────────────────────────────────────┤\n│ C00659441 │ JASON ORTITAY FOR CONGRESS                                     │\n│ C00661249 │ SERVICE AFTER SERVICE                                          │\n│ C00457754 │ U.S. TRAVEL ASSOCIATION PAC                                    │\n│ C00577635 │ ISAKSON VICTORY COMMITTEE                                      │\n│ C00297911 │ TEXAS FORESTRY ASSOCIATION FORESTRY POLITICAL ACTION COMMITTEE │\n│ C00551382 │ VOTECLIMATE.US PAC                                             │\n│ C00414318 │ LOEBSACK FOR CONGRESS                                          │\n│ C00610709 │ AUSTIN INNOVATION 2016                                         │\n│ C00131607 │ FLORIDA CITRUS MUTUAL POLITCAL ACTION COMMITTEE                │\n│ C00136531 │ NATIONAL DEMOCRATIC POLICY COMMITTEE                           │\n│ …         │ …                                                              │\n└───────────┴────────────────────────────────────────────────────────────────┘\n\n\n\nNow add the committee name to the contributions table:\n\ntogether = kept.left_join(comms, \"CMTE_ID\").drop(\"CMTE_ID\", \"CMTE_ID_right\")\ntogether\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ ENTITY_TP ┃ CITY             ┃ STATE  ┃ TRANSACTION_DT ┃ TRANSACTION_AMT ┃ CMTE_NM                                         ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string          │ string    │ string           │ string │ string         │ int64           │ string                                          │\n├─────────────────┼───────────┼──────────────────┼────────┼────────────────┼─────────────────┼─────────────────────────────────────────────────┤\n│ P               │ IND       │ COHASSET         │ MA     │ 01312017       │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ KEY LARGO        │ FL     │ 01042017       │            5000 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ LOOKOUT MOUNTAIN │ GA     │ 01312017       │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ NORTH YARMOUTH   │ ME     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ ALPHARETTA       │ GA     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ FALMOUTH         │ ME     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ FALMOUTH         │ ME     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ HOLLIS CENTER    │ ME     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ FALMOUTH         │ ME     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ P               │ IND       │ ALEXANDRIA       │ VA     │ 01312017       │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │\n│ …               │ …         │ …                │ …      │ …              │               … │ …                                               │\n└─────────────────┴───────────┴──────────────────┴────────┴────────────────┴─────────────────┴─────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/campaign-finance/index.html#cleaning",
    "href": "posts/campaign-finance/index.html#cleaning",
    "title": "Exploring campaign finance data",
    "section": "Cleaning",
    "text": "Cleaning\nFirst, let’s drop any contributions that don’t have a committee name. There are only 6 of them.\n\n# We can do this fearlessly, no .copy() needed, because\n# everything in Ibis is immutable. If we did this in pandas,\n# we might start modifying the original DataFrame accidentally!\ncleaned = together\n\nhas_name = cleaned.CMTE_NM.notnull()\ncleaned = cleaned[has_name]\nhas_name.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ NotNull(CMTE_NM) ┃ NotNull(CMTE_NM)_count ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean          │ int64                  │\n├──────────────────┼────────────────────────┤\n│ True             │               21730724 │\n│ False            │                      6 │\n└──────────────────┴────────────────────────┘\n\n\n\nLet’s look at the ENTITY_TP column. This represents the type of entity that made the contribution:\n\ntogether.ENTITY_TP.value_counts()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ ENTITY_TP ┃ ENTITY_TP_count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ int64           │\n├───────────┼─────────────────┤\n│ IND       │        21687992 │\n│ CCM       │             698 │\n│ CAN       │           13659 │\n│ ORG       │           18555 │\n│ PTY       │              49 │\n│ COM       │             867 │\n│ PAC       │            3621 │\n│ NULL      │            5289 │\n└───────────┴─────────────────┘\n\n\n\nWe only care about contributions from individuals.\nOnce we filter on this column, the contents of it are irrelevant, so let’s drop it.\n\ncleaned = together[_.ENTITY_TP == \"IND\"].drop(\"ENTITY_TP\")\n\nIt looks like the TRANSACTION_DT column was a raw string like “MMDDYYYY”, so let’s convert that to a proper date type.\n\nfrom ibis.expr.types import StringValue, DateValue\n\n\ndef mmddyyyy_to_date(val: StringValue) -&gt; DateValue:\n    return val.cast(str).lpad(8, \"0\").to_timestamp(\"%m%d%Y\").date()\n\n\ncleaned = cleaned.mutate(date=mmddyyyy_to_date(_.TRANSACTION_DT)).drop(\"TRANSACTION_DT\")\ncleaned\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ CITY             ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM                                         ┃ date       ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string          │ string           │ string │ int64           │ string                                          │ date       │\n├─────────────────┼──────────────────┼────────┼─────────────────┼─────────────────────────────────────────────────┼────────────┤\n│ P               │ COHASSET         │ MA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ KEY LARGO        │ FL     │            5000 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-04 │\n│ P               │ LOOKOUT MOUNTAIN │ GA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ NORTH YARMOUTH   │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ ALPHARETTA       │ GA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ HOLLIS CENTER    │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ P               │ ALEXANDRIA       │ VA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │\n│ …               │ …                │ …      │               … │ …                                               │ …          │\n└─────────────────┴──────────────────┴────────┴─────────────────┴─────────────────────────────────────────────────┴────────────┘\n\n\n\nThe TRANSACTION_PGI column represents the type (primary, general, etc) of election, and the year. But it seems to be not very consistent:\n\ncleaned.TRANSACTION_PGI.topk(10)\n\n┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ TRANSACTION_PGI ┃ Count(TRANSACTION_PGI) ┃\n┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string          │ int64                  │\n├─────────────────┼────────────────────────┤\n│ P               │               17013596 │\n│ G2018           │                2095123 │\n│ P2018           │                1677183 │\n│ P2020           │                 208501 │\n│ O2018           │                 161874 │\n│ S2017           │                 124336 │\n│ G2017           │                  98401 │\n│ P2022           │                  91136 │\n│ P2017           │                  61153 │\n│ R2017           │                  54281 │\n└─────────────────┴────────────────────────┘\n\n\n\n\ndef get_election_type(pgi: StringValue) -&gt; StringValue:\n    \"\"\"Use the first letter of the TRANSACTION_PGI column to determine the election type\n\n    If the first letter is not one of the known election stage, then return null.\n    \"\"\"\n    election_types = {\n        \"P\": \"primary\",\n        \"G\": \"general\",\n        \"O\": \"other\",\n        \"C\": \"convention\",\n        \"R\": \"runoff\",\n        \"S\": \"special\",\n        \"E\": \"recount\",\n    }\n    first_letter = pgi[0]\n    return first_letter.substitute(election_types, else_=ibis.NA)\n\n\ncleaned = cleaned.mutate(election_type=get_election_type(_.TRANSACTION_PGI)).drop(\n    \"TRANSACTION_PGI\"\n)\ncleaned\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CITY             ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM                                         ┃ date       ┃ election_type ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string           │ string │ int64           │ string                                          │ date       │ string        │\n├──────────────────┼────────┼─────────────────┼─────────────────────────────────────────────────┼────────────┼───────────────┤\n│ COHASSET         │ MA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ KEY LARGO        │ FL     │            5000 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-04 │ primary       │\n│ LOOKOUT MOUNTAIN │ GA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ NORTH YARMOUTH   │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ ALPHARETTA       │ GA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ HOLLIS CENTER    │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ ALEXANDRIA       │ VA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │\n│ …                │ …      │               … │ …                                               │ …          │ …             │\n└──────────────────┴────────┴─────────────────┴─────────────────────────────────────────────────┴────────────┴───────────────┘\n\n\n\nThat worked well! There are 0 nulls in the resulting column, so we always were able to determine the election type.\n\ncleaned.election_type.topk(10)\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ election_type ┃ Count(election_type) ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string        │ int64                │\n├───────────────┼──────────────────────┤\n│ primary       │             19061953 │\n│ general       │              2216685 │\n│ other         │               161965 │\n│ special       │               149572 │\n│ runoff        │                69637 │\n│ convention    │                22453 │\n│ recount       │                 5063 │\n│ NULL          │                    0 │\n└───────────────┴──────────────────────┘\n\n\n\nAbout 1/20 of transactions are negative. These could represent refunds, or they could be data entry errors. Let’s drop them to keep it simple.\n\nabove_zero = cleaned.TRANSACTION_AMT &gt; 0\ncleaned = cleaned[above_zero]\nabove_zero.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ Greater(TRANSACTION_AMT, 0) ┃ Greater(TRANSACTION_AMT, 0)_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                     │ int64                             │\n├─────────────────────────────┼───────────────────────────────────┤\n│ True                        │                          20669809 │\n│ False                       │                           1018183 │\n└─────────────────────────────┴───────────────────────────────────┘"
  },
  {
    "objectID": "posts/campaign-finance/index.html#adding-features",
    "href": "posts/campaign-finance/index.html#adding-features",
    "title": "Exploring campaign finance data",
    "section": "Adding Features",
    "text": "Adding Features\nNow that the data is cleaned up to a usable format, let’s add some features.\nFirst, it’s useful to categorize donations by size, placing them into buckets of small, medium, large, etc.\n\nedges = [\n    10,\n    50,\n    100,\n    500,\n    1000,\n    5000,\n]\nlabels = [\n    \"&lt;10\",\n    \"10-50\",\n    \"50-100\",\n    \"100-500\",\n    \"500-1000\",\n    \"1000-5000\",\n    \"5000+\",\n]\n\n\ndef bucketize(vals, edges, str_labels):\n    # Uses Ibis's .bucket() method to create a categorical column\n    int_labels = vals.bucket(edges, include_under=True, include_over=True)\n    # Map the integer labels to the string labels\n    int_to_str = {str(i): s for i, s in enumerate(str_labels)}\n    return int_labels.cast(str).substitute(int_to_str)\n\n\nfeatured = cleaned.mutate(amount_bucket=bucketize(_.TRANSACTION_AMT, edges, labels))\nfeatured\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CITY             ┃ STATE  ┃ TRANSACTION_AMT ┃ CMTE_NM                                         ┃ date       ┃ election_type ┃ amount_bucket ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string           │ string │ int64           │ string                                          │ date       │ string        │ string        │\n├──────────────────┼────────┼─────────────────┼─────────────────────────────────────────────────┼────────────┼───────────────┼───────────────┤\n│ COHASSET         │ MA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ KEY LARGO        │ FL     │            5000 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-04 │ primary       │ 1000-5000     │\n│ LOOKOUT MOUNTAIN │ GA     │             230 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ NORTH YARMOUTH   │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ ALPHARETTA       │ GA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ HOLLIS CENTER    │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ FALMOUTH         │ ME     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ ALEXANDRIA       │ VA     │             384 │ UNUM GROUP POLITICAL ACTION COMMITTEE (UNUMPAC) │ 2017-01-31 │ primary       │ 100-500       │\n│ …                │ …      │               … │ …                                               │ …          │ …             │ …             │\n└──────────────────┴────────┴─────────────────┴─────────────────────────────────────────────────┴────────────┴───────────────┴───────────────┘"
  },
  {
    "objectID": "posts/campaign-finance/index.html#analysis",
    "href": "posts/campaign-finance/index.html#analysis",
    "title": "Exploring campaign finance data",
    "section": "Analysis",
    "text": "Analysis\n\nBy donation size\nOne thing we can look at is the donation breakdown by size: - Are most donations small or large? - Where do politicians/committees get most of their money from? Large or small donations?\nWe also will compare performance of Ibis vs pandas during this groupby.\n\ndef summary_by(table, by):\n    return table.group_by(by).agg(\n        n_donations=_.count(),\n        total_amount=_.TRANSACTION_AMT.sum(),\n        mean_amount=_.TRANSACTION_AMT.mean(),\n        median_amount=_.TRANSACTION_AMT.approx_median(),\n    )\n\n\ndef summary_by_pandas(df, by):\n    return df.groupby(by, as_index=False).agg(\n        n_donations=(\"election_type\", \"count\"),\n        total_amount=(\"TRANSACTION_AMT\", \"sum\"),\n        mean_amount=(\"TRANSACTION_AMT\", \"mean\"),\n        median_amount=(\"TRANSACTION_AMT\", \"median\"),\n    )\n\n\n# persist the input data so the following timings of the group_by are accurate.\nsubset = featured[\"election_type\", \"amount_bucket\", \"TRANSACTION_AMT\"]\nsubset = subset.cache()\npandas_subset = subset.execute()\n\nLet’s take a look at what we are actually computing:\n\nby_type_and_bucket = summary_by(subset, [\"election_type\", \"amount_bucket\"])\nby_type_and_bucket\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ election_type ┃ amount_bucket ┃ n_donations ┃ total_amount ┃ mean_amount  ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string        │ string        │ int64       │ int64        │ float64      │ int64         │\n├───────────────┼───────────────┼─────────────┼──────────────┼──────────────┼───────────────┤\n│ primary       │ 50-100        │     2663933 │    155426540 │    58.344763 │            50 │\n│ primary       │ 10-50         │     8115403 │    187666251 │    23.124699 │            25 │\n│ primary       │ 100-500       │     3636287 │    637353634 │   175.275943 │           150 │\n│ primary       │ &lt;10           │     2423728 │     10080721 │     4.159180 │             5 │\n│ primary       │ 500-1000      │      634677 │    334630687 │   527.245649 │           500 │\n│ primary       │ 1000-5000     │      684755 │   1231394874 │  1798.299938 │          1008 │\n│ primary       │ 5000+         │       44085 │   1558371116 │ 35349.237065 │         10000 │\n│ general       │ 100-500       │      700821 │    123174568 │   175.757530 │           150 │\n│ general       │ 50-100        │      304363 │     16184312 │    53.174374 │            50 │\n│ general       │ 10-50         │      660787 │     14411588 │    21.809733 │            25 │\n│ …             │ …             │           … │            … │            … │             … │\n└───────────────┴───────────────┴─────────────┴──────────────┴──────────────┴───────────────┘\n\n\n\nOK, now let’s do our timings.\nOne interesting thing to pay attention to here is the execution time for the following groupby. Before, we could get away with lazy execution: because we only wanted to preview the first few rows, we only had to compute the first few rows, so all our previews were very fast.\nBut now, as soon as we do a groupby, we have to actually go through the whole dataset in order to compute the aggregate per group. So this is going to be slower. BUT, duckdb is still quite fast. It only takes milliseconds to groupby-agg all 20 million rows!\n\n%timeit summary_by(subset, [\"election_type\", \"amount_bucket\"]).execute()  # .execute() so we actually fetch the data\n\n679 ms ± 11.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nNow let’s try the same thing in pandas:\n\n%timeit summary_by_pandas(pandas_subset, [\"election_type\", \"amount_bucket\"])\n\n3.59 s ± 31.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nIt takes about 4 seconds, which is about 10 times slower than duckdb. At this scale, it again doesn’t matter, but you could imagine with a dataset much larger than this, it would matter.\nLet’s also think about memory usage:\n\npandas_subset.memory_usage(deep=True).sum() / 1e9  # GB\n\n2.782586663\n\n\nThe source dataframe is couple gigabytes, so probably during the groupby, the peak memory usage is going to be a bit higher than this. You could use a profiler such as FIL if you wanted an exact number, I was too lazy to use that here.\nAgain, this works on my laptop at this dataset size, but much larger than this and I’d start having problems. Duckdb on the other hand is designed around working out of core so it should scale to datasets into the hundreds of gigabytes, much larger than your computer’s RAM.\n\n\nBack to analysis\nOK, let’s plot the result of that groupby.\nSurprise! (Or maybe not…) Most donations are small. But most of the money comes from donations larger than $1000.\nWell if that’s the case, why do politicians spend so much time soliciting small donations? One explanation is that they can use the number of donations as a marketing pitch, to show how popular they are, and thus how viable of a candidate they are.\nThis also might explain whose interests are being served by our politicians.\n\nimport altair as alt\n\n# Do some bookkeeping so the buckets are displayed smallest to largest on the charts\nbucket_col = alt.Column(\"amount_bucket:N\", sort=labels)\n\nn_by_bucket = (\n    alt.Chart(by_type_and_bucket.execute())\n    .mark_bar()\n    .encode(\n        x=bucket_col,\n        y=\"n_donations:Q\",\n        color=\"election_type:N\",\n    )\n)\ntotal_by_bucket = (\n    alt.Chart(by_type_and_bucket.execute())\n    .mark_bar()\n    .encode(\n        x=bucket_col,\n        y=\"total_amount:Q\",\n        color=\"election_type:N\",\n    )\n)\nn_by_bucket | total_by_bucket\n\n\n\n\n\n\n\n\n\nBy election stage\nLet’s look at how donations break down by election stage. Do people donate differently for primary elections vs general elections?\nLet’s ignore everything but primary and general elections, since they are the most common, and arguably the most important.\n\ngb2 = by_type_and_bucket[_.election_type.isin((\"primary\", \"general\"))]\nn_donations_per_election_type = _.n_donations.sum().over(group_by=\"election_type\")\nfrac = _.n_donations / n_donations_per_election_type\ngb2 = gb2.mutate(frac_n_donations_per_election_type=frac)\ngb2\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ election_type ┃ amount_bucket ┃ n_donations ┃ total_amount ┃ mean_amount  ┃ median_amount ┃ frac_n_donations_per_election_type ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string        │ string        │ int64       │ int64        │ float64      │ int64         │ float64                            │\n├───────────────┼───────────────┼─────────────┼──────────────┼──────────────┼───────────────┼────────────────────────────────────┤\n│ primary       │ 10-50         │     8115403 │    187666251 │    23.124699 │            25 │                           0.445831 │\n│ primary       │ &lt;10           │     2423728 │     10080721 │     4.159180 │             5 │                           0.133151 │\n│ primary       │ 100-500       │     3636287 │    637353634 │   175.275943 │           150 │                           0.199765 │\n│ primary       │ 50-100        │     2663933 │    155426540 │    58.344763 │            50 │                           0.146347 │\n│ primary       │ 500-1000      │      634677 │    334630687 │   527.245649 │           500 │                           0.034867 │\n│ primary       │ 1000-5000     │      684755 │   1231394874 │  1798.299938 │          1008 │                           0.037618 │\n│ primary       │ 5000+         │       44085 │   1558371116 │ 35349.237065 │         10000 │                           0.002422 │\n│ general       │ 50-100        │      304363 │     16184312 │    53.174374 │            50 │                           0.138017 │\n│ general       │ 100-500       │      700821 │    123174568 │   175.757530 │           150 │                           0.317796 │\n│ general       │ 500-1000      │      174182 │     91015697 │   522.532162 │           500 │                           0.078985 │\n│ …             │ …             │           … │            … │            … │             … │                                  … │\n└───────────────┴───────────────┴─────────────┴──────────────┴──────────────┴───────────────┴────────────────────────────────────┘\n\n\n\nIt looks like primary elections get a larger proportion of small donations.\n\nalt.Chart(gb2.execute()).mark_bar().encode(\n    x=\"election_type:O\",\n    y=\"frac_n_donations_per_election_type:Q\",\n    color=bucket_col,\n)\n\n\n\n\n\n\n\n\n\nBy recipient\nLet’s look at the top players. Who gets the most donations?\nFar and away it is ActBlue, which acts as a conduit for donations to Democratic interests.\nBeto O’Rourke is the top individual politician, hats off to him!\n\nby_recip = summary_by(featured, \"CMTE_NM\")\nby_recip\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ CMTE_NM                                                                          ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string                                                                           │ int64       │ int64        │ float64     │ int64         │\n├──────────────────────────────────────────────────────────────────────────────────┼─────────────┼──────────────┼─────────────┼───────────────┤\n│ EXELON CORPORATION POLITICAL ACTION COMMITTEE (EXELON PAC)                       │       13250 │      1939503 │  146.377585 │           118 │\n│ ARCHER DANIELS MIDLAND COMPANY-ADM PAC                                           │        4460 │       275807 │   61.840135 │            25 │\n│ PFIZER INC. PAC                                                                  │       46900 │      1948689 │   41.549872 │            20 │\n│ SUEZ WATER INC. FEDERAL PAC                                                      │         108 │        16873 │  156.231481 │           120 │\n│ INTERNATIONAL WAREHOUSE LOGISTICS ASSOCIATION PAC                                │          90 │       132200 │ 1468.888889 │          1000 │\n│ BAKERY, CONFECTIONERY, TOBACCO WORKERS AND GRAIN MILLERS INTERNATIONAL UNION PAC │         387 │        19091 │   49.330749 │            30 │\n│ UNION PACIFIC CORP. FUND FOR EFFECTIVE GOVERNMENT                                │       16118 │      2436963 │  151.195123 │           114 │\n│ NATIONAL ASSOCIATION OF REALTORS POLITICAL ACTION COMMITTEE                      │       24277 │      5492063 │  226.224945 │           154 │\n│ AMERICAN FINANCIAL SERVICES ASSOCIATION PAC                                      │         690 │       685839 │  993.969565 │            65 │\n│ WEYERHAEUSER COMPANY POLITICAL ACTION COMMITTEE                                  │        5512 │       343244 │   62.272134 │            30 │\n│ …                                                                                │           … │            … │           … │             … │\n└──────────────────────────────────────────────────────────────────────────────────┴─────────────┴──────────────┴─────────────┴───────────────┘\n\n\n\n\ntop_recip = by_recip.order_by(ibis.desc(\"n_donations\")).head(10)\nalt.Chart(top_recip.execute()).mark_bar().encode(\n    x=alt.X(\"CMTE_NM:O\", sort=\"-y\"),\n    y=\"n_donations:Q\",\n)\n\n\n\n\n\n\n\n\n\nBy Location\nWhere are the largest donations coming from?\n\nf2 = featured.mutate(loc=_.CITY + \", \" + _.STATE).drop(\"CITY\", \"STATE\")\nby_loc = summary_by(f2, \"loc\")\n# Drop the places with a small number of donations so we're\n# resistant to outliers for the mean\nby_loc = by_loc[_.n_donations &gt; 1000]\nby_loc\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ loc              ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string           │ int64       │ int64        │ float64     │ int64         │\n├──────────────────┼─────────────┼──────────────┼─────────────┼───────────────┤\n│ DALLAS, TX       │      154038 │     66558403 │  432.090802 │            58 │\n│ PHILADELPHIA, PA │      222938 │     36054977 │  161.726476 │            62 │\n│ MALIBU, CA       │       11699 │      4934763 │  421.810668 │            50 │\n│ SANTEE, CA       │        2454 │       201274 │   82.018745 │            26 │\n│ WINNETKA, IL     │        8589 │      5621809 │  654.535918 │           172 │\n│ OREM, UT         │        2110 │       837475 │  396.907583 │            50 │\n│ MESA, AZ         │       22128 │      1856636 │   83.904375 │            20 │\n│ WAYZATA, MN      │        6488 │      3326275 │  512.681104 │           117 │\n│ MINNETONKA, MN   │        5709 │      1187881 │  208.071641 │            50 │\n│ OJAI, CA         │        4496 │       926422 │  206.054715 │            25 │\n│ …                │           … │            … │           … │             … │\n└──────────────────┴─────────────┴──────────────┴─────────────┴───────────────┘\n\n\n\n\ndef top_by(col):\n    top = by_loc.order_by(ibis.desc(col)).head(10)\n    return (\n        alt.Chart(top.execute())\n        .mark_bar()\n        .encode(\n            x=alt.X('loc:O', sort=\"-y\"),\n            y=col,\n        )\n    )\n\n\ntop_by(\"n_donations\") | top_by(\"total_amount\") | top_by(\"mean_amount\") | top_by(\n    \"median_amount\"\n)\n\n\n\n\n\n\n\n\n\nBy month\nWhen do the donations come in?\n\nby_month = summary_by(featured, _.date.month().name(\"month_int\"))\n# Sorta hacky, .substritute doesn't work to change dtypes (yet?)\n# so we cast to string and then do our mapping\nmonth_map = {\n    \"1\": \"Jan\",\n    \"2\": \"Feb\",\n    \"3\": \"Mar\",\n    \"4\": \"Apr\",\n    \"5\": \"May\",\n    \"6\": \"Jun\",\n    \"7\": \"Jul\",\n    \"8\": \"Aug\",\n    \"9\": \"Sep\",\n    \"10\": \"Oct\",\n    \"11\": \"Nov\",\n    \"12\": \"Dec\",\n}\nby_month = by_month.mutate(month_str=_.month_int.cast(str).substitute(month_map))\nby_month\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ month_int ┃ n_donations ┃ total_amount ┃ mean_amount ┃ median_amount ┃ month_str ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int32     │ int64       │ int64        │ float64     │ int64         │ string    │\n├───────────┼─────────────┼──────────────┼─────────────┼───────────────┼───────────┤\n│      NULL │        1514 │       250297 │  165.321664 │            99 │ NULL      │\n│         1 │      348979 │    174837854 │  500.998209 │           122 │ Jan       │\n│         2 │      581646 │    255997655 │  440.126219 │           100 │ Feb       │\n│         3 │     1042577 │    430906797 │  413.309326 │            81 │ Mar       │\n│         4 │     1088244 │    299252692 │  274.986760 │            50 │ Apr       │\n│         5 │     1374247 │    387317192 │  281.839576 │            48 │ May       │\n│         6 │     1667285 │    465305247 │  279.079610 │            44 │ Jun       │\n│         7 │     1607053 │    320528605 │  199.451172 │            35 │ Jul       │\n│         8 │     2023466 │    473544182 │  234.026261 │            35 │ Aug       │\n│         9 │     2583847 │    697888624 │  270.096729 │            38 │ Sep       │\n│         … │           … │            … │           … │             … │ …         │\n└───────────┴─────────────┴──────────────┴─────────────┴───────────────┴───────────┘\n\n\n\n\nmonths_in_order = list(month_map.values())\nalt.Chart(by_month.execute()).mark_bar().encode(\n    x=alt.X(\"month_str:O\", sort=months_in_order),\n    y=\"n_donations:Q\",\n)"
  },
  {
    "objectID": "posts/campaign-finance/index.html#conclusion",
    "href": "posts/campaign-finance/index.html#conclusion",
    "title": "Exploring campaign finance data",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for following along! I hope you’ve learned something about Ibis, and maybe even about campaign finance.\nIbis is a great tool for exploring data. I now find myself reaching for it when in the past I would have reached for pandas.\nSome of the highlights for me:\n\nFast, lazy execution, a great display format, and good type hinting/editor support for a great REPL experience.\nVery well thought-out API and semantics (e.g. isinstance(val, NumericValue)?? That’s beautiful!)\nFast and fairly complete string support, since I work with a lot of text data.\nExtremely responsive maintainers. Sometimes I’ve submitted multiple feature requests and bug reports in a single day, and a PR has been merged by the next day.\nEscape hatch to SQL. I didn’t have to use that here, but if something isn’t supported, you can always fall back to SQL.\n\nCheck out The Ibis Website for more information."
  },
  {
    "objectID": "posts/torch/index.html",
    "href": "posts/torch/index.html",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "",
    "text": "In this blog post we show how to leverage ecosystem tools to build an end-to-end ML pipeline using Ibis, DuckDB and PyTorch.\nCheck out the live stream of this notebook below!\nLet’s get started!\nimport ibis\nimport ibis.expr.datatypes as dt\n\nfrom ibis import _, selectors as s, udf\n\nibis.options.interactive = True"
  },
  {
    "objectID": "posts/torch/index.html#define-a-function-to-clean-inputs",
    "href": "posts/torch/index.html#define-a-function-to-clean-inputs",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Define a Function to Clean Inputs",
    "text": "Define a Function to Clean Inputs\nLet’s define a function to clean the data in a few different ways:\n\nRemove outliers (Z-score based)\nRemove negative trip distances and negative fare amounts\nCast inputs to float32, since that’s what PyTorch wants\n\nWe use a function here to ensure that we can run the same code on the test data set before prediction.\n\ndef clean_input(path):\n    return (\n        # load parquet\n        ibis.read_parquet(path)\n        # compute fare_amount_zscore and trip_distance_zscore\n        .mutate(s.across([\"fare_amount\", \"trip_distance\"], dict(zscore=(_ - _.mean()) / _.std())))\n        # filter out negative trip distance and bizarre transactions\n        .filter([_.trip_distance &gt; 0.0, _.fare_amount &gt;= 0.0])\n        # keep values that within 2 standard deviations\n        .filter(s.if_all(s.endswith(\"_zscore\"), _.abs() &lt;= 2))\n        # drop columns that aren't necessary for further analysis\n        .drop(s.endswith(\"_zscore\"))\n        # select the columns we care about\n        .select(s.across([\"fare_amount\", \"trip_distance\"], _.cast(\"float32\")))\n    )\n\n\ntraining_data = clean_input(\"https://storage.googleapis.com/ibis-tutorial-data/nyctaxi/yellow/yellow_tripdata_2016-01.parquet\")\ntraining_data\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ trip_distance ┃ fare_amount ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ float32       │ float32     │\n├───────────────┼─────────────┤\n│          3.20 │        14.0 │\n│          1.00 │         9.5 │\n│          0.90 │         6.0 │\n│          0.80 │         5.0 │\n│          1.80 │        11.0 │\n│          2.30 │        11.0 │\n│         13.80 │        43.0 │\n│          3.46 │        20.0 │\n│          0.83 │         5.5 │\n│          0.87 │         7.0 │\n│             … │           … │\n└───────────────┴─────────────┘"
  },
  {
    "objectID": "posts/torch/index.html#execute-the-query-and-convert-to-torch-tensors",
    "href": "posts/torch/index.html#execute-the-query-and-convert-to-torch-tensors",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Execute the Query and Convert to Torch Tensors",
    "text": "Execute the Query and Convert to Torch Tensors\nNew in Ibis 6.0 is the to_torch method, which executes a query and returns the results as a dictionary of torch.Tensors keyed by column names.\nWe’ll use that to get our input data for training.\n\nimport torch\n\ntorch_training_data: dict[str, torch.Tensor] = training_data.to_torch()\ntorch_training_data\n\n{'trip_distance': tensor([3.2000, 1.0000, 0.9000,  ..., 5.6300, 0.7700, 1.2600]),\n 'fare_amount': tensor([14.0000,  9.5000,  6.0000,  ..., 18.5000,  5.0000,  6.5000])}"
  },
  {
    "objectID": "posts/torch/index.html#train-the-model",
    "href": "posts/torch/index.html#train-the-model",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Train the Model",
    "text": "Train the Model\nLet’s assume for now we don’t have access to the model code. Maybe your co-worker wrote the model or it’s part of an API that you don’t control. Either way, it’s a black box to us.\nThe API looks like this:\n\nimport pyarrow\n\n\nclass PredictCabFare:\n    def __init__(self, data: dict[str, torch.Tensor]) -&gt; None:\n        \"\"\"Initialize the model with training data.\"\"\"\n\n    def train(self) -&gt; None:\n        \"\"\"Train the model.\"\"\"\n\n    def __call__(self, input: pyarrow.ChunkedArray) -&gt; pyarrow.Array:\n        \"\"\"Invoke the trained model on unseen input.\"\"\"\n\n\nfrom model import PredictCabFare\n\n\nmodel = PredictCabFare(torch_training_data)\nmodel.train()"
  },
  {
    "objectID": "posts/torch/index.html#define-an-ibis-udf-that-predicts-fares",
    "href": "posts/torch/index.html#define-an-ibis-udf-that-predicts-fares",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Define an Ibis UDF that predicts fares",
    "text": "Define an Ibis UDF that predicts fares\nNow we get to the meaty part: defining an Ibis UDF (user-defined function) that invokes our model on unseen data!\n\nfrom ibis.expr.operations import udf\n\n\n@udf.scalar.pyarrow\ndef predict_fare(distance: dt.float64) -&gt; dt.float32:\n    return model(distance)\n\nLet’s run our UDF\n\nprediction = (\n    clean_input(\"https://storage.googleapis.com/ibis-tutorial-data/nyctaxi/yellow/yellow_tripdata_2016-02.parquet\")\n    .limit(10_000)\n    .mutate(predicted_fare=lambda t: predict_fare(t.trip_distance.cast(\"float32\")))\n)\nprediction\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ trip_distance ┃ fare_amount ┃ predicted_fare ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ float32       │ float32     │ float32        │\n├───────────────┼─────────────┼────────────────┤\n│           9.1 │        27.0 │      29.085516 │\n│           3.3 │        11.5 │      12.626410 │\n│           0.5 │         4.0 │       4.680637 │\n│           7.4 │        26.5 │      24.261295 │\n│           1.6 │         7.5 │       7.802191 │\n│           3.8 │        16.0 │      14.045299 │\n│           1.1 │         6.0 │       6.383303 │\n│           6.8 │        21.0 │      22.558630 │\n│           2.9 │        12.0 │      11.491301 │\n│           1.2 │         6.5 │       6.667080 │\n│             … │           … │              … │\n└───────────────┴─────────────┴────────────────┘"
  },
  {
    "objectID": "posts/torch/index.html#prepare-the-data-for-plotting",
    "href": "posts/torch/index.html#prepare-the-data-for-plotting",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Prepare the Data for Plotting",
    "text": "Prepare the Data for Plotting\nHere we tidy up our data to make it easier to adjust plotting style based on the data.\nIn this case, we’re interested in visually distinguishing the model’s predicted fare amount from the actual fare amount so we pivot the data into a longer form which adds a string column metric that indicates the kind of fare a given row contains.\n\npivoted_prediction = prediction.pivot_longer(\n    s.contains(\"fare\"),\n    values_to=\"fare\",\n    names_to=\"metric\",\n)\npivoted_prediction\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ trip_distance ┃ metric         ┃ fare      ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ float32       │ string         │ float32   │\n├───────────────┼────────────────┼───────────┤\n│           9.1 │ fare_amount    │ 27.000000 │\n│           9.1 │ predicted_fare │ 29.085516 │\n│           3.3 │ fare_amount    │ 11.500000 │\n│           3.3 │ predicted_fare │ 12.626410 │\n│           0.5 │ fare_amount    │  4.000000 │\n│           0.5 │ predicted_fare │  4.680637 │\n│           7.4 │ fare_amount    │ 26.500000 │\n│           7.4 │ predicted_fare │ 24.261295 │\n│           1.6 │ fare_amount    │  7.500000 │\n│           1.6 │ predicted_fare │  7.802191 │\n│             … │ …              │         … │\n└───────────────┴────────────────┴───────────┘"
  },
  {
    "objectID": "posts/torch/index.html#plot-the-results",
    "href": "posts/torch/index.html#plot-the-results",
    "title": "Ibis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch",
    "section": "Plot the Results",
    "text": "Plot the Results\nThere are a bunch of strange and interesting data points and observations that don’t have an obvious explanation:\n\nThere seem to be a good number of \\$50-ish rides regardless of distance. What’s going on there?\nWhat’s going on with the extreme outliers? For instance, the 50 mile ride that only cost about \\$60 or the 25 mile ride that cost about \\$140.\n\n\nfrom plotnine import aes, ggtitle, ggplot, geom_point, xlab, ylab\n\n(\n    ggplot(pivoted_prediction, aes(x=\"trip_distance\", y=\"fare\", color=\"metric\"))\n    + geom_point()\n    + xlab(\"Trip Distance\")\n    + ylab(\"Fare\")\n    + ggtitle(\"Predicted Fare vs Actual Fare by Trip Distance\")\n)\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;"
  },
  {
    "objectID": "posts/bigquery-arrays/index.html",
    "href": "posts/bigquery-arrays/index.html",
    "title": "Working with arrays in Google BigQuery",
    "section": "",
    "text": "Ibis and BigQuery have worked well together for years.\nIn Ibis 7.0.0, they work even better together with the addition of array functionality for BigQuery.\nLet’s look at some examples using BigQuery’s IMDB sample data."
  },
  {
    "objectID": "posts/bigquery-arrays/index.html#introduction",
    "href": "posts/bigquery-arrays/index.html#introduction",
    "title": "Working with arrays in Google BigQuery",
    "section": "",
    "text": "Ibis and BigQuery have worked well together for years.\nIn Ibis 7.0.0, they work even better together with the addition of array functionality for BigQuery.\nLet’s look at some examples using BigQuery’s IMDB sample data."
  },
  {
    "objectID": "posts/bigquery-arrays/index.html#basics",
    "href": "posts/bigquery-arrays/index.html#basics",
    "title": "Working with arrays in Google BigQuery",
    "section": "Basics",
    "text": "Basics\nFirst we’ll connect to BigQuery and pluck out a table to work with.\nWe’ll start with from ibis.interactive import * for maximum convenience.\n\n1from ibis.interactive import *\n\n2con = ibis.connect(\"bigquery://ibis-gbq\")\n3con.set_database(\"bigquery-public-data.imdb\")\n\n\n1\n\nfrom ibis.interactive import * imports Ibis APIs into the global namespace and enables interactive mode.\n\n2\n\nConnect to Google BigQuery. Compute (but not storage) is billed to the project you connect to–ibis-gbq in this case.\n\n3\n\nSet the database to the project and dataset that we will use for analysis.\n\n\n\n\nLet’s look at the tables in this dataset:\n\ncon.tables\n\nTables\n------\n- name_basics\n- reviews\n- title_akas\n- title_basics\n- title_crew\n- title_episode\n- title_principals\n- title_ratings\n\n\nLet’s pull out the name_basics table, which contains names and metadata about people listed on IMDB. We’ll call this ents (short for entities), and remove some columns we won’t need:\n\nents = con.tables.name_basics.drop(\"birth_year\", \"death_year\")\nents\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst    ┃ primary_name      ┃ primary_profession ┃ known_for_titles    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string            │ string             │ string              │\n├───────────┼───────────────────┼────────────────────┼─────────────────────┤\n│ nm7195872 │ Amanda Goetz      │ NULL               │ tt4529508           │\n│ nm7204100 │ Overload          │ NULL               │ tt4828308,tt4538296 │\n│ nm7206569 │ Carl Winter       │ NULL               │ NULL                │\n│ nm7208626 │ Doug Goodin       │ NULL               │ NULL                │\n│ nm7222505 │ Rickard Finndahl  │ NULL               │ tt4519546           │\n│ nm7226759 │ Kenneth Bell      │ NULL               │ tt3545908           │\n│ nm7227158 │ Savannah Gardner  │ NULL               │ tt4028790           │\n│ nm7246216 │ Elisabeth Hofmann │ NULL               │ tt4586074           │\n│ nm7253303 │ Wisda Febriyanti  │ NULL               │ tt4594232           │\n│ nm7255948 │ Charles Myers     │ NULL               │ tt2396758           │\n│ …         │ …                 │ …                  │ …                   │\n└───────────┴───────────────────┴────────────────────┴─────────────────────┘\n\n\n\n\nSplitting strings into arrays\nWe can see that known_for_titles looks sort of like an array, so let’s call the split method on that column and replace the existing column:\n\nents = ents.mutate(known_for_titles=_.known_for_titles.split(\",\"))\nents\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst    ┃ primary_name      ┃ primary_profession ┃ known_for_titles           ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string            │ string             │ array&lt;string&gt;              │\n├───────────┼───────────────────┼────────────────────┼────────────────────────────┤\n│ nm7195872 │ Amanda Goetz      │ NULL               │ ['tt4529508']              │\n│ nm7204100 │ Overload          │ NULL               │ ['tt4828308', 'tt4538296'] │\n│ nm7206569 │ Carl Winter       │ NULL               │ []                         │\n│ nm7208626 │ Doug Goodin       │ NULL               │ []                         │\n│ nm7222505 │ Rickard Finndahl  │ NULL               │ ['tt4519546']              │\n│ nm7226759 │ Kenneth Bell      │ NULL               │ ['tt3545908']              │\n│ nm7227158 │ Savannah Gardner  │ NULL               │ ['tt4028790']              │\n│ nm7246216 │ Elisabeth Hofmann │ NULL               │ ['tt4586074']              │\n│ nm7253303 │ Wisda Febriyanti  │ NULL               │ ['tt4594232']              │\n│ nm7255948 │ Charles Myers     │ NULL               │ ['tt2396758']              │\n│ …         │ …                 │ …                  │ …                          │\n└───────────┴───────────────────┴────────────────────┴────────────────────────────┘\n\n\n\nSimilarly for primary_profession, since people involved in show business often have more than one responsibility on a project:\n\nents = ents.mutate(primary_profession=_.primary_profession.split(\",\"))\n\n\n\nArray length\nLet’s see how many titles each entity is known for, and then show the five people with the largest number of titles they’re known for:\nThis is computed using the length API on array expressions:\n\n(\n    ents.select(\"primary_name\", num_titles=_.known_for_titles.length())\n    .order_by(_.num_titles.desc())\n    .limit(5)\n)\n\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ primary_name     ┃ num_titles ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string           │ int64      │\n├──────────────────┼────────────┤\n│ Sally Sun        │          5 │\n│ Matthew Kavuma   │          5 │\n│ Henry Townsend   │          5 │\n│ Alex Koenigsmark │          5 │\n│ Carrie Schnelker │          5 │\n└──────────────────┴────────────┘\n\n\n\nIt seems like the length of the known_for_titles might be capped at five!\n\n\nIndex\nWe can see the position of \"actor\" in primary_professions:\n\nents.primary_profession.index(\"actor\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayPosition(primary_profession, 'actor') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                                      │\n├────────────────────────────────────────────┤\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                         -1 │\n│                                          … │\n└────────────────────────────────────────────┘\n\n\n\nA return value of -1 indicates that \"actor\" is not present in the value:\nLet’s look for entities that are not primarily actors:\nWe can do this using the index method by checking whether the position of the string \"actor\" is greater than zero:\n\nactor_index = ents.primary_profession.index(\"actor\")\nnot_primarily_actors = actor_index &gt; 0\n1not_primarily_actors.mean()\n\n\n1\n\nThe average of a bool column gives the percentage of True values\n\n\n\n\n\n\n\n\n0.019474437073314168\n\n\n\nWho are they?\n\nents[not_primarily_actors]\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst     ┃ primary_name       ┃ primary_profession  ┃ known_for_titles                    ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ string             │ array&lt;string&gt;       │ array&lt;string&gt;                       │\n├────────────┼────────────────────┼─────────────────────┼─────────────────────────────────────┤\n│ nm2231782  │ Rene Tovar         │ ['legal', 'actor']  │ ['tt21996928']                      │\n│ nm2250015  │ Stephen Clark      │ ['legal', 'actor']  │ ['tt1452628', 'tt14372154', ... +2] │\n│ nm0352162  │ Brett Haber        │ ['legal', 'actor']  │ ['tt1720280', 'tt10928526', ... +2] │\n│ nm12169237 │ Endi Ndini         │ ['editor', 'actor'] │ ['tt4557810', 'tt14137514', ... +2] │\n│ nm14475156 │ Colby White        │ ['editor', 'actor'] │ ['tt26313337']                      │\n│ nm8979480  │ Bartosz Strusewicz │ ['editor', 'actor'] │ ['tt6685946', 'tt7334964', ... +2]  │\n│ nm3116354  │ Robert Marquis     │ ['editor', 'actor'] │ ['tt8376014', 'tt1283513']          │\n│ nm9962305  │ Vino Domi          │ ['editor', 'actor'] │ ['tt8680966', 'tt8669176']          │\n│ nm10346617 │ Lucas Oliveira     │ ['editor', 'actor'] │ ['tt9483226', 'tt7216954', ... +1]  │\n│ nm7206820  │ Prince Sethi       │ ['editor', 'actor'] │ ['tt14396686', 'tt4219300']         │\n│ …          │ …                  │ …                   │ …                                   │\n└────────────┴────────────────────┴─────────────────────┴─────────────────────────────────────┘\n\n\n\nIt’s not 100% clear whether the order of elements in primary_profession matters here.\n\n\nContainment\nWe can get people who are not actors using contains:\n\nnon_actors = ents[~ents.primary_profession.contains(\"actor\")]\nnon_actors\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ nconst     ┃ primary_name          ┃ primary_profession ┃ known_for_titles ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string     │ string                │ array&lt;string&gt;      │ array&lt;string&gt;    │\n├────────────┼───────────────────────┼────────────────────┼──────────────────┤\n│ nm13613518 │ Silvia Vannini        │ ['legal']          │ []               │\n│ nm11482673 │ Umit Yildirim         │ ['legal']          │ []               │\n│ nm14796117 │ Kendall Jackson       │ ['legal']          │ []               │\n│ nm3922637  │ Michael J. Douglas    │ ['legal']          │ []               │\n│ nm5249145  │ Christopher Addy      │ ['legal']          │ []               │\n│ nm9235293  │ Baolu Lan             │ ['legal']          │ []               │\n│ nm14560328 │ Jean Paul S Voilleque │ ['legal']          │ []               │\n│ nm11250663 │ Kelly D. Shapiro      │ ['legal']          │ []               │\n│ nm11355058 │ Sameer Oberoi         │ ['legal']          │ []               │\n│ nm8655635  │ James Madison         │ ['legal']          │ []               │\n│ …          │ …                     │ …                  │ …                │\n└────────────┴───────────────────────┴────────────────────┴──────────────────┘\n\n\n\n\n\nElement removal\nWe can remove elements from arrays too.\n\n\n\n\n\n\nremove() does not mutate the underlying data\n\n\n\n\n\n\nLet’s see who only has “actor” in the list of their primary professions:\n\nents.filter(\n    [\n        _.primary_profession.length() &gt; 0,\n        _.primary_profession.remove(\"actor\").length() == 0,\n    ]\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst    ┃ primary_name       ┃ primary_profession ┃ known_for_titles                    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string             │ array&lt;string&gt;      │ array&lt;string&gt;                       │\n├───────────┼────────────────────┼────────────────────┼─────────────────────────────────────┤\n│ nm7217990 │ Jay Isbell         │ ['actor']          │ ['tt4450682']                       │\n│ nm7218053 │ Eric Crowell       │ ['actor']          │ ['tt4500196']                       │\n│ nm7218081 │ John Wyman         │ ['actor']          │ ['tt4500196']                       │\n│ nm7223556 │ Daniel Hope        │ ['actor']          │ ['tt4558584', 'tt9089514']          │\n│ nm7223623 │ Marcus Troy        │ ['actor']          │ ['tt0120660', 'tt10914400', ... +2] │\n│ nm7241836 │ Havár Csongor      │ ['actor']          │ ['tt4580414']                       │\n│ nm7242608 │ Seigô Uetaki       │ ['actor']          │ ['tt4581192']                       │\n│ nm7245253 │ Mahmoud El Faituri │ ['actor']          │ ['tt2849138']                       │\n│ nm7254729 │ Tom Keesey         │ ['actor']          │ ['tt0924155', 'tt0924156', ... +2]  │\n│ nm7280985 │ Gabriel Garcia     │ ['actor']          │ ['tt4629714']                       │\n│ …         │ …                  │ …                  │ …                                   │\n└───────────┴────────────────────┴────────────────────┴─────────────────────────────────────┘\n\n\n\n\n\nSlicing with square-bracket syntax\nLet’s remove everyone’s first profession from the list, but only if they have more than one profession listed:\n\nents[_.primary_profession.length() &gt; 1].mutate(\n    primary_profession=_.primary_profession[1:],\n)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst     ┃ primary_name           ┃ primary_profession ┃ known_for_titles           ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ string                 │ array&lt;string&gt;      │ array&lt;string&gt;              │\n├────────────┼────────────────────────┼────────────────────┼────────────────────────────┤\n│ nm3146692  │ Keith Sutton           │ ['legal']          │ ['tt0472984']              │\n│ nm2974992  │ Dávid Farkas           │ ['legal']          │ ['tt0114301']              │\n│ nm6819046  │ Walter Batt            │ ['legal']          │ ['tt8592196']              │\n│ nm2231544  │ Don Steele             │ ['actor']          │ ['tt0818746']              │\n│ nm7267327  │ Navarro Gray           │ ['actor']          │ ['tt1718437', 'tt7945012'] │\n│ nm7783929  │ Christopher T. Connell │ ['editor']         │ ['tt7510258', 'tt5262988'] │\n│ nm6894982  │ Jason Robert Moore     │ ['editor']         │ ['tt4177962']              │\n│ nm10526935 │ Damien Mota            │ ['editor']         │ ['tt9889740']              │\n│ nm7151380  │ Chris Villa            │ ['editor']         │ ['tt4479468']              │\n│ nm7641135  │ Curt Champagne         │ ['editor']         │ ['tt5097098']              │\n│ …          │ …                      │ …                  │ …                          │\n└────────────┴────────────────────────┴────────────────────┴────────────────────────────┘"
  },
  {
    "objectID": "posts/bigquery-arrays/index.html#set-operations-and-sorting",
    "href": "posts/bigquery-arrays/index.html#set-operations-and-sorting",
    "title": "Working with arrays in Google BigQuery",
    "section": "Set operations and sorting",
    "text": "Set operations and sorting\nTreating arrays as sets is possible with the union and intersect APIs.\nLet’s take a look at intersect:\n\nIntersection\nLet’s see if we can use array intersection to figure which actors share known-for titles and sort the result:\n\nleft = ents.filter(_.known_for_titles.length() &gt; 0).limit(10_000)\nright = left.view()\nshared_titles = (\n    left\n    .join(right, left.nconst != right.nconst)\n    .select(\n        s.startswith(\"known_for_titles\"),\n        left_name=\"primary_name\",\n        right_name=\"primary_name_right\",\n    )\n    .filter(_.known_for_titles.intersect(_.known_for_titles_right).length() &gt; 0)\n    .group_by(name=\"left_name\")\n    .agg(together_with=_.right_name.collect())\n    .mutate(together_with=_.together_with.unique().sort())\n)\nshared_titles\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ name                  ┃ together_with                                       ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                │ array&lt;string&gt;                                       │\n├───────────────────────┼─────────────────────────────────────────────────────┤\n│ Chief Willie Sellars  │ ['Amy Tan', 'Bruce Williams', ... +13]              │\n│ Rainer Spanagel       │ ['Andra Arnicane', 'Aziz Sheikh', ... +9]           │\n│ Sam Cooper            │ ['Amy Tan', 'Bruce Williams', ... +10]              │\n│ Michelle Porter       │ ['Amy Tan', 'Bruce Williams', ... +8]               │\n│ Bryan Carter          │ ['Andrew Bower', 'Austin Murtha', ... +11]          │\n│ Timothy Johnson       │ ['Betsy Blaney', 'Brendan Halko', ... +19]          │\n│ Colin McLean          │ ['Alison Garnham', 'Ashley Fox', ... +20]           │\n│ Jessica Williams      │ ['Austin Williams']                                 │\n│ Leighann Falcon       │ ['Aaron Green', 'Alejandro Garza y Garza', ... +71] │\n│ Oldham Tuneless Choir │ ['Alex Matvienko', 'Andy McDonald', ... +38]        │\n│ …                     │ …                                                   │\n└───────────────────────┴─────────────────────────────────────────────────────┘"
  },
  {
    "objectID": "posts/bigquery-arrays/index.html#advanced-operations",
    "href": "posts/bigquery-arrays/index.html#advanced-operations",
    "title": "Working with arrays in Google BigQuery",
    "section": "Advanced operations",
    "text": "Advanced operations\n\nFlatten arrays into rows\nThanks to the tireless efforts of the folks working on sqlglot, as of version 7.0.0 Ibis supports unnest for BigQuery!\nYou can use it standalone on a column expression:\n\nents.primary_profession.unnest()\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ primary_profession ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ string             │\n├────────────────────┤\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ actor              │\n│ …                  │\n└────────────────────┘\n\n\n\nYou can also use it in select/mutate calls to expand the table accordingly:\n\nents.mutate(primary_profession=_.primary_profession.unnest())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ nconst    ┃ primary_name       ┃ primary_profession ┃ known_for_titles ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string    │ string             │ string             │ array&lt;string&gt;    │\n├───────────┼────────────────────┼────────────────────┼──────────────────┤\n│ nm7211030 │ Josh Berry         │ actor              │ ['tt4046896']    │\n│ nm7211205 │ Alan Douglas       │ actor              │ ['tt0038449']    │\n│ nm7213536 │ Wilson Recalde     │ actor              │ ['tt2333598']    │\n│ nm7214355 │ Julian Owen        │ actor              │ ['tt3488298']    │\n│ nm7215983 │ Zach Ladouceur     │ actor              │ ['tt4546288']    │\n│ nm7221941 │ Alain Milani       │ actor              │ ['tt4548654']    │\n│ nm7225536 │ Gary Flanzer       │ actor              │ ['tt4521030']    │\n│ nm7236543 │ Bastiaan Schreuder │ actor              │ ['tt4506254']    │\n│ nm7241255 │ Jared Young        │ actor              │ ['tt4579992']    │\n│ nm7241835 │ Fagyas Alex        │ actor              │ ['tt4580414']    │\n│ …         │ …                  │ …                  │ …                │\n└───────────┴────────────────────┴────────────────────┴──────────────────┘\n\n\n\nUnnesting can be useful when joining nested data.\nHere we use unnest to find people known for any of the godfather movies:\n\n1basics = con.tables.title_basics.filter(\n    [\n        _.title_type == \"movie\",\n        _.original_title.lower().startswith(\"the godfather\"),\n        _.genres.lower().contains(\"crime\"),\n    ]\n)\n\nknown_for_the_godfather = (\n2    ents.mutate(tconst=_.known_for_titles.unnest())\n3    .join(basics, \"tconst\")\n4    .select(\"primary_title\", \"primary_name\")\n    .distinct()\n    .order_by([\"primary_title\", \"primary_name\"])\n)\nknown_for_the_godfather\n\n\n1\n\nFilter the title_basics data set to only the Godfather movies\n\n2\n\nUnnest the known_for_titles array column\n\n3\n\nJoin with basics to get movie titles\n\n4\n\nEnsure that each entity is only listed once and sort the results\n\n\n\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ primary_title ┃ primary_name        ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ string        │ string              │\n├───────────────┼─────────────────────┤\n│ The Godfather │ A. Emmett Adams     │\n│ The Godfather │ Abe Vigoda          │\n│ The Godfather │ Al Lettieri         │\n│ The Godfather │ Al Martino          │\n│ The Godfather │ Al Pacino           │\n│ The Godfather │ Albert S. Ruddy     │\n│ The Godfather │ Alex Rocco          │\n│ The Godfather │ Andrea Eastman      │\n│ The Godfather │ Angelo Infanti      │\n│ The Godfather │ Anna Hill Johnstone │\n│ …             │ …                   │\n└───────────────┴─────────────────────┘\n\n\n\nLet’s summarize by showing how many people are known for each Godfather movie:\n\nknown_for_the_godfather.primary_title.value_counts()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┓\n┃ primary_title          ┃ primary_title_count ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━┩\n│ string                 │ int64               │\n├────────────────────────┼─────────────────────┤\n│ The Godfather Part III │                 196 │\n│ The Godfather Part II  │                 117 │\n│ The Godfather          │                  96 │\n└────────────────────────┴─────────────────────┘\n\n\n\n\n\nFiltering array elements\nFiltering array elements can be done with the filter method, which applies a predicate to each array element and returns an array of elements for which the predicate returns True.\nThis method is similar to Python’s filter function.\nLet’s show all people who are neither editors nor actors:\n\nents.mutate(\n1    primary_profession=_.primary_profession.filter(\n        lambda pp: ~pp.isin((\"actor\", \"editor\"))\n    )\n2).filter(_.primary_profession.length() &gt; 0)\n\n\n1\n\nThis filter call is applied to each array element\n\n2\n\nThis filter call is applied to the table\n\n\n\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ nconst     ┃ primary_name       ┃ primary_profession ┃ known_for_titles ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string     │ string             │ array&lt;string&gt;      │ array&lt;string&gt;    │\n├────────────┼────────────────────┼────────────────────┼──────────────────┤\n│ nm14701100 │ Karin Roach        │ ['legal']          │ []               │\n│ nm3709802  │ Kristin L. Holland │ ['legal']          │ []               │\n│ nm13336378 │ Rok Salazar        │ ['legal']          │ []               │\n│ nm7514782  │ Christopher Spicer │ ['legal']          │ []               │\n│ nm11531194 │ J Manuel           │ ['legal']          │ []               │\n│ nm9114713  │ Huy Nguyen         │ ['legal']          │ []               │\n│ nm2230248  │ Jeffrey Galen      │ ['legal']          │ []               │\n│ nm8479496  │ Fatima Amgane      │ ['legal']          │ []               │\n│ nm2229345  │ Harold Brown       │ ['legal']          │ []               │\n│ nm7383201  │ Ashley Silver      │ ['legal']          │ []               │\n│ …          │ …                  │ …                  │ …                │\n└────────────┴────────────────────┴────────────────────┴──────────────────┘\n\n\n\n\n\nApplying a function to array elements\nYou can apply a function to run an ibis expression on each element of an array using the map method.\nLet’s normalize the case of primary_profession to upper case:\n\nents.mutate(\n    primary_profession=_.primary_profession.map(lambda pp: pp.upper())\n).filter(_.primary_profession.length() &gt; 0)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ nconst    ┃ primary_name      ┃ primary_profession ┃ known_for_titles                    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string            │ array&lt;string&gt;      │ array&lt;string&gt;                       │\n├───────────┼───────────────────┼────────────────────┼─────────────────────────────────────┤\n│ nm7199328 │ Renzo Castro      │ ['ACTOR']          │ ['tt4623856', 'tt4494580', ... +1]  │\n│ nm7199362 │ Pankaj            │ ['ACTOR']          │ ['tt4367318']                       │\n│ nm7200119 │ Thibault Péan     │ ['ACTOR']          │ ['tt4534250']                       │\n│ nm7203213 │ Tim Goodman       │ ['ACTOR']          │ ['tt2234701']                       │\n│ nm7207130 │ Ruupertti Arponen │ ['ACTOR']          │ ['tt0185819', 'tt10628202']         │\n│ nm7223822 │ Federico Carghini │ ['ACTOR']          │ ['tt1556087']                       │\n│ nm7232704 │ Ned Jackson       │ ['ACTOR']          │ ['tt0407361', 'tt20453990']         │\n│ nm7238980 │ Rasmus Cassanelli │ ['ACTOR']          │ ['tt0782510']                       │\n│ nm7240979 │ Changyuan Zhou    │ ['ACTOR']          │ ['tt0311913']                       │\n│ nm7242332 │ James Dolbeare    │ ['ACTOR']          │ ['tt10151048', 'tt5769738', ... +2] │\n│ …         │ …                 │ …                  │ …                                   │\n└───────────┴───────────────────┴────────────────────┴─────────────────────────────────────┘"
  },
  {
    "objectID": "posts/bigquery-arrays/index.html#conclusion",
    "href": "posts/bigquery-arrays/index.html#conclusion",
    "title": "Working with arrays in Google BigQuery",
    "section": "Conclusion",
    "text": "Conclusion\nIbis has a sizable collection of array APIs that work with many different backends and as of version 7.0.0, Ibis supports a much larger set of those APIs for BigQuery!\nCheck out the API documentation for the full set of available methods.\nTry it out, and let us know what you think."
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html",
    "href": "posts/ibis_substrait_to_duckdb/index.html",
    "title": "Ibis + Substrait + DuckDB",
    "section": "",
    "text": "Ibis strives to provide a consistent interface for interacting with a multitude of different analytical execution engines, most of which (but not all) speak some dialect of SQL.\nToday, Ibis accomplishes this with a lot of help from sqlalchemy and sqlglot to handle differences in dialect, or we interact directly with available Python bindings (for instance with the pandas, datafusion, and polars backends).\nIbis goes to great lengths to generate sane and consistent SQL for those backends that use it. We are also interested in exploring other means of communicating consistently with those backends.\nSubstrait is a new cross-language serialization format for communicating (among other things) query plans. It’s still in its early days, but there is already nascent support for Substrait in Apache Arrow, DuckDB, and Velox.\nIbis supports producing Substrait plans from Ibis table expressions, with the help of the ibis-substrait library. Let’s take a quick peek at how we might use it for query execution."
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#getting-started",
    "href": "posts/ibis_substrait_to_duckdb/index.html#getting-started",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Getting started",
    "text": "Getting started\nFirst, we can create a conda environment using the latest versions of duckdb, ibis, and ibis_substrait.\nmamba create -n ibis_substrait_duckdb ibis-framework==4.1 ibis-substrait==2.19 ipython python-duckdb parsy==2\nNext, we’ll need to choose a dataset. For this example, we’ll use data from IMDB, available through their dataset portal.\nFor convenience, I used Ready, Set, Data! to grab the data in parquet format and then insert it into a DuckDB database.\nimport duckdb\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\ncon.execute(\n    \"CREATE TABLE ratings AS SELECT * FROM '/home/gil/data/imdb/imdb_ratings.parquet'\"\n)\ncon.execute(\n    \"CREATE TABLE basics AS SELECT * FROM '/home/gil/data/imdb/imdb_basics.parquet'\"\n)"
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#query-creation",
    "href": "posts/ibis_substrait_to_duckdb/index.html#query-creation",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Query Creation",
    "text": "Query Creation\nFor our example, we’ll build up a query using Ibis but without connecting to our execution engine (DuckDB). Once we have an Ibis table expression, we’ll create a Substrait plan, then execute that plan directly on DuckDB to get results.\nTo do this, all we need is some knowledge of the schema of the tables we want to interact with. We might get these schema from a metadata store, or possibly a coworker, or a friendly mouse.\nHowever we arrive at it, if we know the column names and the datatypes, we can build up a query in Ibis, so let’s do that.\nimport ibis\nfrom ibis import _\n\nratings = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"averageRating\", \"str\"),\n        (\"numVotes\", \"str\"),\n    ],\n    name=\"ratings\",\n)\n\nbasics = ibis.table(\n    [\n        (\"tconst\", \"str\"),\n        (\"titleType\", \"str\"),\n        (\"primaryTitle\", \"str\"),\n        (\"originalTitle\", \"str\"),\n        (\"isAdult\", \"str\"),\n        (\"startYear\", \"str\"),\n        (\"endYear\", \"str\"),\n        (\"runtimeMinutes\", \"str\"),\n        (\"genres\", \"str\"),\n    ],\n    name=\"basics\",\n)\nNow that those tables are represented in Ibis, we can start creating our query. We’ll try to recreate the top-ten movies on the IMDB leaderboard. For that, we’ll need movie titles and their respective ratings.\nWe know that the data we have for ratings looks something like the following:\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ tconst    ┃ averageRating ┃ numVotes ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string        │ string   │\n├───────────┼───────────────┼──────────┤\n│ tt0000001 │ 5.7           │ 1919\\n   │\n│ tt0000002 │ 5.8           │ 260\\n    │\n│ tt0000003 │ 6.5           │ 1726\\n   │\n│ tt0000004 │ 5.6           │ 173\\n    │\n│ tt0000005 │ 6.2           │ 2541\\n   │\n└───────────┴───────────────┴──────────┘\nBased on the column names alone, averageRating is almost certainly supposed to be a float, and numVotes should be an integer. We can cast those so we can make useful comparisons between ratings and vote numbers.\nratings = ratings.select(\n    ratings.tconst,\n    avg_rating=ratings.averageRating.cast(\"float\"),\n    num_votes=ratings.numVotes.cast(\"int\"),\n)\nThe first few rows of basics looks like this:\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ tconst    ┃ titleType ┃ primaryTitle           ┃ originalTitle          ┃ isAdult ┃ startYear ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string    │ string    │ string                 │ string                 │ string  │ string    │ … │\n├───────────┼───────────┼────────────────────────┼────────────────────────┼─────────┼───────────┼───┤\n│ tt0000001 │ short     │ Carmencita             │ Carmencita             │ 0       │ 1894      │ … │\n│ tt0000002 │ short     │ Le clown et ses chiens │ Le clown et ses chiens │ 0       │ 1892      │ … │\n│ tt0000003 │ short     │ Pauvre Pierrot         │ Pauvre Pierrot         │ 0       │ 1892      │ … │\n│ tt0000004 │ short     │ Un bon bock            │ Un bon bock            │ 0       │ 1892      │ … │\n│ tt0000005 │ short     │ Blacksmith Scene       │ Blacksmith Scene       │ 0       │ 1893      │ … │\n└───────────┴───────────┴────────────────────────┴────────────────────────┴─────────┴───────────┴───┘\nIn the interest of keeping things family-friendly, we can filter out any adult films. We can filter out any IMDB titles that aren’t movies, then select out the columns tconst and primaryTitle. And we’ll include startYear just in case it’s interesting.\nbasics = basics.filter([basics.titleType == \"movie\", basics.isAdult == \"0\"]).select(\n    \"tconst\",\n    \"primaryTitle\",\n    \"startYear\",\n)\nWith the data (lightly) cleaned up, we can construct our query for top films. We want to join the two tables ratings and basics. Then we’ll order them by avg_rating and num_votes, and include an additional filter that the movie has to have at least 200,000 votes.\ntopfilms = (\n    ratings.join(basics, \"tconst\")\n    .order_by([_.avg_rating.desc(), _.num_votes.desc()])\n    .filter(_.num_votes &gt; 200_000)\n    .limit(10)\n)\nNow that we have an Ibis table expression, it’s time for Substrait to enter the scene."
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#substrait-serialization",
    "href": "posts/ibis_substrait_to_duckdb/index.html#substrait-serialization",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Substrait Serialization",
    "text": "Substrait Serialization\nWe’re going to import ibis_substrait and compile the topfilms table expression into a Substrait plan.\nfrom ibis_substrait.compiler.core import SubstraitCompiler\n\ncompiler = SubstraitCompiler()\n\nplan = compiler.compile(topfilms)\n\n# type(plan) --&gt; &lt;class 'substrait.ibis.plan_pb2.Plan'&gt;\nSubstrait is built using protobuf. If you look at the repr for plan, you’ll see a LOOOONG JSON-ish representation of the Substrait plan. This representation is not really meant for human eyes.\nWe’ll serialize the Substrait plan to disk and open it up in a separate session, or on another machine, entirely. That’s one of the notions of Substrait: plans can be serialized and shuttled around between various systems. It’s similar to Ibis in that it allows a separation of plan creation from plan execution.\nwith open(\"topfilms.proto\", \"wb\") as f:\n    f.write(plan.SerializeToString())"
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#substrait-plan-execution",
    "href": "posts/ibis_substrait_to_duckdb/index.html#substrait-plan-execution",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Substrait Plan Execution",
    "text": "Substrait Plan Execution\nNow we can open up the serialized Substrait plan in a new session where we execute it using DuckDB directly. One important point to note here is that our plan refers to two tables, named basics and ratings. If those tables don’t exist in our execution engine, then this isn’t going to work.\nimport duckdb\n\ncon = duckdb.connect(\"/home/gil/imdb.ddb\")\n\ncon.execute(\"PRAGMA show_tables;\").fetchall()\n\n\n\n\n\n\n\nbasics\n\n\n\n\nratings\n\n\n\n\nLuckily, they do exist! Let’s install and load the DuckDB Substrait extension, then execute the Substrait plan, and finally grab our results.\ncon.install_extension(\"substrait\")\ncon.load_extension(\"substrait\")\n\nwith open(\"topfilms.proto\", \"rb\") as f:\n    plan_blob = f.read()\n\nresult = con.from_substrait(plan_blob)\n\nresult.fetchall()\n\n\n\n\n\n\n\n\n\n\n\ntt0111161\n\n\n9.3\n\n\n2651547\n\n\nThe Shawshank Redemption\n\n\n1994\n\n\n\n\ntt0068646\n\n\n9.2\n\n\n1838044\n\n\nThe Godfather\n\n\n1972\n\n\n\n\ntt0468569\n\n\n9.0\n\n\n2623735\n\n\nThe Dark Knight\n\n\n2008\n\n\n\n\ntt0167260\n\n\n9.0\n\n\n1827464\n\n\nThe Lord of the Rings: The Return of the King\n\n\n2003\n\n\n\n\ntt0108052\n\n\n9.0\n\n\n1343647\n\n\nSchindler’s List\n\n\n1993\n\n\n\n\ntt0071562\n\n\n9.0\n\n\n1259465\n\n\nThe Godfather Part II\n\n\n1974\n\n\n\n\ntt0050083\n\n\n9.0\n\n\n782903\n\n\n12 Angry Men\n\n\n1957\n\n\n\n\ntt0110912\n\n\n8.9\n\n\n2029684\n\n\nPulp Fiction\n\n\n1994\n\n\n\n\ntt1375666\n\n\n8.8\n\n\n2325417\n\n\nInception\n\n\n2010\n\n\n\n\ntt0137523\n\n\n8.8\n\n\n2096752\n\n\nFight Club\n\n\n1999\n\n\n\n\nThat looks about right to me. There may be some small differences with the current Top 10 list on IMDB if our data are a little stale.\nIt’s early days still for Substrait, but it’s exciting to see how far it’s come in the last 18 months!"
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#why-wouldnt-i-just-use-sql-for-this",
    "href": "posts/ibis_substrait_to_duckdb/index.html#why-wouldnt-i-just-use-sql-for-this",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Why wouldn’t I just use SQL for this?",
    "text": "Why wouldn’t I just use SQL for this?\nIt’s a fair question. SQL is everywhere, after all.\nThere are a few reasons we think you shouldn’t ignore Substrait.\n\nStandards\nSQL has a standard, but how closely do engines follow the standard? In our experience, queries don’t translate well between engines (this is one reason Ibis exists!)\n\n\nExtensibility\nSubstrait is more extensible than SQL. Some DBMS have added in some very cool features, but it usually involves diverging (sometimes widely) from the SQL standard. Substrait has an extension system that allows plan producers and plan consumers to agree on a well-typed and well-defined interaction that exists outside of the core Substrait specification.\n\n\nSerialization and parsing\nParsing SQL can be a big pain (trust us). If you send a big string over the wire, you need the engine on the other side to have a SQL parser to understand what the message is. Now, obviously, SQL engines have those. But here, again, standards (or lack of adherence to standards) can bite you. Extensibility is also difficult here, because now the SQL parser needs to understand some new custom syntax.\nProtobuf is hardly a dream to work with, but it’s a lot easier to consistently define behavior AND to validate that behavior is correct. It’s also smaller than raw text."
  },
  {
    "objectID": "posts/ibis_substrait_to_duckdb/index.html#wrapping-up",
    "href": "posts/ibis_substrait_to_duckdb/index.html#wrapping-up",
    "title": "Ibis + Substrait + DuckDB",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThat’s all for now! To quickly summarize:\nSubstrait is a new standard for representing relational algebra queries with support in Apache Arrow, DuckDB, Velox, and more (and more to come!).\nIbis can now generate substrait instead of string SQL, letting it take advantage of this new standard.\nInterested in substrait or ibis? Docs are available at\n\nSubstrait\nIbis Docs\n\nand the relevant GitHub repos are\n\nSubstrait GitHub\nIbis Substrait GitHub\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "posts/Ibis-version-3.0.0-release/index.html",
    "href": "posts/Ibis-version-3.0.0-release/index.html",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "The latest version of Ibis, version 3.0.0, has just been released! This post highlights some of the new features, breaking changes, and performance improvements that come with the new release. 3.0.0 is a major release and includes more changes than those listed in this post. A full list of the changes can be found in the project release notes here.\n\n\nAligned to the roadmap and in response to the community’s requests, Ibis 3.0.0 introduces many new features and functionality.\n\nNow query an Ibis table using inline SQL\nNEW DuckDB backend\nExplore the NEW backend support matrix tool\nImproved support for arrays and tuples in ClickHouse\nSuffixes now supported in join API expressions\nAPIs for creating timestamps and dates from component fields\nPretty printing in ipython/ notebooks\n\nRefer to the sections below for more detail on each new feature.\n\n\nThe most exciting feature of this release is inline SQL! Many data scientists or developers may be familiar with both Python and SQL. However there may be some queries, transformations that they feel comfortable doing in SQL instead of Python. In the updated version of Ibis users can query an Ibis table using SQL! The new .sql method allows users to mix SQL strings with ibis expressions as well as query ibis table expressions in SQL strings.\nThis functionality currently works for the following backends:\n\nPostgreSQL\nDuckDB\nPySpark\nMySQL\n\nIf you’re interested in adding .sql support for other backends please open an issue.\n\n\n\nIbis now supports DuckDB as a backend. DuckDB is a high-performance SQL OLAP database management system. It is designed to be fast, reliable and easy to use and can be embedded. Many Ibis use cases start from getting tables from a single-node backend so directly supporting DuckDB offers a lot of value. As mentioned earlier, the DuckDB backend allows for the new .sql method on tables for mixing sql and Ibis expressions.\n\n\n\nAs the number of backends Ibis supports grows, it can be challenging for users to decide which one best fits their needs. One way to make a more informed decision is for users to find the backend that supports the operations they intend to use. The 3.0.0 release comes with a backend support matrix that allows users to do just that. A screenshot of part of the matrix can be seen below and the full version can be found here.\nIn addition to this users can now call ibis.${backend}.has_operation to find out if a specific operation is supported by a backend.\n\n\n\nbackend support matrix\n\n\n\n\n\nThe 3.0.0 release includes a slew of important improvements for the ClickHouse backend. Most prominently ibis now supports ClickHouse arrays and tuples. Some of the related operations that have been implemented are:\n\nArrayIndex\nArrayConcat\nArrayRepeat\nArraySlice\n\nOther additional operations now supported for the clickhouse backend are string concat, string slicing, table union, trim, pad and string predicates (LIKE and ILIKE) and all remaining joins.\n\n\n\nIn previous versions Ibis’ join API did not accept suffixes as a parameter, leaving backends to either use some default value or raise an error at execution time when column names overlapped. In 3.0.0 suffixes are now directly supported in the join API itself. Along with the removal of materialize, ibis now automatically adds a default suffix to any overlapping column names.\n\n\n\nIt is now possible to create timestamps directly from component fields. This is now possible using the new method ibis.date(y,m,d). A user can pass in a year, month and day and the result is a datetime object. That is we can assert for example that ibis.date (2022, 2, 4).type() == dt.date\n\n\n\nFor users that use jupyter notebooks, repr_html has been added for expressions to enable pretty printing tables in the notebook. This is currently only available for interactive mode (currently delegating to pandas implementation) and should help notebooks become more readable. An example of what this looks like can be seen below.\n\n\n\npretty print repr\n\n\n\n\n\n\n3.0.0 is a major release and according to the project’s use of semantic versioning, breaking changes are on the table. The full list of these changes can be found here. Some of the important changes include:\n\nPython 3.8 is now the minimum supported version\nDeprecation of .materialize()\n\nRefer to the sections below for more detail on these changes.\n\n\nIbis currently follows NEP 29, a community policy standard that recommends Python and Numpy versions to support. NEP 29 suggests that all projects across the Scientific Python ecosystem adopt a common “time window-based” policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning. As part of the 3.0.0 release, support for Python 3.7 has been dropped and the project has now adopted support for version 3.8 and higher.\n\n\n\nThis release sees the deprecation of the .materialize() method from TableExpr. In the past, the materialize method has caused a lot of confusion. Doing simple things like t.join(s, t.foo == s.foo).select([\"unambiguous_column\"]) raised an exception because of it. It turns out that .materialize() isn’t necessary. The materialize method still exists, but is now a no-op and doesn’t need to be used.\n\n\n\n\nThe following changes to the Ibis codebase have resulted in performance improvements.\n\nSpeeding up __str__ and __hash__ datatypes\nCreating a fast path for simple column selection (pandas/dask backends)\nGlobal equality cache\nRemoving full tree repr from rule validator error message\nSpeed up attribute access\nUsing assign instead of concat in projections when possible (pandas/dask backends)\n\nAdditionally, all TPC-H suite queries can be represented in Ibis. All queries are ready-to-run, using the default substitution parameters as specified by the TPC-H spec. Queries have been added here.\n\n\n\nIn summary, the 3.0.0 release includes a number of new features including the ability to query an Ibis table using inline SQL, a DuckDB backend, a backend support matrix tool, support for arrays and tuples, suffixes in joins, timestamps from component fields and prettier tables in ipython. Some breaking changes to take note of are the removal of .materialize() and the switch to Python 3.8 as the minimum supported version. A wide range of changes to the code has also led to significant speed ups in 3.0.0 as well.\nIbis is a community led, open source project. If you’d like to contribute to the project check out the contribution guide here. If you run into a problem and would like to submit an issue you can do so through Ibis’ Github repository. Finally, Ibis relies on community support to grow and to become successful! You can help promote Ibis by following and sharing the project on Twitter, starring the repo or contributing to the code. Ibis continues to improve with every release. Keep an eye on the blog for updates on the next one!"
  },
  {
    "objectID": "posts/Ibis-version-3.0.0-release/index.html#new-features",
    "href": "posts/Ibis-version-3.0.0-release/index.html#new-features",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "Aligned to the roadmap and in response to the community’s requests, Ibis 3.0.0 introduces many new features and functionality.\n\nNow query an Ibis table using inline SQL\nNEW DuckDB backend\nExplore the NEW backend support matrix tool\nImproved support for arrays and tuples in ClickHouse\nSuffixes now supported in join API expressions\nAPIs for creating timestamps and dates from component fields\nPretty printing in ipython/ notebooks\n\nRefer to the sections below for more detail on each new feature.\n\n\nThe most exciting feature of this release is inline SQL! Many data scientists or developers may be familiar with both Python and SQL. However there may be some queries, transformations that they feel comfortable doing in SQL instead of Python. In the updated version of Ibis users can query an Ibis table using SQL! The new .sql method allows users to mix SQL strings with ibis expressions as well as query ibis table expressions in SQL strings.\nThis functionality currently works for the following backends:\n\nPostgreSQL\nDuckDB\nPySpark\nMySQL\n\nIf you’re interested in adding .sql support for other backends please open an issue.\n\n\n\nIbis now supports DuckDB as a backend. DuckDB is a high-performance SQL OLAP database management system. It is designed to be fast, reliable and easy to use and can be embedded. Many Ibis use cases start from getting tables from a single-node backend so directly supporting DuckDB offers a lot of value. As mentioned earlier, the DuckDB backend allows for the new .sql method on tables for mixing sql and Ibis expressions.\n\n\n\nAs the number of backends Ibis supports grows, it can be challenging for users to decide which one best fits their needs. One way to make a more informed decision is for users to find the backend that supports the operations they intend to use. The 3.0.0 release comes with a backend support matrix that allows users to do just that. A screenshot of part of the matrix can be seen below and the full version can be found here.\nIn addition to this users can now call ibis.${backend}.has_operation to find out if a specific operation is supported by a backend.\n\n\n\nbackend support matrix\n\n\n\n\n\nThe 3.0.0 release includes a slew of important improvements for the ClickHouse backend. Most prominently ibis now supports ClickHouse arrays and tuples. Some of the related operations that have been implemented are:\n\nArrayIndex\nArrayConcat\nArrayRepeat\nArraySlice\n\nOther additional operations now supported for the clickhouse backend are string concat, string slicing, table union, trim, pad and string predicates (LIKE and ILIKE) and all remaining joins.\n\n\n\nIn previous versions Ibis’ join API did not accept suffixes as a parameter, leaving backends to either use some default value or raise an error at execution time when column names overlapped. In 3.0.0 suffixes are now directly supported in the join API itself. Along with the removal of materialize, ibis now automatically adds a default suffix to any overlapping column names.\n\n\n\nIt is now possible to create timestamps directly from component fields. This is now possible using the new method ibis.date(y,m,d). A user can pass in a year, month and day and the result is a datetime object. That is we can assert for example that ibis.date (2022, 2, 4).type() == dt.date\n\n\n\nFor users that use jupyter notebooks, repr_html has been added for expressions to enable pretty printing tables in the notebook. This is currently only available for interactive mode (currently delegating to pandas implementation) and should help notebooks become more readable. An example of what this looks like can be seen below.\n\n\n\npretty print repr"
  },
  {
    "objectID": "posts/Ibis-version-3.0.0-release/index.html#other-changes",
    "href": "posts/Ibis-version-3.0.0-release/index.html#other-changes",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "3.0.0 is a major release and according to the project’s use of semantic versioning, breaking changes are on the table. The full list of these changes can be found here. Some of the important changes include:\n\nPython 3.8 is now the minimum supported version\nDeprecation of .materialize()\n\nRefer to the sections below for more detail on these changes.\n\n\nIbis currently follows NEP 29, a community policy standard that recommends Python and Numpy versions to support. NEP 29 suggests that all projects across the Scientific Python ecosystem adopt a common “time window-based” policy for support of Python and NumPy versions. Standardizing a recommendation for project support of minimum Python and NumPy versions will improve downstream project planning. As part of the 3.0.0 release, support for Python 3.7 has been dropped and the project has now adopted support for version 3.8 and higher.\n\n\n\nThis release sees the deprecation of the .materialize() method from TableExpr. In the past, the materialize method has caused a lot of confusion. Doing simple things like t.join(s, t.foo == s.foo).select([\"unambiguous_column\"]) raised an exception because of it. It turns out that .materialize() isn’t necessary. The materialize method still exists, but is now a no-op and doesn’t need to be used."
  },
  {
    "objectID": "posts/Ibis-version-3.0.0-release/index.html#performance-improvements",
    "href": "posts/Ibis-version-3.0.0-release/index.html#performance-improvements",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "The following changes to the Ibis codebase have resulted in performance improvements.\n\nSpeeding up __str__ and __hash__ datatypes\nCreating a fast path for simple column selection (pandas/dask backends)\nGlobal equality cache\nRemoving full tree repr from rule validator error message\nSpeed up attribute access\nUsing assign instead of concat in projections when possible (pandas/dask backends)\n\nAdditionally, all TPC-H suite queries can be represented in Ibis. All queries are ready-to-run, using the default substitution parameters as specified by the TPC-H spec. Queries have been added here."
  },
  {
    "objectID": "posts/Ibis-version-3.0.0-release/index.html#conclusion",
    "href": "posts/Ibis-version-3.0.0-release/index.html#conclusion",
    "title": "Ibis v3.0.0",
    "section": "",
    "text": "In summary, the 3.0.0 release includes a number of new features including the ability to query an Ibis table using inline SQL, a DuckDB backend, a backend support matrix tool, support for arrays and tuples, suffixes in joins, timestamps from component fields and prettier tables in ipython. Some breaking changes to take note of are the removal of .materialize() and the switch to Python 3.8 as the minimum supported version. A wide range of changes to the code has also led to significant speed ups in 3.0.0 as well.\nIbis is a community led, open source project. If you’d like to contribute to the project check out the contribution guide here. If you run into a problem and would like to submit an issue you can do so through Ibis’ Github repository. Finally, Ibis relies on community support to grow and to become successful! You can help promote Ibis by following and sharing the project on Twitter, starring the repo or contributing to the code. Ibis continues to improve with every release. Keep an eye on the blog for updates on the next one!"
  },
  {
    "objectID": "posts/ffill-and-bfill-using-ibis/index.html",
    "href": "posts/ffill-and-bfill-using-ibis/index.html",
    "title": "ffill and bfill using Ibis",
    "section": "",
    "text": "Suppose you have a table of data mapping events and dates to values, and that this data contains gaps in values.\nSuppose you want to forward fill these gaps such that, one-by-one, if a value is null, it is replaced by the non-null value preceding.\nFor example, you might be measuring the total value of an account over time. Saving the same value until that value changes is an inefficient use of space, so you might only measure the value during certain events, like a change in ownership or value.\nIn that case, to view the value of the account by day, you might want to interpolate dates and then ffill or bfill value to show the account value over time by date.\nDate interpolation will be covered in a different guide, but if you already have the dates then you can fill in some values.\nThis was heavily inspired by Gil Forsyth’s writeup on ffill and bfill on the Ibis GitHub Wiki.\n\nSetup\nFirst, we want to make some mock data. To demonstrate this technique in a non-pandas backend, we will use the DuckDB backend.\nOur data will have measurements by date, and these measurements will be grouped by an event id. We will then save this data to data.parquet so we can register that parquet file as a table in our DuckDB connector.\n\nfrom datetime import date\n\nimport numpy as np\nimport pandas as pd\n\nimport ibis\n\n\ndf = pd.DataFrame(\n    {\n        \"event_id\": [0] * 2 + [1] * 3 + [2] * 5 + [3] * 2,\n        \"measured_on\": map(\n            date,\n            [2021] * 12, [6] * 4 + [5] * 6 + [7] * 2,\n            range(1, 13),\n        ),\n        \"measurement\": np.nan,\n    }\n)\n\ndf.head()\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\n\n\n\n\n0\n0\n2021-06-01\nNaN\n\n\n1\n0\n2021-06-02\nNaN\n\n\n2\n1\n2021-06-03\nNaN\n\n\n3\n1\n2021-06-04\nNaN\n\n\n4\n1\n2021-05-05\nNaN\n\n\n\n\n\n\n\n\ndf.loc[[1, 4, 5, 7], \"measurement\"] = [5.0, 42.0, 42.0, 11.0]\ndf\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\n\n\n\n\n0\n0\n2021-06-01\nNaN\n\n\n1\n0\n2021-06-02\n5.0\n\n\n2\n1\n2021-06-03\nNaN\n\n\n3\n1\n2021-06-04\nNaN\n\n\n4\n1\n2021-05-05\n42.0\n\n\n5\n2\n2021-05-06\n42.0\n\n\n6\n2\n2021-05-07\nNaN\n\n\n7\n2\n2021-05-08\n11.0\n\n\n8\n2\n2021-05-09\nNaN\n\n\n9\n2\n2021-05-10\nNaN\n\n\n10\n3\n2021-07-11\nNaN\n\n\n11\n3\n2021-07-12\nNaN\n\n\n\n\n\n\n\nLet’s write that to a parquet file:\n\ndf.to_parquet(\"data.parquet\")\n\nTo use the DuckDB backend with our data, we will spin up a DuckDB connection and then register data.parquet as data:\n\nconn = ibis.connect('duckdb://')\n\nconn.register('data.parquet', table_name='data')\n\ndata = conn.table(\"data\")\n\ndata\n\nDatabaseTable: data\n  event_id    int64\n  measured_on date\n  measurement float64\n\n\n\n\n\nffill Strategy\nTo better understand how we can forward-fill our gaps, let’s take a minute to explain the strategy and then look at the manual result.\nWe will partition our data by event groups and then sort those groups by date.\nOur logic for forward fill is then: let j be an event group sorted by date and let i be a date within j. If i is the first date in j, then continue. If i is not the first date in j, then if measurement in i is null then replace it with measurement for i-1. Otherwise, do nothing.\nLet’s take a look at what this means for the first few rows of our data:\n    event_id measured_on  measurement\n0          0  2021-06-01          NaN # Since this is the first row of the event group (group 0), do nothing\n1          0  2021-06-02          5.0 # Since this is not the first row of the group and is not null: do nothing\n4          1  2021-05-05         42.0 # This is the first row of the event group (group 1): do nothing\n2          1  2021-06-03          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n3          1  2021-06-04          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n5          2  2021-05-06         42.0 # This is the first row of the event group (group 2): do nothing\n6          2  2021-05-07          NaN # This is not the first row and is null: replace it (NaN → 42.0)\n7          2  2021-05-08         11.0 # This is not the first row and is not null: do nothing\n8          2  2021-05-09          NaN # This is not the first row and is null: replace it (NaN → 11.0)\n9          2  2021-05-10          NaN # This is not the first row and is null: replace it (NaN → 11.0)\n10         3  2021-07-11          NaN # This is the first row of the event group (group 3): do nothing\n11         3  2021-07-12          NaN # This is not the first row and is null: replace it (NaN → NaN)\nOur result should for forward fill should look like this:\n\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\n\n\n\n\n0\n0\n2021-06-01\nNaN\n\n\n1\n0\n2021-06-02\n5.0\n\n\n2\n1\n2021-06-03\n5.0\n\n\n3\n1\n2021-06-04\n5.0\n\n\n4\n1\n2021-05-05\n42.0\n\n\n5\n2\n2021-05-06\n42.0\n\n\n6\n2\n2021-05-07\n42.0\n\n\n7\n2\n2021-05-08\n11.0\n\n\n8\n2\n2021-05-09\n11.0\n\n\n9\n2\n2021-05-10\n11.0\n\n\n10\n3\n2021-07-11\n11.0\n\n\n11\n3\n2021-07-12\n11.0\n\n\n\n\n\n\n\nTo accomplish this, we will create a window over our event_id to partition our data into groups. We will take these groups and order them by measured_on:\n\nwin = ibis.window(group_by=data.event_id, order_by=data.measured_on, following=0)\n\nOnce we have our window defined, we can flag the first non-null value in an event group using count, as it will count non-null values row-by-row within our group:\n\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n\ngrouped.execute().sort_values(by=['event_id', 'measured_on'])\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\ngrouper\n\n\n\n\n0\n0\n2021-06-01\nNaN\n0\n\n\n1\n0\n2021-06-02\n5.0\n1\n\n\n7\n1\n2021-05-05\n42.0\n1\n\n\n8\n1\n2021-06-03\nNaN\n1\n\n\n9\n1\n2021-06-04\nNaN\n1\n\n\n2\n2\n2021-05-06\n42.0\n1\n\n\n3\n2\n2021-05-07\nNaN\n1\n\n\n4\n2\n2021-05-08\n11.0\n2\n\n\n5\n2\n2021-05-09\nNaN\n2\n\n\n6\n2\n2021-05-10\nNaN\n2\n\n\n10\n3\n2021-07-11\nNaN\n0\n\n\n11\n3\n2021-07-12\nNaN\n0\n\n\n\n\n\n\n\nTo see this a bit clearer: look at rows 0, 1, and 2. Row 0 is NaN and is the first row of the group (event_id = 0), so at row 0 we have 0 non-null values (grouper = 0). Row 1 is not null (5.0) and is the second row the group, so our count has increased by 1 (grouper = 1). Row 2 is the first row of its group (event_id = 1) and is not null, so our count is 1 (grouper = 1).\nSkip down to rows 9, 10, and 11. Row 9 is the sixth row of group 2 and there are three non-null values in group 2 before row 9. Therefore the count at row 9 is 3.\nRow 10 is the first row of group 3 and is null, therefore its count is 0. Finally: row 11 is the second row of group 3 and is null as well, therefore the count remains 0.\nUnder this design, we now have another partition.\nOur first partition is by event_id. Within each set in that partition, we have a partition by grouper, where each set has up to one non-null value.\nSince there less than or equal to one non-null value in each group of ['event_id', 'grouper'], we can fill values by overwriting all values within the group by the max value in the group.\nSo:\n\nGroup by event_id and grouper\nMutate the data along that grouping by populating a new column ffill with the max value of measurement.\n\n\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n    .execute()\n).sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\n\nresult\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\ngrouper\nffill\n\n\n\n\n0\n0\n2021-06-01\nNaN\n0\nNaN\n\n\n1\n0\n2021-06-02\n5.0\n1\n5.0\n\n\n2\n1\n2021-05-05\n42.0\n1\n42.0\n\n\n3\n1\n2021-06-03\nNaN\n1\n42.0\n\n\n4\n1\n2021-06-04\nNaN\n1\n42.0\n\n\n5\n2\n2021-05-06\n42.0\n1\n42.0\n\n\n6\n2\n2021-05-07\nNaN\n1\n42.0\n\n\n7\n2\n2021-05-08\n11.0\n2\n11.0\n\n\n8\n2\n2021-05-09\nNaN\n2\n11.0\n\n\n9\n2\n2021-05-10\nNaN\n2\n11.0\n\n\n10\n3\n2021-07-11\nNaN\n0\nNaN\n\n\n11\n3\n2021-07-12\nNaN\n0\nNaN\n\n\n\n\n\n\n\n\n\nbfill Strategy\nInstead of sorting the dates ascending, we will sort them descending. This is akin to starting at the last row in an event group and going backwards using the same logic outlined above.\nLet’s take a look:\n    event_id measured_on  measurement  grouper\n0          0  2021-06-01          NaN        1 # null, take the previous row value (NaN → 5.0)\n1          0  2021-06-02          5.0        1 # last row, do nothing\n2          1  2021-05-05         42.0        1 # not null, do nothing\n3          1  2021-06-03          NaN        0 # null, take previous row value (NaN → NaN)\n4          1  2021-06-04          NaN        0 # last row, do nothing\n5          2  2021-05-06         42.0        2 # not null, do nothing\n6          2  2021-05-07          NaN        1 # null, take previous row value (NaN → 11.0)\n7          2  2021-05-08         11.0        1 # not null, do nothing\n8          2  2021-05-09          NaN        0 # null, take previous row value (NaN → NaN)\n9          2  2021-05-10          NaN        0 # not null, do nothing\n10         3  2021-07-11          NaN        0 # null, take previous row value (NaN → NaN)\n11         3  2021-07-12          NaN        0 # last row, do nothing\nCodewise, bfill follows the same strategy as ffill, we need to specify order_by to use ibis.desc. This will flip our dates and our counts (therefore our groupers) will start backwards.\n\nwin = ibis.window(group_by=data.event_id, order_by=ibis.desc(data.measured_on), following=0)\n\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n\ngrouped.execute().sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\ngrouper\n\n\n\n\n0\n0\n2021-06-01\nNaN\n1\n\n\n1\n0\n2021-06-02\n5.0\n1\n\n\n2\n1\n2021-05-05\n42.0\n1\n\n\n3\n1\n2021-06-03\nNaN\n0\n\n\n4\n1\n2021-06-04\nNaN\n0\n\n\n5\n2\n2021-05-06\n42.0\n2\n\n\n6\n2\n2021-05-07\nNaN\n1\n\n\n7\n2\n2021-05-08\n11.0\n1\n\n\n8\n2\n2021-05-09\nNaN\n0\n\n\n9\n2\n2021-05-10\nNaN\n0\n\n\n10\n3\n2021-07-11\nNaN\n0\n\n\n11\n3\n2021-07-12\nNaN\n0\n\n\n\n\n\n\n\nAnd, again, if we take max of our grouper value, we will get the only non-null value if it exists:\n\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(bfill=grouped.measurement.max())\n    .execute()\n).sort_values(by=['event_id', 'measured_on']).reset_index(drop=True)\n\nresult\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\ngrouper\nbfill\n\n\n\n\n0\n0\n2021-06-01\nNaN\n1\n5.0\n\n\n1\n0\n2021-06-02\n5.0\n1\n5.0\n\n\n2\n1\n2021-05-05\n42.0\n1\n42.0\n\n\n3\n1\n2021-06-03\nNaN\n0\nNaN\n\n\n4\n1\n2021-06-04\nNaN\n0\nNaN\n\n\n5\n2\n2021-05-06\n42.0\n2\n42.0\n\n\n6\n2\n2021-05-07\nNaN\n1\n11.0\n\n\n7\n2\n2021-05-08\n11.0\n1\n11.0\n\n\n8\n2\n2021-05-09\nNaN\n0\nNaN\n\n\n9\n2\n2021-05-10\nNaN\n0\nNaN\n\n\n10\n3\n2021-07-11\nNaN\n0\nNaN\n\n\n11\n3\n2021-07-12\nNaN\n0\nNaN\n\n\n\n\n\n\n\n\n\nbfill and ffill without Event Groups\nYou can bfill and ffill without event groups by ignoring that grouping. Remove all references of event_id and you can treat the entire dataset as one event.\nYour window function will increment whenever a new non-null value is observed, creating that partition where each set has up to one non-null value.\nFor example, reasoning through bfill:\n\ndata.execute().sort_values(by=['measured_on'])\n\nwin = ibis.window(order_by=ibis.desc(data.measured_on), following=0)\n\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n\nresult = (\n    grouped\n    .group_by([grouped.grouper])\n    .mutate(bfill=grouped.measurement.max())\n)\n\nresult.execute().sort_values(by=['measured_on'])\n\n\n\n\n\n\n\n\nevent_id\nmeasured_on\nmeasurement\ngrouper\nbfill\n\n\n\n\n10\n1\n2021-05-05\n42.0\n4\n42.0\n\n\n11\n2\n2021-05-06\n42.0\n3\n42.0\n\n\n5\n2\n2021-05-07\nNaN\n2\n11.0\n\n\n4\n2\n2021-05-08\n11.0\n2\n11.0\n\n\n9\n2\n2021-05-09\nNaN\n1\n5.0\n\n\n8\n2\n2021-05-10\nNaN\n1\n5.0\n\n\n7\n0\n2021-06-01\nNaN\n1\n5.0\n\n\n6\n0\n2021-06-02\n5.0\n1\n5.0\n\n\n3\n1\n2021-06-03\nNaN\n0\nNaN\n\n\n2\n1\n2021-06-04\nNaN\n0\nNaN\n\n\n1\n3\n2021-07-11\nNaN\n0\nNaN\n\n\n0\n3\n2021-07-12\nNaN\n0\nNaN\n\n\n\n\n\n\n\nAs an exercise, try to take your time and reason your way through ffill.\nHappy coding!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "presentations/overview.html#ibis-is-a-python-frontend-for",
    "href": "presentations/overview.html#ibis-is-a-python-frontend-for",
    "title": "Ibis overview",
    "section": "Ibis is a Python frontend for:",
    "text": "Ibis is a Python frontend for:\n\nexploratory data analysis (EDA)\nanalytics\ndata engineering\nmachine learning"
  },
  {
    "objectID": "presentations/overview.html#ibis-analytics",
    "href": "presentations/overview.html#ibis-analytics",
    "title": "Ibis overview",
    "section": "ibis-analytics",
    "text": "ibis-analytics\nAnalyzing and predicting on 10M+ rows from 4+ sources."
  },
  {
    "objectID": "presentations/overview.html#dataframe-lore",
    "href": "presentations/overview.html#dataframe-lore",
    "title": "Ibis overview",
    "section": "dataframe lore",
    "text": "dataframe lore\n\nDataframes first appeared in the S programming language, then evolved into the R calculator programming language.\n\n\nThen pandas perfected the dataframe in Python…or did it?\n\n\nSince, dozens of Python dataframes libraries have come and gone…\n\n\nThe pandas API remains the de facto standard for dataframes in Python (alongside PySpark), but it doesn’t scale.\n\n\nThis leads to data scientists frequently “throwing their work over the wall” to data engineers and ML engineers.\n\n\nBut what if there were a new standard?"
  },
  {
    "objectID": "presentations/overview.html#ibis-origins",
    "href": "presentations/overview.html#ibis-origins",
    "title": "Ibis overview",
    "section": "Ibis origins",
    "text": "Ibis origins\n\nfrom Apache Arrow and the “10 Things I Hate About pandas” by Wes McKinney\n\n\n\n…in 2015, I started the Ibis project…to create a pandas-friendly deferred expression system for static analysis and compilation [of] these types of [query planned, multicore execution] operations. Since an efficient multithreaded in-memory engine for pandas was not available when I started Ibis, I instead focused on building compilers for SQL engines (Impala, PostgreSQL, SQLite), similar to the R dplyr package. Phillip Cloud from the pandas core team has been actively working on Ibis with me for quite a long time."
  },
  {
    "objectID": "presentations/overview.html#dataframe-history",
    "href": "presentations/overview.html#dataframe-history",
    "title": "Ibis overview",
    "section": "dataframe history",
    "text": "dataframe history\n\npandas (2008): dataframes in Python\nSpark (2009): distributed dataframes with PySpark\nDask (2014): distributed dataframes with Python\ndplyr (2014): dataframes in R with SQL-like syntax\nIbis (2015): dataframes in Python with SQL-like syntax\ncuDF (2017): pandas on GPUs\nModin (2018): pandas on Ray/Dask\nKoalas (2019): pandas on Spark\nPolars (2020): multicore dataframes in Python"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem",
    "href": "presentations/overview.html#two-world-problem",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\nPython:"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem-1",
    "href": "presentations/overview.html#two-world-problem-1",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\n\n\nPython:\n\nfiles & dataframes"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem-2",
    "href": "presentations/overview.html#two-world-problem-2",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\n\n\nPython:\n\nfiles & dataframes\ndata science"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem-3",
    "href": "presentations/overview.html#two-world-problem-3",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem-4",
    "href": "presentations/overview.html#two-world-problem-4",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\ndashboards\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics\nnotebooks"
  },
  {
    "objectID": "presentations/overview.html#two-world-problem-5",
    "href": "presentations/overview.html#two-world-problem-5",
    "title": "Ibis overview",
    "section": "two world problem",
    "text": "two world problem\n\n\n\nSQL:\n\ndatabases & tables\nanalytics\nmetrics\ndashboards\n\n\nPython:\n\nfiles & dataframes\ndata science\nstatistics\nnotebooks\n\n\n\n\n\nIbis bridges the gap."
  },
  {
    "objectID": "presentations/overview.html#database-history",
    "href": "presentations/overview.html#database-history",
    "title": "Ibis overview",
    "section": "database history",
    "text": "database history\n\nthey got faster"
  },
  {
    "objectID": "presentations/overview.html#duckdb",
    "href": "presentations/overview.html#duckdb",
    "title": "Ibis overview",
    "section": "DuckDB",
    "text": "DuckDB\nimport ibis\ncon = ibis.duckdb.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nAn embeddable, zero-dependency, C++ SQL database engine."
  },
  {
    "objectID": "presentations/overview.html#datafusion",
    "href": "presentations/overview.html#datafusion",
    "title": "Ibis overview",
    "section": "DataFusion",
    "text": "DataFusion\nimport ibis\ncon = ibis.datafusion.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA Rust SQL query engine."
  },
  {
    "objectID": "presentations/overview.html#clickhouse",
    "href": "presentations/overview.html#clickhouse",
    "title": "Ibis overview",
    "section": "ClickHouse",
    "text": "ClickHouse\nimport ibis\ncon = ibis.clickhouse.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA C++ column-oriented database management system."
  },
  {
    "objectID": "presentations/overview.html#polars",
    "href": "presentations/overview.html#polars",
    "title": "Ibis overview",
    "section": "Polars",
    "text": "Polars\nimport ibis\ncon = ibis.polars.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA Rust DataFrame library."
  },
  {
    "objectID": "presentations/overview.html#bigquery",
    "href": "presentations/overview.html#bigquery",
    "title": "Ibis overview",
    "section": "BigQuery",
    "text": "BigQuery\nimport ibis\ncon = ibis.bigquery.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA serverless, highly scalable, and cost-effective cloud data warehouse."
  },
  {
    "objectID": "presentations/overview.html#snowflake",
    "href": "presentations/overview.html#snowflake",
    "title": "Ibis overview",
    "section": "Snowflake",
    "text": "Snowflake\nimport ibis\ncon = ibis.snowflake.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA cloud data platform."
  },
  {
    "objectID": "presentations/overview.html#oracle",
    "href": "presentations/overview.html#oracle",
    "title": "Ibis overview",
    "section": "Oracle",
    "text": "Oracle\nimport ibis\ncon = ibis.oracle.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA relational database management system."
  },
  {
    "objectID": "presentations/overview.html#spark",
    "href": "presentations/overview.html#spark",
    "title": "Ibis overview",
    "section": "Spark",
    "text": "Spark\nimport ibis\ncon = ibis.pyspark.connect(session)\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA unified analytics engine for large-scale data processing."
  },
  {
    "objectID": "presentations/overview.html#trino",
    "href": "presentations/overview.html#trino",
    "title": "Ibis overview",
    "section": "Trino",
    "text": "Trino\nimport ibis\ncon = ibis.trino.connect()\npenguins = con.table(\"penguins\")\npenguins.group_by([\"species\", \"island\"]).agg(ibis._.count().name(\"count\"))\nA distributed SQL query engine."
  },
  {
    "objectID": "presentations/overview.html#and-more",
    "href": "presentations/overview.html#and-more",
    "title": "Ibis overview",
    "section": "and more!",
    "text": "and more!\n\n\n\n\nSQLite\nPostgreSQL\nMySQL\nMSSQL\n\n\n\n\n\nDruid\npandas\nImpala\nDask\n\n\n\n\n\nNew backends are easy to add!*\n\n\n*usually"
  },
  {
    "objectID": "presentations/overview.html#try-it-out-now",
    "href": "presentations/overview.html#try-it-out-now",
    "title": "Ibis overview",
    "section": "try it out now",
    "text": "try it out now\nInstall:\npip install 'ibis-framework[duckdb]'\n\nThen run:\nimport ibis\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\n\nt"
  },
  {
    "objectID": "concepts/internals.html",
    "href": "concepts/internals.html",
    "title": "Internals",
    "section": "",
    "text": "The internals are designed to map the Ibis API to the backend.\n\n\n\nType safety\nExpressiveness\nComposability\nFamiliarity\n\n\n\n\n\nUser writes expression\nEach method or function call builds a new expression\nExpressions are type checked as you create them\nExpressions have some optimizations that happen as the user builds them\nBackend specific rewrites\nExpressions are compiled\nThe SQL string that generated by the compiler is sent to the database and executed (this step is skipped for the pandas backend)\nThe database returns some data that is then turned into a pandas DataFrame by Ibis\n\n\n\n\nThe main user-facing component of Ibis is expressions. The base class of all expressions in Ibis is the Expr class.\nExpressions provide the user facing API, most of which is defined in ibis/expr/api.py.\n\n\nIbis’s type system consists of a set of rules for specifying the types of inputs to ibis.expr.types.Node subclasses. Upon construction of a Node subclass, Ibis performs validation of every input to the node based on the rule that was used to declare the input.\nRules are defined in ibis.expr.rules.\n\n\n\nExpressions are a thin but important abstraction over operations, containing only type information and shape information, i.e., whether they are tables, columns, or scalars.\nExamples of expression types include StringValue and Table.\n\n\n\nNode subclasses make up the core set of operations of Ibis. Each node corresponds to a particular operation.\nMost nodes are defined in the ibis.expr.operations module.\nExamples of nodes include ibis.expr.operations.Add and ibis.expr.operations.Sum.\nNodes (transitively) inherit from a class that allows node authors to define their node’s input arguments directly in the class body.\nAdditionally the output_type member of the class is a rule or method that defines the shape (scalar or column) and element type of the operation.\nAn example of usage is a node that representats a logarithm operation:\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import Value\n\nclass Log(Value):\n   # A double scalar or column\n   arg = rlz.double\n   # Optional argument, defaults to None\n   base = rlz.optional(rlz.double)\n   # Output expression's datatype will correspond to arg's datatype\n   dtype = rlz.dtype_like('arg')\n   # Output expression will be scalar if arg is scalar, column otherwise\n   shape = rlz.shape_like('arg')\nThis class describes an operation called Log that takes one required argument: a double scalar or column, and one optional argument: a double scalar or column named base that defaults to nothing if not provided. The base argument is None by default so that the expression will behave as the underlying database does.\nSimilar objects are instantiated when you use Ibis APIs:\nimport ibis\nt = ibis.table([('a', 'float')], name='t')\nlog_1p = (1 + t.a).log()  # an Add and a Log are instantiated here\n\n\n\nSeparating expressions from their underlying operations makes it easy to generically describe and validate the inputs to particular nodes. In the log example, it doesn’t matter what operation (node) the double-valued arguments are coming from, they must only satisfy the requirement denoted by the rule.\nSeparation of the ibis.expr.types.Node and ibis.expr.types.Expr classes also allows the API to be tied to the physical type of the expression rather than the particular operation, making it easy to define the API in terms of types rather than specific operations.\nFurthermore, operations often have an output type that depends on the input type. An example of this is the greatest function, which takes the maximum of all of its arguments. Another example is CASE statements, whose THEN expressions determine the output type of the expression.\nThis allows Ibis to provide only the APIs that make sense for a particular type, even when an operation yields a different output type depending on its input. Concretely, this means that you cannot perform operations that don’t make sense, like computing the average of a string column.\n\n\n\n\nThe next major component of Ibis is the compilers.\nThe first few versions of Ibis directly generated strings, but the compiler infrastructure was generalized to support compilation of SQLAlchemy based expressions.\nThe compiler works by translating the different pieces of SQL expression into a string or SQLAlchemy expression.\nThe main pieces of a SELECT statement are:\n\nThe set of column expressions (select_set)\nWHERE clauses (where)\nGROUP BY clauses (group_by)\nHAVING clauses (having)\nLIMIT clauses (limit)\nORDER BY clauses (order_by)\nDISTINCT clauses (distinct)\n\nEach of these pieces is translated into a SQL string and finally assembled by the instance of the ibis.sql.compiler.ExprTranslator subclass specific to the backend being compiled. For example, the ibis.impala.compiler.ImpalaExprTranslator is one of the subclasses that will perform this translation.\n\n\n\n\n\n\nNote\n\n\n\nWhile Ibis was designed with an explicit goal of first-class SQL support, Ibis can target other systems such as pandas or Polars.\n\n\n\n\n\nPresumably we want to do something with our compiled expressions. This is where execution comes in.\nThis is least complex part of Ibis, mostly only requiring Ibis to correctly handle whatever the database hands back.\nBy and large, the execution of compiled SQL is handled by the database to which SQL is sent from Ibis.\nHowever, once the data arrives from the database we need to convert that data to a pandas DataFrame.\nThe Query class, with its ibis.sql.client.Query._fetch method, provides a way for Ibis ibis.sql.client.SQLClient objects to do any additional processing necessary after the database returns results to the client."
  },
  {
    "objectID": "concepts/internals.html#primary-goals",
    "href": "concepts/internals.html#primary-goals",
    "title": "Internals",
    "section": "",
    "text": "Type safety\nExpressiveness\nComposability\nFamiliarity"
  },
  {
    "objectID": "concepts/internals.html#flow-of-execution",
    "href": "concepts/internals.html#flow-of-execution",
    "title": "Internals",
    "section": "",
    "text": "User writes expression\nEach method or function call builds a new expression\nExpressions are type checked as you create them\nExpressions have some optimizations that happen as the user builds them\nBackend specific rewrites\nExpressions are compiled\nThe SQL string that generated by the compiler is sent to the database and executed (this step is skipped for the pandas backend)\nThe database returns some data that is then turned into a pandas DataFrame by Ibis"
  },
  {
    "objectID": "concepts/internals.html#expressions",
    "href": "concepts/internals.html#expressions",
    "title": "Internals",
    "section": "",
    "text": "The main user-facing component of Ibis is expressions. The base class of all expressions in Ibis is the Expr class.\nExpressions provide the user facing API, most of which is defined in ibis/expr/api.py.\n\n\nIbis’s type system consists of a set of rules for specifying the types of inputs to ibis.expr.types.Node subclasses. Upon construction of a Node subclass, Ibis performs validation of every input to the node based on the rule that was used to declare the input.\nRules are defined in ibis.expr.rules.\n\n\n\nExpressions are a thin but important abstraction over operations, containing only type information and shape information, i.e., whether they are tables, columns, or scalars.\nExamples of expression types include StringValue and Table.\n\n\n\nNode subclasses make up the core set of operations of Ibis. Each node corresponds to a particular operation.\nMost nodes are defined in the ibis.expr.operations module.\nExamples of nodes include ibis.expr.operations.Add and ibis.expr.operations.Sum.\nNodes (transitively) inherit from a class that allows node authors to define their node’s input arguments directly in the class body.\nAdditionally the output_type member of the class is a rule or method that defines the shape (scalar or column) and element type of the operation.\nAn example of usage is a node that representats a logarithm operation:\nimport ibis.expr.rules as rlz\nfrom ibis.expr.operations import Value\n\nclass Log(Value):\n   # A double scalar or column\n   arg = rlz.double\n   # Optional argument, defaults to None\n   base = rlz.optional(rlz.double)\n   # Output expression's datatype will correspond to arg's datatype\n   dtype = rlz.dtype_like('arg')\n   # Output expression will be scalar if arg is scalar, column otherwise\n   shape = rlz.shape_like('arg')\nThis class describes an operation called Log that takes one required argument: a double scalar or column, and one optional argument: a double scalar or column named base that defaults to nothing if not provided. The base argument is None by default so that the expression will behave as the underlying database does.\nSimilar objects are instantiated when you use Ibis APIs:\nimport ibis\nt = ibis.table([('a', 'float')], name='t')\nlog_1p = (1 + t.a).log()  # an Add and a Log are instantiated here\n\n\n\nSeparating expressions from their underlying operations makes it easy to generically describe and validate the inputs to particular nodes. In the log example, it doesn’t matter what operation (node) the double-valued arguments are coming from, they must only satisfy the requirement denoted by the rule.\nSeparation of the ibis.expr.types.Node and ibis.expr.types.Expr classes also allows the API to be tied to the physical type of the expression rather than the particular operation, making it easy to define the API in terms of types rather than specific operations.\nFurthermore, operations often have an output type that depends on the input type. An example of this is the greatest function, which takes the maximum of all of its arguments. Another example is CASE statements, whose THEN expressions determine the output type of the expression.\nThis allows Ibis to provide only the APIs that make sense for a particular type, even when an operation yields a different output type depending on its input. Concretely, this means that you cannot perform operations that don’t make sense, like computing the average of a string column."
  },
  {
    "objectID": "concepts/internals.html#compilation",
    "href": "concepts/internals.html#compilation",
    "title": "Internals",
    "section": "",
    "text": "The next major component of Ibis is the compilers.\nThe first few versions of Ibis directly generated strings, but the compiler infrastructure was generalized to support compilation of SQLAlchemy based expressions.\nThe compiler works by translating the different pieces of SQL expression into a string or SQLAlchemy expression.\nThe main pieces of a SELECT statement are:\n\nThe set of column expressions (select_set)\nWHERE clauses (where)\nGROUP BY clauses (group_by)\nHAVING clauses (having)\nLIMIT clauses (limit)\nORDER BY clauses (order_by)\nDISTINCT clauses (distinct)\n\nEach of these pieces is translated into a SQL string and finally assembled by the instance of the ibis.sql.compiler.ExprTranslator subclass specific to the backend being compiled. For example, the ibis.impala.compiler.ImpalaExprTranslator is one of the subclasses that will perform this translation.\n\n\n\n\n\n\nNote\n\n\n\nWhile Ibis was designed with an explicit goal of first-class SQL support, Ibis can target other systems such as pandas or Polars."
  },
  {
    "objectID": "concepts/internals.html#execution",
    "href": "concepts/internals.html#execution",
    "title": "Internals",
    "section": "",
    "text": "Presumably we want to do something with our compiled expressions. This is where execution comes in.\nThis is least complex part of Ibis, mostly only requiring Ibis to correctly handle whatever the database hands back.\nBy and large, the execution of compiled SQL is handled by the database to which SQL is sent from Ibis.\nHowever, once the data arrives from the database we need to convert that data to a pandas DataFrame.\nThe Query class, with its ibis.sql.client.Query._fetch method, provides a way for Ibis ibis.sql.client.SQLClient objects to do any additional processing necessary after the database returns results to the client."
  },
  {
    "objectID": "support_matrix.html",
    "href": "support_matrix.html",
    "title": "Operation support matrix",
    "section": "",
    "text": "We provide Ibis’s operation support matrix as a Streamlit app that shows supported operations for each backend. Ibis defines a common API for analytics and data transformation code that is transpiled to native code for each backend. Due to differences in SQL dialects and upstream support for different operations in different backends, support for the full breadth of the Ibis API varies.\nYou can use this page to see which operations are supported on each backend."
  },
  {
    "objectID": "support_matrix.html#raw-data",
    "href": "support_matrix.html#raw-data",
    "title": "Operation support matrix",
    "section": "Raw Data",
    "text": "Raw Data\nYou can also download data from the above tables in CSV format.\nThe code used to generate the linked CSV file is below.\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nimport pandas as pd\n\nimport ibis\nimport ibis.expr.operations as ops\n\n\ndef get_backends(exclude=()):\n    entry_points = sorted(ep.name for ep in ibis.util.backend_entry_points())\n    return [\n        (backend, getattr(ibis, backend))\n        for backend in entry_points\n        if backend not in exclude\n    ]\n\n\ndef get_leaf_classes(op):\n    for child_class in op.__subclasses__():\n        if not child_class.__subclasses__():\n            yield child_class\n        else:\n            yield from get_leaf_classes(child_class)\n\n\ndef main():\n    internal_ops = {\n        # Never translates into anything\n        ops.UnresolvedExistsSubquery,\n        ops.ScalarParameter,\n    }\n\n    public_ops = frozenset(get_leaf_classes(ops.Value)) - internal_ops\n    support = {\"operation\": [f\"{op.__module__}.{op.__name__}\" for op in public_ops]}\n    support.update(\n        (name, list(map(backend.has_operation, public_ops)))\n        for name, backend in get_backends()\n    )\n\n    df = pd.DataFrame(support).set_index(\"operation\").sort_index()\n\n    with Path(ibis.__file__).parents[1].joinpath(\n        \"docs\", \"backends\", \"raw_support_matrix.csv\"\n    ).open(mode=\"w\") as f:\n        df.to_csv(f, index_label=\"FullOperation\")\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "Welcome to the Ibis project!\n\nLearning Ibis for the first time?: Check out the Ibis getting started tutorial!\nComing from SQL?: Take a look at Ibis for SQL users!\nComing from pandas?: Check out Ibis for pandas users!\nComing from R?: See Ibis for dplyr users!\nWant to see some more examples?: We’ve got a repository of examples for that!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "how-to/timeseries/ffill_bfill_w_window.html",
    "href": "how-to/timeseries/ffill_bfill_w_window.html",
    "title": "Forward and backward fill data using window functions",
    "section": "",
    "text": "Forward and backward fill data using window functions\nIf you have gaps in your data and need to fill them in using a simple forward fill (given an order, null values are replaced by the value preceding) or backward fill (given an order, null values are replaced by the value following), then you can do this in Ibis:\n=== “ffill”\n~~~python\n# Create a window that orders your series, default ascending\nwin = ibis.window(order_by=data.measured_on, following=0)\n# Create a grouping that is a rolling count of non-null values\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['measured_on'])\n~~~\n=== “bfill”\n~~~python\n# Create a window that orders your series (use ibis.desc to get descending order)\nwin = ibis.window(order_by=ibis.desc(data.measured_on), following=0)\n# Create a grouping that is a rolling count of non-null values\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['measured_on'])\n~~~\nIf you have an event partition, which means there’s another segment you need to consider for your ffill or bfill operations, you can do this as well:\n=== “ffill with event partition”\n~~~python\n# Group your data by your event partition and then order your series (default ascending)\nwin = ibis.window(group_by=data.event_id, order_by=data.measured_on, following=0)\n# Create a grouping that is a rolling count of non-null values within each event\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['event_id', 'measured_on'])\n~~~\n=== “bfill with event partition”\n~~~python\n# Group your data by your event partition and then order your series (use ibis.desc for desc)\nwin = ibis.window(group_by=data.event_id, order_by=ibis.desc(data.measured_on), following=0)\n# Create a grouping that is a rolling count of non-null values within each event\n# This creates a partition where each set has no more than one non-null value\ngrouped = data.mutate(grouper=data.measurement.count().over(win))\n# Group by your newly-created grouping and, in each set,\n# set all values to the one non-null value in that set (if it exists)\nresult = (\n    grouped\n    .group_by([grouped.event_id, grouped.grouper])\n    .mutate(ffill=grouped.measurement.max())\n)\n# execute to get a pandas dataframe, sort values in case your backend shuffles\nresult.execute().sort_values(by=['event_id', 'measured_on'])\n~~~\nWe wrote a deeper dive into how this works on the ibis-project blog here.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "how-to/configure/basics.html",
    "href": "how-to/configure/basics.html",
    "title": "Basic configuration",
    "section": "",
    "text": "If you don’t have your own data, you can load example data from the ibis.examples module:\n1import ibis\n\n2t = ibis.examples.penguins.fetch()\n\n\n1\n\nEnsure you install Ibis first.\n\n2\n\nLoad a dataset from the built-in examples."
  },
  {
    "objectID": "how-to/configure/basics.html#overview",
    "href": "how-to/configure/basics.html#overview",
    "title": "Basic configuration",
    "section": "Overview",
    "text": "Overview\nIbis configuration happens through the ibis.options attribute. Attributes can be get and set like class attributes."
  },
  {
    "objectID": "how-to/configure/basics.html#interactive-mode",
    "href": "how-to/configure/basics.html#interactive-mode",
    "title": "Basic configuration",
    "section": "Interactive mode",
    "text": "Interactive mode\nIbis out of the box is in deferred mode. Expressions display their internal details when printed to the console.\n\nt.head(3)\n\nr0 := DatabaseTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\nLimit[r0, n=3]\n\n\n\nFor a better interactive experience, set the interactive option:\n\nibis.options.interactive = True\n\nThis will cause expressions to be executed immediately when printed to the console.\n\nt.head(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/configure/basics.html#sql-query-execution",
    "href": "how-to/configure/basics.html#sql-query-execution",
    "title": "Basic configuration",
    "section": "SQL query execution",
    "text": "SQL query execution\nIf an Ibis table expression has no row limit set using the limit API, a default one is applied to prevent too much data from being retrieved from the query engine. The default is currently 10000 rows, but this can be configured with the sql.default_limit option:\n\nibis.options.sql.default_limit = 100\n\nSet this to None to retrieve all rows in all queries\n\n\n\n\n\n\nWarning\n\n\n\nSetting the default limit to None will result in all rows from a query coming back to the client from the backend.\n\n\n\nibis.options.sql.default_limit = None"
  },
  {
    "objectID": "how-to/configure/basics.html#verbose-option-and-logging",
    "href": "how-to/configure/basics.html#verbose-option-and-logging",
    "title": "Basic configuration",
    "section": "Verbose option and logging",
    "text": "Verbose option and logging\nTo see all internal Ibis activity (like queries being executed) set ibis.options.verbose:\n\nibis.options.verbose = True\n\nBy default this information is sent to sys.stdout, but you can set some other logging function:\n\ndef cowsay(msg):\n    print(f\"Cow says: {msg}\")\n\n\nibis.options.verbose_log = cowsay"
  },
  {
    "objectID": "how-to/configure/basics.html#default-backend",
    "href": "how-to/configure/basics.html#default-backend",
    "title": "Basic configuration",
    "section": "Default backend",
    "text": "Default backend\nibis.options.default_backend controls which backend is used by table expressions returned by top-level functions such as ibis.memtable, ibis.read_csv or ibis.read_parquet.\nBy default, it points to an instance of DuckDB backend. Assuming the backend dependencies have been installed, it can be updated by passing the name of the backend to ibis.set_backend as follows:\n\nimport ibis\n\nexpr = ibis.memtable({\"column\": [0, 1, 2, 3, 4]})\nibis.get_backend(expr)\n# &lt;ibis.backends.duckdb.Backend at 0x12fa0fb50&gt;\n\nibis.set_backend(\"sqlite\")\nibis.get_backend(expr)\n# &lt;ibis.backends.sqlite.Backend at 0x158411d10&gt;\n\n&lt;ibis.backends.sqlite.Backend at 0x76251f5a3340&gt;"
  },
  {
    "objectID": "how-to/extending/sql.html",
    "href": "how-to/extending/sql.html",
    "title": "Using SQL strings with Ibis",
    "section": "",
    "text": "While Ibis goes to great lengths to help you avoid the problems associated with hand-building raw SQL strings, there are few use cases where you may need to use SQL strings in your Ibis code:\nFor these situations and others, Ibis has you covered."
  },
  {
    "objectID": "how-to/extending/sql.html#setup",
    "href": "how-to/extending/sql.html#setup",
    "title": "Using SQL strings with Ibis",
    "section": "Setup",
    "text": "Setup\nWe’ll use DuckDB to illustrate the concepts here, but the ideas and code generalize to other SQL backends.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True\n\n1con = ibis.connect(\"duckdb://\")\n\n2t = ibis.examples.penguins.fetch(backend=con, table_name=\"penguins\")\n\n\n1\n\nConnect to an in-memory DuckDB database\n\n2\n\nRead in the penguins example with our DuckDB database, and name it penguins"
  },
  {
    "objectID": "how-to/extending/sql.html#table.sql",
    "href": "how-to/extending/sql.html#table.sql",
    "title": "Using SQL strings with Ibis",
    "section": "Table.sql",
    "text": "Table.sql\nAt the highest level there’s the Table.sql method. This method allows you to run arbitrary SELECT statements against a table expression:\n\nt.sql(\"SELECT * FROM penguins\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nNow, SELECT * on a table expressions is not particularly useful and in fact is a bit wasteful: there’s no need to reselect a table expression.\nThe real power of Table.sql is composing it with other ibis expressions:\n\n(\n   t.sql(\n      \"\"\"\n      SELECT\n        species,\n        island,\n        mad(bill_length_mm) AS bill_mad\n      FROM penguins\n      GROUP BY 1, 2\n      \"\"\"\n   )\n   .filter(_.bill_mad &gt; 2)\n   .order_by(_.bill_mad.desc())\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ species   ┃ island    ┃ bill_mad ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string    │ float64  │\n├───────────┼───────────┼──────────┤\n│ Chinstrap │ Dream     │     2.45 │\n│ Adelie    │ Torgersen │     2.20 │\n│ Gentoo    │ Biscoe    │     2.10 │\n└───────────┴───────────┴──────────┘\n\n\n\nThis method is powerful and you can mix and match Ibis expressions as you like:\n\n(\n   t.sql(\n      \"\"\"\n      SELECT\n        species,\n        island,\n        mad(bill_length_mm) AS bill_mad\n      FROM penguins\n      GROUP BY 1, 2\n      \"\"\"\n   )\n   .filter(_.bill_mad &gt; 2)\n   .alias(\"big_bills\")  # note the alias call for subsequent '.sql' calls\n   .sql(\"SELECT * FROM big_bills ORDER BY 3 DESC\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ species   ┃ island    ┃ bill_mad ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string    │ float64  │\n├───────────┼───────────┼──────────┤\n│ Chinstrap │ Dream     │     2.45 │\n│ Adelie    │ Torgersen │     2.20 │\n│ Gentoo    │ Biscoe    │     2.10 │\n└───────────┴───────────┴──────────┘\n\n\n\n\nWorking with different SQL dialects\nYou can also pass SQL strings from SQL dialects that do not match the backend you’re using by passing a dialect name to the dialect argument of .sql.\nFor example, here’s MySQL syntax running against DuckDB (note the use of backticks for quoting).\n\n(\n   t.sql(\n      \"\"\"\n      SELECT\n        `species`,\n        `island`,\n        mad(`bill_length_mm`) AS bill_mad\n      FROM `penguins`\n      GROUP BY 1, 2\n      \"\"\",\n      dialect=\"mysql\",\n   )\n   .filter(_.bill_mad &gt; 2)\n   .alias(\"big_bills\")  # note the alias call for subsequent '.sql' calls\n1   .sql(\"SELECT * FROM big_bills ORDER BY 3 DESC\")\n)\n\n\n1\n\nBy default the dialect is the backend’s native dialect.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ species   ┃ island    ┃ bill_mad ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string    │ float64  │\n├───────────┼───────────┼──────────┤\n│ Chinstrap │ Dream     │     2.45 │\n│ Adelie    │ Torgersen │     2.20 │\n│ Gentoo    │ Biscoe    │     2.10 │\n└───────────┴───────────┴──────────┘\n\n\n\nThis feature is useful if you’re porting existing SQL from one backend to another."
  },
  {
    "objectID": "how-to/extending/sql.html#backend.sql",
    "href": "how-to/extending/sql.html#backend.sql",
    "title": "Using SQL strings with Ibis",
    "section": "Backend.sql",
    "text": "Backend.sql\nThere’s also the Backend.sql method, which can handle arbitrary SELECT statements as well and returns an Ibis table expression.\nThe main difference with Table.sql is that Backend.sql can only refer to tables that already exist in the database, because the API is defined on Backend instances.\nAfter the Backend.sql call, however, you’re able to mix and match similar to Table.sql:\n\n(\n   con.sql(\n      \"\"\"\n      SELECT\n        species,\n        island,\n        mad(bill_length_mm) AS bill_mad\n      FROM penguins\n      GROUP BY 1, 2\n      \"\"\"\n   )\n   .filter(_.bill_mad &gt; 2)\n   .alias(\"big_bills\")  # note the alias call for subsequent '.sql' calls\n   .sql(\"SELECT * FROM big_bills ORDER BY 3 DESC\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ species   ┃ island    ┃ bill_mad ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string    │ string    │ float64  │\n├───────────┼───────────┼──────────┤\n│ Chinstrap │ Dream     │     2.45 │\n│ Adelie    │ Torgersen │     2.20 │\n│ Gentoo    │ Biscoe    │     2.10 │\n└───────────┴───────────┴──────────┘\n\n\n\n\n\n\n\n\n\nBackend.sql also supports the dialect argument."
  },
  {
    "objectID": "how-to/extending/sql.html#backend.raw_sql",
    "href": "how-to/extending/sql.html#backend.raw_sql",
    "title": "Using SQL strings with Ibis",
    "section": "Backend.raw_sql",
    "text": "Backend.raw_sql\nAt the lowest level there’s Backend.raw_sql which is for those situations where you need to run arbitrary SQL–like a CREATE statement–that cannot be modeled as a table expression.\nBackend.raw_sql accepts a SQL string, executes it and returns the cursor associated with the SQL statement’s execution.\n\n\n\n\n\n\nYou must close the cursor returned from raw_sql to avoid leaking resources\n\n\n\nFailure to do results in variety of errors and hard-to-debug behaviors.\nThe easiest way to do this is to use a context manager:\n\nfrom contextlib import closing\n\nwith closing(con.raw_sql(\"CREATE TEMP TABLE my_table AS SELECT * FROM RANGE(10)\")) as c:\n    ...  # do something with c if necessary\n\n\n\nHere’s an example:\n\ncur = con.raw_sql(\"CREATE TEMP TABLE t AS SELECT * FROM RANGE(10)\")\n1cur.close()\n\n\n1\n\nOnly needed if you’re not using a context manager and the query returns rows. In this case CREATE doesn’t return any rows, so the close() isn’t strictly necessary. However, it’s good practice to always close cursors, even if those close() call isn’t strictly required."
  },
  {
    "objectID": "how-to/extending/builtin.html",
    "href": "how-to/extending/builtin.html",
    "title": "Reference built-in functions",
    "section": "",
    "text": "Functions that aren’t exposed in ibis directly can be accessed using the @ibis.udf.scalar.builtin decorator.\n\n\n\n\n\n\nIbis APIs may already exist for your function.\n\n\n\nBuiltin scalar UDFs are designed to be an escape hatch when Ibis doesn’t have a defined API for a built-in database function.\nSee the reference documentation for existing APIs.\n\n\n\n\nIbis doesn’t directly expose many of the DuckDB text similarity functions. Let’s expose the mismatches API.\n\nfrom ibis import udf\n\n@udf.scalar.builtin\ndef mismatches(left: str, right: str) -&gt; int:\n    ...\n\nThe ... is a visual indicator that the function definition is unknown to Ibis.\n\n\n\n\n\n\nIbis does not do anything with the function body.\n\n\n\n\n\nIbis will not execute the function body or otherwise inspect it. Any code you write in the function body will be ignored.\n\n\n\nWe can now call this function on any ibis expression:\n\nimport ibis\n\n1con = ibis.duckdb.connect()\n\n\n1\n\nConnect to an in-memory DuckDB database\n\n\n\n\n\nexpr = mismatches(\"duck\", \"luck\")\ncon.execute(expr)\n\n1\n\n\nLike any other ibis expression you can inspect the SQL:\n\nimport ibis\n\n1ibis.to_sql(expr, dialect=\"duckdb\")\n\n\n1\n\nThe dialect keyword argument must be passed, because we constructed a literal expression which has no backend attached.\n\n\n\n\nSELECT\n  MISMATCHES('duck', 'luck') AS \"mismatches('duck', 'luck')\"\n\n\nBecause built-in UDFs are ultimately Ibis expressions, they compose with the rest of the library:\n\nibis.options.interactive = True\n\n@udf.scalar.builtin\ndef jaro_winkler_similarity(a: str, b: str) -&gt; float:\n   ...\n\npkgs = ibis.read_parquet(\n   \"https://storage.googleapis.com/ibis-tutorial-data/pypi/packages.parquet\"\n)\npandas_ish = pkgs[jaro_winkler_similarity(pkgs.name, \"pandas\") &gt;= 0.9]\npandas_ish\n\n┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ name     ┃ version           ┃ requires_python ┃ yanked  ┃ has_binary_wheel ┃ has_vulnerabilities ┃ first_uploaded_at   ┃ last_uploaded_at    ┃ recorded_at         ┃ downloads ┃ scorecard_overall ┃ in_google_assured_oss ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string   │ string            │ string          │ boolean │ boolean          │ boolean             │ timestamp           │ timestamp           │ timestamp           │ int32     │ float64           │ boolean               │\n├──────────┼───────────────────┼─────────────────┼─────────┼──────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼───────────┼───────────────────┼───────────────────────┤\n│ bcpandas │ 2.4.1             │ &gt;=3.8.1         │ True    │ False            │ False               │ 2023-07-12 06:14:22 │ 2023-07-12 06:14:23 │ 2023-07-12 14:31:41 │         0 │              NULL │ False                 │\n│ espandas │ 1.0.4             │ ~               │ False   │ False            │ False               │ 2018-12-22 20:52:30 │ 2018-12-22 20:52:30 │ 2023-07-12 14:58:47 │         0 │               3.6 │ False                 │\n│ fpandas  │ 0.5               │ ~               │ False   │ False            │ False               │ 2020-03-09 02:35:31 │ 2020-03-09 02:35:31 │ 2023-07-12 15:04:23 │         0 │              NULL │ False                 │\n│ h3pandas │ 0.2.4             │ &gt;=3.6           │ False   │ False            │ False               │ 2023-03-19 17:58:16 │ 2023-03-19 17:58:16 │ 2023-07-12 15:10:06 │         0 │              NULL │ False                 │\n│ ipandas  │ 0.0.1             │ ~               │ False   │ False            │ False               │ 2019-05-29 18:46:12 │ 2019-05-29 18:46:12 │ 2023-07-12 15:15:34 │         0 │               3.6 │ False                 │\n│ kpandas  │ 0.0.1             │ &gt;=3.6,&lt;4.0      │ False   │ False            │ False               │ 2019-05-02 18:00:29 │ 2019-05-02 18:00:31 │ 2023-07-12 15:20:21 │         0 │              NULL │ False                 │\n│ mpandas  │ 0.0.2.1           │ ~               │ False   │ False            │ False               │ 2022-07-03 16:21:21 │ 2022-07-03 16:21:23 │ 2023-07-12 15:30:35 │         0 │              NULL │ False                 │\n│ mtpandas │ 1.14.202306141807 │ &gt;=3.6           │ False   │ False            │ False               │ 2023-06-14 18:08:01 │ 2023-06-14 18:08:01 │ 2023-07-12 15:31:04 │         0 │               4.6 │ False                 │\n│ mypandas │ 0.1.6             │ &gt;=3.10          │ False   │ False            │ False               │ 2022-10-24 21:01:10 │ 2022-10-24 21:01:12 │ 2023-07-12 15:32:04 │         0 │              NULL │ False                 │\n│ paandas  │ 0.0.3             │ ~               │ False   │ False            │ False               │ 2022-11-24 06:11:15 │ 2022-11-24 06:11:17 │ 2023-07-12 15:43:31 │         0 │              NULL │ False                 │\n│ …        │ …                 │ …               │ …       │ …                │ …                   │ …                   │ …                   │ …                   │         … │                 … │ …                     │\n└──────────┴───────────────────┴─────────────────┴─────────┴──────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────────┴───────────────────────┘\n\n\n\nLet’s count the results:\n\npandas_ish.count()\n\n\n\n\n\n178\n\n\n\nThere are a good number of packages that look similar to pandas!\n\n\n\nSimilarly we can expose Snowflake’s jarowinkler_similarity function.\nLet’s alias it to jw_sim to illustrate some more of the Ibis udf API:\n\n1@udf.scalar.builtin(name=\"jarowinkler_similarity\")\ndef jw_sim(left: str, right: str) -&gt; float:\n    ...\n\n\n1\n\ntarget is the name of the function in the backend. This argument is required in this because the function name is different than the name of the function in ibis.\n\n\n\n\nNow let’s connect to Snowflake and call our jw_sim function:\n\nimport os\n\ncon = ibis.connect(os.environ[\"SNOWFLAKE_URL\"])\n\n\nexpr = jw_sim(\"snow\", \"shoe\")\ncon.execute(expr)\n\n66.0\n\n\nAnd let’s take a look at the SQL\n\nibis.to_sql(expr, dialect=\"snowflake\")\n\nSELECT\n  JAROWINKLER_SIMILARITY('snow', 'shoe') AS \"jw_sim('snow', 'shoe')\"\n\n\n\n\n\nSometimes the input types of builtin functions are difficult to spell.\nConsider a function that computes the length of any array: the elements in the array can be floats, integers, strings and even other arrays. Spelling that type is difficult.\nFortunately the udf.scalar.builtin decorator doesn’t require you to specify input types in these cases:\n\n@udf.scalar.builtin(name=\"array_size\")\ndef cardinality(arr) -&gt; int:\n   ...\n\n\n\n\n\n\n\nThe return type annotation is always required.\n\n\n\n\n\n\nWe can pass arrays with different element types to our cardinality function:\n\ncon.execute(cardinality([1, 2, 3]))\n\n3\n\n\n\ncon.execute(cardinality([\"a\", \"b\"]))\n\n2\n\n\nWhen you bypass input types the errors you get back are backend dependent:\n\ncon.execute(cardinality(\"foo\"))\n\nProgrammingError: (snowflake.connector.errors.ProgrammingError) 001044 (42P13): SQL compilation error: error line 1 at position 7\nInvalid argument types for function 'ARRAY_SIZE': (VARCHAR(3))\n[SQL: SELECT array_size(%(param_1)s) AS \"cardinality('foo')\"]\n[parameters: {'param_1': 'foo'}]\n(Background on this error at: https://sqlalche.me/e/14/f405)\n\n\nHere, Snowflake is informing us that the ARRAY_SIZE function does not accept strings as input."
  },
  {
    "objectID": "how-to/extending/builtin.html#scalar-functions",
    "href": "how-to/extending/builtin.html#scalar-functions",
    "title": "Reference built-in functions",
    "section": "",
    "text": "Functions that aren’t exposed in ibis directly can be accessed using the @ibis.udf.scalar.builtin decorator.\n\n\n\n\n\n\nIbis APIs may already exist for your function.\n\n\n\nBuiltin scalar UDFs are designed to be an escape hatch when Ibis doesn’t have a defined API for a built-in database function.\nSee the reference documentation for existing APIs.\n\n\n\n\nIbis doesn’t directly expose many of the DuckDB text similarity functions. Let’s expose the mismatches API.\n\nfrom ibis import udf\n\n@udf.scalar.builtin\ndef mismatches(left: str, right: str) -&gt; int:\n    ...\n\nThe ... is a visual indicator that the function definition is unknown to Ibis.\n\n\n\n\n\n\nIbis does not do anything with the function body.\n\n\n\n\n\nIbis will not execute the function body or otherwise inspect it. Any code you write in the function body will be ignored.\n\n\n\nWe can now call this function on any ibis expression:\n\nimport ibis\n\n1con = ibis.duckdb.connect()\n\n\n1\n\nConnect to an in-memory DuckDB database\n\n\n\n\n\nexpr = mismatches(\"duck\", \"luck\")\ncon.execute(expr)\n\n1\n\n\nLike any other ibis expression you can inspect the SQL:\n\nimport ibis\n\n1ibis.to_sql(expr, dialect=\"duckdb\")\n\n\n1\n\nThe dialect keyword argument must be passed, because we constructed a literal expression which has no backend attached.\n\n\n\n\nSELECT\n  MISMATCHES('duck', 'luck') AS \"mismatches('duck', 'luck')\"\n\n\nBecause built-in UDFs are ultimately Ibis expressions, they compose with the rest of the library:\n\nibis.options.interactive = True\n\n@udf.scalar.builtin\ndef jaro_winkler_similarity(a: str, b: str) -&gt; float:\n   ...\n\npkgs = ibis.read_parquet(\n   \"https://storage.googleapis.com/ibis-tutorial-data/pypi/packages.parquet\"\n)\npandas_ish = pkgs[jaro_winkler_similarity(pkgs.name, \"pandas\") &gt;= 0.9]\npandas_ish\n\n┏━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ name     ┃ version           ┃ requires_python ┃ yanked  ┃ has_binary_wheel ┃ has_vulnerabilities ┃ first_uploaded_at   ┃ last_uploaded_at    ┃ recorded_at         ┃ downloads ┃ scorecard_overall ┃ in_google_assured_oss ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string   │ string            │ string          │ boolean │ boolean          │ boolean             │ timestamp           │ timestamp           │ timestamp           │ int32     │ float64           │ boolean               │\n├──────────┼───────────────────┼─────────────────┼─────────┼──────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼─────────────────────┼───────────┼───────────────────┼───────────────────────┤\n│ bcpandas │ 2.4.1             │ &gt;=3.8.1         │ True    │ False            │ False               │ 2023-07-12 06:14:22 │ 2023-07-12 06:14:23 │ 2023-07-12 14:31:41 │         0 │              NULL │ False                 │\n│ espandas │ 1.0.4             │ ~               │ False   │ False            │ False               │ 2018-12-22 20:52:30 │ 2018-12-22 20:52:30 │ 2023-07-12 14:58:47 │         0 │               3.6 │ False                 │\n│ fpandas  │ 0.5               │ ~               │ False   │ False            │ False               │ 2020-03-09 02:35:31 │ 2020-03-09 02:35:31 │ 2023-07-12 15:04:23 │         0 │              NULL │ False                 │\n│ h3pandas │ 0.2.4             │ &gt;=3.6           │ False   │ False            │ False               │ 2023-03-19 17:58:16 │ 2023-03-19 17:58:16 │ 2023-07-12 15:10:06 │         0 │              NULL │ False                 │\n│ ipandas  │ 0.0.1             │ ~               │ False   │ False            │ False               │ 2019-05-29 18:46:12 │ 2019-05-29 18:46:12 │ 2023-07-12 15:15:34 │         0 │               3.6 │ False                 │\n│ kpandas  │ 0.0.1             │ &gt;=3.6,&lt;4.0      │ False   │ False            │ False               │ 2019-05-02 18:00:29 │ 2019-05-02 18:00:31 │ 2023-07-12 15:20:21 │         0 │              NULL │ False                 │\n│ mpandas  │ 0.0.2.1           │ ~               │ False   │ False            │ False               │ 2022-07-03 16:21:21 │ 2022-07-03 16:21:23 │ 2023-07-12 15:30:35 │         0 │              NULL │ False                 │\n│ mtpandas │ 1.14.202306141807 │ &gt;=3.6           │ False   │ False            │ False               │ 2023-06-14 18:08:01 │ 2023-06-14 18:08:01 │ 2023-07-12 15:31:04 │         0 │               4.6 │ False                 │\n│ mypandas │ 0.1.6             │ &gt;=3.10          │ False   │ False            │ False               │ 2022-10-24 21:01:10 │ 2022-10-24 21:01:12 │ 2023-07-12 15:32:04 │         0 │              NULL │ False                 │\n│ paandas  │ 0.0.3             │ ~               │ False   │ False            │ False               │ 2022-11-24 06:11:15 │ 2022-11-24 06:11:17 │ 2023-07-12 15:43:31 │         0 │              NULL │ False                 │\n│ …        │ …                 │ …               │ …       │ …                │ …                   │ …                   │ …                   │ …                   │         … │                 … │ …                     │\n└──────────┴───────────────────┴─────────────────┴─────────┴──────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴─────────────────────┴───────────┴───────────────────┴───────────────────────┘\n\n\n\nLet’s count the results:\n\npandas_ish.count()\n\n\n\n\n\n178\n\n\n\nThere are a good number of packages that look similar to pandas!\n\n\n\nSimilarly we can expose Snowflake’s jarowinkler_similarity function.\nLet’s alias it to jw_sim to illustrate some more of the Ibis udf API:\n\n1@udf.scalar.builtin(name=\"jarowinkler_similarity\")\ndef jw_sim(left: str, right: str) -&gt; float:\n    ...\n\n\n1\n\ntarget is the name of the function in the backend. This argument is required in this because the function name is different than the name of the function in ibis.\n\n\n\n\nNow let’s connect to Snowflake and call our jw_sim function:\n\nimport os\n\ncon = ibis.connect(os.environ[\"SNOWFLAKE_URL\"])\n\n\nexpr = jw_sim(\"snow\", \"shoe\")\ncon.execute(expr)\n\n66.0\n\n\nAnd let’s take a look at the SQL\n\nibis.to_sql(expr, dialect=\"snowflake\")\n\nSELECT\n  JAROWINKLER_SIMILARITY('snow', 'shoe') AS \"jw_sim('snow', 'shoe')\"\n\n\n\n\n\nSometimes the input types of builtin functions are difficult to spell.\nConsider a function that computes the length of any array: the elements in the array can be floats, integers, strings and even other arrays. Spelling that type is difficult.\nFortunately the udf.scalar.builtin decorator doesn’t require you to specify input types in these cases:\n\n@udf.scalar.builtin(name=\"array_size\")\ndef cardinality(arr) -&gt; int:\n   ...\n\n\n\n\n\n\n\nThe return type annotation is always required.\n\n\n\n\n\n\nWe can pass arrays with different element types to our cardinality function:\n\ncon.execute(cardinality([1, 2, 3]))\n\n3\n\n\n\ncon.execute(cardinality([\"a\", \"b\"]))\n\n2\n\n\nWhen you bypass input types the errors you get back are backend dependent:\n\ncon.execute(cardinality(\"foo\"))\n\nProgrammingError: (snowflake.connector.errors.ProgrammingError) 001044 (42P13): SQL compilation error: error line 1 at position 7\nInvalid argument types for function 'ARRAY_SIZE': (VARCHAR(3))\n[SQL: SELECT array_size(%(param_1)s) AS \"cardinality('foo')\"]\n[parameters: {'param_1': 'foo'}]\n(Background on this error at: https://sqlalche.me/e/14/f405)\n\n\nHere, Snowflake is informing us that the ARRAY_SIZE function does not accept strings as input."
  },
  {
    "objectID": "how-to/extending/builtin.html#aggregate-functions",
    "href": "how-to/extending/builtin.html#aggregate-functions",
    "title": "Reference built-in functions",
    "section": "Aggregate functions",
    "text": "Aggregate functions\nAggregate functions that aren’t exposed in ibis directly can be accessed using the @ibis.udf.agg.builtin decorator.\n\n\n\n\n\n\nIbis APIs may already exist for your function.\n\n\n\nBuiltin aggregate UDFs are designed to be an escape hatch when Ibis doesn’t have a defined API for a built-in database function.\nSee the reference documentation for existing APIs.\n\n\nLet’s the use the DuckDB backend to demonstrate how to access an aggregate function that isn’t exposed in ibis: kurtosis.\n\nDuckDB\nFirst, define the builtin aggregate function:\n\n@udf.agg.builtin\n1def kurtosis(x: float) -&gt; float:\n   ...\n\n\n1\n\nBoth the input and return type annotations indicate the element type of the input, not the shape (column or scalar). Aggregations can only be called on column expressions.\n\n\n\n\nOne of the powerful features of this API is that you can define your UD(A)Fs at any point during your analysis. You don’t need to connect to the database to define your functions.\nLet’s compute the kurtosis of the number of votes across all movies:\n\nfrom ibis import _\n\nexpr = (\n   ibis.examples.imdb_title_ratings.fetch()\n   .rename(\"snake_case\")\n   .agg(kurt=lambda t: kurtosis(t.num_votes))\n)\nexpr\n\n┏━━━━━━━━━━━━━┓\n┃ kurt        ┃\n┡━━━━━━━━━━━━━┩\n│ float64     │\n├─────────────┤\n│ 4545.349906 │\n└─────────────┘\n\n\n\nSince this is an aggregate function, it has the same capabilities as other, builtin aggregates like sum: it can be used in a group by as well as in a window function expression.\nLet’s compute kurtosis for all the different types of productions (shorts, movies, TV, etc):\n\nbasics = (\n   ibis.examples.imdb_title_basics.fetch()\n   .rename(\"snake_case\")\n   .filter(_.is_adult == 0)\n)\nratings = ibis.examples.imdb_title_ratings.fetch().rename(\"snake_case\")\n\nbasics_ratings = ratings.join(basics, \"tconst\")\n\nexpr = (\n   basics_ratings.group_by(\"title_type\")\n   .agg(kurt=lambda t: kurtosis(t.num_votes))\n   .order_by(_.kurt.desc())\n   .head()\n)\nexpr\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ title_type   ┃ kurt        ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string       │ float64     │\n├──────────────┼─────────────┤\n│ tvEpisode    │ 8043.838209 │\n│ tvSeries     │ 4030.938238 │\n│ short        │ 3645.730119 │\n│ tvMiniSeries │ 1901.614316 │\n│ tvMovie      │ 1316.403908 │\n└──────────────┴─────────────┘\n\n\n\nSimilarly for window functions:\n\nexpr = (\n   basics_ratings.mutate(\n      kurt=lambda t: kurtosis(t.num_votes).over(group_by=\"title_type\")\n   )\n   .relocate(\"kurt\", after=\"tconst\")\n   .filter(\n      [\n         _.original_title.lower().contains(\"godfather\"),\n         _.title_type == \"movie\",\n         _.genres.contains(\"Crime\") & _.genres.contains(\"Drama\"),\n      ]\n   )\n)\nexpr\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ tconst     ┃ kurt        ┃ average_rating ┃ num_votes ┃ title_type ┃ primary_title          ┃ original_title         ┃ is_adult ┃ start_year ┃ end_year ┃ runtime_minutes ┃ genres             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string     │ float64     │ float64        │ int64     │ string     │ string                 │ string                 │ int64    │ int64      │ string   │ int64           │ string             │\n├────────────┼─────────────┼────────────────┼───────────┼────────────┼────────────────────────┼────────────────────────┼──────────┼────────────┼──────────┼─────────────────┼────────────────────┤\n│ tt0250404  │ 1090.363856 │            6.5 │       244 │ movie      │ Godfather              │ Godfather              │        0 │       1992 │ NULL     │            NULL │ Crime,Drama        │\n│ tt0458027  │ 1090.363856 │            3.7 │        27 │ movie      │ Mumbai Godfather       │ Mumbai Godfather       │        0 │       2005 │ NULL     │            NULL │ Action,Crime,Drama │\n│ tt0068646  │ 1090.363856 │            9.2 │   1945537 │ movie      │ The Godfather          │ The Godfather          │        0 │       1972 │ NULL     │             175 │ Crime,Drama        │\n│ tt0071562  │ 1090.363856 │            9.0 │   1321642 │ movie      │ The Godfather Part II  │ The Godfather Part II  │        0 │       1974 │ NULL     │             202 │ Crime,Drama        │\n│ tt0074412  │ 1090.363856 │            5.2 │      1733 │ movie      │ Disco Godfather        │ Disco Godfather        │        0 │       1979 │ NULL     │              98 │ Action,Crime,Drama │\n│ tt0099674  │ 1090.363856 │            7.6 │    412936 │ movie      │ The Godfather Part III │ The Godfather Part III │        0 │       1990 │ NULL     │             162 │ Crime,Drama        │\n│ tt13130308 │ 1090.363856 │            5.2 │      7303 │ movie      │ Godfather              │ Godfather              │        0 │       2022 │ NULL     │             157 │ Action,Crime,Drama │\n└────────────┴─────────────┴────────────────┴───────────┴────────────┴────────────────────────┴────────────────────────┴──────────┴────────────┴──────────┴─────────────────┴────────────────────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html",
    "href": "how-to/analytics/basics.html",
    "title": "Basic analytics",
    "section": "",
    "text": "Assuming you have a table:\n1import ibis\nimport ibis.selectors as s\n\n2ibis.options.interactive = True\n\n3t = ibis.examples.penguins.fetch()\n4t.head(3)\n\n\n1\n\nEnsure you install Ibis first.\n\n2\n\nUse interactive mode for exploratory data analysis (EDA) or demos.\n\n3\n\nLoad a dataset from the built-in examples.\n\n4\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\nYou can perform basic analytics by selecting, grouping, aggregating, filtering, sorting, mutating, and joining data."
  },
  {
    "objectID": "how-to/analytics/basics.html#selecting",
    "href": "how-to/analytics/basics.html#selecting",
    "title": "Basic analytics",
    "section": "Selecting",
    "text": "Selecting\nUse the .select() method to select columns:\n\nt.select(\"species\", \"island\", \"year\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#filtering",
    "href": "how-to/analytics/basics.html#filtering",
    "title": "Basic analytics",
    "section": "Filtering",
    "text": "Filtering\nUse the .filter() method to filter rows:\n\nt.filter(t[\"species\"] != \"Adelie\")\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n│ Gentoo  │ Biscoe │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           48.7 │          14.1 │               210 │        4450 │ female │  2007 │\n│ Gentoo  │ Biscoe │           50.0 │          15.2 │               218 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           47.6 │          14.5 │               215 │        5400 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           46.5 │          13.5 │               210 │        4550 │ female │  2007 │\n│ Gentoo  │ Biscoe │           45.4 │          14.6 │               211 │        4800 │ female │  2007 │\n│ Gentoo  │ Biscoe │           46.7 │          15.3 │               219 │        5200 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           43.3 │          13.4 │               209 │        4400 │ female │  2007 │\n│ Gentoo  │ Biscoe │           46.8 │          15.4 │               215 │        5150 │ male   │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#aggregating",
    "href": "how-to/analytics/basics.html#aggregating",
    "title": "Basic analytics",
    "section": "Aggregating",
    "text": "Aggregating\nUse the .aggregate() method to aggregate data:\n\nt.aggregate(avg_bill_length=t[\"bill_length_mm\"].mean())\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ avg_bill_length ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ float64         │\n├─────────────────┤\n│        43.92193 │\n└─────────────────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#grouping",
    "href": "how-to/analytics/basics.html#grouping",
    "title": "Basic analytics",
    "section": "Grouping",
    "text": "Grouping\nUse the .group_by() method to group data:\n\nt.group_by([\"species\", \"island\"]).aggregate(avg_bill_length=t[\"bill_length_mm\"].mean())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64         │\n├───────────┼───────────┼─────────────────┤\n│ Adelie    │ Torgersen │       38.950980 │\n│ Adelie    │ Biscoe    │       38.975000 │\n│ Adelie    │ Dream     │       38.501786 │\n│ Gentoo    │ Biscoe    │       47.504878 │\n│ Chinstrap │ Dream     │       48.833824 │\n└───────────┴───────────┴─────────────────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#ordering",
    "href": "how-to/analytics/basics.html#ordering",
    "title": "Basic analytics",
    "section": "Ordering",
    "text": "Ordering\nUse the order_by() method to order data:\n\nt.order_by(t[\"bill_length_mm\"].desc())\n\n┏━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string    │ string │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├───────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo    │ Biscoe │           59.6 │          17.0 │               230 │        6050 │ male   │  2007 │\n│ Chinstrap │ Dream  │           58.0 │          17.8 │               181 │        3700 │ female │  2007 │\n│ Gentoo    │ Biscoe │           55.9 │          17.0 │               228 │        5600 │ male   │  2009 │\n│ Chinstrap │ Dream  │           55.8 │          19.8 │               207 │        4000 │ male   │  2009 │\n│ Gentoo    │ Biscoe │           55.1 │          16.0 │               230 │        5850 │ male   │  2009 │\n│ Gentoo    │ Biscoe │           54.3 │          15.7 │               231 │        5650 │ male   │  2008 │\n│ Chinstrap │ Dream  │           54.2 │          20.8 │               201 │        4300 │ male   │  2008 │\n│ Chinstrap │ Dream  │           53.5 │          19.9 │               205 │        4500 │ male   │  2008 │\n│ Gentoo    │ Biscoe │           53.4 │          15.8 │               219 │        5500 │ male   │  2009 │\n│ Chinstrap │ Dream  │           52.8 │          20.0 │               205 │        4550 │ male   │  2008 │\n│ …         │ …      │              … │             … │                 … │           … │ …      │     … │\n└───────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#mutating",
    "href": "how-to/analytics/basics.html#mutating",
    "title": "Basic analytics",
    "section": "Mutating",
    "text": "Mutating\nUse the .mutate() method to create new columns:\n\nt.mutate(bill_length_cm=t[\"bill_length_mm\"] / 10).relocate(\n    t.columns[0:2], \"bill_length_cm\"\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_cm ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           3.91 │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           3.95 │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           4.03 │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           3.67 │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           3.93 │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           3.89 │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           3.92 │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           3.41 │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           4.20 │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#joining",
    "href": "how-to/analytics/basics.html#joining",
    "title": "Basic analytics",
    "section": "Joining",
    "text": "Joining\nUse the .join() method to join data:\n\nt.join(t, t[\"species\"] == t[\"species\"], how=\"left_semi\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/analytics/basics.html#combining-it-all-together",
    "href": "how-to/analytics/basics.html#combining-it-all-together",
    "title": "Basic analytics",
    "section": "Combining it all together",
    "text": "Combining it all together\nWe can use the underscore to chain expressions together.\n\nt.join(t, t[\"species\"] == t[\"species\"], how=\"left_semi\").filter(\n    ibis._[\"species\"] != \"Adelie\"\n).group_by([\"species\", \"island\"]).aggregate(\n    avg_bill_length=ibis._[\"bill_length_mm\"].mean()\n).order_by(\n    ibis._[\"avg_bill_length\"].desc()\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island ┃ avg_bill_length ┃\n┡━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string    │ string │ float64         │\n├───────────┼────────┼─────────────────┤\n│ Chinstrap │ Dream  │       48.833824 │\n│ Gentoo    │ Biscoe │       47.504878 │\n└───────────┴────────┴─────────────────┘\n\n\n\nSince we’ve turned on interactive mode here, this executes the query and displays the result."
  },
  {
    "objectID": "how-to/visualization/seaborn.html",
    "href": "how-to/visualization/seaborn.html",
    "title": "seaborn + Ibis",
    "section": "",
    "text": "If you don’t have data to visualize, you can load an example table:\nCode\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.head(3)\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/visualization/seaborn.html#using-seaborn-with-ibis",
    "href": "how-to/visualization/seaborn.html#using-seaborn-with-ibis",
    "title": "seaborn + Ibis",
    "section": "Using seaborn with Ibis",
    "text": "Using seaborn with Ibis\nRefer to the seaborn documentation. matplotlib has not implemented the dataframe interchange protocol so it is recommended to call to_pandas() on the Ibis table before plotting.\n\nimport seaborn as sns\n\nsns.barplot(\n    t.group_by(\"species\").aggregate(count=ibis._.count()).to_pandas(),\n    x=\"species\",\n    y=\"count\",\n)\n\n&lt;Axes: xlabel='species', ylabel='count'&gt;"
  },
  {
    "objectID": "how-to/visualization/matplotlib.html",
    "href": "how-to/visualization/matplotlib.html",
    "title": "matplotlib + Ibis",
    "section": "",
    "text": "If you don’t have data to visualize, you can load an example table:\nCode\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.head(3)\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/visualization/matplotlib.html#using-matplotlib-with-ibis",
    "href": "how-to/visualization/matplotlib.html#using-matplotlib-with-ibis",
    "title": "matplotlib + Ibis",
    "section": "Using matplotlib with Ibis",
    "text": "Using matplotlib with Ibis\nRefer to the matplotlib documentation. matplotlib has not implemented the dataframe interchange protocol so it is recommended to call to_pandas() on the Ibis table before plotting.\n\nimport matplotlib.pyplot as plt\n\ngrouped = t.group_by(\"species\").aggregate(count=ibis._.count())\ngrouped = grouped.mutate(row_number=ibis.row_number().over()).select(\n    \"row_number\",\n    (\n        ~s.c(\"row_number\") & s.all()\n    ),  # see https://github.com/ibis-project/ibis/issues/6803\n)\ngrouped\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ row_number ┃ species   ┃ count ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ int64      │ string    │ int64 │\n├────────────┼───────────┼───────┤\n│          0 │ Adelie    │   152 │\n│          1 │ Chinstrap │    68 │\n│          2 │ Gentoo    │   124 │\n└────────────┴───────────┴───────┘\n\n\n\n\n# https://stackoverflow.com/questions/9101497/matplotlib-bar-graph-x-axis-wont-plot-string-values\nplt.figure(figsize=(6, 4))\nplt.bar(grouped[\"row_number\"].to_pandas(), grouped[\"count\"].to_pandas())\nplt.title(\"Penguin species counts\")\nplt.xlabel(\"Species\")\nplt.xticks(grouped[\"row_number\"].to_pandas(), grouped[\"species\"].to_pandas())\nplt.ylabel(\"Count\")\nplt.show()"
  },
  {
    "objectID": "how-to/visualization/streamlit.html",
    "href": "how-to/visualization/streamlit.html",
    "title": "Streamlit + Ibis",
    "section": "",
    "text": "Ibis supports the streamlit experimental_connection interface, making it easier than ever to combine the powers of both tools!\nCheck out the example application below that shows the top N ingredients from a corpus of recipes using the ClickHouse backend!\n\n\n\nAnd here’s the source code for the application:\nfrom __future__ import annotations\n\nimport requests\nimport streamlit as st\n\nfrom ibis import _\nfrom ibis.streamlit import IbisConnection\n\nst.set_page_config(page_title=\"Yummy Data\", layout=\"wide\")\nst.title(\"Yummy Data :bacon:\")\n\n\n@st.cache_data\ndef get_emoji():\n    resp = requests.get(\n        \"https://raw.githubusercontent.com/omnidan/node-emoji/master/lib/emoji.json\"\n    )\n    resp.raise_for_status()\n    emojis = resp.json()\n    return emojis\n\n\noptions = [1, 5, 10, 25, 50, 100]\n\n\n@st.cache_data\ndef query():\n    return (\n        con.tables.recipes.relabel(\"snake_case\")\n        .mutate(ner=_.ner.map(lambda n: n.lower()).unnest())\n        .ner.topk(max(options))\n        .relabel(dict(ner=\"ingredient\"))\n        .to_pandas()\n        .assign(\n            emoji=lambda df: df.ingredient.map(\n                lambda emoji: f\"{emojis.get(emoji, '-')}\"\n            )\n        )\n        .set_index(\"ingredient\")\n    )\n\n\nemojis = get_emoji()\n\ncon = st.experimental_connection(\"ch\", type=IbisConnection)\n\nif n := st.radio(\"Ingredients\", options, index=1, horizontal=True):\n    table, whole = st.columns((2, 1))\n    idx = options.index(n)\n    k = 0\n    base = query()\n    for m in options[: idx + 1]:\n        df = base.iloc[k:m]\n        if not k:\n            word = \"first\"\n        elif m &lt; n:\n            word = \"next\"\n        else:\n            word = \"last\"\n\n        uniq_emojis = \" \".join(df.emoji[df.emoji != \"-\"].unique())\n        table.header(f\"{word.title()} {m - k:d}\")\n        table.subheader(uniq_emojis)\n\n        table.dataframe(df, use_container_width=True)\n        k = m\n\n    b = base.iloc[:n]\n    uniq_emojis = \" \".join(b.emoji[b.emoji != \"-\"].unique())\n    whole.header(f\"Top {n:d}\")\n    whole.subheader(uniq_emojis)\n    whole.dataframe(b, use_container_width=True)\n\n\n\n Back to top"
  },
  {
    "objectID": "how-to/input-output/basics.html",
    "href": "how-to/input-output/basics.html",
    "title": "Basic input/output",
    "section": "",
    "text": "If you don’t have your own data, you can load example data from the ibis.examples module:\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()"
  },
  {
    "objectID": "how-to/input-output/basics.html#overview",
    "href": "how-to/input-output/basics.html#overview",
    "title": "Basic input/output",
    "section": "Overview",
    "text": "Overview\nIbis is typically used with a backend that already contains tables, but can import and export data in various formats."
  },
  {
    "objectID": "how-to/input-output/basics.html#data-platforms",
    "href": "how-to/input-output/basics.html#data-platforms",
    "title": "Basic input/output",
    "section": "Data platforms",
    "text": "Data platforms\nYou can connect Ibis to any supported backend to read and write data in backend-native tables.\n\n\nCode\ncon = ibis.duckdb.connect(\"penguins.ddb\")\nt = con.create_table(\"penguins\", t.to_pyarrow(), overwrite=True)\n\n\n\n1con = ibis.duckdb.connect(\"penguins.ddb\")\n2t = con.table(\"penguins\")\n3t.head(3)\n\n\n1\n\nConnect to a backend.\n\n2\n\nLoad a table.\n\n3\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n1grouped = (\n    t.group_by([\"species\", \"island\"])\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n2con.create_table(\"penguins_grouped\", grouped.to_pyarrow(), overwrite=True)\n\n\n1\n\nCreate a lazily evaluated Ibis expression.\n\n2\n\nWrite to a table.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ island    ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ string    │ int64 │\n├───────────┼───────────┼───────┤\n│ Gentoo    │ Biscoe    │   124 │\n│ Chinstrap │ Dream     │    68 │\n│ Adelie    │ Dream     │    56 │\n│ Adelie    │ Torgersen │    52 │\n│ Adelie    │ Biscoe    │    44 │\n└───────────┴───────────┴───────┘"
  },
  {
    "objectID": "how-to/input-output/basics.html#file-formats",
    "href": "how-to/input-output/basics.html#file-formats",
    "title": "Basic input/output",
    "section": "File formats",
    "text": "File formats\nDepending on the backend, you can read and write data in several file formats.\n\nCSVDelta LakeParquet\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_csv(\"penguins.csv\")\n2ibis.read_csv(\"penguins.csv\").head(3)\n\n\n1\n\nWrite the table to a CSV file. Dependent on backend.\n\n2\n\nRead the CSV file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb,deltalake]'\n\n1t.to_delta(\"penguins.delta\", mode=\"overwrite\")\n2ibis.read_delta(\"penguins.delta\").head(3)\n\n\n1\n\nWrite the table to a Delta Lake table. Dependent on backend.\n\n2\n\nRead the Delta Lake table into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\npip install 'ibis-framework[duckdb]'\n\n1t.to_parquet(\"penguins.parquet\")\n2ibis.read_parquet(\"penguins.parquet\").head(3)\n\n\n1\n\nWrite the table to a Parquet file. Dependent on backend.\n\n2\n\nRead the Parquet file into a table. Dependent on backend.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/input-output/basics.html#with-other-python-libraries",
    "href": "how-to/input-output/basics.html#with-other-python-libraries",
    "title": "Basic input/output",
    "section": "With other Python libraries",
    "text": "With other Python libraries\nIbis uses Apache Arrow for efficient data transfer to and from other libraries. Ibis tables implement the __dataframe__ and __array__ protocols, so you can pass them to any library that supports these protocols.\n\npandaspolarspyarrowtorch__dataframe____array__\n\n\nYou can convert Ibis tables to pandas dataframes.\npip install pandas\n\n1df = t.to_pandas()\ndf.head(3)\n\n\n1\n\nReturns a pandas dataframe.\n\n\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n\n\n\n\n\nOr you can convert pandas dataframes to Ibis tables.\n\n1t = ibis.memtable(df)\nt.head(3)\n\n\n1\n\nReturns an Ibis table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │             195.0 │      3250.0 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to Polars dataframes.\npip install polars\n\nimport polars as pl\n\ndf = pl.from_arrow(t.to_pyarrow())\ndf.head(3)\n\n\nshape: (3, 8)\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\nstr\nstr\nf64\nf64\nf64\nf64\nstr\ni64\n\n\n\n\n\"Adelie\"\n\"Torgersen\"\n39.1\n18.7\n181.0\n3750.0\n\"male\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n39.5\n17.4\n186.0\n3800.0\n\"female\"\n2007\n\n\n\"Adelie\"\n\"Torgersen\"\n40.3\n18.0\n195.0\n3250.0\n\"female\"\n2007\n\n\n\n\n\n\nOr Polars dataframes to Ibis tables.\n\nt = ibis.memtable(df)\nt.head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to PyArrow tables.\npip install pyarrow\n\nt.to_pyarrow()\n\npyarrow.Table\ncol0: string\ncol1: string\ncol2: double\ncol3: double\ncol4: double\ncol5: double\ncol6: string\ncol7: int64\n----\ncol0: [[\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",\"Adelie\",...,\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\",\"Chinstrap\"]]\ncol1: [[\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",\"Torgersen\",...,\"Dream\",\"Dream\",\"Dream\",\"Dream\",\"Dream\"]]\ncol2: [[39.1,39.5,40.3,null,36.7,...,55.8,43.5,49.6,50.8,50.2]]\ncol3: [[18.7,17.4,18,null,19.3,...,19.8,18.1,18.2,19,18.7]]\ncol4: [[181,186,195,null,193,...,207,202,193,210,198]]\ncol5: [[3750,3800,3250,null,3450,...,4000,3400,3775,4100,3775]]\ncol6: [[\"male\",\"female\",\"female\",null,\"female\",...,\"male\",\"female\",\"male\",\"male\",\"female\"]]\ncol7: [[2007,2007,2007,2007,2007,...,2009,2009,2009,2009,2009]]\n\n\nOr PyArrow batches:\n\nt.to_pyarrow_batches()\n\n&lt;pyarrow.lib.RecordBatchReader at 0x731983d8f840&gt;\n\n\nAnd you can convert PyArrow tables to Ibis tables.\n\nibis.memtable(t.to_pyarrow()).head(3)\n\n┏━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ col0   ┃ col1      ┃ col2    ┃ col3    ┃ col4    ┃ col5    ┃ col6   ┃ col7  ┃\n┡━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string │ string    │ float64 │ float64 │ float64 │ float64 │ string │ int64 │\n├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼────────┼───────┤\n│ Adelie │ Torgersen │    39.1 │    18.7 │   181.0 │  3750.0 │ male   │  2007 │\n│ Adelie │ Torgersen │    39.5 │    17.4 │   186.0 │  3800.0 │ female │  2007 │\n│ Adelie │ Torgersen │    40.3 │    18.0 │   195.0 │  3250.0 │ female │  2007 │\n└────────┴───────────┴─────────┴─────────┴─────────┴─────────┴────────┴───────┘\n\n\n\n\n\nYou can convert Ibis tables to torch tensors.\npip install torch\nt.select(s.numeric()).limit(3).to_torch()\n{'col2': tensor([39.1000, 39.5000, 40.3000], dtype=torch.float64),\n 'col3': tensor([18.7000, 17.4000, 18.0000], dtype=torch.float64),\n 'col4': tensor([181., 186., 195.], dtype=torch.float64),\n 'col5': tensor([3750., 3800., 3250.], dtype=torch.float64),\n 'col7': tensor([2007, 2007, 2007], dtype=torch.int16)}\n\n\nYou can directly call the __dataframe__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__dataframe__()\n\n&lt;ibis.expr.types.dataframe_interchange.IbisDataFrame at 0x73190a11c2b0&gt;\n\n\n\n\nYou can directly call the __array__ protocol on Ibis tables, though this is typically handled by the library you’re using.\n\nt.__array__()\n\narray([['Adelie', 'Torgersen', 39.1, ..., 3750.0, 'male', 2007],\n       ['Adelie', 'Torgersen', 39.5, ..., 3800.0, 'female', 2007],\n       ['Adelie', 'Torgersen', 40.3, ..., 3250.0, 'female', 2007],\n       ...,\n       ['Chinstrap', 'Dream', 49.6, ..., 3775.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.8, ..., 4100.0, 'male', 2009],\n       ['Chinstrap', 'Dream', 50.2, ..., 3775.0, 'female', 2009]],\n      dtype=object)"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to Ibis",
    "section": "",
    "text": "Contributing to Ibis\nWe love new contributors!\nTo get started:\n\nSet up a development environment\nLearn about the commit workflow\nReview the code style guidelines\nDig into the nitty gritty of being a maintainer\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "how-to/input-output/multiple-backends.html",
    "href": "how-to/input-output/multiple-backends.html",
    "title": "Work with multiple backends",
    "section": "",
    "text": "You can work with multiple backends by creating and using separate connections."
  },
  {
    "objectID": "how-to/input-output/multiple-backends.html#local-example",
    "href": "how-to/input-output/multiple-backends.html#local-example",
    "title": "Work with multiple backends",
    "section": "Local example",
    "text": "Local example\nWe’ll use some of the local backends to demonstrate, but this applies to any backends.\n\nimport ibis\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.to_parquet(\"penguins.parquet\")\nt.head(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nYou can create a connection or several:\n\nddb_con = ibis.duckdb.connect()\nddb_con2 = ibis.duckdb.connect()\n\nYou can use the connection to create a table:\n\nddb_con.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\nddb_con2.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nOr different backends:\n\npl_con = ibis.polars.connect()\npl_con2 = ibis.polars.connect()\n\n\npl_con.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\npl_con2.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nOr a different backend:\n\ndf_con = ibis.datafusion.connect()\ndf_con2 = ibis.datafusion.connect()\n\n\ndf_con.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\ndf_con2.read_parquet(\"penguins.parquet\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/input-output/multiple-backends.html#next-steps",
    "href": "how-to/input-output/multiple-backends.html#next-steps",
    "title": "Work with multiple backends",
    "section": "Next steps",
    "text": "Next steps\nAfter connecting to multiple backends, use them like normal! You can check out input and output formats, including other Python dataframes for more information on how to get data in and out of backends."
  },
  {
    "objectID": "how-to/visualization/plotnine.html",
    "href": "how-to/visualization/plotnine.html",
    "title": "plotnine + Ibis",
    "section": "",
    "text": "If you don’t have data to visualize, you can load an example table:\nCode\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.head(3)\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/visualization/plotnine.html#using-plotnine-with-ibis",
    "href": "how-to/visualization/plotnine.html#using-plotnine-with-ibis",
    "title": "plotnine + Ibis",
    "section": "Using plotnine with Ibis",
    "text": "Using plotnine with Ibis\nRefer to the plotnine documentation. You can pass in Ibis tables or expressions:\n\nfrom plotnine import ggplot, aes, geom_bar, theme\n\nchart = (\n    ggplot(\n        t.group_by(\"species\").agg(count=ibis._.count()),\n        aes(x=\"species\", y=\"count\"),\n    )\n    + geom_bar(stat=\"identity\")\n    + theme(figure_size=(6, 4))\n)\nchart\n\n\n\n\n&lt;Figure Size: (600 x 400)&gt;"
  },
  {
    "objectID": "how-to/visualization/altair.html",
    "href": "how-to/visualization/altair.html",
    "title": "Altair + Ibis",
    "section": "",
    "text": "If you don’t have data to visualize, you can load an example table:\nCode\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.head(3)\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/visualization/altair.html#using-altair-with-ibis",
    "href": "how-to/visualization/altair.html#using-altair-with-ibis",
    "title": "Altair + Ibis",
    "section": "Using Altair with Ibis",
    "text": "Using Altair with Ibis\nRefer to the Altair documentation. You can pass in Ibis tables or expressions:\n\nimport altair as alt\n\nchart = (\n    alt.Chart(t.group_by(\"species\").agg(count=ibis._.count()).to_pandas())\n    .mark_bar()\n    .encode(\n        x=\"species\",\n        y=\"count\",\n        tooltip=[\"species\", \"count\"],\n    )\n    .properties(width=600, height=400)\n    .interactive()\n)\nchart"
  },
  {
    "objectID": "how-to/visualization/plotly.html",
    "href": "how-to/visualization/plotly.html",
    "title": "Plotly + Ibis",
    "section": "",
    "text": "If you don’t have data to visualize, you can load an example table:\nCode\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\n\nt = ibis.examples.penguins.fetch()\nt.head(3)\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "how-to/visualization/plotly.html#using-plotly-with-ibis",
    "href": "how-to/visualization/plotly.html#using-plotly-with-ibis",
    "title": "Plotly + Ibis",
    "section": "Using Plotly with Ibis",
    "text": "Using Plotly with Ibis\nRefer to the Plotly documentation. You can pass in Ibis tables or expressions:\n\nimport plotly.express as px\n\nchart = px.bar(\n    t.group_by(\"species\").agg(count=ibis._.count()).to_pandas(),\n    x=\"species\",\n    y=\"count\",\n    width=600,\n    height=400,\n)\nchart"
  },
  {
    "objectID": "how-to/analytics/chain_expressions.html",
    "href": "how-to/analytics/chain_expressions.html",
    "title": "Chaining expressions",
    "section": "",
    "text": "Expressions can easily be chained using the deferred expression API, also known as the Underscore (_) API.\nIn this guide, we use the _ API to concisely create column expressions and then chain table expressions."
  },
  {
    "objectID": "how-to/analytics/chain_expressions.html#setup",
    "href": "how-to/analytics/chain_expressions.html#setup",
    "title": "Chaining expressions",
    "section": "Setup",
    "text": "Setup\nTo get started, import _ from ibis:\n\nimport ibis\nfrom ibis import _\n\nimport pandas as pd\n\nLet’s create two in-memory tables using [ibis.memtable], an API introduced in 3.2:\n\ndf1 = pd.DataFrame({'x': range(5), 'y': list('ab')*2 + list('e')})\nt1 = ibis.memtable(df1)\n\ndf2 = pd.DataFrame({'x': range(10), 'z': list(reversed(list('ab')*2 + list('e')))*2})\nt2 = ibis.memtable(df2)"
  },
  {
    "objectID": "how-to/analytics/chain_expressions.html#creating-column-expressions",
    "href": "how-to/analytics/chain_expressions.html#creating-column-expressions",
    "title": "Chaining expressions",
    "section": "Creating column expressions",
    "text": "Creating column expressions\nWe can use _ to create new column expressions without explicit reference to the previous table expression:\n\n# We can pass a deferred expression into a function:\ndef modf(t):\n    return t.x % 3\n\nxmod = modf(_)\n\n# We can create ColumnExprs like aggregate expressions:\nymax = _.y.max()\nzmax = _.z.max()\nzct = _.z.count()"
  },
  {
    "objectID": "how-to/analytics/chain_expressions.html#chaining-ibis-expressions",
    "href": "how-to/analytics/chain_expressions.html#chaining-ibis-expressions",
    "title": "Chaining expressions",
    "section": "Chaining Ibis expressions",
    "text": "Chaining Ibis expressions\nWe can also use it to chain Ibis expressions in one Python expression:\n\njoin = (\n    t1\n    # _ is t1\n    .join(t2, _.x == t2.x)\n    # _ is the join result:\n    .mutate(xmod=xmod)\n    # _ is the TableExpression after mutate:\n    .group_by(_.xmod)\n    # `ct` is a ColumnExpression derived from a deferred expression:\n    .aggregate(ymax=ymax, zmax=zmax)\n    # _ is the aggregation result:\n    .filter(_.ymax == _.zmax)\n    # _ is the filtered result, and re-create xmod in t2 using modf:\n    .join(t2, _.xmod == modf(t2))\n    # _ is the second join result:\n    .join(t1, _.xmod == modf(t1))\n    # _ is the third join result:\n    .select(_.x, _.y, _.z)\n    # Finally, _ is the selection result:\n    .order_by(_.x)\n)"
  },
  {
    "objectID": "how-to/extending/reduction.html",
    "href": "how-to/extending/reduction.html",
    "title": "Add a reduction operation",
    "section": "",
    "text": "This notebook will show you how to add a new reduction operation last_date to the existing backend SQLite.\nA reduction operation is a function that maps \\(N\\) rows to 1 row, for example the sum function."
  },
  {
    "objectID": "how-to/extending/reduction.html#description",
    "href": "how-to/extending/reduction.html#description",
    "title": "Add a reduction operation",
    "section": "Description",
    "text": "Description\nWe’re going to add a last_date function to ibis. last_date returns the latest date of a list of dates."
  },
  {
    "objectID": "how-to/extending/reduction.html#step-1-define-the-operation",
    "href": "how-to/extending/reduction.html#step-1-define-the-operation",
    "title": "Add a reduction operation",
    "section": "Step 1: Define the Operation",
    "text": "Step 1: Define the Operation\nLet’s define the last_date operation as a function that takes any date column as input and returns a date:\nfrom __future__ import annotations\n\nfrom datetime import date\n\n\ndef last_date(dates: list[date]) -&gt; date:\n    \"\"\"Latest date.\"\"\"\n\nfrom __future__ import annotations\n\nimport ibis.expr.datatypes as dt\nimport ibis.expr.datashape as ds\nimport ibis.expr.rules as rlz\n\nfrom ibis.expr.operations import Reduction, Value\n\n\nclass LastDate(Reduction):\n    arg: Value[dt.Date, ds.Any]\n    where: Value[dt.Boolean, ds.Any] | None = None\n\n    dtype = rlz.dtype_like(\"arg\")\n    shape = ds.scalar\n\nWe just defined a LastDate class that takes one date column as input, and returns a scalar output of the same type as the input. This matches both the requirements of a reduction and the specifics of the function that we want to implement.\nNote: It is very important that you write the correct argument rules and output type here. The expression will not work otherwise."
  },
  {
    "objectID": "how-to/extending/reduction.html#step-2-define-the-api",
    "href": "how-to/extending/reduction.html#step-2-define-the-api",
    "title": "Add a reduction operation",
    "section": "Step 2: Define the API",
    "text": "Step 2: Define the API\nBecause every reduction in Ibis has the ability to filter out values during aggregation, to make an expression out of LastDate we need to pass an additional argument where to our LastDate constructor.\nAdditionally, reductions should be defined on Column classes because reductions are not always well-defined for a scalar value.\n\nfrom ibis.expr.types import DateColumn\n\n\ndef last_date(date_column, where=None):\n    return LastDate(date_column, where=where).to_expr()\n\n\nDateColumn.last_date = last_date"
  },
  {
    "objectID": "how-to/extending/reduction.html#interlude-create-some-expressions-using-last_date",
    "href": "how-to/extending/reduction.html#interlude-create-some-expressions-using-last_date",
    "title": "Add a reduction operation",
    "section": "Interlude: Create some expressions using last_date",
    "text": "Interlude: Create some expressions using last_date\n\nimport ibis\n\n\npeople = ibis.table(\n    dict(name=\"string\", country=\"string\", date_of_birth=\"date\"),\n    name=\"people\",\n)\n\n\npeople.date_of_birth.last_date()\n\nr0 := UnboundTable: people\n  name          string\n  country       string\n  date_of_birth date\n\nLastDate(date_of_birth): LastDate(r0.date_of_birth)\n\n\n\npeople.date_of_birth.last_date(people.country == \"Indonesia\")\n\nr0 := UnboundTable: people\n  name          string\n  country       string\n  date_of_birth date\n\nLastDate(date_of_birth, Equals(country, 'Indonesia')): LastDate(r0.date_of_birth, where=r0.country == 'Indonesia')"
  },
  {
    "objectID": "how-to/extending/reduction.html#step-3-turn-the-expression-into-sql",
    "href": "how-to/extending/reduction.html#step-3-turn-the-expression-into-sql",
    "title": "Add a reduction operation",
    "section": "Step 3: Turn the Expression into SQL",
    "text": "Step 3: Turn the Expression into SQL\n\nimport sqlalchemy as sa\n\n\n@ibis.sqlite.add_operation(LastDate)\ndef _last_date(translator, expr):\n    # pull out the arguments to the expression\n    op = expr.op()\n\n    arg = op.arg\n    where = op.where\n\n    # compile the argument\n    compiled_arg = translator.translate(arg)\n\n    # call the appropriate SQLite function (`max` for the latest date)\n    agg = sa.func.max(compiled_arg)\n\n    # handle a non-None filter clause\n    if where is not None:\n        return agg.filter(translator.translate(where))\n    return agg"
  },
  {
    "objectID": "how-to/extending/reduction.html#step-4-putting-it-all-together",
    "href": "how-to/extending/reduction.html#step-4-putting-it-all-together",
    "title": "Add a reduction operation",
    "section": "Step 4: Putting it all Together",
    "text": "Step 4: Putting it all Together\nDownload the geography database.\n\n!curl -LsS -o geography.db 'https://storage.googleapis.com/ibis-tutorial-data/geography.db'\n\ncon = ibis.sqlite.connect(\"geography.db\")\n\n\nCreate and execute a bitwise_and expression\n\nind = con.table(\"independence\")\nind\n\nDatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\n\n\nLast country to gain independence in our database:\n\nexpr = ind.independence_date.last_date()\nexpr\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nLastDate(independence_date): LastDate(r0.independence_date)\n\n\n\nibis.to_sql(expr)\n\nSELECT\n  MAX(t0.independence_date) AS \"LastDate(independence_date)\"\nFROM independence AS t0\n\n\nShow the last country to gain independence from the Spanish Empire, using the where parameter:\n\nexpr = ind.independence_date.last_date(\n    where=ind.independence_from == \"Spanish Empire\"\n)\nexpr\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nLastDate(independence_date, Equals(independence_from, 'Spanish Empire')): LastDate(r0.independence_date, where=r0.independence_from == 'Spanish Empire')"
  },
  {
    "objectID": "how-to/extending/elementwise.html",
    "href": "how-to/extending/elementwise.html",
    "title": "Add an elementwise operation",
    "section": "",
    "text": "This notebook will show you how to add a new elementwise operation to an existing backend.\nWe are going to add julianday, a function supported by the SQLite database, to the SQLite Ibis backend.\nThe Julian day of a date, is the number of days since January 1st, 4713 BC. For more information check the Julian day Wikipedia page."
  },
  {
    "objectID": "how-to/extending/elementwise.html#step-1-define-the-operation",
    "href": "how-to/extending/elementwise.html#step-1-define-the-operation",
    "title": "Add an elementwise operation",
    "section": "Step 1: Define the Operation",
    "text": "Step 1: Define the Operation\nLet’s define the julianday operation as a function that takes one string input argument and returns a float.\ndef julianday(date: str) -&gt; float:\n    \"\"\"Return the Julian day from a date.\"\"\"\n\nimport ibis.expr.datatypes as dt\nimport ibis.expr.rules as rlz\nimport ibis.expr.datashape as ds\n\nfrom ibis.expr.operations import Value\n\n\nclass JulianDay(Value):\n    arg: Value[dt.String, ds.Any]\n\n    dtype = dt.float32\n    shape = rlz.shape_like('arg')\n\nWe just defined a JulianDay class that takes one argument of type string or binary, and returns a float."
  },
  {
    "objectID": "how-to/extending/elementwise.html#step-2-define-the-api",
    "href": "how-to/extending/elementwise.html#step-2-define-the-api",
    "title": "Add an elementwise operation",
    "section": "Step 2: Define the API",
    "text": "Step 2: Define the API\nBecause we know the output type of the operation, to make an expression out of JulianDay we can construct it and call its ibis.expr.types.Node.to_expr method.\nWe still need to add a method to StringValue (this needs to work on both scalars and columns).\nWhen you add a method to any of the expression classes whose name matches *Value both the scalar and column child classes will pick it up, making it easy to define operations for both scalars and columns in one place.\nWe can do this by defining a function and assigning it to the appropriate class of expressions.\n\nfrom ibis.expr.types import StringValue\n\n\ndef julianday(string_value):\n    return JulianDay(string_value).to_expr()\n\n\nStringValue.julianday = julianday"
  },
  {
    "objectID": "how-to/extending/elementwise.html#interlude-create-some-expressions-with-julianday",
    "href": "how-to/extending/elementwise.html#interlude-create-some-expressions-with-julianday",
    "title": "Add an elementwise operation",
    "section": "Interlude: Create some expressions with julianday",
    "text": "Interlude: Create some expressions with julianday\n\nimport ibis\n\nt = ibis.table(dict(string_col=\"string\"), name=\"t\")\n\nt.string_col.julianday()\n\nr0 := UnboundTable: t\n  string_col string\n\nJulianDay(string_col): JulianDay(r0.string_col)"
  },
  {
    "objectID": "how-to/extending/elementwise.html#step-3-turn-the-expression-into-sql",
    "href": "how-to/extending/elementwise.html#step-3-turn-the-expression-into-sql",
    "title": "Add an elementwise operation",
    "section": "Step 3: Turn the Expression into SQL",
    "text": "Step 3: Turn the Expression into SQL\n\nimport sqlalchemy as sa\n\n\n@ibis.sqlite.add_operation(JulianDay)\ndef _julianday(translator, expr):\n    # pull out the arguments to the expression\n    (arg,) = expr.args\n\n    # compile the argument\n    compiled_arg = translator.translate(arg)\n\n    # return a SQLAlchemy expression that calls into the SQLite julianday function\n    return sa.func.julianday(compiled_arg)"
  },
  {
    "objectID": "how-to/extending/elementwise.html#step-4-putting-it-all-together",
    "href": "how-to/extending/elementwise.html#step-4-putting-it-all-together",
    "title": "Add an elementwise operation",
    "section": "Step 4: Putting it all Together",
    "text": "Step 4: Putting it all Together\nDownload the geography database.\n\n!curl -LsS -o geography.db 'https://storage.googleapis.com/ibis-tutorial-data/geography.db'\n\ncon = ibis.sqlite.connect(\"geography.db\")\n\n\nCreate and execute a julianday expression\n\nind = con.table(\"independence\")\nind\n\nDatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\n\n\n\nday = ind.independence_date.cast(\"string\")\nday\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\nCast(independence_date, string): Cast(r0.independence_date, to=string)\n\n\n\n\njday_expr = day.julianday().name(\"jday\")\njday_expr\n\nr0 := DatabaseTable: independence\n  country_code      string\n  independence_date date\n  independence_from string\n\njday: JulianDay(Cast(r0.independence_date, to=string))\n\n\n\n\nibis.to_sql(jday_expr)\n\nSELECT\n  JULIANDAY(CAST(t0.independence_date AS TEXT)) AS jday\nFROM independence AS t0\n\n\nBecause we’ve defined our operation on StringValue, and not just on StringColumn we get operations on both string scalars and string columns for free.\n\njday = ibis.literal(\"2010-03-14\").julianday()\ncon.execute(jday)\n\n2455269.5"
  },
  {
    "objectID": "how-to/timeseries/sessionize.html",
    "href": "how-to/timeseries/sessionize.html",
    "title": "Sessionize a log of events",
    "section": "",
    "text": "Suppose you have entities (users, objects, actions, etc) that have event logs through polling or event triggers.\nYou might be interested in partitioning these logs by something called sessions, which can be defined as groups of consecutive event records without long interruptions for a given entity.\nIn the case of a user portal, it might be grouping the navigation events that result in completing a task or buying a product. For online games, it might be a the grouping of activity events of a given user playing the game while remaining logged in.\nSessionization can also be useful on longer time scales, for instance to reconstruct active subscription data from a raw payment or activity log, so as to model customer churn.\nThis guide on sessionization is inspired by The Expressions API in Polars is Amazing, a blog post in the Polars community demonstrating the strength of Polars expressions.\n\n\nFor this example, we use an activity log from the online game “World of Warcraft” with more than 10 million records for 37,354 unique players made available under the CC0 / Public Domain license. A copy of the data can be found at https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet (75 MB) under the parquet format to reduce load times. You can use ibis.read_parquet to quickly get it into a table expression via the default DuckDB backend.\nThis data contains the following fields:\n\nchar : a unique identifier for a character (or a player). This is our entity column.\ntimestamp: a timestamp denoting when a char was polled. This occurs every ~10 minutes.\n\nWe can take this information, along with a definition of what separates two sessions for an entity, and break our dataset up into sessions without using any joins:\n# Imports\nimport ibis\nfrom ibis import deferred as c\n\n# Read files into table expressions with ibis.read_parquet:\ndata = ibis.read_parquet(\n    \"https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet\"\n)\n\n# Integer delay in seconds noting if a row should be included in the previous\n# session for an entity.\nsession_boundary_threshold = 30 * 60\n\n# Window for finding session ids per character\nentity_window = ibis.cumulative_window(group_by=c.char, order_by=c.timestamp)\n\n# Take the previous timestamp within a window (by character ordered by timestamp):\n# Note: the first value in a window will be null.\nts_lag = c.timestamp.lag().over(entity_window)\n\n# Subtract the lag from the current timestamp to get a timedelta.\nts_delta = c.timestamp - ts_lag\n\n# Compare timedelta to our session delay in seconds to determine if the\n# current timestamp falls outside of the session.\n# Cast as int for aggregation.\nis_new_session = (ts_delta &gt; ibis.interval(seconds=session_boundary_threshold))\n\n# Window to compute session min/max and duration.\nsession_window = ibis.window(group_by=[c.char, c.session_id])\n\n# Generate all of the data we need to analyze sessions:\nsessionized = (\n    data\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(new_session=is_new_session.fillna(True))\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(session_id=c.new_session.sum().over(entity_window))\n    # Drop `new_session` because it is no longer needed.\n    .drop(\"new_session\")\n    .mutate(\n        # Get session duration using max(timestamp) - min(timestamp) over our window.\n        session_duration=c.timestamp.max().over(session_window) - c.timestamp.min().over(session_window)\n    )\n    # Sort for convenience.\n    .order_by([c.char, c.timestamp])\n)\nCalling ibis.show_sql(sessionized) displays the SQL query and can be used to confirm that this Ibis table expression does not rely on any join operations.\nCalling sessionized.to_pandas() should complete in less than a minute, depending on the speed of the internet connection to download the data and the number of CPU cores available to parallelize the processing of this nested query."
  },
  {
    "objectID": "how-to/timeseries/sessionize.html#sessionizing-logs-on-a-cadence",
    "href": "how-to/timeseries/sessionize.html#sessionizing-logs-on-a-cadence",
    "title": "Sessionize a log of events",
    "section": "",
    "text": "For this example, we use an activity log from the online game “World of Warcraft” with more than 10 million records for 37,354 unique players made available under the CC0 / Public Domain license. A copy of the data can be found at https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet (75 MB) under the parquet format to reduce load times. You can use ibis.read_parquet to quickly get it into a table expression via the default DuckDB backend.\nThis data contains the following fields:\n\nchar : a unique identifier for a character (or a player). This is our entity column.\ntimestamp: a timestamp denoting when a char was polled. This occurs every ~10 minutes.\n\nWe can take this information, along with a definition of what separates two sessions for an entity, and break our dataset up into sessions without using any joins:\n# Imports\nimport ibis\nfrom ibis import deferred as c\n\n# Read files into table expressions with ibis.read_parquet:\ndata = ibis.read_parquet(\n    \"https://storage.googleapis.com/ibis-tutorial-data/wowah_data/wowah_data_raw.parquet\"\n)\n\n# Integer delay in seconds noting if a row should be included in the previous\n# session for an entity.\nsession_boundary_threshold = 30 * 60\n\n# Window for finding session ids per character\nentity_window = ibis.cumulative_window(group_by=c.char, order_by=c.timestamp)\n\n# Take the previous timestamp within a window (by character ordered by timestamp):\n# Note: the first value in a window will be null.\nts_lag = c.timestamp.lag().over(entity_window)\n\n# Subtract the lag from the current timestamp to get a timedelta.\nts_delta = c.timestamp - ts_lag\n\n# Compare timedelta to our session delay in seconds to determine if the\n# current timestamp falls outside of the session.\n# Cast as int for aggregation.\nis_new_session = (ts_delta &gt; ibis.interval(seconds=session_boundary_threshold))\n\n# Window to compute session min/max and duration.\nsession_window = ibis.window(group_by=[c.char, c.session_id])\n\n# Generate all of the data we need to analyze sessions:\nsessionized = (\n    data\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(new_session=is_new_session.fillna(True))\n    # Create a session id for each character by using a cumulative sum\n    # over the `new_session` column.\n    .mutate(session_id=c.new_session.sum().over(entity_window))\n    # Drop `new_session` because it is no longer needed.\n    .drop(\"new_session\")\n    .mutate(\n        # Get session duration using max(timestamp) - min(timestamp) over our window.\n        session_duration=c.timestamp.max().over(session_window) - c.timestamp.min().over(session_window)\n    )\n    # Sort for convenience.\n    .order_by([c.char, c.timestamp])\n)\nCalling ibis.show_sql(sessionized) displays the SQL query and can be used to confirm that this Ibis table expression does not rely on any join operations.\nCalling sessionized.to_pandas() should complete in less than a minute, depending on the speed of the internet connection to download the data and the number of CPU cores available to parallelize the processing of this nested query."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nIbis versus X: Performance across the ecosystem part 2\n\n\n\nblog\n\n\ncase study\n\n\necosystem\n\n\nperformance\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nDec 11, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis + DuckDB geospatial: a match made on Earth\n\n\n\nblog\n\n\nduckdb\n\n\ngeospatial\n\n\n\n\n\n\n\nNaty Clementi\n\n\nDec 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis versus X: Performance across the ecosystem part 1\n\n\n\nblog\n\n\ncase study\n\n\necosystem\n\n\nperformance\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nDec 6, 2023\n\n\n\n\n\n\n\n\n\n\n\n\ndbt-ibis: Write your dbt models using Ibis\n\n\n\nblog\n\n\ndbt\n\n\ndata engineering\n\n\n\n\n\n\n\nStefan Binder\n\n\nNov 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nQuerying every file in every release on the Python Package Index (redux)\n\n\n\nblog\n\n\n\n\n\n\n\nGil Forsyth\n\n\nNov 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWorking with arrays in Google BigQuery\n\n\n\nblog\n\n\nbigquery\n\n\narrays\n\n\ncloud\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIcy IO: loading local files with Snowflake\n\n\n\nblog\n\n\nsnowflake\n\n\nio\n\n\nproductivity\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nAug 31, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis v6.1.0\n\n\n\nrelease\n\n\nblog\n\n\n\n\n\n\n\nIbis team\n\n\nAug 2, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis v6.0.0\n\n\n\nrelease\n\n\nblog\n\n\n\n\n\n\n\nIbis team\n\n\nJul 3, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis on 🔥: Supercharge Your Workflow with DuckDB and PyTorch\n\n\n\nblog\n\n\ncase study\n\n\nmachine learning\n\n\necosystem\n\n\nnew feature\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nJun 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nExploring campaign finance data\n\n\n\nblog\n\n\ndata engineering\n\n\ncase study\n\n\nduckdb\n\n\nperformance\n\n\n\n\n\n\n\nNick Crews\n\n\nMar 24, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis sneak peek: writing to files\n\n\n\nblog\n\n\nio\n\n\nnew feature\n\n\nsneak peek\n\n\n\n\n\n\n\nKae Suarez\n\n\nMar 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis sneak peek: examples\n\n\n\nblog\n\n\nnew feature\n\n\nsneak peek\n\n\n\n\n\n\n\nKae Suarez\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nMaximizing productivity with selectors\n\n\n\nblog\n\n\nnew feature\n\n\nproductivity\n\n\nduckdb\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nFeb 27, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis + Substrait + DuckDB\n\n\n\nblog\n\n\nsubstrait\n\n\necosystem\n\n\nduckdb\n\n\n\n\n\n\n\nGil Forsyth\n\n\nFeb 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of Ibis’s CI performance\n\n\n\nblog\n\n\nbigquery\n\n\ncontinuous integration\n\n\ndata engineering\n\n\ndogfood\n\n\n\n\n\n\n\nPhillip Cloud\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nIbis v4.0.0\n\n\n\nrelease\n\n\nblog\n\n\n\n\n\n\n\nPatrick Clarke\n\n\nJan 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nffill and bfill using Ibis\n\n\n\nblog\n\n\nwindow functions\n\n\ntime series\n\n\n\n\n\n\n\nPatrick Clarke\n\n\nSep 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIbis v3.1.0\n\n\n\nrelease\n\n\nblog\n\n\n\n\n\n\n\nMarlene Mhangami\n\n\nJul 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIbis v3.0.0\n\n\n\nrelease\n\n\nblog\n\n\n\n\n\n\n\nMarlene Mhangami\n\n\nApr 25, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to top"
  },
  {
    "objectID": "release_notes.html",
    "href": "release_notes.html",
    "title": "2.1.0 (2022-01-12)",
    "section": "",
    "text": "Release notes"
  },
  {
    "objectID": "release_notes.html#section",
    "href": "release_notes.html#section",
    "title": "2.1.0 (2022-01-12)",
    "section": "7.2.0 (2023-12-18)",
    "text": "7.2.0 (2023-12-18)\n\nFeatures\n\napi: add ArrayValue.flatten method and operation (e6e995c)\napi: add ibis.range function for generating sequences (f5a0a5a)\napi: add timestamp range (c567fe0)\nbase: add to_pandas method to BaseBackend (3d1cf66)\nclickhouse: implement array flatten support (d15c6e6)\ncommon: node.replace() now supports mappings for quick lookup-like substitutions (bbc93c7)\ncommon: add node.find_topmost() method to locate matching nodes without descending further to their children (15acf7d)\ncommon: allow matching on dictionaries in possibly nested patterns (1d314f7)\ncommon: expose node.__children__ property to access the flattened list of children of a node (2e91476)\nduckdb: add initial support for geospatial functions (65f496c)\nduckdb: add read_geo function (b19a8ce)\nduckdb: enforce aswkb for projections, coerce to geopandas (33327dc)\nduckdb: implement array flatten support (0a0eecc)\nexasol: add exasol backend (295903d)\nexport: allow passing keyword arguments to PyArrow ParquetWriter and CSVWriter (40558fd)\nflink: implement nested schema support (057fabc)\nflink: implement windowed computations (256767f)\ngeospatial: add support for GeoTransform on duckdb (ec533c1)\ngeospatial: update read_geo to support url (3baf509)\npandas/dask: implement flatten (c2e8d9d)\npolars: add streaming kwarg to to_pandas (703507f)\npolars: implement array flatten support (19b2aa0)\npyspark: enable multiple values in .substitute (291a290)\npyspark: implement array flatten support (5d1fadf)\nsnowflake: implement array flatten support (d3c754f)\nsnowflake: read_csv with https (72752eb)\nsnowflake: support udf arguments for reading from staged files (529a3a2)\nsnowflake: use upstream array_sort (9624341)\nsqlalchemy: support expressions in window bounds (5dbb3b1)\ntrino: implement array flatten support (0d1faaa)\n\n\n\nBug Fixes\n\napi: avoid casting to bool for table.info() nullable column (3b3bd7b)\nbigquery: escape the schema (project ID) for BQ builtin UDFs (8096552)\nbigquery: fully qualified memtable names in compile (a81e432)\nclickhouse: use backwards compatible methods of getting query metadata (975556f)\ndatafusion: bring back UDF registration (43084fa)\ndatafusion: ensure that non-matching re_search calls return bool values when patterns do not match (088b027)\ndatafusion: support computed group by when the aggregation is count distinct (18bdb7e)\ndecompile: handle isin (6857751)\ndeferred: don’t pass expression in fstringified error message (724859d)\ndeps: update dependency datafusion to v33 (57047a2)\ndeps: update dependency sqlglot to v20 (13bc6e2)\nduckdb: ensure that already quoted identifiers are not erased (45ee391)\nduckdb: ensure that parameter names are unlikely to overlap with column names (d93dbe2)\nduckdb: gate geoalchemy import in duckdb geospatial (8f012c4)\nduckdb: render dates, times, timestamps and none literals correctly (5d8866a)\nduckdb: use functions for temporal literals (b1407f8)\nduckdb: use the UDF’s signature instead of arguments’ output type for generating a duckdb signature (233dce1)\nflink: add more test (33e1a31)\nflink: add os to the cache key (1b92b33)\nflink: add test cases for recreate table (1413de9)\nflink: customize the list of base idenitifers (0b5d343)\nflink: fix recreating table/view issue on flink backend (0c9791f)\nflink: implement TypeMapper and SchemaMapper for Flink backend (f983bfa)\nflink: use lazy import to prevent premature loading of pyflink during gen_matrix (d042402)\ngeospatial: pretty print data in interactive mode (afb04ed)\nir: ensure that join projection columns are all always nullable (f5f35c6)\nir: handle renaming for scalar operations (6f77f17)\nir: handle the case of non-overlapping data and add a test (1c9ae1b)\nir: implicitly convert None literals with dt.Null type to the requested type during value coercion (d51ec4e)\nir: merge window frames for bound analytic window functions with a subsequent over call (e12ce8d)\nir: raise if Concrete.copy() receives unexpected arguments (442199a)\nmemtable: ensure column names match provided data (faf99df)\nmemtables: disallow duplicate column names when constructing memtables (4937b48)\nmssql: compute the length of strings correctly (64d2957)\nmssql: render dates, times and timestamps correctly (aca30e1)\nmysql: render dates and timestamps correctly (19e878c)\noracle: ensure that .sql metadata results are in column-definition order (26a3c1f)\noracle: render dates and timestamps correctly (66fbad6)\npandas-format: convert map keys (bb92e9f)\npandas: ensure that empty arrays unnest to nothing (fa9831f)\npandas: fix integer wraparound when extracting epoch seconds (e98fa3c)\npandas: handle non-nullable type mapping (c6a6c56)\nparse_sql: parse IN clauses (8b1f7b5)\npolars: handle new categorical types (5d6d6ae)\npolars: handle the case of an empty InValues list (b26aa55)\npolars: project first when creating computed grouping keys (7f9fdd4)\npostgres: render dates, times, timestamps and none literals correctly (a3c1c07)\npyarrow: avoid catching ValueError and hiding legitimate failures (b7f650c)\npyspark,polars: add packaging extra (bdde3a4)\npyspark: custom format converter to handle pyspark timestamps (758ec25)\nsnowflake: convert arrays, maps and structs using the base class implementation (f361891)\nsnowflake: convert path to str when checking for a prefix (c5f884c)\nsnowflake: ensure that empty arrays unnest to nothing (28c2498)\nsnowflake: fix array printing by using a pyarrow extension type (7d8fe5a)\nsnowflake: fix creating table in a different database (9b65b48)\nsnowflake: fix quoting across all apis (7bf8e84)\nsubstitute: allow mappings with None keys (4b28ff1)\n\n\n\nDocumentation\n\nadd exasol to the backend coverage app (3575858)\narrays: document behavior of unnest in the presence of empty array rows (5526c40)\nbackends: include docs for inherited members (c04bf67)\nblog-post: add blog post comparing ibis to pandas and dask (a7fd32b)\nblog-post: add blogpost ibis duckdb geospatial (def8031)\nblog-post: pydata performance part 2; polars and datafusion (36e1db5)\nblog: add dbt-ibis post (d73c156)\nblog: add pypi compiled file extension blog (751cfcf)\nbuild: allow building individual docs without rendering api docs first (529ee6c)\nbuild: turn off interactive mode before every example (502b88c)\nfix minor typo in sql.qmd (17aa929)\nfix typo in ir.Table docstring (e3b9611)\nfix typos (9a4d1f8)\nmake minor edits to duckdb-geospatial post (2365e10)\nname: improve docstring of ibis.param API (2f9ec90)\nname: improve docstring of Value.name API (dd66af2)\nperf: use an unordered list instead of an ordered one (297be44)\npypi-metadata-post: add Fortran pattern and fix regex (12058f2)\nremove confusing backend page (c1d19c7)\nreplace deprecated relabels with renames (6bc9e15)\nsql: emphasize the need to close a raw_sql cursor only when using SELECT statements (74379a8)\ntests: add API docs for the testing base classes (173e9a9)\ntests: document class variables in BackendTest (e814c6b)\n\n\n\nRefactors\n\nanalysis: always merge frames during windowization (66fd69c)\nbigquery: move BigQueryType to use sqlglot for type parsing and generation (6e3219f)\nclickhouse: clean up session timezone handling (66220c7)\nclickhouse: use isoformat instead of manual specification (a3fac3e)\ncommon: consolidate the finder and replacer inputs for the various graph methods (a1881eb)\ncommon: remove traverse() function’s filter argument since it can be expressed using the visitor (e4e2993)\ncommon: unify the node.find() and node.match() methods to transparently support types and patterns (3c14091)\ndatafusion: simplify execute and to_pyarrow implementations (c572eab)\nduckdb: use pyarrow for all memtable registration (d6a2f09)\nformats: move the TableProxy object to formats from the operations (05964b1)\npandas-format: move to classmethods to pickup super class behavior where possible (7bb0470)\nsnowflake: use upstream map-from-arrays function instead of a custom UDF (318459c)\ntests: remove test rounding mixins (3b730d9)\ntests: remove UnorderedComparator class (ab0a8f6)\n\n\n\nPerformance\n\ncommon: improve the performance of replacing nodes by using a specialized node.__recreate__() method (f3da926)"
  },
  {
    "objectID": "release_notes.html#section-1",
    "href": "release_notes.html#section-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "7.1.0 (2023-11-16)",
    "text": "7.1.0 (2023-11-16)\n\nFeatures\n\napi: add bucket method for timestamps (ca0f7bc)\napi: add Table.sample method for sampling rows from a table (3ce2617)\napi: allow selectors in order_by (359fd5e)\napi: move analytic window functions to top-level (8f2ced1)\napi: support deferred in reduction filters (349f475)\napi: support specifying signature in udf definitions (764977e)\nbigquery: add location parameter (d652dbb)\nbigquery: add read_csv, read_json, read_parquet support (ff83110)\nbigquery: support temporary tables using sessions (eab48a9)\nclickhouse: add support for timestamp bucket (10a5916)\nclickhouse: support Table.fillna (5633660)\ncommon: better inheritance support for Slotted and FrozenSlotted (9165d41)\ncommon: make Slotted and FrozenSlotted pickleable (13cbce0)\ncommon: support Self annotations for Annotable (0c60146)\ncommon: use patterns to filter out nodes during graph traversal (3edd8f7)\ndask: add read_csv and read_parquet (e9260af)\ndask: enable pyarrow conversion (2d36722)\ndask: support Table.sample (09a7626)\ndatafusion: add case and if-else statements (851d560)\ndatafusion: add corr and covar (edc42be)\ndatafusion: add isnull and isnan operations (0076c25)\ndatafusion: add some array functions (0b96b68)\ndatafusion: add StringLength, FindInSet, ArrayStringJoin (fd03831)\ndatafusion: add TimestampFromUNIX and subtract/add operations (2bffa5a)\ndatafusion: add TimestampTruncate / fix broken extract time part functions (940ed21)\ndatafusion: support dropping schemas (cc6870c)\nduckdb: add attach and detach methods for adding and removing databases to the current duckdb session (162b058)\nduckdb: add ntile support (bf08a2a)\nduckdb: add dict-like for DuckDB settings (ea2d317)\nduckdb: add support for specific timestamp scales (3518b78)\nduckdb: allow users to register fsspec filesystem with DuckDB (6172f07)\nduckdb: expose option to force reinstall extension (98080d0)\nduckdb: implement Table.sample as a TABLESAMPLE query (3a80f3a)\nduckdb: implement partial json collection casting (aae28e9)\nflink: add remaining operators for Flink to pass/skip the common tests (b27adc6)\nflink: add several temporal operators (f758228)\nflink: implement the ops.TryCast operation (752e587)\nformats: map ibis JSON type to pyarrow strings (79b6eac)\nimpala/pyspark: implement to_pyarrow (6b33454)\nimpala: implement Table.sample (8e78dfc)\nimplement window table valued functions (a35a756)\nimprove generated column names for methods receiving intervals (c319ed3)\nmssql: add support for timestamp bucket (1ffac11)\nmssql: support cross-db/cross-schema table list (3e0f0fa)\nmysql: support ntile (9a14ba3)\noracle: add fixes after running pre-commit (6538b70)\noracle: add fixes after running pre-commit (e3d14b3)\noracle: add support for loading Oracle RAW and BLOB types (c77eeb2)\noracle: change parsing of Oracle NUMBER data type (649ab86)\noracle: remove redundant brackets (2905484)\npandas: add read_csv and read_parquet (34eeca6)\npandas: support Table.sample (77215be)\npolars: add support for timestamp bucket (c59518c)\npostgres: add support for timestamp bucket (4d34afc)\npyspark: support Table.sample (6aa897e)\nsnowflake: support ntile (39eed1a)\nsnowflake: support cross-db/cross-schema table list (2071897)\nsnowflake: support timestamp bucketing (a95ffa9)\nsql: implement Table.sample as a random() filter across several SQL backends (e1870ea)\ntrino: implement Table.sample as a TABLESAMPLE query (f3d044c)\ntrino: support ntile (2978d1a)\ntrino: support temporal operations (8b8e885)\nudf: improve mypy compatibility for udf functions (65b5bb7)\nuse to_pyarrow instead of to_pandas in the interactive repr (72aa573)\nux: fix long links, add repr links in vscode (734bd91)\nux: implement recursive element conversion for nested types and json (8ddfa94)\nux: render url strings as links in rich table output (1c7a9b6)\nux: show syntax-highlighted SQL if pygments is installed (09881b0)\n\n\n\nBug Fixes\n\nbigquery: apply unnest transformation in other methods that execute SQL (2cc9d0e)\nbigquery: avoid trying to filter separator argument to GroupConcat operation (ed3b017)\nbigquery: ensure that the identifier is parsed according to the dialect (f5bb555)\nbigquery: move sql code to proper argument (abb0bdd)\ndatafusion: do_connect: properly deal with config-is-actually-context (649480c)\ndatafusion: fix some temporal operations (3206dbc)\ndatatypes: correct uint upper bounds (5ca56d5)\ndatatypes: correct unsigned integer bounds (1e40d4e)\ndeps: bump pins lower bound to pickup transitive fsspec upper bound (983e23e)\ndeps: bump sqlglot lower bound (a47be79)\ndeps: pin pyspark to a working version (7eb8a19)\ndeps: update dependency datafusion to v32 (1afbe9c)\ndeps: update dependency pyarrow to v14 (bce86c4)\ndeps: update dependency sqlglot to v19 (1f3ae07)\nduckdb: ensure proper quoting when compiling cross database/schema tables (8d7b5fa)\nduckdb: query table list directly instead of relying on sqlalchemy (5d7822c)\nduckdb: use connect instead of begin to avoid nesting transactions (6889543)\nflink: cast argument to integer for reduction (5059eed)\nflink: correct the filtered count translation (2cbca74)\nflink: re-implement ops.ApproxCountDistinct (2e3a5a0)\nir: ibis.parse_sql() removes where clause (522f3a4)\nir: coerce integers passed to Value[dt.Floating] annotated values as dt.float64 (b8a924a)\nir: ensure that windowization directly wraps the reduction/analytic function (772df36)\nmssql: support translation of ops.Neg() when projecting a field (ca49d2a)\noracle: change filter inside select into case when (c743fa2)\noracle: disable if_exists for Oracle drop view command (973133b)\noracle: fix fallback column type inference (fb5d56d)\npandas: drop __index_level_N__ cols before applying schema (b53feac)\npatterns: Object pattern should match on positional arguments first (96c796f)\npatterns: PatternList should keep the original pattern’s type (6552639)\npolars: bump lower bound to 0.19.8 and clean up a bunch of backcompat code (462bd17)\npolars: various polars enhancements (5948dd6)\nrepr: add dispatch for repr of GeoSpatialBinOps (843d086)\nsnowflake: include views when listing tables for backwards compatibility (094881b)\nsnowflake: support snowflake 3.3.0 (nanoarrow) (a0f24e8)\nsqlalchemy: ensure that limit on .sql calls works (a5e3062)\nsqlite: handle BLOB datatype (d36ed1c)\nsqlite: truncate week to previous week not following (6239794)\nsql: subtract one from ntile output in string-generating backends (1d264dc)\nsupport self joins on memtables (f24e355)\ntrino: enable passing the database argument when accessing tables (e7ce43e)\ntrino: ensure that a schema is not required upon connection when accessing tables with explicit schema (8bde3e0)\nuse pyarrow_hotfix where necessary (0fa1e5d)\n\n\n\nDocumentation\n\nadd .nullif() example (6d405df)\nadd “similar to pandas …” to docstrings (cd7be29)\nadd basic intro docstring to Table class (1a68f31)\nadd callout note for Table.sample (51027d9)\nadd copyright holders to license (ca97dfb)\nadd deprecation to .nullifzero docstring (8502e81)\nadd example to Value.hash() (501ae92)\nadd examples to Value.typeof() (c146381)\nadd more examples to Table.select() (735bbd0)\nadd See Also sections to some APIs (be8938f)\nclickhouse: freeze clickhouse backend docs to avoid rate limit from upstream playground (e3a7eac)\ncontribute: fix instructions for nix environment setup (013cedd)\ncontribute: fix path to conda-lock files for contributors (ef5bdf9)\ndedupe 6.2.0 and 7.0.0 release notes (7ce4b1a)\nfix and improve .isin() docstring (063cfba)\nfix dask compile docstring typo (d38d2c4)\nfix link in Value.type() docstring (43b798c)\nfixup link (d4c97b0)\nflink: add backend back to support matrix df (e846e80)\nimprove .between() docstring (a086134)\nimprove .case() and .cases() docstrings (7fc89e8)\nimprove cast() and try_cast() docstrings (0b686e8)\nimprove cross-linking within reference (9e45194)\nimprove examples for Table.order_by() (9465b2a)\nimprove join() docstring (84c08c6), closes #7424\nimprove re_replace docstring (f55d0db)\nimprove Table.columns docstring (d50558b)\nmysql: render_do_connect mssql to mysql (3c2da6c)\npandas: show methods from BasePandasBackend (20fd120)\nranking: add ranking function docstrings (750bfeb)\nsetup codespace configuration [skip ci] (5363b94)\nstyle: replace Black with Ruff in guidelines (1db3047)\ntemporal: add Literal annotation to display possible units for delta method (ee94cb5)\ntrino: add details for connecting to starburst (ca9873a)\ntrino: add note about SSO configuration (457534b)\nudfs: fix udf interlink locations (c26e48b)\n\n\n\nRefactors\n\nanalysis: remove _rewrite_filter() in favor of using replacement patterns (4c0ac2e)\nanalysis: remove is_reduction() (2acc31f)\nanalysis: remove pushdown_aggregation_filters() (cf95ff7)\nanalysis: remove sub_for(), substitute(), find_toplevel_aggs() (492b296)\nanalysis: remove substitute_parents() (cd91a7e)\nanalysis: remove substitute_unbound() since it is used at a single place (6a6ad19)\nanalysis: simplify and improve pushdown_selection_filters() (2e47738)\nanalysis: vastly simplify windowize_function (998bbaa)\nbackends: move read_delta to base io handler (3d5a684)\nbigquery: add schema kwarg to list_tables (95be62f)\nbigquery: remove session use (60e7900)\nbigquery: remove unused BigQueryTable object (b83e60e)\nclean up lit usage (1bc6cee)\nclickhouse: apply repetitive transformations as pattern replacements (e966af8)\nclickhouse: replace lit with builtin sqlglot functions (221b630)\nclickhouse: use a pattern for one-to-zero index conversion of ranking window functions (732c031)\nclickhouse: use sqlglot for create_table implementation (ea0826d)\ncommon: remove ibis.common.bases.Base in favor of Abstract (8ed313c)\ndatafusion: create registry of time udfs to create them only once (9ed0a89)\ndocker-compose: clean up unused exposed ports and make envar spec uniform (7ee518d)\nduckdb: remove lit (6f77df9)\nflink: use FILTER syntax when counting (815c12f)\nimports: move pandas-importing object to method (103a524)\nir: remove ibis.expr.streaming (70df318)\nir: remove ops.Negatable, ops.NotAny, ops.NotAll, ops.UnresolvedNotExistsSubquery (e31e8fd)\nir: unify ibis.common.pattern builders and ibis.expr.deferred (652ceab)\nmake _WellKnownText not a NamedTuple (9a9e733)\noracle: deprecate database for schema in list_tables (c8ea79f)\npatterns: support more flexible sequence matching (b8e463d)\npostgres: deprecate database for schema in list_tables (d622730)\nremove unused *args in udf functions (e22236c)\nsql: align logic for filtered reductions (0347036)\ntemporal: remove unnecessary Temporal* classes (d3bcf73)\ntrino: support better cross-db/cross-schema table list (d2cf1c9)\nuse rewrite rules to handle fillna/dropna in sql backends (f5e06a6)\n\n\n\nPerformance\n\nbigquery: use more efficient representation for memtables (697d325)"
  },
  {
    "objectID": "release_notes.html#section-2",
    "href": "release_notes.html#section-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "7.0.0 (2023-10-02)",
    "text": "7.0.0 (2023-10-02)\n\n⚠ BREAKING CHANGES\n\napi: the interpolation argument was only supported in the dask and pandas backends; for interpolated quantiles use dask or pandas directly\nir: Dask and Pandas only; cumulative operations that relied on implicit ordering from prior operations such as calls to table.order_by may no longer work, pass order_by=... into the appropriate cumulative method to achieve the same behavior.\napi: UUID, MACADDR and INET are no longer subclasses of strings. Cast those values to string to enable use of the string APIs.\nimpala: ImpalaTable.rename is removed, use Backend.rename_table instead.\npyspark: PySparkTable.rename is removed, use Backend.rename_table instead.\nclickhouse: ClickhouseTable is removed. This class only provided a single insert method. Use the Clickhouse backend’s insert method instead.\ndatatypes: The minimum version of sqlglot is now 17.2.0, to support much faster and more robust backend type parsing.\nir: ibis.expr.selectors module is removed, use ibis.selectors instead\napi: passing a tuple or a sequence of tuples to table.order_by() calls is not allowed anymore; use ibis.asc(key) or ibis.desc(key) instead\nir: the ibis.common.validators module has been removed and all validation rules from ibis.expr.rules, either use typehints or patterns from ibis.common.patterns\n\n\n\nFeatures\n\napi: add .delta method for computing difference in units between two temporal values (18617bf)\napi: add ArrayIntersect operation and corresponding ArrayValue.intersect API (76c95b2)\napi: add Backend.rename_table (0047143)\napi: add levenshtein edit distance API (ab211a8)\napi: add relocate table expression API for moving columns around based on selectors (ee8a86f)\napi: add Table.rename, with support for renaming via keyword arguments (917d7ec)\napi: add to_pandas_batches (740778f)\napi: add support for referencing backend-builtin functions (76f5f4b)\napi: implement negative slice indexing (caee5c1)\napi: improve repr for deferred expressions containing Column/Scalar values (6b1218a)\napi: improve repr of deferred functions (f2b3744)\napi: support deferred and literal values in ibis.ifelse (685dbc1)\napi: support deferred arguments in ibis.case() (6f9f7c5)\napi: support deferred arguments to ibis.array (b1b83f9)\napi: support deferred arguments to ibis.map (86c8669)\napi: support deferred arguments to ibis.struct (7ef870d)\napi: support deferred arguments to udfs (a49d259)\napi: support deferred expressions in ibis.date (f454a71)\napi: support deferred expressions in ibis.time (be1fd65)\napi: support deferred expressions in ibis.timestamp (0e71505)\napi: support deferred values in ibis.coalesce/ibis.greatest/ibis.least (e423480)\nbigquery: implement array functions (04f5a11)\nbigquery: use sqlglot to implement functional unnest to relational unnest (167c3bd)\nclickhouse: add read_parquet and read_csv (dc2ea25)\nclickhouse: add support for .sql methods (f1d004b)\nclickhouse: implement builtin agg functions (eea679a)\nclickhouse: support caching tables with the .cache() method (621bdac)\nclickhouse: support reading parquet and csv globs (4ea1834)\ncommon: match and replace graph nodes (78865c0)\ndatafusion: add coalesce, nullif, ifnull, zeroifnull (1cc67c9)\ndatafusion: add ExtractWeekOfYear, ExtractMicrosecond, ExtractEpochSeconds (5612d48)\ndatafusion: add join support (e2c143a)\ndatafusion: add temporal functions (6be6c2b)\ndatafusion: implement builtin agg functions (0367069)\nduckdb: expose loading extensions (2feecf7)\nexamples: name examples tables according to example name (169d889)\nflink: add batch and streaming mode test fixtures for Flink backend (49485f6)\nflink: allow translation of decimal literals (52f7032)\nflink: fine-tune numeric literal translation (2f2d0d9)\nflink: implement ops.FloorDivide operation (95474e6)\nflink: implement a minimal PyFlink Backend (46d0e33)\nflink: implement insert dml (6bdec79)\nflink: implement table-related ddl in Flink backend to support streaming connectors (8dabefd)\nflink: implement translation of NULLIFZERO (6ad1e96)\nflink: implement translation of ZEROIFNULL (31560eb)\nflink: support translating typed null values (83beb7e)\nimpala: implement Backend.rename_table (309c999)\nintroduce watermarks in ibis api (eaaebb8)\njust chat to open Zulip in terminal (95e164e)\npatterns: support building sequences in replacement patterns (f320c2e)\npatterns: support building sequences in replacement patterns (beab068)\npatterns: support calling methods on builders like a variable (58b2d0e)\npolars: implement new UDF API (becbf41)\npolars: implement support for builtin aggregate udfs (c383f62)\npolars: support reading ndjson (1bda3bd)\npostgres: implement array functions (fe41d57)\npostgres: implement array sort (4791cb4)\npostgres: implement array union (6d3d518)\npyspark: enable reading csv and parquet globs and implement read_json (d487e10)\npyspark: enable the new scalar UDF API (f29a8e7)\npyspark: implement Backend.rename_table (0a8b201)\nselectors: support column references in column selector (d4fae08)\nsnowflake: add ArrayRemove implementation (4f9d9f9)\nsnowflake: allow disabling creation of object UDFs (569aa12)\nsnowflake: handle glob patterns in read_csv, read_parquet and read_json (adb8f4c)\nsnowflake: implement ops.ArrayRepeat (a93cbd6)\nsnowflake: implement read_csv (3323156)\nsnowflake: implement read_json (ec870a2)\nsnowflake: implement read_parquet (e02888b)\nsnowflake: implement array sort (465fae1)\nsnowflake: support literal map key contains check (dbe7d4e)\nsql: add database argument to list_schemas (22ceba7)\nsqlalchemy: support builtin aggregate functions (3b27e23)\nsqlite: implement caching support (0677f8d)\ntests: support defining datatype nullability for hypothesis strategies (ff26fb8)\ntrino: cross-schema table support (9c7c65f)\nudf: add support for builtin aggregate UDFs (8ee12bf)\nudf: support inputs without type annotations (99e531d)\nux: promote lists of strings to any_of selectors (5e11529)\n\n\n\nBug Fixes\n\napi: ensure that deferred objects cannot be converted into literals (b37804a)\napi: ensure that normalization of boolean, ints and floats fail with readable error message (556f7cc)\napi: ensure the order of duplicate non-renamed columns in relocate is preserved (19a59aa)\napi: fail on trying to construct an iterable of a deferred object (89bf919)\napi: improve error message for bad arguments to Table.select (258a289)\napi: support passing functools.partial objects to array .map/.filter methods (28f45d0)\nbigquery: generate the correct temporal literal type based on the presence of timezone information (98a6ae0)\nbigquery: quote struct field names in memtable when necessary (b1fcde8)\nclickhouse: do not always prefix the table name with database, because temp tables cannot be assigned a database (5f88102)\nclickhouse: list temporary tables with list_tables (758a875)\nclickhouse: make sure that array1.union(array2) null handling matches across backends (8d42794)\nclickhouse: workaround clickhouse_connect usage of removed APIs in pandas 2.1.0 (577599a)\nclip: preserve nulls when clipping (c12dfa4)\ncommon: pattern() factory should construct a CoercedTo(type) pattern from coercible types (09be2cd)\ncommon: disallow plain string inputs for SequenceOf patterns (578980d)\ncommon: disallow type coercion when checking for generic type fields (df63e8b)\ncommon: support optional keyword-only parameters when validating callables (519a9e0)\ndatafusion: cast division inputs to float64 before dividing (197342d)\ndatatypes: decimal normalization failed for integers (5213958)\ndeps: update dependency datafusion to v28 (1a8b223)\ndeps: update dependency datafusion to v31 (fa0a8bd)\ndeps: update dependency pyarrow to v13 (43dc1e1)\ndeps: update dependency sqlglot to v18 (5fa0083)\ndrop: support deferred objects in calls to drop (d27374b)\ndruid: avoid double escaping percent-signs in strings (1d1f7bd)\ndruid: convert type strings to lowercase before looking up (4a838f7)\ndruid: ensure that string types are translated to VARCHAR (56e6ffc)\ndtypes: switch scale and timestamp parameter order when formatting a timestamp datatype (302b122)\nduckdb: load httpfs with read_csv from s3 (da1b95f)\nduckdb: make sure that array1.union(array2) null handling matches across backends (849dea4)\nduckdb: remove hack to workaround bug that was fixed upstream (310c521)\nduckdb: workaround aggressive importing on the duckdb side (105e2d6)\nflink: correct ops.RegexSearch translations (a3427a1)\nflink: correct the translation of ops.Power (42d2236)\nflink: correct translation of ops.IfNull op (85de81c)\nflink: fix the pandas conversion in execute (2f6564f)\nflink: fix translation of ops.TimestampDiff (580eff7)\nflink: implement an in-memory table formatter (217a14b)\nflink: remove broken, untested epochseconds (f18c760)\nflink: rewrite ops.Clip using if statements (b7153ea)\nflink: rewrite ops.Date as a cast operation (2470e81)\nflink: translate ops.RandomScalar to rand (c485a92)\nformat: support rendering empty schemas (f8faada)\nhistogram: ensure that the bin width calculation matches numpy (e6a0037)\nimpala: allow arbitrary connection params (f251289)\nmysql: handle null literals (79788c7)\noracle: clarify sid vs service_name handling and allow dsn (d4ea3bf)\noracle: ensure that metadata queries use SQL and not sqlplus-specific syntax (2c1bf93)\npandas: compatibility with 2.1 groupby behavior (ab3fc9e)\npatterns: fix pattern mismatch error for default Pattern (f68079a)\npatterns: support optional keyword arguments in CallableWith (a78aa60)\npatterns: support passing mappings to Getitem builder (25864cf)\npatterns: support string inputs for builder() (3610e52)\npolars: polars no longer panics on a value_counts-ed expression (e14185a)\npyspark: default to inferring the schema of CSV files and assuming they have a header with header=True (0ffda75)\npyspark: gate datediff op to restore pyspark 3.2 support (4a8d611)\npyspark: gate other usage of DayTimeIntervalType for PySpark 3.2 (ab01de0)\nremove pandas license (476a659)\nrepr: ensure that column expressions are not promoted to table when repring non-interactively (d57a162)\nselectors: error when trying to select a non-existent column with s.c (ae3e76e)\nsnowflake: allow backend to choose how to prefix table names during compilation (933fb32)\nsnowflake: disable filter and map (53bc22e)\nsnowflake: ensure that laterals joins with newlines are also rewritten (dfd3c9b)\nsnowflake: ensure the correct compilation of tables from other databases and schemas (0ee68e2)\nsnowflake: fix timestamp scale inference (083bdae)\nsnowflake: use ibis-defined array_sort until upstream lands (6f7e13d)\nsqlalchemy: ignore database when specified with temp=True (04461d5)\nsql: avoid reselecting relations that do not need it to prevent dropping order by clauses (8ae2f03)\nsqlglot: ensure back compat for DataTypeParam import (65851fc)\nstruct-column: make ops.StructColumn dshape depend on its input (7086d58)\ntrino: differentiate between a single column struct and a non-struct column (b1f1939)\ntype hints: improvements to type hints in ibis.expr (297b449)\ntype hints: remove notimplemented as type hints as not valid (57ea7a1)\ntype hints: various improvements to type hints in common (ff00347)\n\n\n\nDocumentation\n\nadd functools.partial and lambda closures to ArrayValue.map and ArrayValue.filter (e245e83)\nadd ibis.connect to top_level API docs (0d197e8)\nadd 404 page (8b2de41)\nadd 6.2.0 release notes (f5a2aed)\nadd back goatcounter to website (a0095bf)\nadd docs issue to navbar (ea41e53)\nadd exending how-to guides (0bad961)\nadd how-to for working with raw sql strings (3e08556)\nadd interactivity to altair example (0037fd8)\nadd interactivity to altair example on homepage (217f080)\nadd more redirects based on Google search console findings (fe890a4)\nadd proper zulip icon (f854327)\nadd some prose and move operation support matrix (48b7e34)\nadd UDF API documentation (5354689)\nadd v6.1.0 release blog (a66f7b7)\narrays: update blog post to include unnest examples (e765712)\nbackends: add support more supported IO types (124f085)\nbackends: explain how to release a cursor and suggest using .sql instead (1e1a574)\nbasic starburst galaxy tutorial (a7a49ca)\nblog: add bigquery arrays 7.0.0 blog post (8f2a40f)\nblog: add tags to blog posts (1527655)\nblog: embed the torch youtube video directly (70515ed)\nblog: snowflake io (ee8c512)\nbring back backend API documentation (df981d5)\nbring back versioning policy doc (9dc8966)\nclean up extending tutorials (8da58d4)\ncommunity to contriute (f02c2fb)\ndark mode for life (2fa181a)\ndefault to short signature but show full path for top level functions (9793862)\ndraft posts to draft and very minor edits (43499e3)\nensure that all parameters elements overflow with a scrollbar (5002d9f)\nexpose API under ibis when possible (bc05ced)\nfix edit this page button (284b48a)\nfix links from install to connect (9bf27fb)\nfix numerics and move connection apis elsewhere (95bb2e0)\nfix setuptools extra install style (d9ab537)\nfix zulip link for new members (3732f46)\ngitter -&gt; zulip (ef79e64)\ngive reference docs a more organized layout (583af94)\nhand roll datatypes.core APIs to avoid documenting private types (6ad8069)\nimport ibis in doctests (7f340de)\ninclude full name of signatures (66a58e2), closes /github.com/ibis-project/ibis/pull/7159#issuecomment-1735845163\ninstall: point to connect anchor instead of do_connect (d680b40)\nIO: add missing word, add line breaks (f4fdfd3)\nlanguage: de-simple-fy prose in docs (0617271)\nlink to zulip in README.md (15112bb)\nmajor home page refactor (a4e4569)\nminor blog fixes/prose update (304edd1)\nminor blog update (df14997)\nminor consistency on capitalization of versioning concept (b838760)\nminor fix in starburst tutorial (54473ba)\nmore redirects and add ibis 3.0.2 (8d900ee)\nmove column selectors closer to relevant expression page (ea3a090)\nnix: add configuration notes to nix environment setup (1c60318)\nonly render file support methods once (98b348c)\nport bigquery ci-analysis blog post to use the delta API (e543b1d)\nquarto: add quartodoc interlinks filter (9f7a1ef)\nquarto: make blog post titles visible in light mode (be2e95f)\nquarto: override api code blocks with custom renderer (b504fee)\nquarto: shorten method signature names (e37bfea)\nquart: remove temporary eval false setting (0b9e3a3)\nrefactor and move to quarto (487a5e5)\nreference: fix ibis.ifelse() docstring (a80bb75)\nreference: improve descriptions of sections (6a4924a)\nreference: move collections API from global (4780536)\nreference: move generic API from global (ba1f72e)\nreference: move numeric/bool API from global (f3f23ac)\nreference: move Table API from global (efcc2fb)\nreference: move temporal API from global (c452fbb)\nreference: move types API from global (662c509)\nreference: rename Complex to Collection (194afa7)\nreference: rename top-level to connection (9b9cd03)\nreference: simplify title of Generic section (ef86165)\nreference: soft-deprecate ibis.where (3c94f7b)\nreference: sort numeric before strings (0df8bba)\nremove duckdb code annotations (b0bcdde)\nremove extra bits in zulip links (29680a3)\nremove final instances of gitter (69f941a)\nremove keywords (53a9f9f)\nremove old S3 comment in impala docs (57c0596)\nremove old-style schema construction from examples and docstrings (1b1c33a)\nremove stray bracket (72c9039)\nremove streamlit app on front page (d6d498e)\nremove underscores that are not deferreds in doctest (5d300a9)\nremove warning on front page (e68ec90)\nset expectations for the impala backend (09c7678)\nsome how-to updates (6627016)\nswap release notes and contribute (a242e31)\nupdate poetry version (15e77f7)\nupdate link to ‘example repository’ instead of ‘tutorial’ (c20d3ee)\nupdate link to sqlalchemy tutorial (047aef7)\nwhy ibis and other edits (a3c1c3f)\n\n\n\nRefactors\n\nadd deferrable decorator (b09d978)\nadd type annotations to set operation functions (13f593b)\nadd types to Case and Window Builders (b85b424)\nanalysis: remove find_memtables function in favor of node.find() (c4658e7)\nanalysis: remove find_phyisical_tables() function in favor of node.find() (4daf2df)\nanalysis: remove is_analytic function in favor of node.find() (0452810)\nanalysis: remove ScalarAggregate, reduction_to_aggregation and has_multiple_bases (ed75866)\nanalysis: rewrite substitute_unbound to use the new pattern system (885d2ff)\napi: remove deprecated tuple syntax for order_by() (57733e0)\napi: remove interpolation argument (7c242af)\napi: remove string as a parent type from expression API (2db98fb)\narray-apply: adjust array map and array filter representation for easier non-recursive compilation (b91ecf0)\nbackends: adjust backends to work with new array representation (90befb2)\nbigquery: make literals less messy (8d8ad87)\nclickhouse: move ClickhouseTable.insert method to clickhouse backend and remove ClickhouseTable class (c9c72ae)\nclickhouse: remove recursion from the compiler (ccbcdc0)\nclickhouse: use more sqlglot constructs (c7ca7cd)\ncommon: disallow None for Annotation.pattern in favor of using Any() (7434068)\ncommon: factor out base classes to ibis.common.bases from ibis.common.grounds (01671d2)\ncommon: ibis.common.patterns.match() should return with the matched value rather than the context (cbb9b2f)\ncommon: improve error messages raised during validation (f95613a)\ncommon: remove ibis.collections.DotDict (fedd4b1)\ncommon: remove Validator mixin for better clarity (4697e7d)\ncommon: remove ibis.common.parse since it is only used by the datatype parser (557414f)\ncommon: restrict implicit traversals to common builtin collections (8531347)\ncommon: turn annotations into slotted classes (0770e92)\ndatatypes: use sqlglot for parsing backend specific types (fe7ba24)\ndelete unexposed ibis.api.category_label function (24ac5e7)\nexamples: replace pooch with lighter weight pins (521669c)\nflink: reorder registry to match SQL one (93dad5b)\nflink: use built-in DEGREES, RADIANS (33518e9)\nformats: turn TypeParser into a TypeMapper implementation for sqlglot (468bed1)\nir: construct ArrayContains instead of Contains for value.isin(array_value) (e826037)\nir: decompose Contains into InValues and InColumn (fe9a289)\nir: glue patterns and rules together (c20ba7f)\nir: remove deprecated ibis.expr.selectors module (d4161d7)\nir: rename .output_dtype and .output_shape to .dtype and .shape respectively (f9d5403)\nir: replace Cumulative operations by adding where, group_by and order_by kwargs to cumulative APIs (26ffc68)\nir: rewrite ibis.expr.format using node.map() (94ee679)\nir: use @annotated decorator to coerce Selection.order_by and Aggregation.order_by arguments (8b841c1)\nmysql: use describe temporary table to retrieve ibis schema from query (a723637)\nrename ops.Where to ops.IfElse (a64b7ad)\nreplace deprecated classes of type hints (25946f9)\nsnowflake: get query schema using describe of last query id (890d54a)\nsnowflake: remove unnecessary schema setting (9b0e6c8)\nsnowflake: replace custom temp table ddl for memtables with read_parquet (41df410)\nsnowflake: sort column names in the database instead of on the client (fb52814)\ntests: move test_visualize.py to ibis/expr/tests (46d74ee)\ntests: reorganize ibis.expr.decompile and ibis.expr.sql test files to be under the ibis.expr subpackage (d0d006e)\ntests: reorganize operation related tests from ibis.tests.exprs to ibis.expr.operations.tests (3cbe2f3)\ntests: simplify pattern matching tests on Value operations (d87e65a)\ntraverse builtin collections for in deferrable (b5ee8f4)\nuse deferrable to implement deferred case statements (5577d51)\n\n\n\nPerformance\n\ncommon: improve Concrete construction performance (2cb1a55)\nduckdb: improve to_pyarrow performance (5970cfe)\nduckdb: speed up metadata access to support the many-columns use case (2854143)\nduckdb: use information_schema instead of describe select (ef7f69f)\nintroduce quicker abstract base classes (47822c6)\nops: early return if two nodes do not hash to the same value (b0b62cc)\nops: store schema on relation ops to avoid large traversals (0b49c96)\nsnowflake: speed up metadata accesses from the existing schema and database (f2ef129)\n\n\n\nDeprecations\n\napi: deprecate ibis.negate in favor of negate method (47cdbe8)\napi: deprecate ibis.where in favor of ibis.ifelse (995c1bc)\napi: deprecate Table.relabel in favor of Table.rename (dcd9772)\napi: deprecate top-level ibis.geo_* functions in favor of their corresponding methods (71b7106)\napi: replace nullifzero with ifnull and zeroifnull with fillna (ac85d11)"
  },
  {
    "objectID": "release_notes.html#section-3",
    "href": "release_notes.html#section-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "6.2.0 (2023-08-31)",
    "text": "6.2.0 (2023-08-31)\n\nFeatures\n\ntrino: add source application to trino backend (cf5fdb9)\n\n\n\nBug Fixes\n\nbigquery,impala: escape all ASCII escape sequences in string literals (402f5ca)\nbigquery: correctly escape ASCII escape sequences in regex patterns (a455203)\nrelease: pin conventional-changelog-conventionalcommits to 6.1.0 (d6526b8)\ntrino: ensure that list_databases look at all catalogs not just the current one (cfbdbf1)\ntrino: override incorrect base sqlalchemy list_schemas implementation (84d38a1)\n\n\n\nDocumentation\n\ntrino: add connection docstring (507a00e)"
  },
  {
    "objectID": "release_notes.html#section-4",
    "href": "release_notes.html#section-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "6.1.0 (2023-08-03)",
    "text": "6.1.0 (2023-08-03)\n\nFeatures\n\napi: add ibis.dtype top-level API (867e5f1)\napi: add table.nunique() for counting unique table rows (adcd762)\napi: allow mixing literals and columns in ibis.array (3355dd8)\napi: improve efficiency of __dataframe__ protocol (15e27da)\napi: support boolean literals in join API (c56376f)\narrays: add concat method equivalent to __add__/__radd__ (0ed0ab1)\narrays: add repeat method equivalent to __mul__/__rmul__ (b457c7b)\nbackends: add current_schema API (955a9d0)\nbigquery: fill out CREATE TABLE DDL options including support for overwrite (5dac7ec)\ndatafusion: add count_distinct, median, approx_median, stddev and var aggregations (45089c4)\ndatafusion: add extract url fields functions (4f5ea98)\ndatafusion: add functions sign, power, nullifzero, log (ef72e40)\ndatafusion: add RegexSearch, StringContains and StringJoin (4edaab5)\ndatafusion: implement in-memory table (d4ec5c2)\nflink: add tests and translation rules for additional operators (fc2aa5d)\nflink: implement translation rules and tests for over aggregation in Flink backend (e173cd7)\nflink: implement translation rules for literal expressions in flink compiler (a8f4880)\nimproved error messages when missing backend dependencies (2fe851b)\nmake output of to_sql a proper str subclass (084bdb9)\npandas: add ExtractURLField functions (e369333)\npolars: implement ops.SelfReference (983e393)\npyspark: read/write delta tables (d403187)\nrefactor ddl for create_database and add create_schema where relevant (d7a857c)\nsqlite: add scalar python udf support to sqlite (92f29e6)\nsqlite: implement extract url field functions (cb1956f)\ntrino: implement support for .sql table expression method (479bc60)\ntrino: support table properties when creating a table (b9d65ef)\n\n\n\nBug Fixes\n\napi: allow scalar window order keys (3d3f4f3)\nbackends: make current_database implementation and API consistent across all backends (eeeeee0)\nbigquery: respect the fully qualified table name at the init (a25f460)\nclickhouse: check dispatching instead of membership in the registry for has_operation (acb7f3f)\ndatafusion: always quote column names to prevent datafusion from normalizing case (310db2b)\ndeps: update dependency datafusion to v27 (3a311cd)\ndruid: handle conversion issues from string, binary, and timestamp (b632063)\nduckdb: avoid double escaping backslashes for bind parameters (8436f57)\nduckdb: cast read_only to string for connection (27e17d6)\nduckdb: deduplicate results from list_schemas() (172520e)\nduckdb: ensure that current_database returns the correct value (2039b1e)\nduckdb: handle conversion from duckdb_engine unsigned int aliases (e6fd0cc)\nduckdb: map hugeint to decimal to avoid information loss (4fe91d4)\nduckdb: run pre-execute-hooks in duckdb before file export (5bdaa1d)\nduckdb: use regexp_matches to ensure that matching checks containment instead of a full match (0a0cda6)\nexamples: remove example datasets that are incompatible with case-insensitive file systems (4048826)\nexprs: ensure that left_semi and semi are equivalent (bbc1eb7)\nforward arguments through __dataframe__ protocol (50f3be9)\nir: change “it not a” to “is not a” in errors (d0d463f)\nmemtable: implement support for translation of empty memtable (05b02da)\nmysql: fix UUID type reflection for sqlalchemy 2.0.18 (12d4039)\nmysql: pass-through kwargs to connect_args (e3f3e2d)\nops: ensure that name attribute is always valid for ops.SelfReference (9068aca)\npolars: ensure that pivot_longer works with more than one column (822c912)\npolars: fix collect implementation (c1182be)\npostgres: by default use domain socket (e44fdfb)\npyspark: make has_operation method a [@classmethod](https://github.com/classmethod) (c1b7dbc)\nrelease: use @google/semantic-release-replace-plugin@1.2.0 to avoid module loading bug (673aab3)\nsnowflake: fix broken unnest functionality (207587c)\nsnowflake: reset the schema and database to the original schema after creating them (54ce26a)\nsnowflake: reset to original schema when resetting the database (32ff832)\nsnowflake: use regexp_instr != 0 instead of REGEXP keyword (06e2be4)\nsqlalchemy: add support for sqlalchemy string subclassed types (8b33b35)\nsql: handle parsing aliases (3645cf4)\ntrino: handle all remaining common datatype parsing (b3778c7)\ntrino: remove filter index warning in Trino dialect (a2ae7ae)\n\n\n\nDocumentation\n\nadd conda/mamba install instructions for specific backends (c643fca)\nadd docstrings to DataType.is_* methods (ed40fdb)\nbackend-matrix: add ability to select a specific subset of backends (f663066)\nbackends: document memtable support and performance for each backend (b321733)\nblog: v6.0.0 release blog (21fc5da)\ndocument versioning policy (242ea15)\ndot-sql: add examples of mixing ibis expressions and SQL strings (5abd30e)\ndplyr: small fixes to the dplyr getting started guide (4b57f7f)\nexpand docstring for dtype function (39b7a24)\nfix functions names in examples of extract url fields (872445e)\nfix heading in 6.0.0 blog (0ad3ce2)\noracle: add note about old password checks in oracle (470b90b)\npostgres: fix postgres memtable docs (7423eb9)\nrelease-notes: fix typo (a319e3a)\nsocial: add social media preview cards (e98a0a6)\nupdate imports/exports for pyspark backend (16d73c4)\n\n\n\nRefactors\n\npyarrow: remove unnecessary calls to combine_chunks (c026d2d)\npyarrow: use schema.empty_table() instead of manually constructing empty tables (c099302)\nresult-handling: remove result_handler in favor of expression specific methods (3dc7143)\nsnowflake: enable multiple statements and clean up duplicated parameter setting code (75824a6)\ntests: clean up backend test setup to make non-data-loading steps atomic (16b4632)"
  },
  {
    "objectID": "release_notes.html#section-5",
    "href": "release_notes.html#section-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "6.0.0 (2023-07-05)",
    "text": "6.0.0 (2023-07-05)\n\n⚠ BREAKING CHANGES\n\nimports: Use of ibis.udf as a module is removed. Use ibis.legacy.udf instead.\nThe minimum supported Python version is now Python 3.9\napi: group_by().count() no longer automatically names the count aggregation count. Use relabel to rename columns.\nbackends: Backend.ast_schema is removed. Use expr.as_table().schema() instead.\nsnowflake/postgres: Postgres UDFs now use the new @udf.scalar.python API. This should be a low-effort replacement for the existing API.\nir: ops.NullLiteral is removed\ndatatypes: dt.Interval has no longer a default unit, dt.interval is removed\ndeps: snowflake-connector-python’s lower bound was increased to 3.0.2, the minimum version needed to avoid a high-severity vulnerability. Please upgrade snowflake-connector-python to at least version 3.0.2.\napi: Table.difference(), Table.intersection(), and Table.union() now require at least one argument.\npostgres: Ibis no longer automatically defines first/last reductions on connection to the postgres backend. Use DDL shown in https://wiki.postgresql.org/wiki/First/last_(aggregate) or one of the pgxn implementations instead.\napi: ibis.examples.&lt;example-name&gt;.fetch no longer forwards arbitrary keyword arguments to read_csv/read_parquet.\ndatatypes: dt.Interval.value_type attribute is removed\napi: Table.count() is no longer automatically named \"count\". Use Table.count().name(\"count\") to achieve the previous behavior.\ntrino: The trino backend now requires at least version 0.321 of the trino Python package.\nbackends: removed AlchemyTable, AlchemyDatabase, DaskTable, DaskDatabase, PandasTable, PandasDatabase, PySparkDatabaseTable, use ops.DatabaseTable instead\ndtypes: temporal unit enums are now available under ibis.common.temporal instead of ibis.common.enums.\nclickhouse: external_tables can no longer be passed in ibis.clickhouse.connect. Pass external_tables directly in raw_sql/execute/to_pyarrow/to_pyarrow_batches().\ndatatypes: dt.Set is now an alias for dt.Array\nbigquery: Before this change, ibis timestamp is mapping to Bigquery TIMESTAMP type and no timezone supports. However, it’s not correct, BigQuery TIMESTAMP type should have UTC timezone, while DATETIME type is the no timezone version. Hence, this change is breaking the ibis timestamp mapping to BigQuery: If ibis timestamp has the UTC timezone, will map to BigQuery TIMESTAMP type. If ibis timestamp has no timezone, will map to BigQuery DATETIME type.\nimpala: Cursors are no longer returned from DDL operations to prevent resource leakage. Use raw_sql if you need specialized operations that return a cursor. Additionally, table-based DDL operations now return the table they’re operating on.\napi: Column.first()/Column.last() are now reductions by default. Code running these expressions in isolation will no longer be windowed over the entire table. Code using this function in select-based APIs should function unchanged.\nbigquery: when using the bigquery backend, casting float to int will no longer round floats to the nearest integer\nops.Hash: The hash method on table columns on longer accepts the how argument. The hashing functions available are highly backend-dependent and the intention of the hash operation is to provide a fast, consistent (on the same backend, only) integer value. If you have been passing in a value for how, you can remove it and you will get the same results as before, as there were no backends with multiple hash functions working.\nduckdb: Some CSV files may now have headers that did not have them previously. Set header=False to get the previous behavior.\ndeps: New environments will have a different default setting for compression in the ClickHouse backend due to removal of optional dependencies. Ibis is still capable of using the optional dependencies but doesn’t include them by default. Install clickhouse-cityhash and lz4 to preserve the previous behavior.\napi: Table.set_column() is removed; use Table.mutate(name=expr) instead\napi: the suffixes argument in all join methods has been removed in favor of lname/rname args. The default renaming scheme for duplicate columns has also changed. To get the exact same behavior as before, pass in lname=\"{name}_x\", rname=\"{name}_y\".\nir: IntervalType.unit is now an enum instead of a string\ntype-system: Inferred types of Python objects may be slightly different. Ibis now use pyarrow to infer the column types of pandas DataFrame and other types.\nbackends: path argument of Backend.connect() is removed, use the database argument instead\napi: removed Table.sort_by() and Table.groupby(), use .order_by() and .group_by() respectively\ndatatypes: DataType.scalar and column class attributes are now strings.\nbackends: Backend.load_data(), Backend.exists_database() and Backend.exists_table() are removed\nir: Value.summary() and NumericValue.summary() are removed\nschema: Schema.merge() is removed, use the union operator schema1 | schema2 instead\napi: ibis.sequence() is removed\ndrop support for Python 3.8 (747f4ca)\n\n\n\nFeatures\n\nadd dask windowing (9cb920a)\nadd easy type hints to GroupBy (da330b1)\nadd microsecond method to TimestampValue and TimeValue (e9df2da)\napi: add __dataframe__ implementation (b3d9619)\napi: add ALL_CAPS option to Table.relabel (c0b30e2)\napi: add first/last reduction APIs (8c01980)\napi: add zip operation and api (fecf695)\napi: allow passing multiple keyword arguments to ibis.interval (22ee854)\napi: better repr and pickle support for deferred expressions (2b1ec9c)\napi: exact median (c53031c)\napi: raise better error on column name collision in joins (e04c38c)\napi: replace suffixes in join with lname/rname (3caf3a1)\napi: support abstract type names in selectors.of_type (f6d2d56)\napi: support list of strings and single strings in the across selector (a6b60e7)\napi: use create_table to load example data (42e09a4)\nbigquery: add client and storage_client params to connect (4cf1354)\nbigquery: enable group_concat over windows (d6a1117)\ncast: add table-level try_cast (5e4d16b)\nclickhouse: add array zip impl (efba835)\nclickhouse: move to clickhouse supported Python client (012557a)\nclickhouse: set default engine to native file (29815fa)\nclickhouse: support pyarrow decimal types (7472dd5)\ncommon: add a pure python egraph implementation (aed2ed0)\ncommon: add pattern matchers (b515d5c)\ncommon: add support for start parameter in StringFind (31ce741)\ncommon: add Topmost and Innermost pattern matchers (90b48fc)\ncommon: implement copy protocol for Immutable base class (e61c66b)\ncreate_table: support pyarrow Table in table creation (9dbb25c)\ndatafusion: add string functions (66c0afb)\ndatafusion: add support for scalar pyarrow UDFs (45935b7)\ndatafusion: minimal decimal support (c550780)\ndatafusion: register tables and datasets in datafusion (cb2cc58)\ndatatypes: add support for decimal values with arrow-based APIs (b4ba6b9)\ndatatypes: support creating Timestamp from units (66f2ff0)\ndeps: load examples lazily (4ea0ddb)\nduckdb: add attach_sqlite method (bd32649)\nduckdb: add support for native and pyarrow UDFs (7e56fc4)\nduckdb: expand map support to .values() and map concatenation (ad49a09)\nduckdb: set header=True by default (e4b515d)\nduckdb: support 0.8.0 (ae9ae7d)\nduckdb: support array zip operation (2d14ccc)\nduckdb: support motherduck (053dc7e)\nduckdb: warn when querying an already consumed RecordBatchReader (5a013ff)\nflink: add initial flink SQL compiler (053a6d2)\nformats: support timestamps in delta output; default to micros for pyarrow conversion (d8d5710)\nimplement read_delta and to_delta for some backends (74fc863)\nimplement read_delta for datafusion (eb4602f)\nimplement try_cast for a few backends (f488f0e)\nio: add to_torch API (685c8fc)\nio: add az/gs prefixes to normalize_filename in utils (e9eebba)\nmysql: add re_extract (5ed40e1)\noracle: add oracle backend (c9b038b)\noracle: support temporary tables (6e64cd0)\npandas: add approx_median (6714b9f)\npandas: support passing memtables to create_table (3ea9a21)\npolars: add any and all reductions (0bd3c01)\npolars: add argmin and argmax (78562d3)\npolars: add correlation operation (05ff488)\npolars: add polars support for identical_to (aab3bae)\npolars: add support for offset, binary literals, and dropna(how='all') (d2298e9)\npolars: allow seamless connection for DataFrame as well as LazyFrame (a2a3e45)\npolars: implement .sql methods (86f2a34)\npolars: lower-latency column return for non-temporal results (b009563)\npolars: support pyarrow decimal types (7e6c365)\npolars: support SQL dialect translation (c87f695)\npolars: support table registration from multiple parquet files (9c0a8be)\npostgres: add ApproxMedian aggregation (887f572)\npyspark: add zip array impl (6c00cbc)\nsnowflake/postgres: scalar UDFs (dbf5b62)\nsnowflake: implement array zip (839e1f0)\nsnowflake: implement proper approx median (b15a6fe)\nsnowflake: support SSO and other forms of passwordless authentication (23ac53d)\nsnowflake: use the client python version as the UDF runtime where possible (69a9101)\nsql: allow any SQL dialect accepted by sqlgllot in Table.sql and Backend.sql (f38c447)\nsqlite: add argmin and argmax functions (c8af9d4)\nsqlite: add arithmetic mode aggregation (6fcac44)\nsqlite: add ops.DateSub, ops.DateAdd, ops.DateDiff (cfd65a0)\nstreamlit: add support for streamlit connection interface (05c9449)\ntrino: implement zip (cd11daa)\n\n\n\nBug Fixes\n\nadd issue write permission to assign.yml (9445cee)\nalchemy: close the cursor on error during dataframe construction (cc7dffb)\nbackends: fix capitalize to lowercase subsequent characters (49978f9)\nbackends: fix notall/notany translation (56b56b3)\nbigquery: add srid=4326 to the geography dtype mapping (57a825b)\nbigquery: allow passing both schema and obj in create_table (49cc2c4)\nbigquery: bigquery timestamp and datetime dtypes (067e8a5)\nbigquery: ensure that bigquery temporal ops work with the new timeunit/dateunit/intervalunit enums (0e00d86)\nbigquery: ensure that generated names are used when compiling columns and allow flexible column names (c7044fe)\nbigquery: fix table naming from count rename removal refactor (5b009d2)\nbigquery: raise OperationNotDefinedError for IntervalAdd and IntervalSubtract (501aaf7)\nbigquery: support capture group functionality (3f4f05b)\nbigquery: truncate when casting float to int (267d8e1)\nci: use mariadb-admin instead of mysqladmin in mariadb 11.x (d4ccd3d)\nclickhouse: avoid generating names for structs (5d11f48)\nclickhouse: clean up external tables per query to avoid leaking them across queries (6d32edd)\nclickhouse: close cursors more aggressively (478a40f)\nclickhouse: use correct functions for milli and micro extraction (49b3136)\nclickhouse: use named rather than positional group by (1f7e309)\nclickhouse: use the correct dialect to generate subquery string for Contains operation (f656bd5)\ncommon: fix bug in re_extract (6ebaeab), closes #6167\ncore: interval resolution should upcast to smallest unit (f7f844d), closes #6139\ndatafusion: fix incorrect order of predicate -&gt; select compilation (0092304)\ndeps: make pyarrow a required dependency (b217cde)\ndeps: prevent vulnerable snowflake-connector-python versions (6dedb45)\ndeps: support multipledispatch version 1 (805a7d7)\ndeps: update dependency atpublic to v4 (3a44755)\ndeps: update dependency datafusion to v22 (15d8d11)\ndeps: update dependency datafusion to v23 (e4d666d)\ndeps: update dependency datafusion to v24 (c158b78)\ndeps: update dependency datafusion to v25 (c3a6264)\ndeps: update dependency datafusion to v26 (7e84ffe)\ndeps: update dependency deltalake to &gt;=0.9.0,&lt;0.11.0 (9817a83)\ndeps: update dependency pyarrow to v12 (3cbc239)\ndeps: update dependency sqlglot to v12 (5504bd4)\ndeps: update dependency sqlglot to v13 (1485dd0)\ndeps: update dependency sqlglot to v14 (9c40c06)\ndeps: update dependency sqlglot to v15 (f149729)\ndeps: update dependency sqlglot to v16 (46601ef)\ndeps: update dependency sqlglot to v17 (9b50fb4)\ndocs: fix failing doctests (04b9f19)\ndocs: typo in code without selectors (b236893)\ndocs: typo in docstrings and comments (0d3ed86)\ndocs: typo in snowflake do_connect kwargs (671bc31)\nduckdb: better types for null literals (7b9d85e)\nduckdb: disable map values and map merge for columns (b5472b3)\nduckdb: ensure to_timestamp returns a UTC timestamp (0ce0b9f)\nduckdb: ensure connection lifetime is greater than or equal to record batch reader lifetime (6ed353e)\nduckdb: ensure that quoted struct field names work (47de1c3)\nduckdb: ensure that types are inferred correctly across duckdb_engine versions (9c3d173)\nduckdb: fix check for literal maps (b2b229b)\nduckdb: fix exporting pyarrow record batches by bumping duckdb to 0.8.1 (aca52ab)\nduckdb: fix read_csv problem with kwargs (6f71735), closes #6190\nexamples: move lockfile creation to data directory (b8f6e6b)\nexamples: use filelock to prevent pooch from clobbering files when fetching concurrently (e14662e)\nexpr: fix graphviz rendering (6d4a34f)\nimpala: do not cast ca_cert None value to string (bfdfb0e)\nimpala: expose hdfs_connect function as ibis.impala.hdfs_connect (27a0d12)\nimpala: more aggressively clean up cursors internally (bf5687e)\nimpala: replace time_mapping with TIME_MAPPING and backwards compatible check (4c3ca20)\nir: force an alias if projecting or aggregating columns (9fb1e88)\nir: raise Exception for group by with no keys (845f7ab), closes #6237\nmssql: dont yield from inside a cursor (4af0731)\nmysql: do not fail when we cannot set the session timezone (930f8ab)\nmysql: ensure enum string functions are coerced to the correct type (e499c7f)\nmysql: ensure that floats and double do not come back as Python Decimal objects (a3c329f)\nmysql: fix binary literals (e081252)\nmysql: handle the zero timestamp value (9ac86fd)\noperations: ensure that self refs have a distinct name from the table they are referencing (bd8eb88)\noracle: disable autoload when cleaning up temp tables (b824142)\noracle: disable statement cache (41d3857)\noracle: disable temp tables to get inserts working (f9985fe)\npandas, dask: allow overlapping non-predicate columns in asof join (09e26a0)\npandas: fix first and last over windows (9079bc4), closes #5417\npandas: fix string translate function (12b9569), closes #6157\npandas: grouped aggregation using a case statement (d4ac345)\npandas: preserve RHS values in asof join when column names collide (4514668)\npandas: solve problem with first and last window function (dfdede5), closes #4918\npolars: avoid implode deprecation warning (ce3bdad)\npolars: ensure that to_pyarrow is called from the backend (41bacf2)\npolars: make list column operations backwards compatible (35fc5f7)\npostgres: ensure that alias method overwrites view even if types are different (7d5845b)\npostgres: ensure that backend still works when create/drop first/last aggregates fails (eb5d534)\npyspark: enable joining on columns with different names as well as complex predicates (dcee821)\nsnowflake: always use pyarrow for memtables (da34d6f)\nsnowflake: ensure connection lifetime is greater than or equal to record batch reader lifetime (34a0c59)\nsnowflake: ensure that _pandas_converter attribute is resolved correctly (9058bbe)\nsnowflake: ensure that temp tables are only created once (43b8152)\nsnowflake: ensure unnest works for nested struct/object types (fc6ffc2)\nsnowflake: ensure use of the right timezone value (40426bf)\nsnowflake: fix tmpdir construction for python &lt;3.10 (a507ae2)\nsnowflake: fix incorrect arguments to snowflake regexp_substr (9261f70)\nsnowflake: fix invalid attribute access when using pyarrow (bfd90a8)\nsnowflake: handle broken upstream behavior when a table can’t be found (31a8366)\nsnowflake: resolve import error from interval datatype refactor (3092012)\nsnowflake: use convert_timezone for timezone conversion instead of invalid postgres AT TIME ZONE syntax (1595e7b)\nsqlalchemy: ensure that backends don’t clobber tables needed by inputs (76e38a3)\nsqlalchemy: ensure that union_all-generated memtables use the correct column names (a4f546b)\nsqlalchemy: prepend the table’s schema when querying metadata (d8818e2)\nsqlalchemy: quote struct field names (f5c91fc)\ntests: ensure that record batch readers are cleaned up (d230a8d)\ntrino: bump lower bound to avoid having to handle experimental_python_types (bf6eeab)\ntrino: ensure that nested array types are inferred correctly (030f76d)\ntrino: fix incorrect version computation (04d3a89)\ntrino: support trino 0.323 special tuple type for struct results (ea1529d)\ntype-system: infer in-memory object types using pyarrow (f7018ee)\ntypehint: update type hint for class instance (2e1e14f)\n\n\n\nDocumentation\n\nacross: add documentation for across (b8941d3)\nadd allowed input for memtable constructor (69cdee5)\nadd disclaimer on no row order guarantees (75dd8b0)\nadd examples to if_any and if_all (5015677)\nadd platform comment in conda env creation (e38eacb)\nadd read_delta and related to backends docs (90eaed2)\napi: ensure all top-level items have a description (c83d783)\napi: hide dunder methods in API docs (6724b7b)\napi: manually add inherited mixin methods to timey classes (7dbc96d)\napi: show source for classes to allow dunder method inspection (4cef0f8)\nbackends: fix typo in pip install command (6a7207c)\nbigquery: add connection explainer to bigquery backend docs (84caa5b)\nblog: add Ibis + PyTorch + DuckDB blog post (1ad946c)\nchange plural variable name cols to col (c33a3ed), closes #6115\nclarify map refers to Python Mapping container (f050a61)\ncss: enable code block copy button, don’t select prompt (3510abe)\nde-template remaining backends (except pandas, dask, impala) (82b7408)\ndescribe NULL differences with pandas (688b293)\ndev-env: remove python 3.8 from environment support matrix (4f89565)\ndrop docker-compose install for conda dev env setup (e19924d)\nduckdb: add quick explainer on connecting to motherduck (4ef710e)\nfile support: add badge and docstrings for read_* methods (0767b7c)\nfill out more docstrings (dc0289c)\nfix errors and add ‘table’ before ‘expression’ (096b568)\nfix some redirects (3a23c1f)\nfix typo in Table.relabel return description (05cc51e)\ngeneric: add docstring examples in types/generic (1d87292)\nguides: add brief installation instructions at top of notebooks (dc3e694)\nguides: update ibis-for-dplyr-users.ipynb with latest (1aa172e), closes #6125\nimprove docstrings for BooleanValue and BoleanColumn (30c1009)\nimprove docstrings to map types (72a49b0)\ninstall: add quotes to all bracketed installs for shell compatibility (bb5c075)\nintersphinx: add mapping to autolink pyarrow and pandas refs (cd92019)\nintro: create Ibis for dplyr users document (e02a6f2)\nintroguides: use DuckDB for intro pandas notebook, remove iris (a7e845a)\nlink to Ibis for dplyr users (6e7c6a2)\nmake pandas.md filename lowercase (4937d45)\nmore group_by() and NULL in pandas guide (486b696)\nmore spelling fixes (564abbe)\nmove API docs to top-level (dcc409f)\nnumeric: add examples to numeric methods (39b470f)\noracle: add basic backend documentation (c871790)\noracle: add oracle to matrix (89aecf2)\npython-versions: document how we decide to drop support for Python versions (3474dbc)\nredirect Pandas to pandas (4074284)\nremove trailing whitespace (63db643)\nreorder sections in pandas guide (3b66093)\nrestructure and consistency (351d424)\nsnowflake: add connection explainer to snowflake backend docs (a62bbcd)\nstreamlit: fix ibis-framework install (a8cf773)\nupdate copyright and some minor edits (b9aed44)\nupdate notany/notall docstrings with arg (a5ec986), closes #5993\nupdate structs and fix constructor docstrings (493437a)\nuse lowercase pandas (19b5d10)\nuse to_pandas instead of execute (882949e)\n\n\n\nRefactors\n\nalchemy: abstract out custom type mapping and fix sqlite (d712e2e)\napi: consolidate ibis.date(), ibis.time() and ibis.timestamp() functions (20f71bf)\napi: enforce at least one argument for Table set operations (57e948f)\napi: remove automatic count name from relations (2cb19ec)\napi: remove automatic group by count naming (15d9e50)\napi: remove deprecated ibis.sequence() function (de0bf69)\napi: remove deprecated Table.set_column() method (aa5ed94)\napi: remove deprecated Table.sort_by() and Table.groupby() methods (1316635)\nbackends: remove ast_schema method (51b5ef8)\nbackends: remove backend specific DatabaseTable operations (d1bab97)\nbackends: remove deprecated Backend.load_data(), .exists_database() and .exists_table() methods (755555f)\nbackends: remove deprecated path argument of Backend.connect() (6737ea8)\nbigquery: align datatype conversions with the new convention (70b8232)\nbigquery: support a broader range of interval units in temporal binary operations (f78ce73)\ncommon: add sanity checks for creating ENodes and Patterns (fc89cc3)\ncommon: cleanup unit conversions (73de24e)\ncommon: disallow unit conversions between days and hours (5619ce0)\ncommon: move ibis.collections.DisjointSet to ibis.common.egraph (07dde21)\ncommon: move tests for re_extract to general suite (acd1774)\ncommon: use an enum as a sentinel value instead of NoMatch class (6674353), closes #6049\ndask/pandas: align datatype conversions with the new convention (cecc24c)\ndatatypes: make pandas conversion backend specific if needed (544d27c)\ndatatypes: normalize interval values to integers (80a40ab)\ndatatypes: remove Set() in favor of Array() datatype (30a4f7e)\ndatatypes: remove value_type parametrization of the Interval datatype (463cdc3)\ndatatypes: remove direct ir dependency from datatypes (d7f0be0)\ndatatypes: use typehints instead of rules (704542e)\ndeps: remove optional dependency on clickhouse-cityhash and lz4 (736fe26)\ndtypes: add normalize_datetime() and normalize_timezone() common utilities (c00ab38)\ndtypes: turn dt.dtype() into lazily dispatched factory function (5261003)\nformats: consolidate the dataframe conversion logic (53ed88e)\nformats: encapsulate conversions to TypeMapper, SchemaMapper and DataMapper subclasses (ab35311)\nformats: introduce a standalone subpackage to deal with common in-memory formats (e8f45f5)\nimpala: rely on impyla cursor for _wait_synchronous (a1b8736)\nimports: move old UDF implementation to ibis.legacy module (cf93d5d)\nir: encapsulate temporal unit handling in enums (1b8fa7b)\nir: remove rlz.column_from, rlz.base_table_of and rlz.function_of rules (ed71d51)\nir: remove deprecated Value.summary() and NumericValue.summary() expression methods (6cd8050)\nir: remove redundant ops.NullLiteral() operation (a881703)\nir: simplify Expr._find_backends() implementation by using the ibis.common.graph utilities (91ff8d4)\nir: use dt.normalize() to construct literals (bf72f16)\nops.Hash: remove how from backend-specific hash operation (46a55fc)\npandas: solve and remove stale TODOs (92d979e)\npolars: align datatype conversion functions with the new convention (5d61159)\npostgres: fail at execute time for UDFs to avoid db connections in .compile() (e3a4d4d)\npyspark: align datatype conversion functions with the new convention (3437bb6)\npyspark: remove useless window branching in compiler (ad08da4)\nreplace custom _merge using pd.merge (fe74f76)\nschema: remove deprecated Schema.merge() method (d307722)\nschema: use type annotations instead of rules (98cd539)\nsnowflake: add flags to supplemental JavaScript UDFs (054add4)\nsql: align datatype conversions with the new convention (0ef145b)\nsqlite: remove roundtripping for DayOfWeekIndex and DayOfWeekName (b5a2bc5)\ntest: cleanup test data (7ae2b24)\nto-pyarrow-batches: ensure that batch readers are always closed and exhausted (35a391f)\ntrino: always clean up prepared statements created when accessing query metadata (4f3a4cd)\nutil: use base32 to compress uuid table names (ba039a3)\n\n\n\nPerformance\n\nimports: speed up checking for geospatial support (aa601af)\nsnowflake: use pyarrow for all transport (1fb89a1)\nsqlalchemy: lazily construct the inspector object (8db5624)\n\n\n\nDeprecations\n\napi: deprecate tuple syntax for order by keys (5ed5110)"
  },
  {
    "objectID": "release_notes.html#section-6",
    "href": "release_notes.html#section-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "5.1.0 (2023-04-11)",
    "text": "5.1.0 (2023-04-11)\n\nFeatures\n\napi: expand distinct API for dropping duplicates based on column subsets (3720ea5)\napi: implement pyarrow memtables (9d4fbbd)\napi: support passing a format string to Table.relabel (0583959)\napi: thread kwargs around properly to support more complex connection arguments (7e0e15b)\nbackends: add more array functions (5208801)\nbigquery: make to_pyarrow_batches() smarter (42f5987)\nbigquery: support bignumeric type (d7c0f49)\ndefault repr to showing all columns in Jupyter notebooks (91a0811)\ndruid: add re_search support (946202b)\nduckdb: add map operations (a4c4e77)\nduckdb: support sqlalchemy 2 (679bb52)\nmssql: implement ops.StandardDev, ops.Variance (e322f1d)\npandas: support memtable in pandas backend (6e4d621), closes #5467\npolars: implement count distinct (aea4ccd)\npostgres: implement ops.Arbitrary (ee8dbab)\npyspark: pivot_longer (f600c90)\npyspark: add ArrayFilter operation (2b1301e)\npyspark: add ArrayMap operation (e2c159c)\npyspark: add DateDiff operation (bfd6109)\npyspark: add partial support for interval types (067120d)\npyspark: add read_csv, read_parquet, and register (7bd22af)\npyspark: implement count distinct (db29e10)\npyspark: support basic caching (ab0df7a)\nsnowflake: add optional ‘connect_args’ param (8bf2043)\nsnowflake: native pyarrow support (ce3d6a4)\nsqlalchemy: support unknown types (fde79fa)\nsqlite: implement ops.Arbitrary (9bcdf77)\nsql: use temp views where possible (5b9d8c0)\ntable: implement pivot_wider API (60e7731)\nux: move ibis.expr.selectors to ibis.selectors and deprecate for removal in 6.0 (0ae639d)\n\n\n\nBug Fixes\n\napi: disambiguate attribute errors from a missing resolve method (e12c4df)\napi: support filter on literal followed by aggregate (68d65c8)\nclickhouse: do not render aliases when compiling aggregate expression components (46caf3b)\nclickhouse: ensure that clickhouse depends on sqlalchemy for make_url usage (ea10a27)\nclickhouse: ensure that truncate works (1639914)\nclickhouse: fix create_table implementation (5a54489)\nclickhouse: workaround sqlglot issue with calling match (762f4d6)\ndeps: support pandas 2.0 (4f1d9fe)\nduckdb: branch to avoid unnecessary dataframe construction (9d5d943)\nduckdb: disable the progress bar by default (1a1892c)\nduckdb: drop use of experimental parallel csv reader (47d8b92)\nduckdb: generate SIMILAR TO instead of tilde to workaround sqlglot issue (434da27)\nimprove typing signature of .dropna() (e11de3f)\nmssql: improve aggregation on expressions (58aa78d)\nmssql: remove invalid aggregations (1ce3ef9)\npolars: backwards compatibility for the time_zone and time_unit properties (3a2c4df)\npostgres: allow inference of unknown types (343fb37)\npyspark: fail when aggregation contains a having filter (bd81a9f)\npyspark: raise proper error when trying to generate sql (51afc13)\nsnowflake: fix new array operations; remove ArrayRemove operation (772668b)\nsnowflake: make sure ephemeral tables following backend quoting rules (9a845df)\nsnowflake: make sure pyarrow is used when possible (01f5154)\nsql: ensure that set operations resolve to a single relation (3a02965)\nsql: generate consistent pivot_longer semantics in the presence of multiple unnests (6bc301a)\nsqlglot: work with newer versions (6f7302d)\ntrino,duckdb,postgres: make cumulative notany/notall aggregations work (c2e985f)\ntrino: only support how='first' with arbitrary reduction (315b5e7)\nux: use guaranteed length-1 characters for NULL values (8618789)\n\n\n\nRefactors\n\napi: remove explicit use of .projection in favor of the shorter .select (73df8df)\ncache: factor out ref counted cache (c816f00)\nduckdb: simplify to_pyarrow_batches implementation (d6235ee)\nduckdb: source loaded and installed extensions from duckdb (fb06262)\nduckdb: use native duckdb parquet reader unless auth required (e9f57eb)\ngenerate uuid-based names for temp tables (a1164df)\nmemtable: clean up dispatch code (9a19302)\nmemtable: dedup table proxy code (3bccec0)\nsqlalchemy: remove unused _meta instance attributes (523e198)\n\n\n\nDeprecations\n\napi: deprecate Table.set_column in favor of Table.mutate (954a6b7)\n\n\n\nDocumentation\n\nadd a getting started guide (8fd03ce)\nadd warning about comparisons to None (5cf186a)\nblog: add campaign finance blog post (383c708)\nblog: add campaign finance to SUMMARY.md (0bdd093)\nclean up agg argument descriptions and add join examples (93d3059)\ncomparison: add a “why ibis” page (011cc19)\nmove conda before nix in dev setup instructions (6b2cbaa)\nnth: improve docstring for nth() (fb7b34b)\npatch docs build to fix anchor links (51be459)\npenguins: add citation for palmer penguins data (679848d)\npenguins: change to flipper (eec3706)\nrefresh environment setup pages (b609571)\nselectors: make doctests more complete and actually run them (c8f2964)\nstyle and review fixes in getting started guide (3b0f8db)"
  },
  {
    "objectID": "release_notes.html#section-7",
    "href": "release_notes.html#section-7",
    "title": "2.1.0 (2022-01-12)",
    "section": "5.0.0 (2023-03-15)",
    "text": "5.0.0 (2023-03-15)\n\n⚠ BREAKING CHANGES\n\napi: Snowflake identifiers are now kept as is from the database. Many table names and column names may now be in SHOUTING CASE. Adjust code accordingly.\nbackend: Backends now raise ibis.common.exceptions.UnsupportedOperationError in more places during compilation. You may need to catch this error type instead of the previous type, which differed between backends.\nux: Table.info now returns an expression\nux: Passing a sequence of column names to Table.drop is removed. Replace drop(cols) with drop(*cols).\nThe spark plugin alias is removed. Use pyspark instead\nir: removed ibis.expr.scope and ibis.expr.timecontext modules, access them under ibis.backends.base.df.&lt;module&gt;\nsome methods have been removed from the top-level ibis.&lt;backend&gt; namespaces, access them on a connected backend instance instead.\ncommon: removed ibis.common.geospatial, import the functions from ibis.backends.base.sql.registry.geospatial\ndatatypes: JSON is no longer a subtype of String\ndatatype: Category, CategoryValue/Column/Scalar are removed. Use string types instead.\nux: The metric_name argument to value_counts is removed. Use Table.relabel to change the metric column’s name.\ndeps: the minimum version of parsy is now 2.0\nir/backends: removed the following symbols:\nibis.backends.duckdb.parse_type() function\nibis.backends.impala.Backend.set_database() method\nibis.backends.pyspark.Backend.set_database() method\nibis.backends.impala.ImpalaConnection.ping() method\nibis.expr.operations.DatabaseTable.change_name() method\nibis.expr.operations.ParseURL class\nibis.expr.operations.Value.to_projection() method\nibis.expr.types.Table.get_column() method\nibis.expr.types.Table.get_columns() method\nibis.expr.types.StringValue.parse_url() method\nschema: Schema.from_dict(), .delete() and .append() methods are removed\ndatatype: struct_type.pairs is removed, use struct_type.fields instead\ndatatype: Struct(names, types) is not supported anymore, pass a dictionary to Struct constructor instead\n\n\n\nFeatures\n\nadd max_columns option for table repr (a3aa236)\nadd examples API (b62356e)\napi: add map/array accessors for easy conversion of JSON to stronger-typed values (d1e9d11)\napi: add array to string join operation (74de349)\napi: add builtin support for relabeling columns to snake case (1157273)\napi: add support for passing a mapping to ibis.map (d365fd4)\napi: allow single argument set operations (bb0a6f0)\napi: implement to_pandas() API for ecosystem compatibility (cad316c)\napi: implement isin (ac31db2)\napi: make cache evaluate only once per session per expression (5a8ffe9)\napi: make create_table uniform (833c698)\napi: more selectors (5844304)\napi: upcast pandas DataFrames to memtables in rlz.table rule (8dcfb8d)\nbackends: implement ops.Time for sqlalchemy backends (713cd33)\nbigquery: add BIGNUMERIC type support (5c98ea4)\nbigquery: add UUID literal support (ac47c62)\nbigquery: enable subqueries in select statements (ef4dc86)\nbigquery: implement create and drop table method (5f3c22c)\nbigquery: implement create_view and drop_view method (a586473)\nbigquery: support creating tables from in-memory tables (c3a25f1)\nbigquery: support in-memory tables (37e3279)\nchange Rich repr of dtypes from blue to dim (008311f)\nclickhouse: implement ArrayFilter translation (f2144b6)\nclickhouse: implement ops.ArrayMap (45000e7)\nclickhouse: implement ops.MapLength (fc82eaa)\nclickhouse: implement ops.Capitalize (914c64c)\nclickhouse: implement ops.ExtractMillisecond (ee74e3a)\nclickhouse: implement ops.RandomScalar (104aeed)\nclickhouse: implement ops.StringAscii (a507d17)\nclickhouse: implement ops.TimestampFromYMDHMS, ops.DateFromYMD (05f5ae5)\nclickhouse: improve error message for invalid types in literal (e4d7799)\nclickhouse: support asof_join (7ed5143)\ncommon: add abstract mapping collection with support for set operations (7d4aa0f)\ncommon: add support for variadic positional and variadic keyword annotations (baea1fa)\ncommon: hold typehint in the annotation objects (b3601c6)\ncommon: support Callable arguments and return types in Validator.from_annotable() (ae57c36)\ncommon: support positional only and keyword only arguments in annotations (340dca1)\ndask/pandas: raise OperationNotDefinedError exc for not defined operations (2833685)\ndatafusion: implement ops.Degrees, ops.Radians (7e61391)\ndatafusion: implement ops.Exp (7cb3ade)\ndatafusion: implement ops.Pi, ops.E (5a74cb4)\ndatafusion: implement ops.RandomScalar (5d1cd0f)\ndatafusion: implement ops.StartsWith (8099014)\ndatafusion: implement ops.StringAscii (b1d7672)\ndatafusion: implement ops.StrRight (016a082)\ndatafusion: implement ops.Translate (2fe3fc4)\ndatafusion: support substr without end (a19fd87)\ndatatype/schema: support datatype and schema declaration using type annotated classes (6722c31)\ndatatype: enable inference of Decimal type (8761732)\ndatatype: implement Mapping abstract base class for StructType (5df2022)\ndeps: add Python 3.11 support and tests (6f3f759)\ndruid: add Apache Druid backend (c4cc2a6)\ndruid: implement bitwise operations (3ac7447)\ndruid: implement ops.Pi, ops.Modulus, ops.Power, ops.Log10 (090ff03)\ndruid: implement ops.Sign (35f52cc)\ndruid: implement ops.StringJoin (42cd9a3)\nduckdb: add support for reading tables from sqlite databases (9ba2211)\nduckdb: add UUID type support (5cd6d76)\nduckdb: implement ArrayFilter translation (5f35d5c)\nduckdb: implement ops.ArrayMap (063602d)\nduckdb: implement create_view and drop_view method (4f73953)\nduckdb: implement ops.Capitalize (b17116e)\nduckdb: implement ops.TimestampDiff, ops.IntervalAdd, ops.IntervalSubtract (a7fd8fb)\nduckdb: implement uuid result type (3150333)\nduckdb: support dt.MACADDR, dt.INET as string (c4739c7)\nduckdb: use read_json_auto when reading json (4193867)\nexamples: add imdb dataset examples (3d63203)\nexamples: add movielens small dataset (5f7c15c)\nexamples: add wowah_data data to examples (bf9a7cc)\nexamples: enable progressbar and faster hashing (4adfe29)\nimpala: implement ops.Clip (279fd78)\nimpala: implement ops.Radians, ops.Degrees (a794ace)\nimpala: implement ops.RandomScalar (874f2ff)\nio: add to_parquet, to_csv to backends (fecca42)\nir: add ArrayFilter operation (e719d60)\nir: add ArrayMap operation (49e5f7a)\nmysql: support in-memory tables (4dfabbd)\npandas/dask: implement bitwise operations (4994add)\npandas/dask: implement ops.Pi, ops.E (091be3c)\npandas: add basic unnest support (dd36b9d)\npandas: implement ops.StartsWith, ops.EndsWith (2725423)\npandas: support more pandas extension dtypes (54818ef)\npolars: implement ops.Union (17c6011)\npolars: implement ops.Pi, ops.E (6d8fc4a)\npostgres: allow connecting with an explicit schema (39c9ea8)\npostgres: fix interval literal (c0fa933)\npostgres: implement argmin/argmax (82668ec)\npostgres: parse tsvector columns as strings (fac8c47), closes #5402\npyspark: add support for ops.ArgMin and ops.ArgMax (a3fa57c)\npyspark: implement ops.Between (ed83465)\nreturn Table from create_table(), create_view() (e4ea597)\nschema: implement Mapping abstract base class for Schema (167d85a)\nselectors: support ranges (e10caf4)\nsnowflake: add support for alias in snowflake (b1b947a)\nsnowflake: add support for bulk upload for temp tables in snowflake (6cc174f)\nsnowflake: add UUID literal support (436c781)\nsnowflake: implement argmin/argmax (8b998a5)\nsnowflake: implement ops.BitwiseAnd, ops.BitwiseNot, ops.BitwiseOr, ops.BitwiseXor (1acd4b7)\nsnowflake: implement ops.GroupConcat (2219866)\nsnowflake: implement remaining map functions (c48c9a6)\nsnowflake: support binary variance reduction with filters (eeabdee)\nsnowflake: support cross-database table access (79cb445)\nsqlalchemy: generalize unnest to work on backends that don’t support it (5943ce7)\nsqlite: add sqlite type support (addd6a9)\nsqlite: support in-memory tables (1b24848)\nsql: support for creating temporary tables in sql based backends (466cf35)\ntables: cast table using schema (96ce109)\ntables: implement pivot_longer API (11c5736)\ntrino: enable MapLength operation (a7ad1db)\ntrino: implement ArrayFilter translation (50f6fcc)\ntrino: implement ops.ArrayMap (657bf61)\ntrino: implement ops.Between (d70b9c0)\ntrino: support sqlalchemy 2 (0d078c1)\nux: accept selectors in Table.drop (325140f)\nux: allow creating unbound tables using annotated class definitions (d7bf6a2)\nux: easy interactive setup (6850146)\nux: expose between, rows and range keyword arguments in value.over() (5763063)\n\n\n\nBug Fixes\n\nanalysis: extract Limit subqueries (62f6e14)\napi: add a name attribute to backend proxy modules (d6d8e7e)\napi: fix broken __radd__ array concat operation (121d9a0)\napi: only include valid python identifiers in struct tab completion (8f33775)\napi: only include valid python identifiers in table tab completion (031a48c)\nbackend: provide useful error if default backend is unavailable (1dbc682)\nbackends: fix capitalize implementations across all backends (d4f0275)\nbackends: fix null literal handling (7f46342)\nbigquery: ensure that memtables are translated correctly (d6e56c5)\nbigquery: fix decimal literals (4a04c9b)\nbigquery: regenerate negative string index sql snapshots (3f02c73)\nbigquery: regenerate sql for predicate pushdown fix (509806f)\ncache: remove bogus schema argument and validate database argument type (c4254f6)\nci: fix invalid test id (f70de1d)\nclickhouse: fix decimal literal (4dcd2cb)\nclickhouse: fix set ops with table operands (86bcf32)\nclickhouse: raise OperationNotDefinedError if operation is not supported (71e2570)\nclickhouse: register in-memory tables in pyarrow-related calls (09a045c)\nclickhouse: use a bool type supported by clickhouse_driver (ab8f064)\nclickhouse: workaround sqlglot’s insistence on uppercasing (6151f37)\ncompiler: generate aliases in a less clever way (04a4aa5)\ndatafusion: support sum aggregation on bool column (9421400)\ndeps: bump duckdb to 0.7.0 (38d2276)\ndeps: bump snowflake-connector-python upper bound (b368b04)\ndeps: ensure that pyspark depends on sqlalchemy (60c7382)\ndeps: update dependency pyarrow to v11 (2af5d8d)\ndeps: update dependency sqlglot to v11 (e581e2f)\ndon’t expose backend methods on ibis.&lt;backend&gt; directly (5a16431)\ndruid: remove invalid operations (19f214c)\nduckdb: add null to duckdb datatype parser (07d2a86)\nduckdb: ensure that temp_directory exists (00ba6cb)\nduckdb: explicitly set timezone to UTC on connection (6ae4a06)\nduckdb: fix blob type in literal (f66e8a1)\nduckdb: fix memtable to_pyarrow/to_pyarrow_batches (0e8b066)\nduckdb: in-memory objects registered with duckdb show up in list_tables (7772f79)\nduckdb: quote identifiers if necessary in struct_pack (6e598cc)\nduckdb: support casting to unsigned integer types (066c158)\nduckdb: treat g re_replace flag as literal text (aa3c31c)\nduckdb: workaround an ownership bug at the interaction of duckdb, pandas and pyarrow (2819cff)\nduckdb: workaround duckdb bug that prevents multiple substitutions (0e09220)\nimports: remove top-level import of sqlalchemy from base backend (b13cf25)\nio: add read_parquet and read_csv to base backend mixin (ce80d36), closes #5420\nir: incorrect predicate pushdown (9a9204f)\nir: make find_subqueries return in topological order (3587910)\nir: properly raise error if literal cannot be coerced to a datatype (e16b91f)\nir: reorder the right schema of set operations to align with the left schema (58e60ae)\nir: use rlz.map_to() rule instead of isin to normalize temporal units (a1c46a2)\nir: use static connection pooling to prevent dropping temporary state (6d2ae26)\nmssql: set sqlglot to tsql (1044573)\nmysql: remove invalid operations (8f34a2b)\npandas/dask: handle non numpy scalar results in wrap_case_result (a3b82f7)\npandas: don’t try to dispatch on arrow dtype if not available (d22ae7b)\npandas: handle casting to arrays with None elements (382b90f)\npandas: handle NAs in array conversion (06bd15d)\npolars: back compat for concat_str separator argument (ced5a61)\npolars: back compat for the reverse/descending argument (f067d81)\npolars: polars execute respect limit kwargs (d962faf)\npolars: properly infer polars categorical dtype (5a4707a)\npolars: use metric name in aggregate output to dedupe columns (234d8c1)\npyspark: fix incorrect ops.EndsWith translation rule (4c0a5a2)\npyspark: fix isnan and isinf to work on bool (8dc623a)\nsnowflake: allow loose casting of objects and arrays (1cf8df0)\nsnowflake: ensure that memtables are translated correctly (b361e07)\nsnowflake: ensure that null comparisons are correct (9b83699)\nsnowflake: ensure that quoting matches snowflake behavior, not sqlalchemy (b6b67f9)\nsnowflake: ensure that we do not try to use a None schema or database (03e0265)\nsnowflake: handle the case where pyarrow isn’t installed (b624fa3)\nsnowflake: make array_agg preserve nulls (24b95bf)\nsnowflake: quote column names on construction of sa.Column (af4db5c)\nsnowflake: remove broken pyarrow fetch support (c440adb)\nsnowflake: return NULL when trying to call map functions on non-object JSON (d85fb28)\nsnowflake: use _flatten to avoid overriding unrelated function in other backends (8c31594)\nsqlalchemy: ensure that isin contains full column expression (9018eb6)\nsqlalchemy: get builtin dialects working; mysql/mssql/postgres/sqlite (d2356bc)\nsqlalchemy: make strip family of functions behave like Python (dd0a04c)\nsqlalchemy: reflect most recent schema when view is replaced (62c8dea)\nsqlalchemy: use sa.true instead of Python literal (8423eba)\nsqlalchemy: use indexed group by key references everywhere possible (9f1ddd8)\nsql: ensure that set operations generate valid sql in the presence of additional constructs such as sort keys (3e2c364)\nsqlite: explicitly disallow array in literal (de73b37)\nsqlite: fix random scalar range (26d0dde)\nsupport negative string indices (f84a54d)\ntrino: workaround broken dialect (b502faf)\ntypes: fix argument types of Table.order_by() (6ed3a97)\nutil: make convert_unit work with python types (cb3a90c)\nux: give the value_counts aggregate column a better name (abab1d7)\nux: make string range selectors inclusive (7071669)\nux: make top level set operations work (f5976b2)\n\n\n\nPerformance\n\nduckdb: faster to_parquet/to_csv implementations (6071bb5)\nfix duckdb insert-from-dataframe performance (cd27b99)\ndeps: bump minimum required version of parsy (22020cb)\nremove spark alias to pyspark and associated cruft (4b286bd)\n\n\n\nRefactors\n\nanalysis: slightly simplify find_subqueries() (ab3712f)\nbackend: normalize exceptions (065b66d)\nclickhouse: clean up parsing rules (6731772)\ncommon: move frozendict and DotDict to ibis.common.collections (4451375)\ncommon: move the geospatial module to the base SQL backend (3e7bfa3)\ndask: remove unneeded create_table() (86885a6)\ndatatype: clean up parsing rules (c15fb5f)\ndatatype: remove Category type and related APIs (bb0ee78)\ndatatype: remove StructType.pairs property in favor of identical fields attribute (6668122)\ndatatypes: move sqlalchemy datatypes to specific backend (d7b49eb)\ndatatypes: remove String parent type from JSON type (34f3898)\ndatatype: use a dictionary to store StructType fields rather than names and types tuples (84455ac)\ndatatype: use lazy dispatch when inferring pandas Timedelta objects (e5280ea)\ndrop limit kwarg from to_parquet/to_csv (a54460c)\nduckdb: clean up parsing rules (30da8f9)\nduckdb: handle parsing timestamp scale (16c1443)\nduckdb: remove unused list&lt;...&gt; parsing rule (f040b86)\nduckdb: use a proper sqlalchemy construct for structs and reduce casting (8daa4a1)\nir/api: introduce window frame operation and revamp the window API (2bc5e5e)\nir/backends: remove various deprecated functions and methods (a8d3007)\nir: reorganize the scope and timecontext utilities (80bd494)\nir: update ArrayMap to use the new callable_with validation rule (560474e)\nmove pretty repr tests back to their own file (4a75988)\nnix: clean up marker argument construction (12eb916)\npostgres: clean up datatype parsing (1f61661)\npostgres: clean up literal arrays (21b122d)\npyspark: remove another private function (c5081cf)\nremove unnecessary top-level rich console (8083a6b)\nrules: remove unused non_negative_integer and pair rules (e00920a)\nschema: remove deprecated Schema.from_dict(), .delete() and .append() methods (8912b24)\nsnowflake: remove the need for parsy (c53403a)\nsqlalchemy: set session parameters once per connection (ed4b476)\nsqlalchemy: use backend-specific startswith/endswith implementations (6101de2)\ntest_sqlalchemy.py: move to snapshot testing (96998f0)\ntests: reorganize rules test file to the ibis.expr subpackage (47f0909)\ntests: reorganize schema test file to the ibis.expr subpackage (40033e1)\ntests: reorganize datatype test files to the datatypes subpackage (16199c6)\ntrino: clean up datatype parsing (84c0e35)\nux: return expression from Table.info (71cc0e0)\n\n\n\nDeprecations\n\napi: deprecate summary API (e449c07)\napi: mark ibis.sequence() for removal (3589f80)\n\n\n\nDocumentation\n\nadd a bunch of string expression examples (18d3112)\nadd Apache Druid to backend matrix (764d9c3)\nadd CNAME file to mkdocs source (6d19111)\nadd druid to the backends index docs page (ad0b6a3)\nadd missing DataFusion entry to the backends in the README (8ce025a)\nadd redirects for common old pages (c9087f2)\napi: document deferred API and its pitfalls (8493604)\napi: improve collect method API documentation (b4fcef1)\narray expression examples (6812c17)\nbackends: document default backend configuration (6d917d3)\nbackends: link to configuration from the backends list (144044d)\nblob: blog on ibis + substrait + duckdb (5dc7a0a)\nblog: adds examples sneak peek blog + assets folder (fcbb3d5)\nblog: adds to file sneak peek blog (128194f)\nblog: specify parsy 2.0 in substrait blog article (c264477)\nbump query engine count in README and use project-preferred names (11169f7)\ndon’t sort backends by coverage percentage by default (68f73b1)\ndrop docs versioning (d7140e7)\nduckdb: fix broken docstring examples (51084ad)\nenable light/dark mode toggle in docs (b9e812a)\nfill out table API with working examples (16fc8be)\nfix notebook logging example (04b75ef)\nhow-to: fix sessionize.md to use ibis.read_parquet (ff9cbf7)\nimprove Expr.substitute() docstring (b954edd)\nimprove/update pandas walkthrough (80b05d8)\nio: doc/ux improvements for read_parquet and friends (2541556), closes #5420\nio: update README.md to recommend installing duckdb as default backend (0a72ec0), closes #5423 #5420\nmove tutorial from docs to external ibis-examples repo (11b0237)\nparquet: add docstring examples for to_parquet incl. partitioning (8040164)\npoint to ibis-examples repo in the README (1205636)\nREADME.md: clean up readme, fix typos, alter the example (383a3d3)\nremove duplicate “or” (b6ef3cc)\nremove duplicate spark backend in install docs (5954618)\nrender __dunder__ method API documentation (b532c63)\nrerender ci-analysis notebook with new table header colors (50507b6)\nstreamlit: fix url for support matrix (594199b)\ntutorial: remove impala from sql tutorial (7627c13)\nuse teal for primary & accent colors (24be961)"
  },
  {
    "objectID": "release_notes.html#section-8",
    "href": "release_notes.html#section-8",
    "title": "2.1.0 (2022-01-12)",
    "section": "4.1.0 (2023-01-25)",
    "text": "4.1.0 (2023-01-25)\n\nFeatures\n\nadd ibis.get_backend function (2d27df8)\nadd py.typed to allow mypy to type check packages that use ibis (765d42e)\napi: add ibis.set_backend function (e7fabaf)\napi: add selectors for easier selection of columns (306bc88)\nbigquery: add JS UDF support (e74328b)\nbigquery: add SQL UDF support (db24173)\nbigquery: add to_pyarrow method (30157c5)\nbigquery: implement bitwise operations (55b69b1)\nbigquery: implement ops.Typeof (b219919)\nbigquery: implement ops.ZeroIfNull (f4c5607)\nbigquery: implement struct literal (c5f2a1d)\nclickhouse: properly support native boolean types (31cc7ba)\ncommon: add support for annotating with coercible types (ae4a415)\ncommon: make frozendict truly immutable (1c25213)\ncommon: support annotations with typing.Literal (6f89f0b)\ncommon: support generic mapping and sequence type annotations (ddc6603)\ndask: support connect() with no arguments (67eed42)\ndatatype: add optional timestamp scale parameter (a38115a)\ndatatypes: add as_struct method to convert schemas to structs (64be7b1)\nduckdb: add read_json function for consuming newline-delimited JSON files (65e65c1)\nmssql: add a bunch of missing types (c698d35)\nmssql: implement inference for DATETIME2 and DATETIMEOFFSET (aa9f151)\nnicer repr for Backend.tables (0d319ca)\npandas: support connect() with no arguments (78cbbdd)\npolars: allow ibis.polars.connect() to function without any arguments (d653a07)\npolars: handle casting to scaled timestamps (099d1ec)\npostgres: add Map(string, string) support via the built-in HSTORE extension (f968f8f)\npyarrow: support conversion to pyarrow map and struct types (54a4557)\nsnowflake: add more array operations (8d8bb70)\nsnowflake: add more map operations (7ae6e25)\nsnowflake: any/all/notany/notall reductions (ba1af5e)\nsnowflake: bitwise reductions (5aba997)\nsnowflake: date from ymd (035f856)\nsnowflake: fix array slicing (bd7af2a)\nsnowflake: implement ArrayCollect (c425f68)\nsnowflake: implement NthValue (0dca57c)\nsnowflake: implement ops.Arbitrary (45f4f05)\nsnowflake: implement ops.StructColumn (41698ed)\nsnowflake: implement StringSplit (e6acc09)\nsnowflake: implement StructField and struct literals (286a5c3)\nsnowflake: implement TimestampFromUNIX (314637d)\nsnowflake: implement TimestampFromYMDHMS (1eba8be)\nsnowflake: implement typeof operation (029499c)\nsnowflake: implement exists/not exists (7c8363b)\nsnowflake: implement extract millisecond (3292e91)\nsnowflake: make literal maps and params work (dd759d3)\nsnowflake: regex extract, search and replace (9c82179)\nsnowflake: string to timestamp (095ded6)\nsqlite: implement _get_schema_using_query in SQLite backend (7ff84c8)\ntrino: compile timestamp types with scale (67683d3)\ntrino: enable ops.ExistsSubquery and ops.NotExistsSubquery (9b9b315)\ntrino: map parameters (53bd910)\nux: improve error message when column is not found (b527506)\n\n\n\nBug Fixes\n\nbackend: read the default backend setting in _default_backend (11252af)\nbigquery: move connection logic to do_connect (42f2106)\nbigquery: remove invalid operations from registry (911a080)\nbigquery: resolve deprecation warnings for StructType and Schema (c9e7078)\nclickhouse: fix position call (702de5d)\ncorrectly visualize array type (26b0b3f)\ndeps: make sure pyarrow is not an implicit dependency (10373f4)\nduckdb: make read_csv on URLs work (9e61816)\nduckdb: only try to load extensions when necessary for csv (c77bde7)\nduckdb: remove invalid operations from registry (ba2ec59)\nfallback to default backend with to_pyarrow/to_pyarrow_batches (a1a6902)\nimpala: remove broken alias elision (32b120f)\nir: error for order_by on nonexistent column (57b1dd8)\nir: ops.Where output shape should consider all arguments (6f87064)\nmssql: infer bit as boolean everywhere (24f9d7c)\nmssql: pull nullability from column information (490f8b4)\nmysql: fix mysql query schema inference (12f6438)\npolars: remove non-working Binary and Decimal literal inference (0482d15)\npostgres: use permanent views to avoid connection pool defeat (49a4991)\npyspark: fix substring constant translation (40d2072)\nset ops: raise if no tables passed to set operations (bf4bdde)\nsnowflake: bring back bitwise operations (260facd)\nsnowflake: don’t always insert a cast (ee8817b)\nsnowflake: implement working TimestampNow (42d95b0)\nsnowflake: make sqlalchemy 2.0 compatible (8071255)\nsnowflake: re-enable ops.TableArrayView (a1ad2b7)\nsnowflake: remove invalid operations from registry (2831559)\nsql: add typeof test and bring back implementations (7dc5356)\nsqlalchemy: 2.0 compatibility (837a736)\nsqlalchemy: fix view creation with select stmts that have bind parameters (d760e69)\nsqlalchemy: handle correlated exists sanely (efa42bd)\nsqlalchemy: handle generic geography/geometry by name instead of geotype (23c35e1)\nsqlalchemy: use exec_driver_sql in view teardown (2599c9b)\nsqlalchemy: use the backend’s compiler instead of AlchemyCompiler (9f4ff54)\nsql: fix broken call to ibis.map (045edc7)\nsqlite: interpolate pathlib.Path correctly in attach (0415bd3)\ntrino: ensure connecting works with trino 0.321 (07cee38)\ntrino: remove invalid operations from registry (665265c)\nux: remove extra trailing newline in expression repr (ee6d58a)\n\n\n\nDocumentation\n\nadd BigQuery backend docs (09d8995)\nadd streamlit app for showing the backend operation matrix (3228f64)\nallow deselecting geospatial ops in backend support matrix (012da8c)\napi: document more public expression APIs (337018f)\nbackend-info: prevent app from trying install duckdb extensions (3d94082)\nclean up gen_matrix.py after adding streamlit app (deb80f2)\nduckdb: add to_pyarrow_batches documentation (ec1ffce)\nembed streamlit operation matrix app to docs (469a50d)\nmake firefox render the proper iframe height (ff1d4dc)\npublish raw data for operation matrix (62e68da)\nre-order when to download test data (8ce8c16)\nrelease: update breaking changes in the release notes for 4.0.0 (4e91401)\nremove trailing parenthesis (4294397)\nupdate ibis-version-4.0.0-release.md (f6701df)\nupdate links to contributing guides (da615e4)\n\n\n\nRefactors\n\nbigquery: explicitly disallow INT64 in JS UDF (fb33bf9)\ndatatype: add custom sqlalchemy nested types for backend differentiation (dec70f5)\ndatatype: introduce to_sqla_type dispatching on dialect (a8bbc00)\ndatatypes: remove Geography and Geometry types in favor of GeoSpatial (d44978c)\ndatatype: use a mapping to store StructType fields rather than names and types tuples (ff34c7b)\ndtypes: expose nbytes property for integer and floating point datatypes (ccf80fd)\nduckdb: remove .raw_sql call (abc939e)\nduckdb: use sqlalchemy-views to reduce string hacking (c162750)\nir: remove UnnamedMarker (dd352b1)\npostgres: use a bindparam for metadata queries (b6b4669)\nremove empty unused file (9d63fd6)\nschema: use a mapping to store Schema fields rather than names and types tuples (318179a)\nsimplify _find_backend implementation (60f1a1b)\nsnowflake: remove unnecessary parse_json call in ops.StructField impl (9e80231)\nsnowflake: remove unnecessary casting (271554c)\nsnowflake: use unary instead of fixed_arity(..., 1) (4a1c7c9)\nsqlalchemy: clean up quoting implementation (506ce01)\nsqlalchemy: generalize handling of failed type inference (b0f4e4c)\nsqlalchemy: move _get_schema_using_query to base class (296cd7d)\nsqlalchemy: remove the need for deferred columns (e4011aa)\nsqlalchemy: remove use of deprecated isnot (4ec53a4)\nsqlalchemy: use exec_driver_sql everywhere (e8f96b6)\nsql: finally remove _CorrelatedRefCheck (f49e429)\n\n\n\nDeprecations\n\napi: deprecate .to_projection in favor of .as_table (7706a86)\napi: deprecate get_column/s in favor of __getitem__/__getattr__ syntax (e6372e2)\nir: schedule DatabaseTable.change_name for removal (e4bae26)\nschema: schedule Schema.delete() and Schema.append() for removal (45ac9a9)"
  },
  {
    "objectID": "release_notes.html#section-9",
    "href": "release_notes.html#section-9",
    "title": "2.1.0 (2022-01-12)",
    "section": "4.0.0 (2023-01-09)",
    "text": "4.0.0 (2023-01-09)\n\n⚠ BREAKING CHANGES\n\nfunctions, methods and classes marked as deprecated are removed now\nir: replace HLLCardinality with ApproxCountDistinct and CMSMedian with ApproxMedian operations.\nbackends: the datatype of returned execution results now more closely matches that of the ibis expression’s type. Downstream code may need to be adjusted.\nir: the JSONB type is replaced by the JSON type.\ndev-deps: expression types have been removed from ibis.expr.api. Use import ibis.expr.types as ir to access these types.\ncommon: removed @immutable_property decorator, use @attribute.default instead\ntimestamps: the timezone argument to to_timestamp is gone. This was only supported in the BigQuery backend. Append %Z to the format string and the desired time zone to the input column if necessary.\ndeps: ibis now supports at minimum duckdb 0.3.3. Please upgrade your duckdb install as needed.\napi: previously ibis.connect would return a Table object when calling connect on a parquet/csv file. This now returns a backend containing a single table created from that file. When possible users may use ibis.read instead to read files into ibis tables.\napi: histogram()’s closed argument no longer exists because it never had any effect. Remove it from your histogram method calls.\npandas/dask: the pandas and Dask backends now interpret casting ints to/from timestamps as seconds since the unix epoch, matching other backends.\ndatafusion: register_csv and register_parquet are removed. Pass filename to register method instead.\nir: ops.NodeList and ir.List are removed. Use tuples to represent sequence of expressions instead.\napi: re_extract now follows re.match behavior. In particular, the 0th group is now the entire string if there’s a match, otherwise the groups are 1-based.\ndatatypes: enums are now strings. Likely no action needed since no functionality existed.\nir: Replace t[t.x.topk(...)] with t.semi_join(t.x.topk(...), \"x\").\nir: ir.Analytic.type() and ir.TopK.type() methods are removed.\napi: the default limit for table/column expressions is now None (meaning no limit).\nir: join changes: previously all column names that collided between left and right tables were renamed with an appended suffix. Now for the case of inner joins with only equality predicates, colliding columns that are known to be equal due to the join predicates aren’t renamed.\nimpala: kerberos support is no longer installed by default for the impala backend. To add support you’ll need to install the kerberos package separately.\nir: ops.DeferredSortKey is removed. Use ops.SortKey directly instead.\nir: ibis.common.grounds.Annotable is mutable by default now\nir: node.has_resolved_name() is removed, use isinstance(node, ops.Named) instead; node.resolve_name() is removed use node.name instead\nir: removed ops.Node.flat_args(), directly use node.args property instead\nir: removed ops.Node.inputs property, use the multipledispatched get_node_arguments() function in the pandas backend\nir: Node.blocks() method has been removed.\nir: HasSchema mixin class is no longer available, directly subclass ops.TableNode and implement schema property instead\nir: Removed Node.output_type property in favor of abstractmethod Node.to_expr() which now must be explicitly implemented\nir: Expr(Op(Expr(Op(Expr(Op))))) is now represented as Expr(Op(Op(Op))), so code using ibis internals must be migrated\npandas: Use timezone conversion functions to compute the original machine localized value\ncommon: use ibis.common.validators.{Parameter, Signature} instead\nir: ibis.expr.lineage.lineage() is now removed\nir: removed ir.DestructValue, ir.DestructScalar and ir.DestructColumn, use table.unpack() instead\nir: removed Node.root_tables() method, use ibis.expr.analysis.find_immediate_parent_tables() instead\nimpala: use other methods for pinging the database\n\n\n\nFeatures\n\nadd experimental decorator (791335f)\nadd to_pyarrow and to_pyarrow_batches (a059cf9)\nadd unbind method to expressions (4b91b0b), closes #4536\nadd way to specify sqlglot dialect on backend (f1c0608)\nalchemy: implement json getitem for sqlalchemy backends (7384087)\napi: add agg alias for aggregate (907583f)\napi: add agg alias to group_by (6b6367c)\napi: add ibis.read top level API function (e67132c)\napi: add JSON __getitem__ operation (3e2efb4)\napi: implement __array__ (1402347)\napi: make drop variadic (1d69702)\napi: return object from to_sql to support notebook syntax highlighting (87c9833)\napi: use rich for interactive __repr__ (04758b8)\nbackend: make ArrayCollect filterable (1e1a5cf)\nbackends/mssql: add backend support for Microsoft Sql Server (fc39323)\nbigquery: add ops.DateFromYMD, ops.TimeFromHMS, ops.TimestampFromYMDHMS (a4a7936)\nbigquery: add ops.ExtractDayOfYear (30c547a)\nbigquery: add support for correlation (4df9f8b)\nbigquery: implement argmin and argmax (40c5f0d)\nbigquery: implement pi and e (b91370a)\nbigquery: implement array repeat (09d1e2f)\nbigquery: implement JSON getitem functionality (9c0e775)\nbigquery: implement ops.ArraySlice (49414ef)\nbigquery: implement ops.Capitalize (5757bb0)\nbigquery: implement ops.Clip (5495d6d)\nbigquery: implement ops.Degrees, ops.Radians (5119b93)\nbigquery: implement ops.ExtractWeekOfYear (477d287)\nbigquery: implement ops.RandomScalar (5dc8482)\nbigquery: implement ops.StructColumn, ops.ArrayColumn (2bbf73c)\nbigquery: implement ops.Translate (77a4b3e)\nbigquery: implementt ops.NthValue (b43ba28)\nbigquery: move bigquery backend back into the main repo (cd5e881)\nclickhouse: handle more options in parse_url implementation (874c5c0)\nclickhouse: implement INTERSECT ALL/EXCEPT ALL (f65fbc3)\nclickhouse: implement quantile/multiquantile (96d7d1b)\ncommon: support function annotations with both typehints and rules (7e23f3e)\ndask: implement mode aggregation (017f07a)\ndask: implement json getitem (381d805)\ndatafusion: convert column expressions to pyarrow (0a888de)\ndatafusion: enable topk (d44903f)\ndatafusion: implement Limit (1ddc876)\ndatafusion: implement ops.StringConcat (6bb5b4f)\ndecompile: support rendering ibis expression as python code (7eebc67)\ndeps: support shapely 2.0 (68dff10)\ndisplay qualified named in deprecation warnings (a6e2a49)\ndocs: first draft of Ibis for pandas users (7f7c9b5)\nduckdb: enable registration of parquet files from s3 (fced465)\nduckdb: implement mode aggregation (36fd152)\nduckdb: implement to_timestamp (26ca1e4)\nduckdb: implement quantile/multiquantile (fac9705)\nduckdb: overwrite views when calling register (ae07438)\nduckdb: pass through kwargs to file loaders (14fa2aa)\nduckdb: support out of core execution for in-memory connections (a4d4ba2)\nduckdb: support registering external postgres tables with duckdb (8633e6b)\nexpr: split ParseURL operation into multiple URL extract operations (1f0fcea)\nimpala: implement strftime (d3ede8d)\nimpala: support date literals (cd334c4)\ninsert: add support for list+dict to sqlalchemy backends (15d399e)\nir/pandas/dask/clickhouse: revamp Map type support (62b6f2d)\nir: add is_* methods to DataTypes (79f5c2b)\nir: prototype for parsing SQL into an ibis expression (1301183)\nir: support python 3.10 pattern matching on Annotable nodes (eca93eb)\nmssql: add window function support (ef1be45)\nmssql: detect schema from SQL (ff79928)\nmssql: extract quarter (7d04266)\nmssql: implement ops.DayOfWeekIndex (4125593)\nmssql: implement ops.ExtractDayOfYear (ae026d5)\nmssql: implement ops.ExtractEpochSeconds (4f49b5b)\nmssql: implement ops.ExtractWeekOfYear (f1394bc)\nmssql: implement ops.Ln, ops.Log, ops.Log2, ops.Log10 (f8ee1d8)\nmssql: implement ops.RandomScalar (4149450)\nmssql: implement ops.TimestampTruncate, ops.DateTruncate (738e496)\nmssql: implementt ops.DateFromYMD, ops.TimestampFromYMDHMS, ops.TimeFromHMS (e84f2ce)\nopen *.db files with sqlite in ibis.connect (37baf05)\npandas: implement mode aggregation (fc023b5)\npandas: implement RegexReplace for str (23713cc)\npandas: implement json getitem (8fa1190)\npandas: implement quantile/multiquantile (cd4dcaa)\npandas: support histogram API (5bfc0fe)\npolars: enable topk (8bfb16a)\npolars: implement mode aggregation (7982ba2)\npolars: initial support for polars backend (afecb0a)\npostgres: implement mode aggregation (b2f1c2d)\npostgres: implement quantile and multiquantile (82ed4f5)\npostgres: prettify array literals (cdc60d5)\npyspark: add support for struct operations (ce05987)\npyspark: enable topk (0f748e0)\npyspark: implement pi and e (fea81c6)\npyspark: implement json getitem (9bfb748)\npyspark: implement quantile and multiquantile (743f411)\npyspark: support histogram API (8f4808c)\nsnowflake: enable day-of-week column expression (6fd9c33)\nsnowflake: handle date and timestamp literals (ec2392d)\nsnowflake: implement mode aggregation (f35915e)\nsnowflake: implement parse_url (a9746e3)\nsnowflake: implement rowid scalar (7e1425a)\nsnowflake: implement time literal (068fc50)\nsnowflake: implement scalar (cc07d91)\nsnowflake: initial commit for snowflake backend (a8687dd)\nsnowflake: support reductions in window functions via automatic ordering (0234e5c)\nsql: add ops.StringSQLILike (7dc4924)\nsqlalchemy: implement ops.Where using IF/IFF functions (4cc9c15)\nsqlalchemy: in-memory tables have name in generated SQL (01b4c60)\nsql: improve error message in fixed_arity helper (891a1ad)\nsqlite: add type_map arg to override type inference (1961bad)\nsqlite: fix impl for missing pi and e functions (24b6d2f)\nsqlite: support con.sql with explicit schema specified (7ca82f3)\nsqlite: support wider range of datetime formats (f65093a)\nsupport both postgresql:// and postgres:// in ibis.connect (2f7a7b4)\nsupport deferred predicates in join (b51a64b)\nsupport more operations with unsigned integers (9992953)\nsupport passing callable to relabel (0bceefd)\nsupport tab completion for getitem access of table columns (732dba4)\nsupport Table.fillna for SQL backends (26d4cac)\ntrino: add bit_xor aggregation (830acf4)\ntrino: add EXTRACT-based functionality (6549657)\ntrino: add millisecond scale to *_trunc function (3065248)\ntrino: add some basic aggregation ops (7ecf7ab)\ntrino: extract milliseconds (09517a5)\ntrino: implement approx_median (1cba8bd)\ntrino: implement parse_url (2bc87fc)\ntrino: implement round, cot, pi, and e (c0e8736)\ntrino: implement arbitrary first support (0c7d3b3)\ntrino: implement array collect support (dfeb600)\ntrino: implement array column support (dadf9a8)\ntrino: implement array concat (240c55d)\ntrino: implement array index (c5f3a96)\ntrino: implement array length support (2d7cc65)\ntrino: implement array literal support (2182177)\ntrino: implement array repeat (2ee3d10)\ntrino: implement array slicing (643792e)\ntrino: implement basic struct operations (cc3c937)\ntrino: implement bitwise agg support (5288b35)\ntrino: implement bitwise scalar/column ops (ac4876c)\ntrino: implement default precision and scale (37f8a47)\ntrino: implement group concat support (5c41439)\ntrino: implement json getitem support (7c41566)\ntrino: implement map operations (4efc5ce)\ntrino: implement more generic and numeric ops (63b45c8)\ntrino: implement ops.Capitalize (dff14fc)\ntrino: implement ops.DateFromYMD (edd2994)\ntrino: implement ops.DateTruncate, ops.TimestampTruncate (32f4862)\ntrino: implement ops.DayOfWeekIndex, ops.DayOfWeekName (a316d6d)\ntrino: implement ops.ExtractDayOfYear (b0a3465)\ntrino: implement ops.ExtractEpochSeconds (10b82f1)\ntrino: implement ops.ExtractWeekOfYear (cf719b8)\ntrino: implement ops.Repeat (e9f6851)\ntrino: implement ops.Strftime (a436823)\ntrino: implement ops.StringAscii (93fd32d)\ntrino: implement ops.StringContains (d5cb2ec)\ntrino: implement ops.StringSplit (62d79a6)\ntrino: implement ops.StringToTimestamp (b766f62)\ntrino: implement ops.StrRight (691b39c)\ntrino: implement ops.TimeFromHMS (e5cacc2)\ntrino: implement ops.TimestampFromUNIX (ce5d726)\ntrino: implement ops.TimestampFromYMDHMS (9fa7304)\ntrino: implement ops.TimestampNow (c832e4c)\ntrino: implement ops.Translate (410ae1e)\ntrino: implement quantile/multiquantile (bc7fdab)\ntrino: implement regex functions (9e493c5)\ntrino: implement window function support (5b6cc45)\ntrino: initial trino backend (c367865)\ntrino: support string date scalar parameter (9092530)\ntrino: use proper approx_distinct function (3766fff)\n\n\n\nBug Fixes\n\nibis.connect always returns a backend (2d5b155)\nallow inserting memtable with alchemy backends (c02fcc3)\nalways display at least one column in the table repr (5ea9e5a)\nanalysis: only lower sort keys that are in an agg’s output (6bb4f66)\napi: allow arbitrary sort keys (a980b34)\napi: allow boolean scalars in predicate APIs (2a2636b)\napi: allow deferred instances as input to ibis.desc and ibis.asc (6861347)\napi: ensure that window functions are propagated (4fb1106)\napi: make re_extract conform to semantics of Python’s re.match (5981227)\nauto-register csv and parquet with duckdb using ibis.connect (67c4f87)\navoid renaming known equal columns for inner joins with equality predicates (5d4b0ed)\nbackends: fix casting and execution result types in many backends (46c21dc)\nbigquery: don’t try to parse database when name is already fully qualified (ae3c113)\nbigquery: fix integer to timestamp casting (f5bacad)\nbigquery: normalize W frequency in *_trunc (893cd49)\ncatch TypeError instead of more specific error (6db19d8)\nchange default limit to None (8d1526a)\nclarify and normalize behavior of Table.rowid (92b03d6)\nclickhouse: ensure that correlated subqueries’ columns can be referenced (708d682)\nclickhouse: fix list_tables to use database name (edc3511)\nclickhouse: make any/all filterable and reduce code size (99b10e2)\nclickhouse: use clickhouse’s dbapi (bd0da12)\ncommon: support copying variadic annotable instances (ee0d9ad)\ndask: make filterable reductions work (0f759fc)\ndask: raise TypeError with informative message in ibis.dask.connect (4e67f7a)\ndefine to_pandas/to_pyarrow on DataType/Schema classes directly (22f3b4d)\ndeps: bound shapely to a version that doesn’t segfault (be5a779)\ndeps: update dependency datafusion to &gt;=0.6,&lt;0.8 (4c73870)\ndeps: update dependency geopandas to &gt;=0.6,&lt;0.13 (58a32dc)\ndeps: update dependency packaging to v22 (e0b6177)\ndeps: update dependency rich to v13 (4f313dd)\ndeps: update dependency sqlglot to v10 (db19d43)\ndeps: update dependency sqlglot to v9 (cf330ac)\ndocs: make sure data can be downloaded when building notebooks (fa7da17)\ndon’t fuse filters & selections that contain window functions (d757069)\ndrop snowflake support for RowID (dd378f1)\nduckdb: drop incorrect translate implementation (8690151)\nduckdb: fix bug in json getitem for duckdb (49ce739)\nduckdb: keep ibis.now() type semantics (eca4a2c)\nduckdb: make array repeat actually work (021f4de)\nduckdb: replace all in re_replace (c138f0f)\nduckdb: rereflect sqla table on re-registration (613b311), closes #4729\nduckdb: s3 priority (a2d03d1)\nduckdb: silence duckdb-engine warnings (359adc3)\nensure numpy ops dont accidentally cast ibis types (a7ca6c8)\nexclude geospatial ops from pandas/dask/polars has_operation (6f1d265)\nfix table.mutate with deferred named expressions (5877d0b)\nfix bug when disabling show_types in interactive repr (2402506)\nfix expression repr for table -&gt; value operations (dbf92f5)\nhandle dimensionality of empty outputs (3a88170)\nimprove rich repr support (522db9c)\nir: normalize date types (39056b5)\nir: normalize timestamps to datetime.datetime values (157efde)\nmake col.day_of_week not an expr (96e1580)\nmssql: fix integer to timestamp casting (9122eef)\nmssql: fix ops.TimeFromHMS (d2188e1)\nmssql: fix ops.TimestampFromUNIX (ec28add)\nmssql: fix round without argument (52a60ce)\nmssql: use double-dollar sign to prevent from interpolating a value (b82da5d)\nmysql: fix mysql startswith/endswith to be case sensitive (d7469cc)\nmysql: handle out of bounds timestamps and fix milliseconds calculation (1f7649a)\nmysql: upcast bool agg args (8c5f9a5)\npandas/dask now cast int&lt;-&gt;timestamp as seconds since epoch (bbfe998)\npandas: drop RowID implementation (05f5016)\npandas: make quantile/multiquantile with filter work (6b5abd6)\npandas: support substr with no length (b2c2922)\npandas: use localized UTC time for now operation (f6d7327)\npandas: use the correct context when aggregating over a window (e7fa5c0)\npolars: fix polars startswith to call the right method (9e6f397)\npolars: workaround passing pl.Null to the null type (fd9633b)\npostgres/duckdb: fix negative slicing by copying the trino impl (39e3962)\npostgres: fix array repeat to work with literals (3c46eb1)\npostgres: fix array_index operation (63ef892)\npostgres: make any/all translation rules use reduction helper (78bfd1d)\npyspark: handle datetime.datetime literals (4f94abe)\nremove kerberos extra for impala dialect (6ed3e5f)\nrepr: don’t repeat value in repr for literals (974eeb6)\nrepr: fix off by one in repr (322c8dc)\ns3: fix quoting and autonaming for s3 (ce09266)\nselect: raise error on attempt to select no columns in projection (94ac10e)\nsnowflake: fix extracting query parameter by (75af240)\nsnowflake: fix failing snowflake url extraction functions (2eee50b)\nsnowflake: fix snowflake list_databases (680cd24)\nsnowflake: handle schema when getting table (f6fff5b)\nsnowflake: snowflake now likes Tuesdays (1bf9d7c)\nsqlalchemy: allow passing pd.DataFrame to create (1a083f6)\nsqlalchemy: ensure that arbitrary expressions are valid sort keys (cb1a013)\nsql: avoid generating cartesian products yet again (fdc52a2)\nsqlite: fix sqlite startswith/endswith to be case sensitive (fd4a88d)\nstandardize list_tables signature everywhere (abafe1b), closes #2877\nsupport arbitrary with no arguments (45156f5)\nsupport dtype in __array__ methods (1294b76)\ntest: ensure that file-based url tests don’t expect data to exist (c2b635a)\ntrino: fix integer to timestamp casting (49321a6)\ntrino: make filterable any/all reductions work (992bd18)\ntruncate columns in repr for wide tables (aadcba1)\ntypo: in StringValue helpstr (b2e2093)\nux: improve error messages for rlz.comparable failures (5ca41d2)\nux: prevent infinite looping when formatting a floating column of all nans (b6afe98)\nvisualize(label_edges=True) works for NodeList ops (a91ceae)\nvisualize: dedup nodes and edges and add verbose argument for debugging (521e188)\nvisualize: handle join predicates in visualize (d63cb57)\nwindow: allow window range tuples in preceding or following (77172b3)\n\n\n\nDeprecations\n\ndeprecate Table.groupby alias in favor of Table.group_by (39cea3b)\ndeprecate Table.sort_by in favor of Table.order_by (7ac7103)\n\n\n\nPerformance\n\nadd benchmark for known-slow table expression (e9617f0)\nexpr: traverse nodes only once during compilation (69019ed)\nfix join performance by avoiding Projection construction (ed532bf)\nnode: give Nodes the default Python repr (eb26b11)\nux: remove pandas import overhead from import ibis (ea452fc)\ndeps: bump duckdb lower bound (4539683)\ndev-deps: replace flake8 et al with ruff and fix lints (9c1b282)\n\n\n\nRefactors\n\nadd lazy_singledispatch utility (180ecff)\nadd rlz.lazy_instance_of (4e30480)\nadd Temporal base class for temporal data types (694eec4)\napi: add deprecated Node.op() #4519 (2b0826b)\navoid roundtripping to expression for IFF (3068ae2)\nclean up cot implementations to have one less function call (0f304e5)\nclean up timezone support in ops.TimestampFromYMDHMS (2e183a9)\ncleanup str method docstrings (36bd36c)\nclickhouse: implement sqlglot-based compiler (5cc5d4b)\nclickhouse: simplify Quantile and MultiQuantile implementation (9e16e9e)\ncommon: allow traversal and substitution of tuple and dictionary arguments (60f4806)\ncommon: enforce slots definitions for Base subclasses (6c3df91)\ncommon: move Parameter and Signature to validators.py (da20537)\ncommon: reduce implementation complexity of annotations (27cee71)\ndatafusion: align register API across backends (08046aa)\ndatafusion: get name from expr (fea3e5b)\ndatatypes: remove Enum (145e706)\ndev-deps: remove unnecessary poetry2nix overrides (5ed95bc)\ndon’t sort new columns in mutate (72ec96a)\nduckdb: use lambda to define backend operations (5d14de6)\nimpala: move impala SQL tests to snapshots (927bf65)\nimpala: replace custom pooling with sqlalchemy QueuePool (626cdca)\nir: ops.List -&gt; ops.NodeList (6765bd2)\nir: better encapsulate graph traversal logic, schema and datatype objects are not traversable anymore (1a07725)\nir: generalize handling and traversal of node sequences (e8bcd0f)\nir: make all value operations ‘Named’ for more consistent naming semantics (f1eb4d2)\nir: move random() to api.py (e136f1b)\nir: remove ops.DeferredSortKey (e629633)\nir: remove ops.TopKNode and ir.TopK (d4dc544)\nir: remove Analytic expression’s unused type() method (1864bc1)\nir: remove DecimalValue.precision(), DecimalValue.scale() method (be975bc)\nir: remove DestructValue expressions (762d384)\nir: remove duplicated literal creation code (7dfb56f)\nir: remove intermediate expressions (c6fb0c0)\nir: remove lin.lineage() since it’s not used anywhere (120b1d7)\nir: remove node.blocks() in favor of more explicit type handling (37d8ce4)\nir: remove Node.inputs since it is an implementation detail of the pandas backend (6d2c49c)\nir: remove node.root_tables() and unify parent table handling (fbb07c1)\nir: remove ops.AggregateSelection in favor of an.simplify_aggregation (ecf6ed3)\nir: remove ops.NodeList and ir.List in favor of builtin tuples (a90ce35)\nir: remove pydantic dependency and make grounds more composable (9da0f41)\nir: remove sch.HasSchema and introduce ops.Projection base class for ops.Selection (c3b0139)\nir: remove unnecessary complexity introduced by variadic annotation (698314b)\nir: resolve circular imports so operations can be globally imported for types (d2a3919)\nir: simplify analysis.substitute_unbound() (a6c7406)\nir: simplify SortKey construction using rules (4d63280)\nir: simplify switch-case builders (9acf717)\nir: split datatypes package into multiple submodules (cce6535)\nir: split out table count into CountStar operation (e812e6e)\nir: support replacing nodes in the tree (6a0df5a)\nir: support variadic annotable arguments and add generic graph traversal routines (5d6a289)\nir: unify aggregation construction to use AggregateSelection (c7d6a6f)\nmake quantile, any, and all reductions filterable (1bafc9e)\nmake sure value_counts always has a projection (a70a302)\nmssql: use lambda to define backend operations (1437cfb)\nmysql: dedup extract code (d551944)\nmysql: use lambda to define backend operations (d10bff8)\npolars: match duckdb registration api (ac59dac)\npostgres: use lambda to define backend operations (4c85d7b)\nremove dead compat.py module (eda0fdb)\nremove deprecated approximate aggregation classes (53fc6cb)\nremove deprecated functions and classes (be1cdda)\nremove duplicate _random_identifier calls (26e7942)\nremove setup.py and related infrastructure (adfcce1)\nremove the JSONB type (c4fc0ec)\nrename some infer methods for consistency (a8f5579)\nreplace isinstance dtype checking with is_* methods (386adc2)\nrework registration / file loading (c60e30d)\nrules: generalize field referencing using rlz.ref() (0afb8b9)\nsimplify ops.ArrayColumn in postgres backend (f9677cc)\nsimplify histogram implementation by using window functions (41cbc29)\nsimplify ops.ArrayColumn in alchemy backend (28ff4a8)\nsnowflake: use lambda to define backend operations (cb33fce)\nsplit up custom nix code; remove unused derivations (57dff10)\nsqlite: use lambda to define backend operations (b937391)\ntest: make clickhouse tests use pytest-snapshot (413dbd2)\ntests: move sql output to golden dir (6a6a453)\ntest: sort regex test cases by name instead of posix-ness (0dfb0e7)\ntests: replace sqlgolden with pytest-snapshot (5700eb0)\ntimestamps: remove timezone argument to to_timestamp API (eb4762e)\ntrino: use lambda to define backend operations (dbd61a5)\nuncouple MultiQuantile class from Quantile (9c48f8c)\nuse rlz.lazy_instance_of to delay shapely import (d14badc)\nuse lazy dispatch for dt.infer (2e56540)\n\n\n\nDocumentation\n\nadd backend_sensitive decorator (836f237)\nadd pip install poetry dev env setup step (69940b1)\nadd bigquery ci data analysis notebook (2b1d4e5)\nadd how to sessionize guide (18989dd)\nadd issue templates (4480c18)\nadd missing argument descriptions (ea757fa)\nadd mssql backend page (63c0f19)\nadded 4.0 release blog post (bcc0eca)\nadded memtable howto guide (5dde9bd)\nbackends: add duckdb and mssql to the backend index page (7b13218)\nbring back git revision localized date plugin (e4fc2c9)\ncreated how to guide for deferred expressions (2a9f6ab)\ndev: python-duckdb now available for windows with conda (7f76b09)\ndocument how to create a table from a pandas dataframe using ibis.memtable (c6521ec)\nfix backends label in feature request issue form (cf852d3)\nfix broken docstrings; reduce docstring noise; workaround griffe (bd1c637)\nfix docs for building docs (23af567)\nfix feature-request issue template (6fb62f5)\nfix installation section for conda (7af6ac1)\nfix landing page links (1879362)\nfix links to make docs work locally and remotely (13c7810)\nfix pyarrow batches docstring (dba9594)\nfix single line docstring summaries (8028201)\nfix snowflake doc link in readme.md (9aff68e)\nfix the inline example for ibis.dask.do_connect (6a533f0)\nfix tutorial link on install page (b34811a)\nfix typo in first example of the homepage (9a8a25a)\nformatting and syntax highlighting fixes (50864da)\nfront page rework (24b795a)\nhow-to: use parquet data source for sessionization, fix typos, more deferred usage (974be37)\nimprove the docstring of the generic connect method (ee87802)\nissue template cleanups (fed37da)\nlist (e331247)\npolars: add backend docs page (e303b68)\nremove hrs (4c30de4)\nrenamed how to guides to be more consistent (1bdc5bd)\nsentence structure in the Notes section (ac20232)\nshow interactive prompt for python (5d7d913)\nsplit out geospatial operations in the support matrix docs (0075c28)\ntrino: add backend docs (2f262cd)\ntypo (6bac645)\ntypos headers and formatting (9566cbb)\nudf: examples in pandas have the incorrect import path (49028b8)\nupdate filename (658a296)\nupdate line (4edfce0)\nupdate readme (19a3f3c)\nuse buf/feat prefix only (2561a29)\nuse components instead of pieces (179ca1e)\nuse heading instead of bulleted bold (99b044e)\nuse library instead of project (fd2d915)\nuse present tense for use cases and “why” section (6cc7416)\nwww: fix frontpage example (7db39e8)"
  },
  {
    "objectID": "release_notes.html#section-10",
    "href": "release_notes.html#section-10",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.2.0 (2022-09-15)",
    "text": "3.2.0 (2022-09-15)\n\nFeatures\n\nadd api to get backend entry points (0152f5e)\napi: add and_ and or_ helpers (94bd4df)\napi: add argmax and argmin column methods (b52216a)\napi: add distinct to Intersection and Difference operations (cd9a34c)\napi: add ibis.memtable API for constructing in-memory table expressions (0cc6948)\napi: add ibis.sql to easily get a formatted SQL string (d971cc3)\napi: add Table.unpack() and StructValue.lift() APIs for projecting struct fields (ced5f53)\napi: allow transmute-style select method (d5fc364)\napi: implement all bitwise operators (7fc5073)\napi: promote psql to a show_sql public API (877a05d)\nclickhouse: add dataframe external table support for memtables (bc86aa7)\nclickhouse: add enum, ipaddr, json, lowcardinality to type parser (8f0287f)\nclickhouse: enable support for working window functions (310a5a8)\nclickhouse: implement argmin and argmax (ee7c878)\nclickhouse: implement bitwise operations (348cd08)\nclickhouse: implement struct scalars (1f3efe9)\ndask: implement StringReplace execution (1389f4b)\ndask: implement ungrouped argmin and argmax (854aea7)\ndeps: support duckdb 0.5.0 (47165b2)\nduckdb: handle query parameters in ibis.connect (fbde95d)\nduckdb: implement argmin and argmax (abf03f1)\nduckdb: implement bitwise xor (ca3abed)\nduckdb: register tables from pandas/pyarrow objects (36e48cc)\nduckdb: support unsigned integer types (2e67918)\nimpala: implement bitwise operations (c5302ab)\nimplement dropna for SQL backends (8a747fb)\nlog: make BaseSQLBackend._log print by default (12de5bb)\nmysql: register BLOB types (1e4fb92)\npandas: implement argmin and argmax (bf9b948)\npandas: implement NotContains on grouped data (976dce7)\npandas: implement StringReplace execution (578795f)\npandas: implement Contains with a group by (c534848)\npostgres: implement bitwise xor (9b1ebf5)\npyspark: add option to treat nan as null in aggregations (bf47250)\npyspark: implement ibis.connect for pyspark (a191744)\npyspark: implement Intersection and Difference (9845a3c)\npyspark: implement bitwise operators (33cadb1)\nsqlalchemy: implement bitwise operator translation (bd9f64c)\nsqlalchemy: make ibis.connect with sqlalchemy backends (b6cefb9)\nsqlalchemy: properly implement Intersection and Difference (2bc0b69)\nsql: implement StringReplace translation (29daa32)\nsqlite: implement bitwise xor and bitwise not (58c42f9)\nsupport table.sort_by(ibis.random()) (693005d)\ntype-system: infer pandas’ string dtype (5f0eb5d)\nux: add duckdb as the default backend (8ccb81d)\nux: use rich to format Table.info() output (67234c3)\nux: use sqlglot for pretty printing SQL (a3c81c5)\nvariadic union, intersect, & difference functions (05aca5a)\n\n\n\nBug Fixes\n\napi: make sure column names that are already inferred are not overwritten (6f1cb16)\napi: support deferred objects in existing API functions (241ce6a)\nbackend: ensure that chained limits respect prior limits (02a04f5)\nbackends: ensure select after filter works (e58ca73)\nbackends: only recommend installing ibis-foo when foo is a known backend (ac6974a)\nbase-sql: fix String-generating backend string concat implementation (3cf78c1)\nclickhouse: add IPv4/IPv6 literal inference (0a2f315)\nclickhouse: cast repeat times argument to UInt64 (b643544)\nclickhouse: fix listing tables from databases with no tables (08900c3)\ncompilers: make sure memtable rows have names in the SQL string compilers (18e7f95)\ncompiler: use repr for SQL string VALUES data (75af658)\ndask: ensure predicates are computed before projections (5cd70e1)\ndask: implement timestamp-date binary comparisons (48d5058)\ndask: set dask upper bound due to large scale test breakage (796c645), closes #9221\ndecimal: add decimal type inference (3fe3fd8)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.4.0 (113dc8f)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.5.0 (ef97c9d)\ndeps: update dependency parsy to v2 (9a06131)\ndeps: update dependency shapely to &gt;=1.6,&lt;1.8.4 (0c787d2)\ndeps: update dependency shapely to &gt;=1.6,&lt;1.8.5 (d08c737)\ndeps: update dependency sqlglot to v5 (f210bb8)\ndeps: update dependency sqlglot to v6 (5ca4533)\nduckdb: add missing types (59bad07)\nduckdb: ensure that in-memory connections remain in their creating thread (39bc537)\nduckdb: use fetch_arrow_table() to be able to handle big timestamps (85a76eb)\nfix bug in pandas & dask difference implementation (88a78fa)\nfix dask where implementation (49f8845)\nimpala: add date column dtype to impala to ibis type dict (c59e94e), closes #4449\npandas where supports scalar for left (48f6c1e)\npandas: fix anti-joins (10a659d)\npandas: implement timestamp-date binary comparisons (4fc666d)\npandas: properly handle empty groups when aggregating with GroupConcat (6545f4d)\npyspark: fix broken StringReplace implementation (22cb297)\npyspark: make sure ibis.connect works with pyspark (a7ab107)\npyspark: translate predicates before projections (b3d1c80)\nsqlalchemy: fix float64 type mapping (8782773)\nsqlalchemy: handle reductions with multiple arguments (5b2039b)\nsqlalchemy: implement SQLQueryResult translation (786a50f)\nsql: fix sql compilation after making InMemoryTable a subclass of PhysicalTable (aac9524)\nsquash several bugs in sort_by asc/desc handling (222b2ba)\nsupport chained set operations in SQL backends (227aed3)\nsupport filters on InMemoryTable exprs (abfaf1f)\ntypo: in BaseSQLBackend.compile docstring (0561b13)\n\n\n\nDeprecations\n\nright kwarg in union/intersect/difference (719a5a1)\nduckdb: deprecate path argument in favor of database (fcacc20)\nsqlite: deprecate path argument in favor of database (0f85919)\n\n\n\nPerformance\n\npandas: remove reexecution of alias children (64efa53)\npyspark: ensure that pyspark DDL doesn’t use VALUES (422c98d)\nsqlalchemy: register DataFrames cheaply where possible (ee9f1be)\n\n\n\nDocumentation\n\nadd to_sql (e2821a5)\nadd back constraints for transitive doc dependencies and fix docs (350fd43)\nadd coc reporting information (c2355ba)\nadd community guidelines documentation (fd0893f)\nadd HeavyAI to the readme (4c5ca80)\nadd how-to bfill and ffill (ff84027)\nadd how-to for ibis+duckdb register (73a726e)\nadd how-to section to docs (33c4b93)\nduckdb: add installation note for duckdb &gt;= 0.5.0 (608b1fb)\nfix memtable docstrings (72bc0f5)\nfix flake8 line length issues (fb7af75)\nfix markdown (4ab6b95)\nfix relative links in tutorial (2bd075f), closes #4064 #4201\nmake attribution style uniform across the blog (05561e0)\nmove the blog out to the top level sidebar for visibility (417ba64)\nremove underspecified UDF doc page (0eb0ac0)"
  },
  {
    "objectID": "release_notes.html#section-11",
    "href": "release_notes.html#section-11",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.1.0 (2022-07-26)",
    "text": "3.1.0 (2022-07-26)\n\nFeatures\n\nadd __getattr__ support to StructValue (75bded1)\nallow selection subclasses to define new node args (2a7dc41)\napi: accept Schema objects in public ibis.schema (0daac6c)\napi: add .tables accessor to BaseBackend (7ad27f0)\napi: add e function to public API (3a07e70)\napi: add ops.StructColumn operation (020bfdb)\napi: add cume_dist operation (6b6b185)\napi: add toplevel ibis.connect() (e13946b)\napi: handle literal timestamps with timezone embedded in string (1ae976b)\napi: ibis.connect() default to duckdb for parquet/csv extensions (ff2f088)\napi: make struct metadata more convenient to access (3fd9bd8)\napi: support tab completion for backends (eb75fc5)\napi: underscore convenience api (81716da)\napi: unnest (98ecb09)\nbackends: allow column expressions from non-foreign tables on the right side of isin/notin (e1374a4)\nbase-sql: implement trig and math functions (addb2c1)\nclickhouse: add ability to pass arbitrary kwargs to Clickhouse do_connect (583f599)\nclickhouse: implement ops.StructColumn operation (0063007)\nclickhouse: implement array collect (8b2577d)\nclickhouse: implement ArrayColumn (1301f18)\nclickhouse: implement bit aggs (f94a5d2)\nclickhouse: implement clip (12dfe50)\nclickhouse: implement covariance and correlation (a37c155)\nclickhouse: implement degrees (7946c0f)\nclickhouse: implement proper type serialization (80f4ab9)\nclickhouse: implement radians (c7b7f08)\nclickhouse: implement strftime (222f2b5)\nclickhouse: implement struct field access (fff69f3)\nclickhouse: implement trig and math functions (c56440a)\nclickhouse: support subsecond timestamp literals (e8698a6)\ncompiler: restore intersect_class and difference_class overrides in base SQL backend (2c46a15)\ndask: implement trig functions (e4086bb)\ndask: implement zeroifnull (38487db)\ndatafusion: implement negate (69dd64d)\ndatafusion: implement trig functions (16803e1)\nduckdb: add register method to duckdb backend to load parquet and csv files (4ccc6fc)\nduckdb: enable find_in_set test (377023d)\nduckdb: enable group_concat test (4b9ad6c)\nduckdb: implement ops.StructColumn operation (211bfab)\nduckdb: implement approx_count_distinct (03c89ad)\nduckdb: implement approx_median (894ce90)\nduckdb: implement arbitrary first and last aggregation (8a500bc)\nduckdb: implement NthValue (1bf2842)\nduckdb: implement strftime (aebc252)\nduckdb: return the ir.Table instance from DuckDB’s register API (0d05d41)\nmysql: implement FindInSet (e55bbbf)\nmysql: implement StringToTimestamp (169250f)\npandas: implement bitwise aggregations (37ff328)\npandas: implement degrees (25b4f69)\npandas: implement radians (6816b75)\npandas: implement trig functions (1fd52d2)\npandas: implement zeroifnull (48e8ed1)\npostgres/duckdb: implement covariance and correlation (464d3ef)\npostgres: implement ArrayColumn (7b0a506)\npyspark: implement approx_count_distinct (1fe1d75)\npyspark: implement approx_median (07571a9)\npyspark: implement covariance and correlation (ae818fb)\npyspark: implement degrees (f478c7c)\npyspark: implement nth_value (abb559d)\npyspark: implement nullifzero (640234b)\npyspark: implement radians (18843c0)\npyspark: implement trig functions (fd7621a)\npyspark: implement Where (32b9abb)\npyspark: implement xor (550b35b)\npyspark: implement zeroifnull (db13241)\npyspark: topk support (9344591)\nsqlalchemy: add degrees and radians (8b7415f)\nsqlalchemy: add xor translation rule (2921664)\nsqlalchemy: allow non-primitive arrays (4e02918)\nsqlalchemy: implement approx_count_distinct as count distinct (4e8bcab)\nsqlalchemy: implement clip (8c02639)\nsqlalchemy: implement trig functions (34c1514)\nsqlalchemy: implement Where (7424704)\nsqlalchemy: implement zeroifnull (4735e9a)\nsqlite: implement BitAnd, BitOr and BitXor (e478479)\nsqlite: implement cotangent (01e7ce7)\nsqlite: implement degrees and radians (2cf9c5e)\n\n\n\nBug Fixes\n\napi: bring back null datatype parsing (fc131a1)\napi: compute the type from both branches of Where expressions (b8f4120)\napi: ensure that Deferred objects work in aggregations (bbb376c)\napi: ensure that nulls can be cast to any type to allow caller promotion (fab4393)\napi: make ExistSubquery and NotExistsSubquery pure boolean operations (dd70024)\nbackends: make execution transactional where possible (d1ea269)\nclickhouse: cast empty result dataframe (27ae68a)\nclickhouse: handle empty IN and NOT IN expressions (2c892eb)\nclickhouse: return null instead of empty string for group_concat when values are filtered out (b826b40)\ncompiler: fix bool bool comparisons (1ac9a9e)\ndask/pandas: allow limit to be None (9f91d6b)\ndask: aggregation with multi-key groupby fails on dask backend (4f8bc70)\ndatafusion: handle predicates in aggregates (4725571)\ndeps: update dependency datafusion to &gt;=0.4,&lt;0.7 (f5b244e)\ndeps: update dependency duckdb to &gt;=0.3.2,&lt;0.5.0 (57ee818)\ndeps: update dependency duckdb-engine to &gt;=0.1.8,&lt;0.3.0 (3e379a0)\ndeps: update dependency geoalchemy2 to &gt;=0.6.3,&lt;0.13 (c04a533)\ndeps: update dependency geopandas to &gt;=0.6,&lt;0.12 (b899c37)\ndeps: update dependency Shapely to &gt;=1.6,&lt;1.8.3 (87a49ad)\ndeps: update dependency toolz to &gt;=0.11,&lt;0.13 (258a641)\ndon’t mask udf module in init.py (3e567ba)\nduckdb: ensure that paths with non-extension . chars are parsed correctly (9448fd3)\nduckdb: fix struct datatype parsing (5124763)\nduckdb: force string_agg separator to be a constant (21cdf2f)\nduckdb: handle multiple dotted extensions; quote names; consolidate implementations (1494246)\nduckdb: remove timezone function invocation (33d38fc)\ngeospatial: ensure that later versions of numpy are compatible with geospatial code (33f0afb)\nimpala: a delimited table explicitly declare stored as textfile (04086a4), closes #4260\nimpala: remove broken nth_value implementation (dbc9cc2)\nir: don’t attempt fusion when projections aren’t exactly equivalent (3482ba2)\nmysql: cast mysql timestamp literals to ensure correct return type (8116e04)\nmysql: implement integer to timestamp using from_unixtime (1b43004)\npandas/dask: look at pre_execute for has_operation reporting (cb44efc)\npandas: execute negate on bool as not (330ab4f)\npandas: fix struct inference from dict in the pandas backend (5886a9a)\npandas: force backend options registration on trace.enable() calls (8818fe6)\npandas: handle empty boolean column casting in Series conversion (f697e3e)\npandas: handle struct columns with NA elements (9a7c510)\npandas: handle the case of selection from a join when remapping overlapping column names (031c4c6)\npandas: perform correct equality comparison (d62e7b9)\npostgres/duckdb: cast after milliseconds computation instead of after extraction (bdd1d65)\npyspark: handle predicates in Aggregation (842c307)\npyspark: prevent spark from trying to convert timezone of naive timestamps (dfb4127)\npyspark: remove xpassing test for #2453 (c051e28)\npyspark: specialize implementation of has_operation (5082346)\npyspark: use empty check for collect_list in GroupConcat rule (df66acb)\nrepr: allow DestructValue selections to be formatted by fmt (4b45d87)\nrepr: when formatting DestructValue selections, use struct field names as column names (d01fe42)\nsqlalchemy: fix parsing and construction of nested array types (e20bcc0)\nsqlalchemy: remove unused second argument when creating temporary views (8766b40)\nsqlite: register conversion to isoformat for pandas.Timestamp (fe95dca)\nsqlite: test case with whitespace at the end of the line (7623ae9)\nsql: use isoformat for timestamp literals (70d0ba6)\ntype-system: infer null datatype for empty sequence of expressions (f67d5f9)\nuse bounded precision for decimal aggregations (596acfb)\n\n\n\nPerformance Improvements\n\nanalysis: add _projection as cached_property to avoid reconstruction of projections (98510c8)\nlineage: ensure that expressions are not traversed multiple times in most cases (ff9708c)\n\n\n\nReverts\n\nci: install sqlite3 on ubuntu (1f2705f)\n\n\n\n3.0.2 (2022-04-28)\n\n\nBug Fixes\n\ndocs: fix tempdir location for docs build (dcd1b22)\n\n\n\n3.0.1 (2022-04-28)\n\n\nBug Fixes\n\nbuild: replace version before exec plugin runs (573139c)"
  },
  {
    "objectID": "release_notes.html#section-14",
    "href": "release_notes.html#section-14",
    "title": "2.1.0 (2022-01-12)",
    "section": "3.0.0 (2022-04-25)",
    "text": "3.0.0 (2022-04-25)\n\n⚠ BREAKING CHANGES\n\nir: The following are breaking changes due to simplifying expression internals\n\nibis.expr.datatypes.DataType.scalar_type and DataType.column_type factory methods have been removed, DataType.scalar and DataType.column class fields can be used to directly construct a corresponding expression instance (though prefer to use operation.to_expr())\nibis.expr.types.ValueExpr._name and ValueExpr._dtype`` fields are not   accassible anymore. While these were not supposed to used directly nowValueExpr.has_name(),ValueExpr.get_name()andValueExpr.type()` methods are the only way to retrieve the expression’s name and datatype.\nibis.expr.operations.Node.output_type is a property now not a method, decorate those methods with @property\nibis.expr.operations.Value subclasses must define output_shape and output_dtype properties from now on (note the datatype abbreviation dtype in the property name)\nibis.expr.rules.cast(), scalar_like() and array_like() rules have been removed\n\napi: Replace t[\"a\"].distinct() with t[[\"a\"]].distinct().\ndeps: The sqlalchemy lower bound is now 1.4\nir: Schema.names and Schema.types attributes now have tuple type rather than list\nexpr: Columns that were added or used in an aggregation or mutation would be alphabetically sorted in compiled SQL outputs. This was a vestige from when Python dicts didn’t preserve insertion order. Now columns will appear in the order in which they were passed to aggregate or mutate\napi: dt.float is now dt.float64; use dt.float32 for the previous behavior.\nir: Relation-based execute_node dispatch rules must now accept tuples of expressions.\nir: removed ibis.expr.lineage.{roots,find_nodes} functions\nconfig: Use ibis.options.graphviz_repr = True to enable\nhdfs: Use fsspec instead of HDFS from ibis\nudf: Vectorized UDF coercion functions are no longer a public API.\nThe minimum supported Python version is now Python 3.8\nconfig: register_option is no longer supported, please submit option requests upstream\nbackends: Read tables with pandas.read_hdf and use the pandas backend\nThe CSV backend is removed. Use Datafusion for CSV execution.\nbackends: Use the datafusion backend to read parquet files\nExpr() -&gt; Expr.pipe()\ncoercion functions previously in expr/schema.py are now in udf/vectorized.py\napi: materialize is removed. Joins with overlapping columns now have suffixes.\nkudu: use impala instead: https://kudu.apache.org/docs/kudu_impala_integration.html\nAny code that was relying implicitly on string-y behavior from UUID datatypes will need to add an explicit cast first.\n\n\n\nFeatures\n\nadd repr_html for expressions to print as tables in ipython (cd6fa4e)\nadd duckdb backend (667f2d5)\nallow construction of decimal literals (3d9e865)\napi: add ibis.asc expression (efe177e), closes #1454\napi: add has_operation API to the backend (4fab014)\napi: implement type for SortExpr (ab19bd6)\nclickhouse: implement string concat for clickhouse (1767205)\nclickhouse: implement StrRight operation (67749a0)\nclickhouse: implement table union (e0008d7)\nclickhouse: implement trim, pad and string predicates (a5b7293)\ndatafusion: implement Count operation (4797a86)\ndatatypes: unbounded decimal type (f7e6f65)\ndate: add ibis.date(y,m,d) functionality (26892b6), closes #386\nduckdb/postgres/mysql/pyspark: implement .sql on tables for mixing sql and expressions (00e8087)\nduckdb: add functionality needed to pass integer to interval test (e2119e8)\nduckdb: implement _get_schema_using_query (93cd730)\nduckdb: implement now() function (6924f50)\nduckdb: implement regexp replace and extract (18d16a7)\nimplement force argument in sqlalchemy backend base class (9df7f1b)\nimplement coalesce for the pyspark backend (8183efe)\nimplement semi/anti join for the pandas backend (cb36fc5)\nimplement semi/anti join for the pyspark backend (3e1ba9c)\nimplement the remaining clickhouse joins (b3aa1f0)\nir: rewrite and speed up expression repr (45ce9b2)\nmysql: implement _get_schema_from_query (456cd44)\nmysql: move string join impl up to alchemy for mysql (77a8eb9)\npostgres: implement _get_schema_using_query (f2459eb)\npyspark: implement Distinct for pyspark (4306ad9)\npyspark: implement log base b for pyspark (527af3c)\npyspark: implement percent_rank and enable testing (c051617)\nrepr: add interval info to interval repr (df26231)\nsqlalchemy: implement ilike (43996c0)\nsqlite: implement date_truncate (3ce4f2a)\nsqlite: implement ISO week of year (714ff7b)\nsqlite: implement string join and concat (6f5f353)\nsupport of arrays and tuples for clickhouse (db512a8)\nver: dynamic version identifiers (408f862)\n\n\n\nBug Fixes\n\nadded wheel to pyproject toml for venv users (b0b8e5c)\nallow major version changes in CalVer dependencies (9c3fbe5)\nannotable: allow optional arguments at any position (778995f), closes #3730\napi: add ibis.map and .struct (327b342), closes #3118\napi: map string multiplication with integer to repeat method (b205922)\napi: thread suffixes parameter to individual join methods (31a9aff)\nchange TimestampType to Timestamp (e0750be)\nclickhouse: disconnect from clickhouse when computing version (11cbf08)\nclickhouse: use a context manager for execution (a471225)\ncombine windows during windowization (7fdd851)\nconform epoch_seconds impls to expression return type (18a70f1)\ncontext-adjustment: pass scope when calling adjust_context in pyspark backend (33aad7b), closes #3108\ndask: fix asof joins for newer version of dask (50711cc)\ndask: workaround dask bug (a0f3bd9)\ndeps: update dependency atpublic to v3 (3fe8f0d)\ndeps: update dependency datafusion to &gt;=0.4,&lt;0.6 (3fb2194)\ndeps: update dependency geoalchemy2 to &gt;=0.6.3,&lt;0.12 (dc3c361)\ndeps: update dependency graphviz to &gt;=0.16,&lt;0.21 (3014445)\nduckdb: add casts to literals to fix binding errors (1977a55), closes #3629\nduckdb: fix array column type discovery on leaf tables and add tests (15e5412)\nduckdb: fix log with base b impl (4920097)\nduckdb: support both 0.3.2 and 0.3.3 (a73ccce)\nenforce the schema’s column names in apply_to (b0f334d)\nexpose ops.IfNull for mysql backend (156c2bd)\nexpr: add more binary operators to char list and implement fallback (b88184c)\nexpr: fix formatting of table info using tabulate (b110636)\nfix float vs real data type detection in sqlalchemy (24e6774)\nfix list_schemas argument (69c1abf)\nfix postgres udfs and re-enable ci tests (7d480d2)\nfix tablecolumn execution for filter following join (064595b)\nformat: remove some newlines from formatted expr repr (ed4fa78)\nhistogram: cross_join needs onclause=True (5d36a58), closes #622\nibis.expr.signature.Parameter is not pickleable (828fd54)\nimplement coalesce properly in the pandas backend (aca5312)\nimplement count on tables for pyspark (7fe5573), closes #2879\ninfer coalesce types when a non-null expression occurs after the first argument (c5f2906)\nmutate: do not lift table column that results from mutate (ba4e5e5)\npandas: disable range windows with order by (e016664)\npandas: don’t reassign the same column to silence SettingWithCopyWarning warning (75dc616)\npandas: implement percent_rank correctly (d8b83e7)\nprevent unintentional cross joins in mutate + filter (83eef99)\npyspark: fix range windows (a6f2aa8)\nregression in Selection.sort_by with resolved_keys (c7a69cd)\nregression in sort_by with resolved_keys (63f1382), closes #3619\nremove broken csv pre_execute (93b662a)\nremove importorskip call for backend tests (2f0bcd8)\nremove incorrect fix for pandas regression (339f544)\nremove passing schema into register_parquet (bdcbb08)\nrepr: add ops.TimeAdd to repr binop lookup table (fd94275)\nrepr: allow ops.TableNode in fmt_value (6f57003)\nreverse the predicate pushdown substitution (f3cd358)\nsort_index to satisfy pandas 1.4.x (6bac0fc)\nsqlalchemy: ensure correlated subqueries FROM clauses are rendered (3175321)\nsqlalchemy: use corresponding_column to prevent spurious cross joins (fdada21)\nsqlalchemy: use replace selectables to prevent semi/anti join cross join (e8a1a71)\nsql: retain column names for named ColumnExprs (f1b4b6e), closes #3754\nsql: walk right join trees and substitute joins with right-side joins with views (0231592)\nstore schema on the pandas backend to allow correct inference (35070be)\n\n\n\nPerformance Improvements\n\ndatatypes: speed up str and hash (262d3d7)\nfast path for simple column selection (d178498)\nir: global equality cache (13c2bb2)\nir: introduce CachedEqMixin to speed up equality checks (b633925)\nrepr: remove full tree repr from rule validator error message (65885ab)\nspeed up attribute access (89d1c05)\nuse assign instead of concat in projections when possible (985c242)\n\n\n\nMiscellaneous Chores\n\ndeps: increase sqlalchemy lower bound to 1.4 (560854a)\ndrop support for Python 3.7 (0afd138)\n\n\n\nCode Refactoring\n\napi: make primitive types more cohesive (71da8f7)\napi: remove distinct ColumnExpr API (3f48cb8)\napi: remove materialize (24285c1)\nbackends: remove the hdf5 backend (ff34f3e)\nbackends: remove the parquet backend (b510473)\nconfig: disable graphviz-repr-in-notebook by default (214ad4e)\nconfig: remove old config code and port to pydantic (4bb96d1)\ndt.UUID inherits from DataType, not String (2ba540d)\nexpr: preserve column ordering in aggregations/mutations (668be0f)\nhdfs: replace HDFS with fsspec (cc6eddb)\nir: make Annotable immutable (1f2b3fa)\nir: make schema annotable (b980903)\nir: remove unused lineage roots and find_nodes functions (d630a77)\nir: simplify expressions by not storing dtype and name (e929f85)\nkudu: remove support for use of kudu through kudu-python (36bd97f)\nmove coercion functions from schema.py to udf (58eea56), closes #3033\nremove blanket call for Expr (3a71116), closes #2258\nremove the csv backend (0e3e02e)\nudf: make coerce functions in ibis.udf.vectorized private (9ba4392)"
  },
  {
    "objectID": "release_notes.html#section-15",
    "href": "release_notes.html#section-15",
    "title": "2.1.0 (2022-01-12)",
    "section": "2.1.1 (2022-01-12)",
    "text": "2.1.1 (2022-01-12)\n\nBug Fixes\n\nsetup.py: set the correct version number for 2.1.0 (f3d267b)"
  },
  {
    "objectID": "release_notes.html#features-14",
    "href": "release_notes.html#features-14",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nSerialization-deserialization of Node via pickle is now byte compatible between different processes (#2938)\nSupport joining on different columns in ClickHouse backend (#2916)\nSupport summarization of empty data in pandas backend (#2908)\nUnify implementation of fillna and isna in Pyspark backend (#2882)\nSupport binary operation with Timedelta in Pyspark backend (#2873)\nAdd group_concat operation for Clickhouse backend (#2839)\nSupport comparison of ColumnExpr to timestamp literal (#2808)\nMake op schema a cached property (#2805)\nImplement .insert() for SQLAlchemy backends (#2613, #2613)\nInfer categorical and decimal Series to more specific Ibis types in pandas backend (#2792)\nAdd startswith and endswith operations (#2790)\nAllow more flexible return type for UDFs (#2776, #2797)\nImplement Clip in the Pyspark backend (#2779)\nUse ndarray as array representation in pandas backend (#2753)\nSupport Spark filter with window operation (#2687)\nSupport context adjustment for udfs for pandas backend (#2646)\nAdd auth_local_webserver, auth_external_data, and auth_cache parameters to BigQuery connect method. Set auth_local_webserver to use a local server instead of copy-pasting an authorization code. Set auth_external_data to true to request additional scopes required to query Google Drive and Sheets. Set auth_cache to reauth or none to force reauthentication. (#2655)\nAdd bit_and, bit_or, and bit_xor integer column aggregates (BigQuery and MySQL backends) (#2641)\nBackends are defined as entry points (#2379)\nAdd ibis.array for creating array expressions (#2615)\nImplement Not operation in PySpark backend (#2607)\nAdded support for case/when in PySpark backend (#2610)\nAdd support for np.array as literals for backends that already support lists as literals (#2603)"
  },
  {
    "objectID": "release_notes.html#bugs",
    "href": "release_notes.html#bugs",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix data races in impala connection pool accounting (#2991)\nFix null literal compilation in the Clickhouse backend (#2985)\nFix order of limit and offset parameters in the Clickhouse backend (#2984)\nReplace equals operation for geospatial datatype to geo_equals (#2956)\nFix .drop(fields). The argument can now be either a list of strings or a string. (#2829)\nFix projection on differences and intersections for SQL backends (#2845)\nBackends are loaded in a lazy way, so third-party backends can import Ibis without circular imports (#2827)\nDisable aggregation optimization due to N squared performance (#2830)\nFix .cast() to array outputting list instead of np.array in pandas backend (#2821)\nFix aggregation with mixed reduction datatypes (array + scalar) on Dask backend (#2820)\nFix error when using reduction UDF that returns np.array in a grouped aggregation (#2770)\nFix time context trimming error for multi column udfs in pandas backend (#2712)\nFix error during compilation of range_window in base_sql backends (:issue:2608) (#2710)\nFix wrong row indexing in the result for ‘window after filter’ for timecontext adjustment (#2696)\nFix aggregate exploding the output of Reduction ops that return a list/ndarray (#2702)\nFix issues with context adjustment for filter with PySpark backend (#2693)\nAdd temporary struct col in pyspark backend to ensure that UDFs are executed only once (#2657)\nFix BigQuery connect bug that ignored project ID parameter (#2588)\nFix overwrite logic to account for DestructColumn inside mutate API (#2636)\nFix fusion optimization bug that incorrectly changes operation order (#2635)\nFixes a NPE issue with substr in PySpark backend (#2610)\nFixes binary data type translation into BigQuery bytes data type (#2354)\nMake StructValue picklable (#2577)"
  },
  {
    "objectID": "release_notes.html#support",
    "href": "release_notes.html#support",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nImprovement of the backend API. The former Client subclasses have been replaced by a Backend class that must subclass ibis.backends.base.BaseBackend. The BaseBackend class contains abstract methods for the minimum subset of methods that backends must implement, and their signatures have been standardized across backends. The Ibis compiler has been refactored, and backends don’t need to implement all compiler classes anymore if the default works for them. Only a subclass of ibis.backends.base.sql.compiler.Compiler is now required. Backends now need to register themselves as entry points. (#2678)\nDeprecate exists_table(table) in favor of table in list_tables() (#2905)\nRemove handwritten type parser; parsing errors that were previously IbisTypeError are now parsy.ParseError. parsy is now a hard requirement. (#2977)\nMethods current_database and list_databases raise an exception for backends that do not support databases (#2962)\nMethod set_database has been deprecated, in favor of creating a new connection to a different database (#2913)\nRemoved log method of clients, in favor of verbose_log option (#2914)\nOutput of Client.version returned as a string, instead of a setuptools Version (#2883)\nDeprecated list_schemas in SQLAlchemy backends in favor of list_databases (#2862)\nDeprecated ibis.&lt;backend&gt;.verify() in favor of capturing exception in ibis.&lt;backend&gt;.compile() (#2865)\nSimplification of data fetching. Backends don’t need to implement Query anymore (#2789)\nMove BigQuery backend to a separate repository &lt;https://github.com/ibis-project/ibis-bigquery&gt;_. The backend will be released separately, use pip install ibis-bigquery or conda install ibis-bigquery to install it, and then use as before. (#2665)\nSupporting SQLAlchemy 1.4, and requiring minimum 1.3 (#2689)\nNamespace time_col config, fix type check for trim_with_timecontext for pandas window execution (#2680)\nRemove deprecated ibis.HDFS, ibis.WebHDFS and ibis.hdfs_connect (#2505)"
  },
  {
    "objectID": "release_notes.html#features-15",
    "href": "release_notes.html#features-15",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd Struct.from_dict (#2514)\nAdd hash and hashbytes support for BigQuery backend (#2310)\nSupport reduction UDF without groupby to return multiple columns for pandas backend (#2511)\nSupport analytic and reduction UDF to return multiple columns for pandas backend (#2487)\nSupport elementwise UDF to return multiple columns for pandas and PySpark backend (#2473)\nFEAT: Support Ibis interval for window in pyspark backend (#2409)\nUse Scope class for scope in pyspark backend (#2402)\nAdd PySpark support for ReductionVectorizedUDF (#2366)\nAdd time context in scope in execution for pandas backend (#2306)\nAdd start_point and end_point to PostGIS backend. (#2081)\nAdd set difference to general ibis api (#2347)\nAdd rowid expression, supported by SQLite and OmniSciDB (#2251)\nAdd intersection to general ibis api (#2230)\nAdd application_name argument to ibis.bigquery.connect to allow attributing Google API requests to projects that use Ibis. (#2303)\nAdd support for casting category dtype in pandas backend (#2285)\nAdd support for Union in the PySpark backend (#2270)\nAdd support for implementign custom window object for pandas backend (#2260)\nImplement two level dispatcher for execute_node (#2246)\nAdd ibis.pandas.trace module to log time and call stack information. (#2233)\nValidate that the output type of a UDF is a single element (#2198)\nZeroIfNull and NullIfZero implementation for OmniSciDB (#2186)\nIsNan implementation for OmniSciDB (#2093)\n[OmnisciDB] Support add_columns and drop_columns for OmnisciDB table (#2094)\nCreate ExtractQuarter operation and add its support to Clickhouse, CSV, Impala, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark (#2175)\nAdd translation rules for isnull() and notnull() for pyspark backend (#2126)\nAdd window operations support to SQLite (#2232)\nImplement read_csv for omniscidb backend (#2062)\n[OmniSciDB] Add support to week extraction (#2171)\nDate, DateDiff and TimestampDiff implementations for OmniSciDB (#2097)\nCreate ExtractWeekOfYear operation and add its support to Clickhouse, CSV, MySQL, pandas, Parquet, PostgreSQL, PySpark and Spark (#2177)\nAdd initial support for ibis.random function (#2060)\nAdded epoch_seconds extraction operation to Clickhouse, CSV, Impala, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite, Spark and BigQuery :issue:2273 (#2178)\n[OmniSciDB] Add “method” parameter to load_data (#2165)\nAdd non-nullable info to schema output (#2117)\nfillna and nullif implementations for OmnisciDB (#2083)\nAdd load_data to sqlalchemy’s backends and fix database parameter for load/create/drop when database parameter is the same than the current database (#1981)\n[OmniSciDB] Add support for within, d_fully_within and point (#2125)\nOmniSciDB - Refactor DDL and Client; Add temporary parameter to create_table and “force” parameter to drop_view (#2086)\nCreate ExtractDayOfYear operation and add its support to Clickhouse, CSV, MySQL, OmniSciDB, pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark (#2173)\nImplementations of Log Log2 Log10 for OmniSciDB backend (#2095)"
  },
  {
    "objectID": "release_notes.html#bugs-1",
    "href": "release_notes.html#bugs-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nTable expressions do not recognize inet datatype (Postgres backend) (#2462)\nTable expressions do not recognize macaddr datatype (Postgres backend) (#2461)\nFix aggcontext.Summarize not always producing scalar (pandas backend) (#2410)\nFix same window op with different window size on table lead to incorrect results for pyspark backend (#2414)\nFix same column with multiple aliases not showing properly in repr (#2229)\nFix reduction UDFs over ungrouped, bounded windows on pandas backend (#2395)\nFEAT: Support rolling window UDF with non numeric inputs for pandas backend. (#2386)\nFix scope get to use hashmap lookup instead of list lookup (#2386)\nFix equality behavior for Literal ops (#2387)\nFix analytic ops over ungrouped and unordered windows on pandas backend (#2376)\nFix the covariance operator in the BigQuery backend. (#2367)\nUpdate impala kerberos dependencies (#2342)\nAdded verbose logging to SQL backends (#1320)\nFix issue with sql_validate call to OmnisciDB. (#2256)\nAdd missing float types to pandas backend (#2237)\nAllow group_by and order_by as window operation input in pandas backend (#2252)\nFix PySpark compiler error when elementwise UDF output_type is Decimal or Timestamp (#2223)\nFix interactive mode returning a expression instead of the value when used in Jupyter (#2157)\nFix PySpark error when doing alias after selection (#2127)\nFix millisecond issue for OmniSciDB :issue:2167, MySQL :issue:2169, PostgreSQL :issue:2166, pandas :issue:2168, BigQuery :issue:2273 backends (#2170)\n[OmniSciDB] Fix TopK when used as filter (#2134)"
  },
  {
    "objectID": "release_notes.html#support-1",
    "href": "release_notes.html#support-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nMove ibis.HDFS, ibis.WebHDFS and ibis.hdfs_connect to ibis.impala.* (#2497)\nDrop support to Python 3.6 (#2288)\nSimplifying tests directories structure (#2351)\nUpdate google-cloud-bigquery dependency minimum version to 1.12.0 (#2304)\nRemove “experimental” mentions for OmniSciDB and pandas backends (#2234)\nUse an OmniSciDB image stable on CI (#2244)\nAdded fragment_size to table creation for OmniSciDB (#2107)\nAdded round() support for OmniSciDB (#2096)\nEnabled cumulative ops support for OmniSciDB (#2113)"
  },
  {
    "objectID": "release_notes.html#features-16",
    "href": "release_notes.html#features-16",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nImprove many arguments UDF performance in pandas backend. (#2071)\nAdd DenseRank, RowNumber, MinRank, Count, PercentRank/CumeDist window operations to OmniSciDB (#1976)\nIntroduce a top level vectorized UDF module (experimental). Implement element-wise UDF for pandas and PySpark backend. (#2047)\nAdd support for multi arguments window UDAF for the pandas backend (#2035)\nClean up window translation logic in pyspark backend (#2004)\nAdd docstring check to CI for an initial subset files (#1996)\nPyspark backend bounded windows (#2001)\nAdd more POSTGIS operations (#1987)\nSQLAlchemy Default precision and scale to decimal types for PostgreSQL and MySQL (#1969)\nAdd support for array operations in PySpark backend (#1983)\nImplement sort, if_null, null_if and notin for PySpark backend (#1978)\nAdd support for date/time operations in PySpark backend (#1974)\nAdd support for params, query_schema, and sql in PySpark backend (#1973)\nImplement join for PySpark backend (#1967)\nValidate AsOfJoin tolerance and attempt interval unit conversion (#1952)\nfilter for PySpark backend (#1943)\nwindow operations for pyspark backend (#1945)\nImplement IntervalSub for pandas backend (#1951)\nPySpark backend string and column ops (#1942)\nPySpark backend (#1913)\nDDL support for Spark backend (#1908)\nSupport timezone aware arrow timestamps (#1923)\nAdd shapely geometries as input for literals (#1860)\nAdd geopandas as output for omniscidb (#1858)\nSpark UDFs (#1885)\nAdd support for Postgres UDFs (#1871)\nSpark tests (#1830)\nSpark client (#1807)\nUse pandas rolling apply to implement rows_with_max_lookback (#1868)"
  },
  {
    "objectID": "release_notes.html#bugs-2",
    "href": "release_notes.html#bugs-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nPin “clickhouse-driver” to “&gt;=0.1.3” (#2089)\nFix load data stage for Linux CI (#2069)\nFix datamgr.py fail if IBIS_TEST_OMNISCIDB_DATABASE=omnisci (#2057)\nChange pymapd connection parameter from “session_id” to “sessionid” (#2041)\nFix pandas backend to treat trailing_window preceding arg as window bound rather than window size (e.g. preceding=0 now indicates current row rather than window size 0) (#2009)\nFix handling of Array types in Postgres UDF (#2015)\nFix pydocstyle config (#2010)\nPinning clickhouse-driver&lt;0.1.2 (#2006)\nFix CI log for database (#1984)\nFixes explain operation (#1933)\nFix incorrect assumptions about attached SQLite databases (#1937)\nUpgrade to JDK11 (#1938)\nsql method doesn’t work when the query uses LIMIT clause (#1903)\nFix union implementation (#1910)\nFix failing com imports on master (#1912)\nOmniSci/MapD - Fix reduction for bool (#1901)\nPass scope to grouping execution in the pandas backend (#1899)\nFix various Spark backend issues (#1888)\nMake Nodes enforce the proper signature (#1891)\nFix according to bug in pd.to_datetime when passing the unit flag (#1893)\nFix small formatting buglet in PR merge tool (#1883)\nFix the case where we do not have an index when using preceding with intervals (#1876)\nFixed issues with geo data (#1872)\nRemove -x from pytest call in linux CI (#1869)\nFix return type of Struct.from_tuples (#1867)"
  },
  {
    "objectID": "release_notes.html#support-2",
    "href": "release_notes.html#support-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nAdd support to Python 3.8 (#2066)\nPin back version of isort (#2079)\nUse user-defined port variables for Omnisci and PostgreSQL tests (#2082)\nChange omniscidb image tag from v5.0.0 to v5.1.0 on docker-compose recipe (#2077)\n[Omnisci] The same SRIDs for test_geo_spatial_binops (#2051)\nUnpin rtree version (#2078)\nLink pandas issues with xfail tests in pandas/tests/test_udf.py (#2074)\nDisable Postgres tests on Windows CI. (#2075)\nuse conda for installation black and isort tools (#2068)\nCI: Fix CI builds related to new pandas 1.0 compatibility (#2061)\nFix data map for int8 on OmniSciDB backend (#2056)\nAdd possibility to run tests for separate backend via make test BACKENDS=[YOUR BACKEND] (#2052)\nFix “cudf” import on OmniSciDB backend (#2055)\nCI: Drop table only if it exists (OmniSciDB) (#2050)\nAdd initial documentation for OmniSciDB, MySQL, PySpark and SparkSQL backends, add initial documentation for geospatial methods and add links to Ibis wiki page (#2034)\nImplement covariance for bigquery backend (#2044)\nAdd Spark to supported backends list (#2046)\nPing dependency of rtree to fix CI failure (#2043)\nDrop support for Python 3.5 (#2037)\nHTML escape column names and types in png repr. (#2023)\nAdd geospatial tutorial notebook (#1991)\nChange omniscidb image tag from v4.7.0 to v5.0.0 on docker-compose recipe (#2031)\nPin “semantic_version” to “&lt;2.7” in the docs build CI, fix “builddoc” and “doc” section inside “Makefile” and skip mysql tzinfo on CI to allow to run MySQL using docker container on a hard disk drive. (#2030)\nFixed impala start up issues (#2012)\ncache all ops in translate() (#1999)\nAdd black step to CI (#1988)\nJson UUID any (#1962)\nAdd log for database services (#1982)\nFix BigQuery backend fixture so batting and awards_players fixture re… (#1972)\nDisable BigQuery explicitly in all/test_join.py (#1971)\nRe-formatting all files using pre-commit hook (#1963)\nDisable codecov report upload during CI builds (#1961)\nDeveloper doc enhancements (#1960)\nMissing geospatial ops for OmniSciDB (#1958)\nRemove pandas deprecation warnings (#1950)\nAdd developer docs to get docker setup (#1948)\nMore informative IntegrityError on duplicate columns (#1949)\nImprove geospatial literals and smoke tests (#1928)\nPostGIS enhancements (#1925)\nRename mapd to omniscidb backend (#1866)\nFix failing BigQuery tests (#1926)\nAdded missing null literal op (#1917)\nUpdate link to Presto website (#1895)\nRemoving linting from windows (#1896)\nFix link to NUMFOCUS CoC (#1884)\nAdded CoC section (#1882)\nRemove pandas exception for rows_with_max_lookback (#1859)\nMove CI pipelines to Azure (#1856)"
  },
  {
    "objectID": "release_notes.html#features-17",
    "href": "release_notes.html#features-17",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd new geospatial functions to OmniSciDB backend (#1836)\nallow pandas timedelta in rows_with_max_lookback (#1838)\nAccept rows-with-max-lookback as preceding parameter (#1825)\nPostGIS support (#1787)"
  },
  {
    "objectID": "release_notes.html#bugs-3",
    "href": "release_notes.html#bugs-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix call to psql causing failing CI (#1855)\nFix nested array literal repr (#1851)\nFix repr of empty schema (#1850)\nAdd max_lookback to window replace and combine functions (#1843)\nPartially revert #1758 (#1837)"
  },
  {
    "objectID": "release_notes.html#support-3",
    "href": "release_notes.html#support-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nSkip SQLAlchemy backend tests in connect method in backends.py (#1847)\nValidate order_by when using rows_with_max_lookback window (#1848)\nGenerate release notes from commits (#1845)\nRaise exception on backends where rows_with_max_lookback can’t be implemented (#1844)\nTighter version spec for pytest (#1840)\nAllow passing a branch to ci/feedstock.py (#1826)"
  },
  {
    "objectID": "release_notes.html#features-18",
    "href": "release_notes.html#features-18",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nConslidate trailing window functions (#1809)\nCall to_interval when casting integers to intervals (#1766)\nAdd session feature to mapd client API (#1796)\nAdd min periods parameter to Window (#1792)\nAllow strings for types in pandas UDFs (#1785)\nAdd missing date operations and struct field operation for the pandas backend (#1790)\nAdd window operations to the OmniSci backend (#1771)\nReimplement the pandas backend using topological sort (#1758)\nAdd marker for xfailing specific backends (#1778)\nEnable window function tests where possible (#1777)\nis_computable_arg dispatcher (#1743)\nAdded float32 and geospatial types for create table from schema (#1753)"
  },
  {
    "objectID": "release_notes.html#bugs-4",
    "href": "release_notes.html#bugs-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nFix group_concat test and implementations (#1819)\nFix failing strftime tests on Python 3.7 (#1818)\nRemove unnecessary (and erroneous in some cases) frame clauses (#1757)\nChained mutate operations are buggy (#1799)\nAllow projections from joins to attempt fusion (#1783)\nFix Python 3.5 dependency versions (#1798)\nFix compatibility and bugs associated with pandas toposort reimplementation (#1789)\nFix outer_join generating LEFT join instead of FULL OUTER (#1772)\nNullIf should enforce that its arguments are castable to a common type (#1782)\nFix conda create command in documentation (#1775)\nFix preceding and following with None (#1765)\nPostgreSQL interval type not recognized (#1661)"
  },
  {
    "objectID": "release_notes.html#support-4",
    "href": "release_notes.html#support-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nRemove decorator hacks and add custom markers (#1820)\nAdd development deps to setup.py (#1814)\nFix design and developer docs (#1805)\nPin sphinx version to 2.0.1 (#1810)\nAdd pep8speaks integration (#1793)\nFix typo in UDF signature specification (#1821)\nClean up most xpassing tests (#1779)\nUpdate omnisci container version (#1781)\nConstrain PyMapD version to get passing builds (#1776)\nRemove warnings and clean up some docstrings (#1763)\nAdd StringToTimestamp as unsupported (#1638)\nAdd isort pre-commit hooks (#1759)\nAdd Python 3.5 testing back to CI (#1750)\nRe-enable CI for building step (#1700)\nUpdate README reference to MapD to say OmniSci (#1749)"
  },
  {
    "objectID": "release_notes.html#features-19",
    "href": "release_notes.html#features-19",
    "title": "2.1.0 (2022-01-12)",
    "section": "Features",
    "text": "Features\n\nAdd black as a pre-commit hook (#1735)\nAdd support for the arbitrary aggregate in the mapd backend (#1680)\nAdd SQL method for the MapD backend (#1731)\nClean up merge PR script and use the actual merge feature of GitHub (#1744)\nAdd cross join to the pandas backend (#1723)\nImplement default handler for multiple client pre_execute (#1727)\nImplement BigQuery auth using pydata_google_auth (#1728)\nTimestamp literal accepts a timezone parameter (#1712)\nRemove support for passing integers to ibis.timestamp (#1725)\nAdd find_nodes to lineage (#1704)\nRemove a bunch of deprecated APIs and clean up warnings (#1714)\nImplement table distinct for the pandas backend (#1716)\nImplement geospatial functions for MapD (#1678)\nImplement geospatial types for MapD (#1666)\nAdd pre commit hook (#1685)\nGetting started with mapd, mysql and pandas (#1686)\nSupport column names with special characters in mapd (#1675)\nAllow operations to hide arguments from display (#1669)\nRemove implicit ordering requirements in the PostgreSQL backend (#1636)\nAdd cross join operator to MapD (#1655)\nFix UDF bugs and add support for non-aggregate analytic functions (#1637)\nSupport string slicing with other expressions (#1627)\nPublish the ibis roadmap (#1618)\nImplement approx_median in BigQuery (#1604)\nMake ibis node instances hashable (#1611)\nAdd range_window and trailing_range_window to docs (#1608)"
  },
  {
    "objectID": "release_notes.html#bugs-5",
    "href": "release_notes.html#bugs-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bugs",
    "text": "Bugs\n\nMake dev/merge-pr.py script handle PR branches (#1745)\nFix NULLIF implementation for the pandas backend (#1742)\nFix casting to float in the MapD backend (#1737)\nFix testing for BigQuery after auth flow update (#1741)\nFix skipping for new BigQuery auth flow (#1738)\nFix bug in TableExpr.drop (#1732)\nFilter the raw warning from newer pandas to support older pandas (#1729)\nFix BigQuery credentials link (#1706)\nAdd Union as an unsuppoted operation for MapD (#1639)\nFix visualizing an ibis expression when showing a selection after a table join (#1705)\nFix MapD exception for toDateTime (#1659)\nUse == to compare strings (#1701)\nResolves joining with different column names (#1647)\nFix map get with compatible types (#1643)\nFixed where operator for MapD (#1653)\nRemove parameters from mapd (#1648)\nMake sure we cast when NULL is else in CASE expressions (#1651)\nFix equality (#1600)"
  },
  {
    "objectID": "release_notes.html#support-5",
    "href": "release_notes.html#support-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Support",
    "text": "Support\n\nDo not build universal wheels (#1748)\nRemove tag prefix from versioneer (#1747)\nUse releases to manage documentation (#1746)\nUse cudf instead of pygdf (#1694)\nFix multiple CI issues (#1696)\nUpdate mapd ci to v4.4.1 (#1681)\nEnabled mysql CI on azure pipelines (#1672)\nRemove support for Python 2 (#1670)\nFix flake8 and many other warnings (#1667)\nUpdate README.md for impala and kudu (#1664)\nRemove defaults as a channel from azure pipelines (#1660)\nFixes a very typo in the pandas/core.py docstring (#1658)\nUnpin clickhouse-driver version (#1657)\nAdd test for reduction returning lists (#1650)\nFix Azure VM image name (#1646)\nUpdated MapD server-CI (#1641)\nAdd TableExpr.drop to API documentation (#1645)\nFix Azure deployment step (#1642)\nSet up CI with Azure Pipelines (#1640)\nFix conda builds (#1609)"
  },
  {
    "objectID": "release_notes.html#new-features",
    "href": "release_notes.html#new-features",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAllow keyword arguments in Node subclasses (#968)\nSplat args into Node subclasses instead of requiring a list (#969)\nAdd support for UNION in the BigQuery backend (#1408, #1409)\nSupport for writing UDFs in BigQuery (#1377). See the BigQuery UDF docs for more details.\nSupport for cross-project expressions in the BigQuery backend. (#1427, #1428)\nAdd strftime and to_timestamp support for BigQuery (#1422, #1410)\nRequire google-cloud-bigquery &gt;=1.0 (#1424)\nLimited support for interval arithmetic in the pandas backend (#1407)\nSupport for subclassing TableExpr (#1439)\nFill out pandas backend operations (#1423)\nAdd common DDL APIs to the pandas backend (#1464)\nImplement the sql method for BigQuery (#1463)\nAdd to_timestamp for BigQuery (#1455)\nAdd the mapd backend (#1419)\nImplement range windows (#1349)\nSupport for map types in the pandas backend (#1498)\nAdd mean and sum for boolean types in BigQuery (#1516)\nAll recent versions of SQLAlchemy are now supported (#1384)\nAdd support for NUMERIC types in the BigQuery backend (#1534)\nSpeed up grouped and rolling operations in the pandas backend (#1549)\nImplement TimestampNow for BigQuery and pandas (#1575)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-17",
    "href": "release_notes.html#bug-fixes-17",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nNullable property is now propagated through value types (#1289)\nImplicit casting between signed and unsigned integers checks boundaries\nFix precedence of case statement (#1412)\nFix handling of large timestamps (#1440)\nFix identical_to precedence (#1458)\npandas 0.23 compatibility (#1458)\nPreserve timezones in timestamp-typed literals (#1459)\nFix incorrect topological ordering of UNION expressions (#1501)\nFix projection fusion bug when attempting to fuse columns of the same name (#1496)\nFix output type for some decimal operations (#1541)"
  },
  {
    "objectID": "release_notes.html#api-changes",
    "href": "release_notes.html#api-changes",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nThe previous, private rules API has been rewritten (#1366)\nDefining input arguments for operations happens in a more readable fashion instead of the previous input_type list.\nRemoved support for async query execution (only Impala supported)\nRemove support for Python 3.4 (#1326)\nBigQuery division defaults to using IEEE_DIVIDE (#1390)\nAdd tolerance parameter to asof_join (#1443)"
  },
  {
    "objectID": "release_notes.html#new-backends",
    "href": "release_notes.html#new-backends",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Backends",
    "text": "New Backends\n\nFile Support for CSV & HDF5 (#1165, #1194)\nFile Support for Parquet Format (#1175, #1194)\nExperimental support for MySQL thanks to @kszucs (#1224)"
  },
  {
    "objectID": "release_notes.html#new-features-1",
    "href": "release_notes.html#new-features-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nSupport for Unsigned Integer Types (#1194)\nSupport for Interval types and expressions with support for execution on the Impala and Clickhouse backends (#1243)\nIsnan, isinf operations for float and double values (#1261)\nSupport for an interval with a quarter period (#1259)\nibis.pandas.from_dataframe convenience function (#1155)\nRemove the restriction on ROW_NUMBER() requiring it to have an ORDER BY clause (#1371)\nAdd .get() operation on a Map type (#1376)\nAllow visualization of custom defined expressions\nAdd experimental support for pandas UDFs/UDAFs (#1277)\nFunctions can be used as groupby keys (#1214, #1215)\nGeneralize the use of the where parameter to reduction operations (#1220)\nSupport for interval operations thanks to @kszucs (#1243, #1260, #1249)\nSupport for the PARTITIONTIME column in the BigQuery backend (#1322)\nAdd arbitrary() method for selecting the first non null value in a column (#1230, #1309)\nWindowed MultiQuantile operation in the pandas backend thanks to @DiegoAlbertoTorres (#1343)\nRules for validating table expressions thanks to @DiegoAlbertoTorres (#1298)\nComplete end-to-end testing framework for all supported backends (#1256)\ncontains/not contains now supported in the pandas backend (#1210, #1211)\nCI builds are now reproducible locally thanks to @kszucs (#1121, #1237, #1255, #1311)\nisnan/isinf operations thanks to @kszucs (#1261)\nFramework for generalized dtype and schema inference, and implicit casting thanks to @kszucs (#1221, #1269)\nGeneric utilities for expression traversal thanks to @kszucs (#1336)\nday_of_week API (#306, #1047)\nDesign documentation for ibis (#1351)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-18",
    "href": "release_notes.html#bug-fixes-18",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nUnbound parameters were failing in the simple case of a ibis.expr.types.TableExpr.mutate call with no operation (#1378)\nFix parameterized subqueries (#1300, #1331, #1303, #1378)\nFix subquery extraction, which wasn’t happening in topological order (#1342)\nFix parenthesization if isnull (#1307)\nCalling drop after mutate did not work (#1296, #1299)\nSQLAlchemy backends were missing an implementation of ibis.expr.operations.NotContains.\nSupport REGEX_EXTRACT in PostgreSQL 10 (#1276, #1278)"
  },
  {
    "objectID": "release_notes.html#api-changes-1",
    "href": "release_notes.html#api-changes-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nFixing #1378 required the removal of the name parameter to the ibis.param function. Use the ibis.expr.types.Expr.name method instead."
  },
  {
    "objectID": "release_notes.html#new-backends-1",
    "href": "release_notes.html#new-backends-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Backends",
    "text": "New Backends\n\nBigQuery backend (#1170), thanks to @tsdlovell.\nClickhouse backend (#1127), thanks to @kszucs."
  },
  {
    "objectID": "release_notes.html#new-features-2",
    "href": "release_notes.html#new-features-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAdd support for Binary data type (#1183)\nAllow users of the BigQuery client to define their own API proxy classes (#1188)\nAdd support for HAVING in the pandas backend (#1182)\nAdd struct field tab completion (#1178)\nAdd expressions for Map/Struct types and columns (#1166)\nSupport Table.asof_join (#1162)\nAllow right side of arithmetic operations to take over (#1150)\nAdd a data_preload step in pandas backend (#1142)\nexpressions in join predicates in the pandas backend (#1138)\nScalar parameters (#1075)\nLimited window function support for pandas (#1083)\nImplement Time datatype (#1105)\nImplement array ops for pandas (#1100)\nsupport for passing multiple quantiles in .quantile() (#1094)\nsupport for clip and quantile ops on DoubleColumns (#1090)\nEnable unary math operations for pandas, sqlite (#1071)\nEnable casting from strings to temporal types (#1076)\nAllow selection of whole tables in pandas joins (#1072)\nImplement comparison for string vs date and timestamp types (#1065)\nImplement isnull and notnull for pandas (#1066)\nAllow like operation to accept a list of conditions to match (#1061)\nAdd a pre_execute step in pandas backend (#1189)"
  },
  {
    "objectID": "release_notes.html#bug-fixes-19",
    "href": "release_notes.html#bug-fixes-19",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nRemove global expression caching to ensure repeatable code generation (#1179, #1181)\nFix ORDER BY generation without a GROUP BY (#1180, #1181)\nEnsure that ~ibis.expr.datatypes.DataType and subclasses hash properly (#1172)\nEnsure that the pandas backend can deal with unary operations in groupby\n(#1182)\nIncorrect impala code generated for NOT with complex argument (#1176)\nBUG/CLN: Fix predicates on Selections on Joins (#1149)\nDon't use SET LOCAL to allow redshift to work (#1163)\nAllow empty arrays as arguments (#1154)\nFix column renaming in groupby keys (#1151)\nEnsure that we only cast if timezone is not None (#1147)\nFix location of conftest.py (#1107)\nTST/Make sure we drop tables during postgres testing (#1101)\nFix misleading join error message (#1086)\nBUG/TST: Make hdfs an optional dependency (#1082)\nMemoization should include expression name where available (#1080)"
  },
  {
    "objectID": "release_notes.html#performance-enhancements",
    "href": "release_notes.html#performance-enhancements",
    "title": "2.1.0 (2022-01-12)",
    "section": "Performance Enhancements",
    "text": "Performance Enhancements\n\nSpeed up imports (#1074)\nFix execution perf of groupby and selection (#1073)\nUse normalize for casting to dates in pandas (#1070)\nSpeed up pandas groupby (#1067)"
  },
  {
    "objectID": "release_notes.html#contributors",
    "href": "release_notes.html#contributors",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\nThe following people contributed to the 0.12.0 release :\n$ git shortlog -sn --no-merges v0.11.2..v0.12.0\n63  Phillip Cloud\n 8  Jeff Reback\n 2  Krisztián Szűcs\n 2  Tory Haavik\n 1  Anirudh\n 1  Szucs Krisztian\n 1  dlovell\n 1  kwangin"
  },
  {
    "objectID": "release_notes.html#new-features-3",
    "href": "release_notes.html#new-features-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nExperimental pandas backend to allow execution of ibis expression against pandas DataFrames\nGraphviz visualization of ibis expressions. Implements _repr_png_ for Jupyter Notebook functionality\nAbility to create a partitioned table from an ibis expression\nSupport for missing operations in the SQLite backend: sqrt, power, variance, and standard deviation, regular expression functions, and missing power support for PostgreSQL\nSupport for schemas inside databases with the PostgreSQL backend\nAppveyor testing on core ibis across all supported Python versions\nAdd year/month/day methods to date types\nAbility to sort, group by and project columns according to positional index rather than only by name\nAdded a type parameter to ibis.literal to allow user specification of literal types"
  },
  {
    "objectID": "release_notes.html#bug-fixes-20",
    "href": "release_notes.html#bug-fixes-20",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nFix broken conda recipe\nFix incorrectly typed fillna operation\nFix postgres boolean summary operations\nFix kudu support to reflect client API Changes\nFix equality of nested types and construction of nested types when the value type is specified as a string"
  },
  {
    "objectID": "release_notes.html#api-changes-2",
    "href": "release_notes.html#api-changes-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nDeprecate passing integer values to the ibis.timestamp literal constructor, this will be removed in 0.12.0\nAdded the admin_timeout parameter to the kudu client connect function"
  },
  {
    "objectID": "release_notes.html#contributors-1",
    "href": "release_notes.html#contributors-1",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git shortlog --summary --numbered v0.10.0..v0.11.0\n\n  58 Phillip Cloud\n   1 Greg Rahn\n   1 Marius van Niekerk\n   1 Tarun Gogineni\n   1 Wes McKinney"
  },
  {
    "objectID": "release_notes.html#new-features-4",
    "href": "release_notes.html#new-features-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nInitial PostgreSQL backend contributed by Phillip Cloud.\nAdd groupby as an alias for group_by to table expressions"
  },
  {
    "objectID": "release_notes.html#bug-fixes-21",
    "href": "release_notes.html#bug-fixes-21",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nFix an expression error when filtering based on a new field\nFix Impala's SQL compilation of using OR with compound filters\nVarious fixes with the having(...) function in grouped table expressions\nFix CTE (WITH) extraction inside UNION ALL expressions.\nFix ImportError on Python 2 when mock library not installed"
  },
  {
    "objectID": "release_notes.html#api-changes-3",
    "href": "release_notes.html#api-changes-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nThe deprecated ibis.impala_connect and ibis.make_client APIs have been removed"
  },
  {
    "objectID": "release_notes.html#new-features-5",
    "href": "release_notes.html#new-features-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nApache Kudu (incubating) integration for Impala users. Will add some documentation here when possible.\nAdd use_https option to ibis.hdfs_connect for WebHDFS connections in secure (Kerberized) clusters without SSL enabled.\nCorrectly compile aggregate expressions involving multiple subqueries.\n\nTo explain this last point in more detail, suppose you had:\ntable = ibis.table([('flag', 'string'),\n                    ('value', 'double')],\n                   'tbl')\n\nflagged = table[table.flag == '1']\nunflagged = table[table.flag == '0']\n\nfv = flagged.value\nuv = unflagged.value\n\nexpr = (fv.mean() / fv.sum()) - (uv.mean() / uv.sum())\nThe last expression now generates the correct Impala or SQLite SQL:\nSELECT t0.`tmp` - t1.`tmp` AS `tmp`\nFROM (\n  SELECT avg(`value`) / sum(`value`) AS `tmp`\n  FROM tbl\n  WHERE `flag` = '1'\n) t0\n  CROSS JOIN (\n    SELECT avg(`value`) / sum(`value`) AS `tmp`\n    FROM tbl\n    WHERE `flag` = '0'\n  ) t1"
  },
  {
    "objectID": "release_notes.html#bug-fixes-22",
    "href": "release_notes.html#bug-fixes-22",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nCHAR(n) and VARCHAR(n) Impala types now correctly map to Ibis string expressions\nFix inappropriate projection-join-filter expression rewrites resulting in incorrect generated SQL.\nImpalaClient.create_table correctly passes STORED AS PARQUET for format='parquet'.\nFixed several issues with Ibis dependencies (impyla, thriftpy, sasl, thrift_sasl), especially for secure clusters. Upgrading will pull in these new dependencies.\nDo not fail in ibis.impala.connect when trying to create the temporary Ibis database if no HDFS connection passed.\nFix join predicate evaluation bug when column names overlap with table attributes.\nFix handling of fully-materialized joins (aka select * joins) in SQLAlchemy / SQLite."
  },
  {
    "objectID": "release_notes.html#contributors-2",
    "href": "release_notes.html#contributors-2",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\nThank you to all who contributed patches to this release.\n$ git log v0.6.0..v0.7.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n    21 Wes McKinney\n     1 Uri Laserson\n     1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-6",
    "href": "release_notes.html#new-features-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nNew integrated Impala functionality. See Ibis for Impala Users for more details on these things.\n\nImproved Impala-pandas integration. Create tables or insert into existing tables from pandas DataFrame objects.\nPartitioned table metadata management API. Add, drop, alter, and insert into table partitions.\nAdd is_partitioned property to ImpalaTable.\nAdded support for LOAD DATA DDL using the load_data function, also supporting partitioned tables.\nModify table metadata (location, format, SerDe properties etc.) using ImpalaTable.alter\nInterrupting Impala expression execution with Control-C will attempt to cancel the running query with the server.\nSet the compression codec (e.g. snappy) used with ImpalaClient.set_compression_codec.\nGet and set query options for a client session with ImpalaClient.get_options and ImpalaClient.set_options.\nAdd ImpalaTable.metadata method that parses the output of the DESCRIBE FORMATTED DDL to simplify table metadata inspection.\nAdd ImpalaTable.stats and ImpalaTable.column_stats to see computed table and partition statistics.\nAdd CHAR and VARCHAR handling\nAdd refresh, invalidate_metadata DDL options and add incremental option to compute_stats for COMPUTE INCREMENTAL STATS.\n\nAdd substitute method for performing multiple value substitutions in an array or scalar expression.\nDivision is by default true division like Python 3 for all numeric data. This means for SQL systems that use C-style division semantics, the appropriate CAST will be automatically inserted in the generated SQL.\nEasier joins on tables with overlapping column names. See Ibis for SQL Programmers.\nExpressions like string_expr[:3] now work as expected.\nAdd coalesce instance method to all value expressions.\nPassing limit=None to the execute method on expressions disables any default row limits."
  },
  {
    "objectID": "release_notes.html#api-changes-4",
    "href": "release_notes.html#api-changes-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nImpalaTable.rename no longer mutates the calling table expression."
  },
  {
    "objectID": "release_notes.html#contributors-3",
    "href": "release_notes.html#contributors-3",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.5.0..v0.6.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n46 Wes McKinney\n 3 Uri Laserson\n 1 Phillip Cloud\n 1 mariusvniekerk\n 1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-7",
    "href": "release_notes.html#new-features-7",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nSQLite client and built-in function support\nIbis now supports Python 3.4 as well as 2.6 and 2.7\nIbis can utilize Impala user-defined aggregate (UDA) functions\nSQLAlchemy-based translation toolchain to enable more SQL engines having SQLAlchemy dialects to be supported\nMany window function usability improvements (nested analytic functions and deferred binding conveniences)\nMore convenient aggregation with keyword arguments in aggregate functions\nBuilt preliminary wrapper API for MADLib-on-Impala\nAdd var and std aggregation methods and support in Impala\nAdd nullifzero numeric method for all SQL engines\nAdd rename method to Impala tables (for renaming tables in the Hive metastore)\nAdd close method to ImpalaClient for session cleanup (#533)\nAdd relabel method to table expressions\nAdd insert method to Impala tables\nAdd compile and verify methods to all expressions to test compilation and ability to compile (since many operations are unavailable in SQLite, for example)"
  },
  {
    "objectID": "release_notes.html#api-changes-5",
    "href": "release_notes.html#api-changes-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nImpala Ibis client creation now uses only ibis.impala.connect, and ibis.make_client has been deprecated"
  },
  {
    "objectID": "release_notes.html#contributors-4",
    "href": "release_notes.html#contributors-4",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.4.0..v0.5.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n      55 Wes McKinney\n      9 Uri Laserson\n      1 Kristopher Overholt"
  },
  {
    "objectID": "release_notes.html#new-features-8",
    "href": "release_notes.html#new-features-8",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nAdd tooling to use Impala C++ scalar UDFs within Ibis (#262, #195)\nSupport and testing for Kerberos-enabled secure HDFS clusters\nMany table functions can now accept functions as parameters (invoked on the calling table) to enhance composability and emulate late-binding semantics of languages (like R) that have non-standard evaluation (#460)\nAdd any, all, notany, and notall reductions on boolean arrays, as well as cumany and cumall\nUsing topk now produces an analytic expression that is executable (as an aggregation) but can also be used as a filter as before (#392, #91)\nAdded experimental database object \"usability layer\", see ImpalaClient.database.\nAdd TableExpr.info\nAdd compute_stats API to table expressions referencing physical Impala tables\nAdd explain method to ImpalaClient to show query plan for an expression\nAdd chmod and chown APIs to HDFS interface for superusers\nAdd convert_base method to strings and integer types\nAdd option to ImpalaClient.create_table to create empty partitioned tables\nibis.cross_join can now join more than 2 tables at once\nAdd ImpalaClient.raw_sql method for running naked SQL queries\nImpalaClient.insert now validates schemas locally prior to sending query to cluster, for better usability.\nAdd conda installation recipes"
  },
  {
    "objectID": "release_notes.html#contributors-5",
    "href": "release_notes.html#contributors-5",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.3.0..v0.4.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     38 Wes McKinney\n      9 Uri Laserson\n      2 Meghana Vuyyuru\n      2 Kristopher Overholt\n      1 Marius van Niekerk"
  },
  {
    "objectID": "release_notes.html#new-features-9",
    "href": "release_notes.html#new-features-9",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\nImplement window / analytic function support\nEnable non-equijoins (join clauses with operations other than ==).\nAdd remaining string functions supported by Impala.\nAdd pipe method to tables (hat-tip to the pandas dev team).\nAdd mutate convenience method to tables.\nFleshed out WebHDFS implementations: get/put directories, move files, etc. See the full HDFS API.\nAdd truncate method for timestamp values\nImpalaClient can execute scalar expressions not involving any table.\nCan also create internal Impala tables with a specific HDFS path.\nMake Ibis's temporary Impala database and HDFS paths configurable (see ibis.options).\nAdd truncate_table function to client (if the user's Impala cluster supports it).\nPython 2.6 compatibility\nEnable Ibis to execute concurrent queries in multithreaded applications (earlier versions were not thread-safe).\nTest data load script in scripts/load_test_data.py\nAdd an internal operation type signature API to enhance developer productivity."
  },
  {
    "objectID": "release_notes.html#contributors-6",
    "href": "release_notes.html#contributors-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.2.0..v0.3.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     59 Wes McKinney\n     29 Uri Laserson\n      4 Isaac Hodes\n      2 Meghana Vuyyuru"
  },
  {
    "objectID": "release_notes.html#new-features-10",
    "href": "release_notes.html#new-features-10",
    "title": "2.1.0 (2022-01-12)",
    "section": "New Features",
    "text": "New Features\n\ninsert method on Ibis client for inserting data into existing tables.\nparquet_file, delimited_file, and avro_file client methods for querying datasets not yet available in Impala\nNew ibis.hdfs_connect method and HDFS client API for WebHDFS for writing files and directories to HDFS\nNew timedelta API and improved timestamp data support\nNew bucket and histogram methods on numeric expressions\nNew category logical datatype for handling bucketed data, among other things\nAdd summary API to numeric expressions\nAdd value_counts convenience API to array expressions\nNew string methods like, rlike, and contains for fuzzy and regex searching\nAdd options.verbose option and configurable options.verbose_log callback function for improved query logging and visibility\nSupport for new SQL built-in functions\n\nibis.coalesce\nibis.greatest and ibis.least\nibis.where for conditional logic (see also ibis.case and ibis.cases)\nnullif method on value expressions\nibis.now\n\nNew aggregate functions: approx_median, approx_nunique, and group_concat\nwhere argument in aggregate functions\nAdd having method to group_by intermediate object\nAdded group-by convenience table.group_by(exprs).COLUMN_NAME.agg_function()\nAdd default expression names to most aggregate functions\nNew Impala database client helper methods\n\ncreate_database\ndrop_database\nexists_database\nlist_databases\nset_database\n\nClient list_tables searching / listing method\nAdd add, sub, and other explicit arithmetic methods to value expressions"
  },
  {
    "objectID": "release_notes.html#api-changes-6",
    "href": "release_notes.html#api-changes-6",
    "title": "2.1.0 (2022-01-12)",
    "section": "API Changes",
    "text": "API Changes\n\nNew Ibis client and Impala connection workflow. Client now combined from an Impala connection and an optional HDFS connection"
  },
  {
    "objectID": "release_notes.html#bug-fixes-23",
    "href": "release_notes.html#bug-fixes-23",
    "title": "2.1.0 (2022-01-12)",
    "section": "Bug Fixes",
    "text": "Bug Fixes\n\nNumerous expression API bug fixes and rough edges fixed"
  },
  {
    "objectID": "release_notes.html#contributors-7",
    "href": "release_notes.html#contributors-7",
    "title": "2.1.0 (2022-01-12)",
    "section": "Contributors",
    "text": "Contributors\n$ git log v0.1.0..v0.2.0 --pretty=format:%aN | sort | uniq -c | sort -rn\n     71 Wes McKinney\n      1 Juliet Hougland\n      1 Isaac Hodes"
  },
  {
    "objectID": "concepts/versioning.html",
    "href": "concepts/versioning.html",
    "title": "Versioning policy",
    "section": "",
    "text": "Ibis follows a Semantic Versioning scheme (MAJOR.MINOR.PATCH, like 6.1.0).\n\nAn increase in the MAJOR version number will happen when a release contains breaking changes in the public API. This includes anything documented in the reference documentation, excluding any features explicitly marked as “experimental”. Features not part of the public API (e.g. anything in ibis.expr.operations may make breaking changes at any time).\nAn increase in the MINOR or PATCH version number indicate changes to public APIs that should remain compatible with previous Ibis versions with the same MAJOR version number.\n\n\n\nIbis follows NEP29 with respect to supported Python versions.\nThis has been in-place since Ibis version 3.0.0.\nThe support table shows the schedule for dropping support for Python versions.\nThe next major release of Ibis that occurs on or after the NEP29 drop date removes support for the specified Python version."
  },
  {
    "objectID": "concepts/versioning.html#supported-python-versions",
    "href": "concepts/versioning.html#supported-python-versions",
    "title": "Versioning policy",
    "section": "",
    "text": "Ibis follows NEP29 with respect to supported Python versions.\nThis has been in-place since Ibis version 3.0.0.\nThe support table shows the schedule for dropping support for Python versions.\nThe next major release of Ibis that occurs on or after the NEP29 drop date removes support for the specified Python version."
  },
  {
    "objectID": "why.html",
    "href": "why.html",
    "title": "Why Ibis?",
    "section": "",
    "text": "Ibis is the portable Python dataframe library.\nIf you’ve had issues with scaling data transformation code in Python, need to work with data in multiple data platforms, find yourself translating between other Python dataframe APIs, or just want a great Python dataframe experience, Ibis is for you."
  },
  {
    "objectID": "why.html#portability",
    "href": "why.html#portability",
    "title": "Why Ibis?",
    "section": "Portability",
    "text": "Portability\nYou can reuse the same code across different backends.\nMost Python dataframes are tightly coupled to their execution engine. And many databases only support SQL, with no Python API. Ibis solves this problem by providing a common API for data manipulation in Python, and compiling that API into the backend’s native language. This means you can learn a single API and use it across any supported backend (execution engine).\n\nWhile portability with Ibis isn’t perfect, commonalities across backends and SQL dialects combined with years of engineering effort produce a full-featured and robust framework for data manipulation in Python.\nIn the long-term, we aim for a standard query plan Intermediate Representation (IR) like Substrait to simplify this further."
  },
  {
    "objectID": "why.html#ecosystem",
    "href": "why.html#ecosystem",
    "title": "Why Ibis?",
    "section": "Ecosystem",
    "text": "Ecosystem\nIbis is part of a larger ecosystem of Python data tools.\nIt is designed to work well with other tools in this ecosystem, and we continue to make it easier to use Ibis with other tools over time.\n\nLocal experience\nOut of the box, Ibis offers a great local experience for working with many file formats.\nDuckDB is the default backend, with Polars and DataFusion as two other great local options. Many of the backends can run locally but require more setup than a pip installation.\n\n\nScaling up and out\nAfter prototyping on a local backend, directly scale in the cloud.\nYou can prototype on DuckDB and deploy with MotherDuck. You can scale from any Python client with Ibis installed to whatever your backend supports."
  },
  {
    "objectID": "why.html#use-cases",
    "href": "why.html#use-cases",
    "title": "Why Ibis?",
    "section": "Use cases",
    "text": "Use cases\nYou can use Ibis at any stage of your data workflow.\nUse the same framework for local exploration on a few files or production workloads on the most advanced data platforms.\nIbis helps with:\n\ndata catalog exploration\nexploratory data analysis\ntransforming data\nvisualizing data\ndata science and machine learning"
  },
  {
    "objectID": "why.html#supported-backends",
    "href": "why.html#supported-backends",
    "title": "Why Ibis?",
    "section": "Supported backends",
    "text": "Supported backends\nYou can install Ibis and a supported backend with pip, conda, mamba, or pixi.\n\npipcondamambapixi\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall with the bigquery extra:\npip install 'ibis-framework[bigquery]'\nConnect using ibis.bigquery.connect.\n\n\nInstall with the clickhouse extra:\npip install 'ibis-framework[clickhouse]'\nConnect using ibis.clickhouse.connect.\n\n\nInstall with the dask extra:\npip install 'ibis-framework[dask]'\nConnect using ibis.dask.connect.\n\n\nInstall with the datafusion extra:\npip install 'ibis-framework[datafusion]'\nConnect using ibis.datafusion.connect.\n\n\nInstall with the druid extra:\npip install 'ibis-framework[druid]'\nConnect using ibis.druid.connect.\n\n\nInstall with the duckdb extra:\npip install 'ibis-framework[duckdb]'\nConnect using ibis.duckdb.connect.\n\n\nInstall with the impala extra:\npip install 'ibis-framework[impala]'\nConnect using ibis.impala.connect.\n\n\nInstall with the mssql extra:\npip install 'ibis-framework[mssql]'\nConnect using ibis.mssql.connect.\n\n\nInstall with the mysql extra:\npip install 'ibis-framework[mysql]'\nConnect using ibis.mysql.connect.\n\n\nInstall with the oracle extra:\npip install 'ibis-framework[oracle]'\nConnect using ibis.oracle.connect.\n\n\nInstall with the pandas extra:\npip install 'ibis-framework[pandas]'\nConnect using ibis.pandas.connect.\n\n\nInstall with the polars extra:\npip install 'ibis-framework[polars]'\nConnect using ibis.polars.connect.\n\n\nInstall with the postgres extra:\npip install 'ibis-framework[postgres]'\nConnect using ibis.postgres.connect.\n\n\nInstall with the pyspark extra:\npip install 'ibis-framework[pyspark]'\nConnect using ibis.pyspark.connect.\n\n\nInstall with the snowflake extra:\npip install 'ibis-framework[snowflake]'\nConnect using ibis.snowflake.connect.\n\n\nInstall with the sqlite extra:\npip install 'ibis-framework[sqlite]'\nConnect using ibis.sqlite.connect.\n\n\nInstall with the trino extra:\npip install 'ibis-framework[trino]'\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nconda install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nconda install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nconda install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nconda install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nconda install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nconda install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nconda install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nconda install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nconda install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nconda install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nconda install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nconda install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nconda install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nconda install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nconda install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nconda install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nconda install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nInstall the ibis-bigquery package:\nmamba install -c conda-forge ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nInstall the ibis-clickhouse package:\nmamba install -c conda-forge ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nInstall the ibis-dask package:\nmamba install -c conda-forge ibis-dask\nConnect using ibis.dask.connect.\n\n\nInstall the ibis-datafusion package:\nmamba install -c conda-forge ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nInstall the ibis-druid package:\nmamba install -c conda-forge ibis-druid\nConnect using ibis.druid.connect.\n\n\nInstall the ibis-duckdb package:\nmamba install -c conda-forge ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nInstall the ibis-impala package:\nmamba install -c conda-forge ibis-impala\nConnect using ibis.impala.connect.\n\n\nInstall the ibis-mssql package:\nmamba install -c conda-forge ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nInstall the ibis-mysql package:\nmamba install -c conda-forge ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nInstall the ibis-oracle package:\nmamba install -c conda-forge ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nInstall the ibis-pandas package:\nmamba install -c conda-forge ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nInstall the ibis-polars package:\nmamba install -c conda-forge ibis-polars\nConnect using ibis.polars.connect.\n\n\nInstall the ibis-postgres package:\nmamba install -c conda-forge ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nInstall the ibis-pyspark package:\nmamba install -c conda-forge ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nInstall the ibis-snowflake package:\nmamba install -c conda-forge ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nInstall the ibis-sqlite package:\nmamba install -c conda-forge ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nInstall the ibis-trino package:\nmamba install -c conda-forge ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nBigQueryClickHouseDaskDataFusionDruidDuckDBImpalaMSSQLMySQLOraclepandasPolarsPostgreSQLPySparkSnowflakeSQLiteTrino\n\n\nAdd the ibis-bigquery package:\npixi add ibis-bigquery\nConnect using ibis.bigquery.connect.\n\n\nAdd the ibis-clickhouse package:\npixi add ibis-clickhouse\nConnect using ibis.clickhouse.connect.\n\n\nAdd the ibis-dask package:\npixi add ibis-dask\nConnect using ibis.dask.connect.\n\n\nAdd the ibis-datafusion package:\npixi add ibis-datafusion\nConnect using ibis.datafusion.connect.\n\n\nAdd the ibis-druid package:\npixi add ibis-druid\nConnect using ibis.druid.connect.\n\n\nAdd the ibis-duckdb package:\npixi add ibis-duckdb\nConnect using ibis.duckdb.connect.\n\n\nAdd the ibis-impala package:\npixi add ibis-impala\nConnect using ibis.impala.connect.\n\n\nAdd the ibis-mssql package:\npixi add ibis-mssql\nConnect using ibis.mssql.connect.\n\n\nAdd the ibis-mysql package:\npixi add ibis-mysql\nConnect using ibis.mysql.connect.\n\n\nAdd the ibis-oracle package:\npixi add ibis-oracle\nConnect using ibis.oracle.connect.\n\n\nAdd the ibis-pandas package:\npixi add ibis-pandas\nConnect using ibis.pandas.connect.\n\n\nAdd the ibis-polars package:\npixi add ibis-polars\nConnect using ibis.polars.connect.\n\n\nAdd the ibis-postgres package:\npixi add ibis-postgres\nConnect using ibis.postgres.connect.\n\n\nAdd the ibis-pyspark package:\npixi add ibis-pyspark\nConnect using ibis.pyspark.connect.\n\n\nAdd the ibis-snowflake package:\npixi add ibis-snowflake\nConnect using ibis.snowflake.connect.\n\n\nAdd the ibis-sqlite package:\npixi add ibis-sqlite\nConnect using ibis.sqlite.connect.\n\n\nAdd the ibis-trino package:\npixi add ibis-trino\nConnect using ibis.trino.connect.\n\n\n\n\n\n\nSee the backend support matrix for details on operations supported. Open a feature request if you’d like to see support for an operation in a given backend. If the backend supports it, we’ll do our best to add it quickly!"
  },
  {
    "objectID": "why.html#community",
    "href": "why.html#community",
    "title": "Why Ibis?",
    "section": "Community",
    "text": "Community\nCommunity discussions primarily take place on GitHub."
  },
  {
    "objectID": "why.html#getting-started",
    "href": "why.html#getting-started",
    "title": "Why Ibis?",
    "section": "Getting started",
    "text": "Getting started\nIf you’re interested in trying Ibis we recommend the getting started tutorial."
  },
  {
    "objectID": "posts/selectors/index.html",
    "href": "posts/selectors/index.html",
    "title": "Maximizing productivity with selectors",
    "section": "",
    "text": "Before Ibis 5.0 it’s been challenging to concisely express whole-table operations with ibis. Happily this is no longer the case in ibis 5.0.\nLet’s jump right in!\nWe’ll look at selectors examples using the palmerpenguins data set with the DuckDB backend."
  },
  {
    "objectID": "posts/selectors/index.html#setup",
    "href": "posts/selectors/index.html#setup",
    "title": "Maximizing productivity with selectors",
    "section": "Setup",
    "text": "Setup\n\nfrom ibis.interactive import *\n\nt = ex.penguins.fetch()\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "posts/selectors/index.html#examples",
    "href": "posts/selectors/index.html#examples",
    "title": "Maximizing productivity with selectors",
    "section": "Examples",
    "text": "Examples\n\nNormalization\nLet’s say you want to compute the z-score of every numeric column and replace the existing data with that normalized value. Here’s how you’d do that with selectors:\n\nt.mutate(s.across(s.numeric(), (_ - _.mean()) / _.std()))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year      ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ float64   │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │ -1.257484 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │ -1.257484 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │ -1.257484 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │         … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────┘\n\n\n\n\n\nWhat’s Up With the year Column?\nWhoops, looks like we included year in our normalization because it’s an int64 column (and therefore numeric) but normalizing the year doesn’t make sense.\nWe can exclude year from the normalization using another selector:\n\nt.mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      -0.883205 │      0.784300 │         -1.416272 │   -0.563317 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.809939 │      0.126003 │         -1.060696 │   -0.500969 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.663408 │      0.429833 │         -0.420660 │   -1.186793 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -1.322799 │      1.088129 │         -0.562890 │   -0.937403 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.846572 │      1.746426 │         -0.776236 │   -0.688012 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -0.919837 │      0.328556 │         -1.416272 │   -0.719186 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.864888 │      1.240044 │         -0.420660 │    0.590115 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -1.799025 │      0.480471 │         -0.562890 │   -0.906229 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      -0.352029 │      1.543873 │         -0.776236 │    0.060160 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nc is short for “column” and the ~ means “negate”. Combining those we get “not the year column”!\nPretty neat right?\n\n\nComposable Group By\nThe power of this approach comes in when you want the grouped version. Perhaps we think some of these columns vary by species.\nWith selectors, all you need to do is slap a .group_by(\"species\") onto t:\n\nt.group_by(\"species\").mutate(\n    s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std())\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │       0.791697 │     -1.270997 │          0.160007 │   -0.001444 │ female │  2008 │\n│ Adelie  │ Biscoe    │       1.467524 │     -0.038103 │          0.924596 │    0.816322 │ male   │  2009 │\n│ Adelie  │ Dream     │       0.378692 │      0.619441 │         -0.910418 │    2.070231 │ male   │  2007 │\n│ Adelie  │ Dream     │      -0.860324 │     -0.284681 │         -1.216254 │   -1.200835 │ female │  2007 │\n│ Adelie  │ Torgersen │      -0.785232 │      0.783827 │          0.465843 │   -0.546622 │ female │  2007 │\n│ Adelie  │ Torgersen │       0.190962 │      1.852335 │          0.007089 │   -0.110480 │ male   │  2007 │\n│ Adelie  │ Torgersen │       0.040778 │     -0.449067 │         -1.369172 │   -0.164997 │ female │  2007 │\n│ Adelie  │ Torgersen │       0.153416 │      1.030405 │          0.771678 │    2.124749 │ male   │  2007 │\n│ Adelie  │ Torgersen │      -1.761426 │     -0.202489 │          0.465843 │   -0.492104 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │       1.204702 │      1.523563 │          0.007089 │    1.197947 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nSince ibis translates this into a run-of-the-mill selection as if you had called select or mutate without selectors, nothing special is needed for a backend to work with these new constructs.\nLet’s look at some more examples.\n\n\nMin-max Normalization\nGrouped min/max normalization? Easy:\n\nt.group_by(\"species\").mutate(\n    s.across(s.numeric() & ~s.c(\"year\"), (_ - _.min()) / (_.max() - _.min()))\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │       0.633094 │      0.216667 │          0.500000 │    0.441558 │ female │  2008 │\n│ Adelie  │ Biscoe    │       0.762590 │      0.466667 │          0.631579 │    0.636364 │ male   │  2009 │\n│ Adelie  │ Dream     │       0.553957 │      0.600000 │          0.315789 │    0.935065 │ male   │  2007 │\n│ Adelie  │ Dream     │       0.316547 │      0.416667 │          0.263158 │    0.155844 │ female │  2007 │\n│ Adelie  │ Torgersen │       0.330935 │      0.633333 │          0.552632 │    0.311688 │ female │  2007 │\n│ Adelie  │ Torgersen │       0.517986 │      0.850000 │          0.473684 │    0.415584 │ male   │  2007 │\n│ Adelie  │ Torgersen │       0.489209 │      0.383333 │          0.236842 │    0.402597 │ female │  2007 │\n│ Adelie  │ Torgersen │       0.510791 │      0.683333 │          0.605263 │    0.948052 │ male   │  2007 │\n│ Adelie  │ Torgersen │       0.143885 │      0.433333 │          0.552632 │    0.324675 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │       0.712230 │      0.783333 │          0.473684 │    0.727273 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nCasting and Munging\nHow about casting every column whose name ends with any of the strings \"mm\" or \"g\" to a float32? No problem!\n\nt.mutate(s.across(s.endswith((\"mm\", \"g\")), _.cast(\"float32\")))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float32        │ float32       │ float32           │ float32     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │      39.099998 │     18.700001 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      39.500000 │     17.400000 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      40.299999 │     18.000000 │             195.0 │      3250.0 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │               nan │         nan │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      36.700001 │     19.299999 │             193.0 │      3450.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.299999 │     20.600000 │             190.0 │      3650.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      38.900002 │     17.799999 │             181.0 │      3625.0 │ female │  2007 │\n│ Adelie  │ Torgersen │      39.200001 │     19.600000 │             195.0 │      4675.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │      34.099998 │     18.100000 │             193.0 │      3475.0 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │      42.000000 │     20.200001 │             190.0 │      4250.0 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nWe can make all string columns have the same case too!\n\nt.mutate(s.across(s.of_type(\"string\"), _.lower()))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ adelie  │ torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ adelie  │ torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ adelie  │ torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ adelie  │ torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ adelie  │ torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ adelie  │ torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ adelie  │ torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ adelie  │ torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ adelie  │ torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ adelie  │ torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nMultiple Computations per Column\nWhat if I want to compute multiple things? Heck yeah!\n\nt.group_by(\"sex\").mutate(\n    s.across(\n        s.numeric() & ~s.c(\"year\"),\n        dict(centered=_ - _.mean(), zscore=(_ - _.mean()) / _.std()),\n    )\n).select(\"sex\", s.endswith((\"_centered\", \"_zscore\")))\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ sex    ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string │ float64                 │ float64                │ float64                    │ float64              │ float64               │ float64              │ float64                  │ float64            │\n├────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ female │                 4.10303 │              -1.925455 │                  11.636364 │           937.727273 │              0.836760 │            -1.072270 │                 0.930851 │           1.407635 │\n│ female │                 1.20303 │              -2.425455 │                  10.636364 │           712.727273 │              0.245342 │            -1.350716 │                 0.850856 │           1.069885 │\n│ female │                -8.09697 │               0.674545 │                 -12.363636 │          -462.272727 │             -1.651271 │             0.375649 │                -0.989030 │          -0.693924 │\n│ female │                -5.89697 │               0.874545 │                 -10.363636 │          -562.272727 │             -1.202610 │             0.487027 │                -0.829039 │          -0.844035 │\n│ female │                -0.99697 │               1.174545 │                 -15.363636 │          -662.272727 │             -0.203319 │             0.654095 │                -1.229015 │          -0.994147 │\n│ female │                -5.49697 │               1.374545 │                 -12.363636 │          -162.272727 │             -1.121035 │             0.765473 │                -0.989030 │          -0.243590 │\n│ female │                -3.39697 │               2.574545 │                  -2.363636 │          -412.272727 │             -0.692768 │             1.433743 │                -0.189079 │          -0.618868 │\n│ female │                -7.69697 │               1.974545 │                 -13.363636 │          -537.272727 │             -1.569697 │             1.099608 │                -1.069025 │          -0.806507 │\n│ female │                -4.29697 │               1.874545 │                 -23.363636 │          -462.272727 │             -0.876311 │             1.043919 │                -1.868975 │          -0.693924 │\n│ female │                -6.19697 │               2.774545 │                  -8.363636 │           -62.272727 │             -1.263791 │             1.545122 │                -0.669049 │          -0.093478 │\n│ …      │                       … │                      … │                          … │                    … │                     … │                    … │                        … │                  … │\n└────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘\n\n\n\nDon’t like the naming convention?\nPass a function to make your own name!\n\nt.select(s.startswith(\"bill\")).mutate(\n    s.across(\n        s.all(),\n        dict(x=_ - _.mean(), y=_.max()),\n        names=lambda col, fn: f\"{col}_{fn}_improved\",\n    )\n)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n│           40.3 │          18.0 │                  -3.62193 │                  0.84883 │                      59.6 │                     21.5 │\n│            nan │           nan │                       nan │                      nan │                      59.6 │                     21.5 │\n│           36.7 │          19.3 │                  -7.22193 │                  2.14883 │                      59.6 │                     21.5 │\n│           39.3 │          20.6 │                  -4.62193 │                  3.44883 │                      59.6 │                     21.5 │\n│           38.9 │          17.8 │                  -5.02193 │                  0.64883 │                      59.6 │                     21.5 │\n│           39.2 │          19.6 │                  -4.72193 │                  2.44883 │                      59.6 │                     21.5 │\n│           34.1 │          18.1 │                  -9.82193 │                  0.94883 │                      59.6 │                     21.5 │\n│           42.0 │          20.2 │                  -1.92193 │                  3.04883 │                      59.6 │                     21.5 │\n│              … │             … │                         … │                        … │                         … │                        … │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\n\n\n\nDon’t like lambda functions? We support a format string too!\n\nt.select(s.startswith(\"bill\")).mutate(\n    s.across(\n        s.all(),\n        func=dict(x=_ - _.mean(), y=_.max()),\n        names=\"{col}_{fn}_improved\",\n    )\n).head(2)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ bill_length_mm ┃ bill_depth_mm ┃ bill_length_mm_x_improved ┃ bill_depth_mm_x_improved ┃ bill_length_mm_y_improved ┃ bill_depth_mm_y_improved ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64        │ float64       │ float64                   │ float64                  │ float64                   │ float64                  │\n├────────────────┼───────────────┼───────────────────────────┼──────────────────────────┼───────────────────────────┼──────────────────────────┤\n│           39.1 │          18.7 │                  -4.82193 │                  1.54883 │                      59.6 │                     21.5 │\n│           39.5 │          17.4 │                  -4.42193 │                  0.24883 │                      59.6 │                     21.5 │\n└────────────────┴───────────────┴───────────────────────────┴──────────────────────────┴───────────────────────────┴──────────────────────────┘\n\n\n\n\n\nWorking with other Ibis APIs\nWe’ve seen lots of mutate use, but selectors also work with .agg:\n\nt.group_by(\"year\").agg(s.across(s.numeric() & ~s.c(\"year\"), _.mean())).order_by(\"year\")\n\n┏━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ year  ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ int64 │ float64        │ float64       │ float64           │ float64     │\n├───────┼────────────────┼───────────────┼───────────────────┼─────────────┤\n│  2007 │      43.740367 │     17.427523 │        196.880734 │ 4124.541284 │\n│  2008 │      43.541228 │     16.914035 │        202.798246 │ 4266.666667 │\n│  2009 │      44.452941 │     17.125210 │        202.806723 │ 4210.294118 │\n└───────┴────────────────┴───────────────┴───────────────────┴─────────────┘\n\n\n\nNaturally, selectors work in grouping keys too, for even more convenience:\n\nt.group_by(~s.numeric() | s.c(\"year\")).mutate(\n    s.across(s.numeric() & ~s.c(\"year\"), dict(centered=_ - _.mean(), std=_.std()))\n).select(\"species\", s.endswith((\"_centered\", \"_std\")))\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ species ┃ bill_length_mm_centered ┃ bill_depth_mm_centered ┃ flipper_length_mm_centered ┃ body_mass_g_centered ┃ bill_length_mm_std ┃ bill_depth_mm_std ┃ flipper_length_mm_std ┃ body_mass_g_std ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string  │ float64                 │ float64                │ float64                    │ float64              │ float64            │ float64           │ float64               │ float64         │\n├─────────┼─────────────────────────┼────────────────────────┼────────────────────────────┼──────────────────────┼────────────────────┼───────────────────┼───────────────────────┼─────────────────┤\n│ Adelie  │                   -1.46 │               0.400000 │                  -1.600000 │          -170.000000 │           1.327780 │          0.681909 │              2.302173 │      189.076704 │\n│ Adelie  │                   -0.96 │              -0.200000 │                   3.400000 │           180.000000 │           1.327780 │          0.681909 │              2.302173 │      189.076704 │\n│ Adelie  │                   -0.36 │              -1.100000 │                  -1.600000 │            30.000000 │           1.327780 │          0.681909 │              2.302173 │      189.076704 │\n│ Adelie  │                    1.44 │               0.300000 │                   1.400000 │          -220.000000 │           1.327780 │          0.681909 │              2.302173 │      189.076704 │\n│ Adelie  │                    1.34 │               0.600000 │                  -1.600000 │           180.000000 │           1.327780 │          0.681909 │              2.302173 │      189.076704 │\n│ Gentoo  │                    1.00 │               0.935294 │                  11.117647 │           147.058824 │           3.056755 │          0.670766 │              4.973459 │      349.763576 │\n│ Gentoo  │                    1.00 │              -0.164706 │                  -0.882353 │           147.058824 │           3.056755 │          0.670766 │              4.973459 │      349.763576 │\n│ Gentoo  │                   -1.40 │              -0.864706 │                  -3.882353 │          -152.941176 │           3.056755 │          0.670766 │              4.973459 │      349.763576 │\n│ Gentoo  │                   -2.30 │              -0.064706 │                   0.117647 │          -352.941176 │           3.056755 │          0.670766 │              4.973459 │      349.763576 │\n│ Gentoo  │                   -2.20 │               0.035294 │                  -3.882353 │          -402.941176 │           3.056755 │          0.670766 │              4.973459 │      349.763576 │\n│ …       │                       … │                      … │                          … │                    … │                  … │                 … │                     … │               … │\n└─────────┴─────────────────────────┴────────────────────────┴────────────────────────────┴──────────────────────┴────────────────────┴───────────────────┴───────────────────────┴─────────────────┘\n\n\n\n\n\nFiltering Selectors\nYou can also express complex filters more concisely.\nLet’s say we only want to keep rows where all the bill size z-score related columns’ absolute values are greater than 2.\n\nt.drop(\"year\").group_by(\"species\").mutate(\n    s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std()))\n).filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ float64               │ float64              │ float64                  │ float64            │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ Adelie  │ Torgersen │           46.0 │          21.5 │               194 │        4200 │ male   │              2.706539 │             2.592071 │                 0.618760 │           1.088911 │\n│ Adelie  │ Dream     │           32.1 │          15.5 │               188 │        3050 │ female │             -2.512345 │            -2.339505 │                -0.298747 │          -1.418906 │\n│ Gentoo  │ Biscoe    │           55.9 │          17.0 │               228 │        5600 │ male   │              2.724046 │             2.056508 │                 1.667394 │           1.039411 │\n│ Gentoo  │ Biscoe    │           59.6 │          17.0 │               230 │        6050 │ male   │              3.924621 │             2.056508 │                 1.975799 │           1.932062 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘\n\n\n\n\n\nBonus: Generated SQL\nThe SQL for that last expression is pretty gnarly:\n\nibis.to_sql(\n    t.drop(\"year\")\n    .group_by(\"species\")\n    .mutate(s.across(s.numeric(), dict(zscore=(_ - _.mean()) / _.std())))\n    .filter(s.if_all(s.startswith(\"bill\") & s.endswith(\"_zscore\"), _.abs() &gt; 2))\n)\n\nWITH t0 AS (\n  SELECT\n    t2.species AS species,\n    t2.island AS island,\n    t2.bill_length_mm AS bill_length_mm,\n    t2.bill_depth_mm AS bill_depth_mm,\n    t2.flipper_length_mm AS flipper_length_mm,\n    t2.body_mass_g AS body_mass_g,\n    t2.sex AS sex\n  FROM main._ibis_examples_penguins_mqqdnfaydfbevoowdsf7djbsom AS t2\n), t1 AS (\n  SELECT\n    t0.species AS species,\n    t0.island AS island,\n    t0.bill_length_mm AS bill_length_mm,\n    t0.bill_depth_mm AS bill_depth_mm,\n    t0.flipper_length_mm AS flipper_length_mm,\n    t0.body_mass_g AS body_mass_g,\n    t0.sex AS sex,\n    (\n      t0.bill_length_mm - AVG(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_length_mm_zscore,\n    (\n      t0.bill_depth_mm - AVG(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.bill_depth_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS bill_depth_mm_zscore,\n    (\n      t0.flipper_length_mm - AVG(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.flipper_length_mm) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS flipper_length_mm_zscore,\n    (\n      t0.body_mass_g - AVG(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING)\n    ) / STDDEV_SAMP(t0.body_mass_g) OVER (PARTITION BY t0.species ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS body_mass_g_zscore\n  FROM t0\n)\nSELECT\n  t1.species,\n  t1.island,\n  t1.bill_length_mm,\n  t1.bill_depth_mm,\n  t1.flipper_length_mm,\n  t1.body_mass_g,\n  t1.sex,\n  t1.bill_length_mm_zscore,\n  t1.bill_depth_mm_zscore,\n  t1.flipper_length_mm_zscore,\n  t1.body_mass_g_zscore\nFROM t1\nWHERE\n  ABS(t1.bill_length_mm_zscore) &gt; CAST(2 AS TINYINT)\n  AND ABS(t1.bill_depth_mm_zscore) &gt; CAST(2 AS TINYINT)\n\n\nGood thing you didn’t have to write that by hand!"
  },
  {
    "objectID": "posts/selectors/index.html#summary",
    "href": "posts/selectors/index.html#summary",
    "title": "Maximizing productivity with selectors",
    "section": "Summary",
    "text": "Summary\nThis blog post illustrates the ability to apply computations to many columns at once and the power of ibis as a composable, expressive library for analytics.\n\nGet involved!\nReport issues!"
  },
  {
    "objectID": "posts/snowflake-io/index.html",
    "href": "posts/snowflake-io/index.html",
    "title": "Icy IO: loading local files with Snowflake",
    "section": "",
    "text": "It can be challenging to load local files into Snowflake from Python.\nHere’s how to load a CSV file into Snowflake without Ibis.\n1CREATE TEMP STAGE load_csv_stage;\n\n2CREATE TEMP FILE FORMAT load_csv_format\nTYPE = CSV PARSE_HEADER = TRUE;\n\n3PUT 'file:///path/to/my.csv' @load_csv_stage;\n\n4CREATE TEMP TABLE my_table\n5USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE(\n6        INFER_SCHEMA(\n            LOCATION =&gt; '@load_csv_stage',\n            FILE_FORMAT =&gt; 'load_csv_format'\n        )\n    )\n);\n\n7COPY INTO my_table\nFROM @load_csv_stage\nFILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1);\n\n1\n\nCreates a temporary stage in Snowflake. Stages are locations in Snowflake that hold files. They can be used to store raw files to load into tables. TEMP stages are only accessible to the current session and will be dropped when the session ends.\n\n2\n\nA file format is a set of instructions for how to interpret a file. File formats are where you specify parsing and some loading options for your files.\n\n3\n\nPUT copies a file or glob pattern matching one or more files to a stage.\n\n4\n\nCreates a temporary table with schema inferred using Snowflake’s INFER_SCHEMA table function.\n\n5\n\nUSING TEMPLATE is a Snowflake-specific syntax that allows you to specify a set of column definitions computed from staged files.\n\n6\n\nINFER_SCHEMA is a powerful feature of Snowflake that allows you to load files without having to compute the schema in client code.\n\n7\n\nCOPY INTO loads the staged data into the created temporary table.\n\n\nSnowflake provides the full set of primitives required to achieve this, but composing them together can be challenging. Some users struggle to remember the sequence of steps."
  },
  {
    "objectID": "posts/snowflake-io/index.html#loading-files-without-ibis",
    "href": "posts/snowflake-io/index.html#loading-files-without-ibis",
    "title": "Icy IO: loading local files with Snowflake",
    "section": "",
    "text": "It can be challenging to load local files into Snowflake from Python.\nHere’s how to load a CSV file into Snowflake without Ibis.\n1CREATE TEMP STAGE load_csv_stage;\n\n2CREATE TEMP FILE FORMAT load_csv_format\nTYPE = CSV PARSE_HEADER = TRUE;\n\n3PUT 'file:///path/to/my.csv' @load_csv_stage;\n\n4CREATE TEMP TABLE my_table\n5USING TEMPLATE (\n    SELECT ARRAY_AGG(OBJECT_CONSTRUCT(*))\n    FROM TABLE(\n6        INFER_SCHEMA(\n            LOCATION =&gt; '@load_csv_stage',\n            FILE_FORMAT =&gt; 'load_csv_format'\n        )\n    )\n);\n\n7COPY INTO my_table\nFROM @load_csv_stage\nFILE_FORMAT = (TYPE = CSV SKIP_HEADER = 1);\n\n1\n\nCreates a temporary stage in Snowflake. Stages are locations in Snowflake that hold files. They can be used to store raw files to load into tables. TEMP stages are only accessible to the current session and will be dropped when the session ends.\n\n2\n\nA file format is a set of instructions for how to interpret a file. File formats are where you specify parsing and some loading options for your files.\n\n3\n\nPUT copies a file or glob pattern matching one or more files to a stage.\n\n4\n\nCreates a temporary table with schema inferred using Snowflake’s INFER_SCHEMA table function.\n\n5\n\nUSING TEMPLATE is a Snowflake-specific syntax that allows you to specify a set of column definitions computed from staged files.\n\n6\n\nINFER_SCHEMA is a powerful feature of Snowflake that allows you to load files without having to compute the schema in client code.\n\n7\n\nCOPY INTO loads the staged data into the created temporary table.\n\n\nSnowflake provides the full set of primitives required to achieve this, but composing them together can be challenging. Some users struggle to remember the sequence of steps."
  },
  {
    "objectID": "posts/snowflake-io/index.html#loading-files-with-ibis",
    "href": "posts/snowflake-io/index.html#loading-files-with-ibis",
    "title": "Icy IO: loading local files with Snowflake",
    "section": "Loading files with Ibis",
    "text": "Loading files with Ibis\nLet’s take a look at how ibis turns the above process into a single line of Python.\nFirst, we connect to snowflake:\n\nimport os\nfrom ibis.interactive import *\n\ncon = ibis.connect(os.environ[\"SNOWFLAKE_URL\"])\n\n\nread_csv\nLoading CSV files is now a single line of familiar Python code:\n\ndiamonds = con.read_csv(\"diamonds.csv\")\ndiamonds\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ carat         ┃ cut       ┃ color  ┃ clarity ┃ depth         ┃ table         ┃ price ┃ x             ┃ y             ┃ z             ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ decimal(3, 2) │ string    │ string │ string  │ decimal(3, 1) │ decimal(3, 1) │ int64 │ decimal(4, 2) │ decimal(4, 2) │ decimal(4, 2) │\n├───────────────┼───────────┼────────┼─────────┼───────────────┼───────────────┼───────┼───────────────┼───────────────┼───────────────┤\n│          0.23 │ Ideal     │ E      │ SI2     │          61.5 │          55.0 │   326 │          3.95 │          3.98 │          2.43 │\n│          0.21 │ Premium   │ E      │ SI1     │          59.8 │          61.0 │   326 │          3.89 │          3.84 │          2.31 │\n│          0.23 │ Good      │ E      │ VS1     │          56.9 │          65.0 │   327 │          4.05 │          4.07 │          2.31 │\n│          0.29 │ Premium   │ I      │ VS2     │          62.4 │          58.0 │   334 │          4.20 │          4.23 │          2.63 │\n│          0.31 │ Good      │ J      │ SI2     │          63.3 │          58.0 │   335 │          4.34 │          4.35 │          2.75 │\n│          0.24 │ Very Good │ J      │ VVS2    │          62.8 │          57.0 │   336 │          3.94 │          3.96 │          2.48 │\n│          0.24 │ Very Good │ I      │ VVS1    │          62.3 │          57.0 │   336 │          3.95 │          3.98 │          2.47 │\n│          0.26 │ Very Good │ H      │ SI1     │          61.9 │          55.0 │   337 │          4.07 │          4.11 │          2.53 │\n│          0.22 │ Fair      │ E      │ VS2     │          65.1 │          61.0 │   337 │          3.87 │          3.78 │          2.49 │\n│          0.23 │ Very Good │ H      │ VS1     │          59.4 │          61.0 │   338 │          4.00 │          4.05 │          2.39 │\n│             … │ …         │ …      │ …       │             … │             … │     … │             … │             … │             … │\n└───────────────┴───────────┴────────┴─────────┴───────────────┴───────────────┴───────┴───────────────┴───────────────┴───────────────┘\n\n\n\n\n\nread_parquet\nSimilarly, loading Parquet files is now a single line of code:\n\ndiamonds = con.read_parquet(\"diamonds.parquet\")\ndiamonds\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━┓\n┃ carat   ┃ cut       ┃ color  ┃ clarity ┃ depth   ┃ table   ┃ price ┃ x       ┃ y       ┃ z       ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━╇━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━┩\n│ float64 │ string    │ string │ string  │ float64 │ float64 │ int64 │ float64 │ float64 │ float64 │\n├─────────┼───────────┼────────┼─────────┼─────────┼─────────┼───────┼─────────┼─────────┼─────────┤\n│    0.23 │ Ideal     │ E      │ SI2     │    61.5 │    55.0 │   326 │    3.95 │    3.98 │    2.43 │\n│    0.21 │ Premium   │ E      │ SI1     │    59.8 │    61.0 │   326 │    3.89 │    3.84 │    2.31 │\n│    0.23 │ Good      │ E      │ VS1     │    56.9 │    65.0 │   327 │    4.05 │    4.07 │    2.31 │\n│    0.29 │ Premium   │ I      │ VS2     │    62.4 │    58.0 │   334 │    4.20 │    4.23 │    2.63 │\n│    0.31 │ Good      │ J      │ SI2     │    63.3 │    58.0 │   335 │    4.34 │    4.35 │    2.75 │\n│    0.24 │ Very Good │ J      │ VVS2    │    62.8 │    57.0 │   336 │    3.94 │    3.96 │    2.48 │\n│    0.24 │ Very Good │ I      │ VVS1    │    62.3 │    57.0 │   336 │    3.95 │    3.98 │    2.47 │\n│    0.26 │ Very Good │ H      │ SI1     │    61.9 │    55.0 │   337 │    4.07 │    4.11 │    2.53 │\n│    0.22 │ Fair      │ E      │ VS2     │    65.1 │    61.0 │   337 │    3.87 │    3.78 │    2.49 │\n│    0.23 │ Very Good │ H      │ VS1     │    59.4 │    61.0 │   338 │    4.00 │    4.05 │    2.39 │\n│       … │ …         │ …      │ …       │       … │       … │     … │       … │       … │       … │\n└─────────┴───────────┴────────┴─────────┴─────────┴─────────┴───────┴─────────┴─────────┴─────────┘\n\n\n\n\n\nread_json\nLastly, loading JSON files is now – surprise 🥳 – a single line of code!\nLine delimited JSON is supported:\n\ndiamonds = con.read_json(\"diamonds.ndjson\")\ndiamonds\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ carat         ┃ clarity ┃ color  ┃ cut       ┃ depth         ┃ price ┃ table         ┃ x             ┃ y             ┃ z             ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ decimal(3, 2) │ string  │ string │ string    │ decimal(3, 1) │ int64 │ decimal(3, 1) │ decimal(4, 2) │ decimal(4, 2) │ decimal(4, 2) │\n├───────────────┼─────────┼────────┼───────────┼───────────────┼───────┼───────────────┼───────────────┼───────────────┼───────────────┤\n│          0.23 │ SI2     │ E      │ Ideal     │          61.5 │   326 │          55.0 │          3.95 │          3.98 │          2.43 │\n│          0.21 │ SI1     │ E      │ Premium   │          59.8 │   326 │          61.0 │          3.89 │          3.84 │          2.31 │\n│          0.23 │ VS1     │ E      │ Good      │          56.9 │   327 │          65.0 │          4.05 │          4.07 │          2.31 │\n│          0.29 │ VS2     │ I      │ Premium   │          62.4 │   334 │          58.0 │          4.20 │          4.23 │          2.63 │\n│          0.31 │ SI2     │ J      │ Good      │          63.3 │   335 │          58.0 │          4.34 │          4.35 │          2.75 │\n│          0.24 │ VVS2    │ J      │ Very Good │          62.8 │   336 │          57.0 │          3.94 │          3.96 │          2.48 │\n│          0.24 │ VVS1    │ I      │ Very Good │          62.3 │   336 │          57.0 │          3.95 │          3.98 │          2.47 │\n│          0.26 │ SI1     │ H      │ Very Good │          61.9 │   337 │          55.0 │          4.07 │          4.11 │          2.53 │\n│          0.22 │ VS2     │ E      │ Fair      │          65.1 │   337 │          61.0 │          3.87 │          3.78 │          2.49 │\n│          0.23 │ VS1     │ H      │ Very Good │          59.4 │   338 │          61.0 │          4.00 │          4.05 │          2.39 │\n│             … │ …       │ …      │ …         │             … │     … │             … │             … │             … │             … │\n└───────────────┴─────────┴────────┴───────────┴───────────────┴───────┴───────────────┴───────────────┴───────────────┴───────────────┘\n\n\n\nAs well as strict JSON arrays of objects:\n\ndiamonds = con.read_json(\"diamonds.json\")\ndiamonds\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ carat         ┃ clarity ┃ color  ┃ cut       ┃ depth         ┃ price ┃ table         ┃ x             ┃ y             ┃ z             ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ decimal(3, 2) │ string  │ string │ string    │ decimal(3, 1) │ int64 │ decimal(3, 1) │ decimal(4, 2) │ decimal(4, 2) │ decimal(4, 2) │\n├───────────────┼─────────┼────────┼───────────┼───────────────┼───────┼───────────────┼───────────────┼───────────────┼───────────────┤\n│          0.23 │ SI2     │ E      │ Ideal     │          61.5 │   326 │          55.0 │          3.95 │          3.98 │          2.43 │\n│          0.21 │ SI1     │ E      │ Premium   │          59.8 │   326 │          61.0 │          3.89 │          3.84 │          2.31 │\n│          0.23 │ VS1     │ E      │ Good      │          56.9 │   327 │          65.0 │          4.05 │          4.07 │          2.31 │\n│          0.29 │ VS2     │ I      │ Premium   │          62.4 │   334 │          58.0 │          4.20 │          4.23 │          2.63 │\n│          0.31 │ SI2     │ J      │ Good      │          63.3 │   335 │          58.0 │          4.34 │          4.35 │          2.75 │\n│          0.24 │ VVS2    │ J      │ Very Good │          62.8 │   336 │          57.0 │          3.94 │          3.96 │          2.48 │\n│          0.24 │ VVS1    │ I      │ Very Good │          62.3 │   336 │          57.0 │          3.95 │          3.98 │          2.47 │\n│          0.26 │ SI1     │ H      │ Very Good │          61.9 │   337 │          55.0 │          4.07 │          4.11 │          2.53 │\n│          0.22 │ VS2     │ E      │ Fair      │          65.1 │   337 │          61.0 │          3.87 │          3.78 │          2.49 │\n│          0.23 │ VS1     │ H      │ Very Good │          59.4 │   338 │          61.0 │          4.00 │          4.05 │          2.39 │\n│             … │ …       │ …      │ …         │             … │     … │             … │             … │             … │             … │\n└───────────────┴─────────┴────────┴───────────┴───────────────┴───────┴───────────────┴───────────────┴───────────────┴───────────────┘"
  },
  {
    "objectID": "posts/snowflake-io/index.html#conclusion",
    "href": "posts/snowflake-io/index.html#conclusion",
    "title": "Icy IO: loading local files with Snowflake",
    "section": "Conclusion",
    "text": "Conclusion\nIbis 7.0.0 adds support for read_csv, read_parquet and read_json to the Snowflake backend.\nWe think you’ll enjoy the increase in productivity these new features bring to the Snowflake backend and we’d love to hear your feedback!"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html",
    "href": "posts/v6.1.0-release/index.html",
    "title": "Ibis v6.1.0",
    "section": "",
    "text": "Ibis 6.1.0 is a minor release that includes new features, backend improvements, bug fixes, documentation improvements, and refactors. We are excited to see further adoption of the dataframe interchange protocol enabling visualization and other libraries to be used more easily with Ibis.\nYou can view the full changelog in the release notes.\nIf you’re new to Ibis, see how to install and the getting started tutorial.\nTo follow along with this blog, ensure you’re on 'ibis-framework&gt;=6.1,&lt;7'. First, we'll setup Ibis and fetch some sample data to use.\n\nimport ibis\nimport ibis.selectors as s\n\nibis.__version__\n\n'6.1.0'\n\n\n\n# interactive mode for demo purposes\nibis.options.interactive = True\n\n\nt = ibis.examples.penguins.fetch()\nt = t.mutate(year=t[\"year\"].cast(\"str\"))\nt.limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ string │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ 2007   │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ 2007   │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┘"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#overview",
    "href": "posts/v6.1.0-release/index.html#overview",
    "title": "Ibis v6.1.0",
    "section": "",
    "text": "Ibis 6.1.0 is a minor release that includes new features, backend improvements, bug fixes, documentation improvements, and refactors. We are excited to see further adoption of the dataframe interchange protocol enabling visualization and other libraries to be used more easily with Ibis.\nYou can view the full changelog in the release notes.\nIf you’re new to Ibis, see how to install and the getting started tutorial.\nTo follow along with this blog, ensure you’re on 'ibis-framework&gt;=6.1,&lt;7'. First, we'll setup Ibis and fetch some sample data to use.\n\nimport ibis\nimport ibis.selectors as s\n\nibis.__version__\n\n'6.1.0'\n\n\n\n# interactive mode for demo purposes\nibis.options.interactive = True\n\n\nt = ibis.examples.penguins.fetch()\nt = t.mutate(year=t[\"year\"].cast(\"str\"))\nt.limit(3)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ string │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ 2007   │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ 2007   │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┘"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#ecosystem-integrations",
    "href": "posts/v6.1.0-release/index.html#ecosystem-integrations",
    "title": "Ibis v6.1.0",
    "section": "Ecosystem integrations",
    "text": "Ecosystem integrations\nWith the introduction of __dataframe__ support in v6.0.0 and efficiency improvements in this release, Ibis now works with Altair, Plotly, plotnine, and any other visualization library that implements the protocol. This enables passing Ibis tables directly to visualization libraries without a .to_pandas() or to_pyarrow() call for any of the 15+ backends supported, with data efficiently transferred through Apache Arrow.\n\n\nCode\n1width = 640\nheight = 480\n\n\n\n1\n\nSet the width and height of the plots.\n\n\n\n\n\n1grouped = (\n    t.group_by(\"species\")\n    .aggregate(count=ibis._.count())\n    .order_by(ibis.desc(\"count\"))\n)\n2grouped\n\n\n1\n\nSetup data to plot.\n\n2\n\nDisplay the table.\n\n\n\n\n┏━━━━━━━━━━━┳━━━━━━━┓\n┃ species   ┃ count ┃\n┡━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ int64 │\n├───────────┼───────┤\n│ Adelie    │   152 │\n│ Gentoo    │   124 │\n│ Chinstrap │    68 │\n└───────────┴───────┘\n\n\n\n\nAltairPlotlyplotnine\n\n\npip install altair\n\nimport altair as alt\n\nchart = (\n    alt.Chart(grouped)\n    .mark_bar()\n    .encode(\n        x=\"species\",\n        y=\"count\",\n    )\n    .properties(width=width, height=height)\n)\nchart\n\nExpressionError: Use .count() instead\n\n\nalt.Chart(...)\n\n\n\n\npip install plotly\n\nimport plotly.express as px\n\npx.bar(\n    grouped.to_pandas(),\n    x=\"species\",\n    y=\"count\",\n    width=width,\n    height=height,\n)\n\n                                                \n\n\n\n\npip install plotnine\n\nfrom plotnine import ggplot, aes, geom_bar, theme\n\n(\n    ggplot(\n        grouped,\n        aes(x=\"species\", y=\"count\"),\n    )\n    + geom_bar(stat=\"identity\")\n    + theme(figure_size=(width / 100, height / 100))\n)\n\n\n\n\n&lt;Figure Size: (640 x 480)&gt;\n\n\n\n\n\nA more modular, composable, and scalable way of working with data is taking shape with __dataframe__ and __array__ support in Ibis and increasingly the Python data ecosystem. Let's combine the above with PCA after some preprocessing in Ibis to visualize all numeric columns in 2D.\n\n1import ibis.selectors as s\n\n\n2def transform(t):\n    t = t.mutate(\n        s.across(s.numeric(), {\"zscore\": lambda x: (x - x.mean()) / x.std()})\n    ).dropna()\n    return t\n\n\n3f = transform(t)\n4f\n\n\n1\n\nImport the selectors module.\n\n2\n\nDefine a function to transform the table for code reuse (compute z-scores on numeric columns).\n\n3\n\nApply the function to the table and assign it to a new variable.\n\n4\n\nDisplay the transformed table.\n\n\n\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃ bill_length_mm_zscore ┃ bill_depth_mm_zscore ┃ flipper_length_mm_zscore ┃ body_mass_g_zscore ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ string │ float64               │ float64              │ float64                  │ float64            │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼────────┼───────────────────────┼──────────────────────┼──────────────────────────┼────────────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │             -0.883205 │             0.784300 │                -1.416272 │          -0.563317 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ 2007   │             -0.809939 │             0.126003 │                -1.060696 │          -0.500969 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ 2007   │             -0.663408 │             0.429833 │                -0.420660 │          -1.186793 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │ 2007   │             -1.322799 │             1.088129 │                -0.562890 │          -0.937403 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │ 2007   │             -0.846572 │             1.746426 │                -0.776236 │          -0.688012 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │ 2007   │             -0.919837 │             0.328556 │                -1.416272 │          -0.719186 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │ 2007   │             -0.864888 │             1.240044 │                -0.420660 │           0.590115 │\n│ Adelie  │ Torgersen │           41.1 │          17.6 │               182 │        3200 │ female │ 2007   │             -0.516876 │             0.227280 │                -1.345156 │          -1.249141 │\n│ Adelie  │ Torgersen │           38.6 │          21.2 │               191 │        3800 │ male   │ 2007   │             -0.974787 │             2.050255 │                -0.705121 │          -0.500969 │\n│ Adelie  │ Torgersen │           34.6 │          21.1 │               198 │        4400 │ male   │ 2007   │             -1.707443 │             1.999617 │                -0.207315 │           0.247203 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │ …      │                     … │                    … │                        … │                  … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┴───────────────────────┴──────────────────────┴──────────────────────────┴────────────────────┘\n\n\n\npip install scikit-learn\n\n1import plotly.express as px\nfrom sklearn.decomposition import PCA\n\n2X = f.select(s.contains(\"zscore\"))\n\n3n_components = 3\npca = PCA(n_components=n_components).fit(X)\n\n4t_pca = ibis.memtable(pca.transform(X)).relabel(\n    {\"col0\": \"pc1\", \"col1\": \"pc2\", \"col2\": \"pc3\"}\n)\n\n5f = f.mutate(row_number=ibis.row_number().over()).join(\n    t_pca.mutate(row_number=ibis.row_number().over()), \"row_number\"\n)\n\n6px.scatter_3d(\n    f.to_pandas(),\n    x=\"pc1\",\n    y=\"pc2\",\n    z=\"pc3\",\n    color=\"species\",\n)\n\n\n1\n\nImport data science libraries\n\n2\n\nSelect “features” (numeric columns) as X\n\n3\n\nCompute PCA\n\n4\n\nCreate a table from the PCA results\n\n5\n\nJoin the PCA results to the original table\n\n6\n\nPlot the results"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#backends",
    "href": "posts/v6.1.0-release/index.html#backends",
    "title": "Ibis v6.1.0",
    "section": "Backends",
    "text": "Backends\nNumerous backends received improvements. See the release notes for more details.\n\nDataFusionBigQueryPySparkTrinoSQLitepandas\n\n\nThe DataFusion backend (and a few others) received several improvements from community member @mesejo with memtables and many new operations now supported. Some highlights include:\n\nurl = ibis.literal(\"https://ibis-project.org/concepts/why_ibis\")\ncon = ibis.datafusion.connect()\n\ncon.execute(url.host())\n\n'ibis-project.org'\n\n\n\ncon.execute(url.path())\n\n'/concepts/why_ibis'\n\n\n\ncon.execute(ibis.literal(\"aaabbbaaa\").re_search(\"bbb\"))\n\nTrue\n\n\n\ncon.execute(ibis.literal(5.56).ln())\n\n1.715598108262491\n\n\n\ncon.execute(ibis.literal(5.56).log10())\n\n0.7450747915820575\n\n\n\ncon.execute(ibis.literal(5.56).radians())\n\n0.09704030641088471\n\n\n\n\nSome remaining gaps in CREATE TABLE DDL options for BigQuery have been filled in, including the ability to pass in overwrite=True for table creation.\n\n\nThe PySpark backend now supports reading/writing Delta Lake tables. Your PySpark session must be configured to use the Delta Lake package and you must have the delta package installed in your environment.\nt = ibis.read_delta(\"/path/to/delta\")\n\n...\n\nt.to_delta(\"/path/to/delta\", mode=\"overwrite\")\n\n\nThe .sql API is now supported in Trino, enabling you to chain Ibis and SQL together.\n\n\nScalar Python UDFs are now supported in SQLite.\nAdditionally, URL parsing has been added:\n\ncon = ibis.sqlite.connect()\n\ncon.execute(url.host())\n\n'ibis-project.org'\n\n\n\ncon.execute(url.path())\n\n'/concepts/why_ibis'\n\n\n\n\nURL parsing support was added.\n\ncon = ibis.pandas.connect()\n\ncon.execute(url.host())\n\n'ibis-project.org'\n\n\n\ncon.execute(url.path())\n\n'/concepts/why_ibis'"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#functionality",
    "href": "posts/v6.1.0-release/index.html#functionality",
    "title": "Ibis v6.1.0",
    "section": "Functionality",
    "text": "Functionality\nVarious new features and were added.\n\n.nunique() supported on tables\nYou can now call .nunique() on tables to get the number of unique rows.\n\n# how many unique rows are there? equivalent to `.count()` in this case\nt.nunique()\n\n\n\n\n\n344\n\n\n\n\n# how many unique species/island/year combinations are there?\nt.select(\"species\", \"island\", \"year\").nunique()\n\n\n\n\n\n15\n\n\n\n\n\nto_sql returns a str type\nThe ibis.expr.sql.SQLString type resulting from to_sql is now a proper str subclass, enabling use without casting to str first.\n\ntype(ibis.to_sql(t))\n\nibis.expr.sql.SQLString\n\n\n\nissubclass(type(ibis.to_sql(t)), str)\n\nTrue\n\n\n\n\nAllow mixing literals and columns in ibis.array\nNote that arrays must still be of a single type.\n\nibis.array([t[\"species\"], \"hello\"])\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayColumn()       ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;       │\n├─────────────────────┤\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ ['Adelie', 'hello'] │\n│ …                   │\n└─────────────────────┘\n\n\n\n\nibis.array([t[\"flipper_length_mm\"], 42])\n\n┏━━━━━━━━━━━━━━━┓\n┃ ArrayColumn() ┃\n┡━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt;  │\n├───────────────┤\n│ [181, 42]     │\n│ [186, 42]     │\n│ [195, 42]     │\n│ [None, 42]    │\n│ [193, 42]     │\n│ [190, 42]     │\n│ [181, 42]     │\n│ [195, 42]     │\n│ [193, 42]     │\n│ [190, 42]     │\n│ …             │\n└───────────────┘\n\n\n\n\n\nArray concat and repeat methods\nYou can still use + or * in typical Python fashion, with new and more explicit concat and repeat methods added in this release.\n\na = ibis.array([1, 2, 3])\nb = ibis.array([4, 5])\n\nc = a.concat(b)\nc\n\n\n\n\n\n[1, 2, 3, 4, 5]\n\n\n\n\nc = a + b\nc\n\n\n\n\n\n[1, 2, 3, 4, 5]\n\n\n\n\nb.repeat(2)\n\n\n\n\n\n[4, 5, 4, 5]\n\n\n\n\nb * 2\n\n\n\n\n\n[4, 5, 4, 5]\n\n\n\n\n\nSupport boolean literals in the join API\nThis allows for joins with boolean predicates.\n\nt.join(t, True)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃ species_right ┃ island_right ┃ bill_length_mm_right ┃ bill_depth_mm_right ┃ flipper_length_mm_right ┃ body_mass_g_right ┃ sex_right ┃ year_right ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ string │ string        │ string       │ float64              │ float64             │ int64                   │ int64             │ string    │ string     │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼────────┼───────────────┼──────────────┼──────────────────────┼─────────────────────┼─────────────────────────┼───────────────────┼───────────┼────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 39.1 │                18.7 │                     181 │              3750 │ male      │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 39.5 │                17.4 │                     186 │              3800 │ female    │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 40.3 │                18.0 │                     195 │              3250 │ female    │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                  nan │                 nan │                    NULL │              NULL │ NULL      │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 36.7 │                19.3 │                     193 │              3450 │ female    │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 39.3 │                20.6 │                     190 │              3650 │ male      │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 38.9 │                17.8 │                     181 │              3625 │ female    │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 39.2 │                19.6 │                     195 │              4675 │ male      │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 34.1 │                18.1 │                     193 │              3475 │ NULL      │ 2007       │\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ Adelie        │ Torgersen    │                 42.0 │                20.2 │                     190 │              4250 │ NULL      │ 2007       │\n│ …       │ …         │              … │             … │                 … │           … │ …      │ …      │ …             │ …            │                    … │                   … │                       … │                 … │ …         │ …          │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┴───────────────┴──────────────┴──────────────────────┴─────────────────────┴─────────────────────────┴───────────────────┴───────────┴────────────┘\n\n\n\n\nt.join(t, False)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃ species_right ┃ island_right ┃ bill_length_mm_right ┃ bill_depth_mm_right ┃ flipper_length_mm_right ┃ body_mass_g_right ┃ sex_right ┃ year_right ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ int64             │ int64       │ string │ string │ string        │ string       │ float64              │ float64             │ int64                   │ int64             │ string    │ string     │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┴───────────────┴──────────────┴──────────────────────┴─────────────────────┴─────────────────────────┴───────────────────┴───────────┴────────────┘\n\n\n\n\nt.join(t, False, how=\"outer\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year   ┃ species_right ┃ island_right ┃ bill_length_mm_right ┃ bill_depth_mm_right ┃ flipper_length_mm_right ┃ body_mass_g_right ┃ sex_right ┃ year_right ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ string │ string        │ string       │ float64              │ float64             │ int64                   │ int64             │ string    │ string     │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼────────┼───────────────┼──────────────┼──────────────────────┼─────────────────────┼─────────────────────────┼───────────────────┼───────────┼────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │ 2007   │ NULL          │ NULL         │                  nan │                 nan │                    NULL │              NULL │ NULL      │ NULL       │\n│ …       │ …         │              … │             … │                 … │           … │ …      │ …      │ …             │ …            │                    … │                   … │                       … │                 … │ …         │ …          │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴────────┴───────────────┴──────────────┴──────────────────────┴─────────────────────┴─────────────────────────┴───────────────────┴───────────┴────────────┘"
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#refactors",
    "href": "posts/v6.1.0-release/index.html#refactors",
    "title": "Ibis v6.1.0",
    "section": "Refactors",
    "text": "Refactors\nSeveral internal refactors that shouldn't affect normal usage were made. See the release notes for more details."
  },
  {
    "objectID": "posts/v6.1.0-release/index.html#wrapping-up",
    "href": "posts/v6.1.0-release/index.html#wrapping-up",
    "title": "Ibis v6.1.0",
    "section": "Wrapping up",
    "text": "Wrapping up\nIbis v6.1.0 brings exciting enhancements to the library that enable broader ecosystem adoption of Python standards.\nAs always, try Ibis by installing and getting started.\nIf you run into any issues or find support is lacking for your backend, open an issue or discussion and let us know!"
  },
  {
    "objectID": "posts/ci-analysis/index.html",
    "href": "posts/ci-analysis/index.html",
    "title": "Analysis of Ibis’s CI performance",
    "section": "",
    "text": "This notebook takes you through an analysis of Ibis’s CI data using ibis on top of Google BigQuery.\n\nFirst, we load some data and poke around at it to see what’s what.\nSecond, we figure out some useful things to calculate based on our poking.\nThird, we’ll visualize the results of calculations to showcase what changed and how."
  },
  {
    "objectID": "posts/ci-analysis/index.html#summary",
    "href": "posts/ci-analysis/index.html#summary",
    "title": "Analysis of Ibis’s CI performance",
    "section": "",
    "text": "This notebook takes you through an analysis of Ibis’s CI data using ibis on top of Google BigQuery.\n\nFirst, we load some data and poke around at it to see what’s what.\nSecond, we figure out some useful things to calculate based on our poking.\nThird, we’ll visualize the results of calculations to showcase what changed and how."
  },
  {
    "objectID": "posts/ci-analysis/index.html#imports",
    "href": "posts/ci-analysis/index.html#imports",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Imports",
    "text": "Imports\nLet’s start out by importing ibis and turning on interactive mode.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True"
  },
  {
    "objectID": "posts/ci-analysis/index.html#connect-to-bigquery",
    "href": "posts/ci-analysis/index.html#connect-to-bigquery",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Connect to BigQuery",
    "text": "Connect to BigQuery\nWe connect to BigQuery using the ibis.connect API, which accepts a URL string indicating the backend and various bit of information needed to connect to the backend. Here we’re using BigQuery, so we need the project id (ibis-gbq) and the dataset id (workflows).\nDatasets are analogous to schemas in other systems.\n\nurl = \"bigquery://ibis-gbq/workflows\"\ncon = ibis.connect(url)\n\nLet’s see what tables are available.\n\ncon.list_tables()\n\n['analysis', 'jobs', 'workflows']"
  },
  {
    "objectID": "posts/ci-analysis/index.html#analysis",
    "href": "posts/ci-analysis/index.html#analysis",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Analysis",
    "text": "Analysis\nHere we’ve got our first bit of interesting information: the jobs and workflows tables.\n\nTerminology\nBefore we jump in, it helps to lay down some terminology.\n\nA workflow corresponds to an individual GitHub Actions YAML file in a GitHub repository under the .github/workflows directory.\nA job is a named set of steps to run inside a workflow file.\n\n\n\nWhat’s in the workflows table?\nEach row in the workflows table corresponds to a workflow run.\n\nA workflow run is an instance of a workflow that was triggered by some entity: a GitHub user, bot, or other entity. Each row of the workflows table is a workflow run.\n\n\n\nWhat’s in the jobs table?\nSimilarly, each row in the jobs table is a job run. That is, for a given workflow run there are a set of jobs run with it.\n\nA job run is an instance of a job in a workflow. It is associated with a single workflow run."
  },
  {
    "objectID": "posts/ci-analysis/index.html#rationale",
    "href": "posts/ci-analysis/index.html#rationale",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Rationale",
    "text": "Rationale\nThe goal of this analysis is to try to understand ibis’s CI performance, and whether the amount of time we spent waiting on CI has decreased, stayed the same or increased. Ideally, we can understand the pieces that contribute to the change or lack thereof.\n\nMetrics\nTo that end there are a few interesting metrics to look at:\n\njob run duration: this is the amount of time it takes for a given job to complete\nworkflow run duration: the amount of time it takes for all job runs in a workflow run to complete.\nqueueing duration: the amount time time spent waiting for the first job run to commence.\n\n\n\nMitigating Factors\n\nAround October 2021, we changed our CI infrastructure to use Poetry instead of Conda. The goal there was to see if we could cache dependencies using the lock file generated by poetry. We should see whether that had any effect.\nAt the end of November 2022, we switch to the Team Plan (a paid GitHub plan) for the Ibis organzation. This tripled the amount of job runs that could execute in parallel. We should see if that helped anything.\n\nAlright, let’s jump into some data!\n\njobs = con.tables.jobs[_.started_at &lt; \"2023-01-09\"]\njobs\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ url                                                                   ┃ steps                                                                            ┃ status    ┃ started_at                ┃ runner_group_name ┃ run_attempt ┃ name                      ┃ labels        ┃ node_id                      ┃ id        ┃ runner_id ┃ run_url                                                               ┃ run_id    ┃ check_run_url                                                       ┃ html_url                                                                   ┃ runner_name ┃ runner_group_id ┃ head_sha                                 ┃ conclusion ┃ completed_at              ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                                                                │ array&lt;struct&lt;status: string, conclusion: string, started_at: timestamp('UTC'), … │ string    │ timestamp('UTC')          │ string            │ int64       │ string                    │ array&lt;string&gt; │ string                       │ int64     │ int64     │ string                                                                │ int64     │ string                                                              │ string                                                                     │ string      │ int64           │ string                                   │ string     │ timestamp('UTC')          │\n├───────────────────────────────────────────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┼───────────┼───────────────────────────┼───────────────────┼─────────────┼───────────────────────────┼───────────────┼──────────────────────────────┼───────────┼───────────┼───────────────────────────────────────────────────────────────────────┼───────────┼─────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────┼─────────────┼─────────────────┼──────────────────────────────────────────┼────────────┼───────────────────────────┤\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947152589 │ [{...}, {...}, ... +12]                                                          │ completed │ 2020-08-04 23:54:37+00:00 │ NULL              │           1 │ Test Conda                │ []            │ MDg6Q2hlY2tSdW45NDcxNTI1ODk= │ 947152589 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195478382 │ 195478382 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947152589 │ https://github.com/ibis-project/ibis/runs/947152589?check_suite_focus=true │ NULL        │            NULL │ 29c148e9679a53c7bb99755347e336d6c1f4d8c8 │ success    │ 2020-08-04 23:56:50+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947152601 │ [{...}, {...}, ... +5]                                                           │ completed │ 2020-08-04 23:54:37+00:00 │ NULL              │           1 │ Test setup miniconda task │ []            │ MDg6Q2hlY2tSdW45NDcxNTI2MDE= │ 947152601 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195478382 │ 195478382 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947152601 │ https://github.com/ibis-project/ibis/runs/947152601?check_suite_focus=true │ NULL        │            NULL │ 29c148e9679a53c7bb99755347e336d6c1f4d8c8 │ failure    │ 2020-08-04 23:56:59+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947147571 │ [{...}, {...}, ... +5]                                                           │ completed │ 2020-08-04 23:51:54+00:00 │ NULL              │           1 │ Test setup miniconda task │ []            │ MDg6Q2hlY2tSdW45NDcxNDc1NzE= │ 947147571 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195476517 │ 195476517 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947147571 │ https://github.com/ibis-project/ibis/runs/947147571?check_suite_focus=true │ NULL        │            NULL │ 501d9cc1f2f8d016b1e4fe44ebfc3e1facaa3c91 │ failure    │ 2020-08-04 23:54:48+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947147586 │ [{...}, {...}, ... +12]                                                          │ completed │ 2020-08-04 23:51:53+00:00 │ NULL              │           1 │ Test Conda                │ []            │ MDg6Q2hlY2tSdW45NDcxNDc1ODY= │ 947147586 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195476517 │ 195476517 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947147586 │ https://github.com/ibis-project/ibis/runs/947147586?check_suite_focus=true │ NULL        │            NULL │ 501d9cc1f2f8d016b1e4fe44ebfc3e1facaa3c91 │ failure    │ 2020-08-04 23:54:15+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947144553 │ [{...}, {...}, ... +11]                                                          │ completed │ 2020-08-04 23:50:19+00:00 │ NULL              │           1 │ Test Conda                │ []            │ MDg6Q2hlY2tSdW45NDcxNDQ1NTM= │ 947144553 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195475525 │ 195475525 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947144553 │ https://github.com/ibis-project/ibis/runs/947144553?check_suite_focus=true │ NULL        │            NULL │ 963821c370fb8f10f915e4b29e1c78f053c6e7b0 │ failure    │ 2020-08-04 23:52:45+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947144585 │ [{...}, {...}, ... +5]                                                           │ completed │ 2020-08-04 23:50:20+00:00 │ NULL              │           1 │ Test setup miniconda task │ []            │ MDg6Q2hlY2tSdW45NDcxNDQ1ODU= │ 947144585 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195475525 │ 195475525 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947144585 │ https://github.com/ibis-project/ibis/runs/947144585?check_suite_focus=true │ NULL        │            NULL │ 963821c370fb8f10f915e4b29e1c78f053c6e7b0 │ failure    │ 2020-08-04 23:52:53+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947123154 │ [{...}, {...}, ... +5]                                                           │ completed │ 2020-08-04 23:39:58+00:00 │ NULL              │           1 │ Test setup miniconda task │ []            │ MDg6Q2hlY2tSdW45NDcxMjMxNTQ= │ 947123154 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195468677 │ 195468677 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947123154 │ https://github.com/ibis-project/ibis/runs/947123154?check_suite_focus=true │ NULL        │            NULL │ fcab3265e8afd70dddc518601b33661d15e19f62 │ failure    │ 2020-08-04 23:42:36+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947123167 │ [{...}, {...}, ... +9]                                                           │ completed │ 2020-08-04 23:39:57+00:00 │ NULL              │           1 │ Test Conda                │ []            │ MDg6Q2hlY2tSdW45NDcxMjMxNjc= │ 947123167 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195468677 │ 195468677 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947123167 │ https://github.com/ibis-project/ibis/runs/947123167?check_suite_focus=true │ NULL        │            NULL │ fcab3265e8afd70dddc518601b33661d15e19f62 │ failure    │ 2020-08-04 23:42:15+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947111435 │ [{...}, {...}, ... +9]                                                           │ completed │ 2020-08-04 23:34:19+00:00 │ NULL              │           1 │ Test Conda                │ []            │ MDg6Q2hlY2tSdW45NDcxMTE0MzU= │ 947111435 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195465343 │ 195465343 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947111435 │ https://github.com/ibis-project/ibis/runs/947111435?check_suite_focus=true │ NULL        │            NULL │ 0b496d97d80f22c5a1a7db27cffd3f71a0d28941 │ failure    │ 2020-08-04 23:36:26+00:00 │\n│ https://api.github.com/repos/ibis-project/ibis/actions/jobs/947111464 │ [{...}, {...}, ... +5]                                                           │ completed │ 2020-08-04 23:34:19+00:00 │ NULL              │           1 │ Test setup miniconda task │ []            │ MDg6Q2hlY2tSdW45NDcxMTE0NjQ= │ 947111464 │      NULL │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195465343 │ 195465343 │ https://api.github.com/repos/ibis-project/ibis/check-runs/947111464 │ https://github.com/ibis-project/ibis/runs/947111464?check_suite_focus=true │ NULL        │            NULL │ 0b496d97d80f22c5a1a7db27cffd3f71a0d28941 │ failure    │ 2020-08-04 23:36:28+00:00 │\n│ …                                                                     │ …                                                                                │ …         │ …                         │ …                 │           … │ …                         │ …             │ …                            │         … │         … │ …                                                                     │         … │ …                                                                   │ …                                                                          │ …           │               … │ …                                        │ …          │ …                         │\n└───────────────────────────────────────────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴───────────┴───────────────────────────┴───────────────────┴─────────────┴───────────────────────────┴───────────────┴──────────────────────────────┴───────────┴───────────┴───────────────────────────────────────────────────────────────────────┴───────────┴─────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────┴─────────────┴─────────────────┴──────────────────────────────────────────┴────────────┴───────────────────────────┘\n\n\n\nThese first few columns in the jobs table aren’t that interesting so we should look at what else is there\n\njobs.columns\n\n['url',\n 'steps',\n 'status',\n 'started_at',\n 'runner_group_name',\n 'run_attempt',\n 'name',\n 'labels',\n 'node_id',\n 'id',\n 'runner_id',\n 'run_url',\n 'run_id',\n 'check_run_url',\n 'html_url',\n 'runner_name',\n 'runner_group_id',\n 'head_sha',\n 'conclusion',\n 'completed_at']\n\n\nA bunch of these aren’t that useful for our purposes. However, run_id, started_at, completed_at are useful for us. The GitHub documentation for job information provides useful detail about the meaning of these fields.\n\nrun_id: the workflow run associated with this job run\nstarted_at: when the job started\ncompleted_at: when the job completed\n\nWhat we’re interested in to a first degree is the job duration, so let’s compute that.\nWe also need to compute when the last job for a given run_id started and when it completed. We’ll use the former to compute the queueing duration, and the latter to compute the total time it took for a given workflow run to complete.\n\nrun_id_win = ibis.window(group_by=_.run_id)\njobs = jobs.select(\n    _.run_id,\n    job_duration=_.completed_at.delta(_.started_at, \"microsecond\"),\n    last_job_started_at=_.started_at.max().over(run_id_win),\n    last_job_completed_at=_.completed_at.max().over(run_id_win),\n)\njobs\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ run_id    ┃ job_duration ┃ last_job_started_at       ┃ last_job_completed_at     ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64     │ int64        │ timestamp('UTC')          │ timestamp('UTC')          │\n├───────────┼──────────────┼───────────────────────────┼───────────────────────────┤\n│ 236503770 │   1109000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │    636000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │    594000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │    459000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │    430000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │   3268000000 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 236503770 │            0 │ 2020-09-02 19:06:50+00:00 │ 2020-09-02 19:06:50+00:00 │\n│ 243835537 │    565000000 │ 2020-09-08 02:45:31+00:00 │ 2020-09-08 02:45:31+00:00 │\n│ 243835537 │    581000000 │ 2020-09-08 02:45:31+00:00 │ 2020-09-08 02:45:31+00:00 │\n│ 243835537 │    644000000 │ 2020-09-08 02:45:31+00:00 │ 2020-09-08 02:45:31+00:00 │\n│         … │            … │ …                         │ …                         │\n└───────────┴──────────────┴───────────────────────────┴───────────────────────────┘\n\n\n\nLet’s take a look at workflows\n\nworkflows = con.tables.workflows\nworkflows\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ workflow_url                                                             ┃ workflow_id ┃ triggering_actor ┃ run_number ┃ run_attempt ┃ updated_at                ┃ cancel_url                                                                   ┃ rerun_url                                                                   ┃ check_suite_node_id              ┃ pull_requests                                                                    ┃ id        ┃ node_id                          ┃ status    ┃ repository                                                                                                                                                    ┃ jobs_url                                                                   ┃ previous_attempt_url ┃ artifacts_url                                                                   ┃ html_url                                                    ┃ head_sha                                 ┃ head_repository                                                                                                                                                   ┃ run_started_at            ┃ head_branch   ┃ url                                                                   ┃ event        ┃ name   ┃ actor ┃ created_at                ┃ check_suite_url                                                        ┃ check_suite_id ┃ conclusion ┃ head_commit                                                                                                                           ┃ logs_url                                                                   ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                                                                   │ int64       │ struct&lt;subscrip… │ int64      │ int64       │ timestamp('UTC')          │ string                                                                       │ string                                                                      │ string                           │ array&lt;struct&lt;number: int64, url: string, id: int64, head: struct&lt;sha: string, r… │ int64     │ string                           │ string    │ struct&lt;trees_url: string, teams_url: string, statuses_url: string, subscribers_…                                                                              │ string                                                                     │ string               │ string                                                                          │ string                                                      │ string                                   │ struct&lt;trees_url: string, teams_url: string, statuses_url: string, subscribers_…                                                                                  │ timestamp('UTC')          │ string        │ string                                                                │ string       │ string │ stru… │ timestamp('UTC')          │ string                                                                 │ int64          │ string     │ struct&lt;tree_id: string, timestamp: timestamp('UTC'), message: string, id: strin…                                                      │ string                                                                     │\n├──────────────────────────────────────────────────────────────────────────┼─────────────┼──────────────────┼────────────┼─────────────┼───────────────────────────┼──────────────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────────────────────┼──────────────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┼───────────┼──────────────────────────────────┼───────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────┼──────────────────────┼─────────────────────────────────────────────────────────────────────────────────┼─────────────────────────────────────────────────────────────┼──────────────────────────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼───────────────────────────┼───────────────┼───────────────────────────────────────────────────────────────────────┼──────────────┼────────┼───────┼───────────────────────────┼────────────────────────────────────────────────────────────────────────┼────────────────┼────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┼────────────────────────────────────────────────────────────────────────────┤\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         28 │           1 │ 2020-08-05 20:01:17+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191/rerun │ MDEwOkNoZWNrU3VpdGUxMDEzMDI5NDkw │ []                                                                               │ 196667191 │ MDExOldvcmtmbG93UnVuMTk2NjY3MTkx │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196667191 │ 08855609f1e9ebdeb6197887cf64ecda015d99a8 │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 19:01:08+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191 │ pull_request │ Main   │ NULL  │ 2020-08-05 19:01:08+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1013029490 │     1013029490 │ success    │ {'tree_id': 'c4277198178ae73c3d9611af464ee75eadbceedc', 'timestamp': datetime.datetime(2020, 8, 5, 19, 0, 57, tzinfo=&lt;UTC&gt;), ... +4}  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196667191/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         27 │           1 │ 2020-08-05 18:59:37+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyODM0OTQ3 │ []                                                                               │ 196617109 │ MDExOldvcmtmbG93UnVuMTk2NjE3MTA5 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196617109 │ 7472797f3e4da39d18e53c09566dba5e373094b0 │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 18:12:34+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109 │ pull_request │ Main   │ NULL  │ 2020-08-05 18:12:34+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012834947 │     1012834947 │ success    │ {'tree_id': '451472455efc6f20b81f6e1762ac712ec75e77b3', 'timestamp': datetime.datetime(2020, 8, 5, 18, 12, 25, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196617109/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         26 │           1 │ 2020-08-05 18:05:32+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyNzc0OTQ4 │ []                                                                               │ 196598751 │ MDExOldvcmtmbG93UnVuMTk2NTk4NzUx │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196598751 │ 7452ea048908149a672f681ffd94e3fd0953ab2c │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 17:58:48+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751 │ pull_request │ Main   │ NULL  │ 2020-08-05 17:58:48+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012774948 │     1012774948 │ failure    │ {'tree_id': 'e753c3d693a15eeb99a0d2bd074414ab90dbc85d', 'timestamp': datetime.datetime(2020, 8, 5, 17, 58, 39, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196598751/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         25 │           1 │ 2020-08-05 18:23:52+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyNzY2NTk2 │ []                                                                               │ 196596932 │ MDExOldvcmtmbG93UnVuMTk2NTk2OTMy │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196596932 │ 59daccd16de041b14fa48b9ba53e8aac6495a578 │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 17:56:34+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932 │ pull_request │ Main   │ NULL  │ 2020-08-05 17:56:34+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012766596 │     1012766596 │ failure    │ {'tree_id': '342312b7ce508d4d7c91259dd7919cee06508f19', 'timestamp': datetime.datetime(2020, 8, 5, 17, 56, 25, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196596932/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         24 │           1 │ 2020-08-05 17:54:55+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyNzU5MjQ1 │ []                                                                               │ 196595357 │ MDExOldvcmtmbG93UnVuMTk2NTk1MzU3 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196595357 │ 1d4f2db372834da7fb33b53c60b59d3f3e40cf7c │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 17:54:35+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357 │ pull_request │ Main   │ NULL  │ 2020-08-05 17:54:35+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012759245 │     1012759245 │ failure    │ {'tree_id': '82f0fdad2916ec10d33f5b6c589dbfe8e4decccd', 'timestamp': datetime.datetime(2020, 8, 5, 17, 54, 26, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196595357/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         23 │           1 │ 2020-08-05 17:17:46+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyNDExMDA0 │ []                                                                               │ 196505866 │ MDExOldvcmtmbG93UnVuMTk2NTA1ODY2 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196505866 │ c36dd6504d86d1994fb36d6a84fb3f302a57642c │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 16:28:57+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866 │ pull_request │ Main   │ NULL  │ 2020-08-05 16:28:57+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012411004 │     1012411004 │ failure    │ {'tree_id': 'c3bf70f2809e48fc5c1dd5b0e7e2321bae4879ea', 'timestamp': datetime.datetime(2020, 8, 5, 16, 28, 48, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196505866/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         22 │           1 │ 2020-08-05 16:47:06+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206/rerun │ MDEwOkNoZWNrU3VpdGUxMDEyMzQ4MjUz │ []                                                                               │ 196491206 │ MDExOldvcmtmbG93UnVuMTk2NDkxMjA2 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196491206 │ 5ffc4dcb3857eae64b5b36f46b378149c0bb2d74 │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44}     │ 2020-08-05 16:15:44+00:00 │ actions-lint  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206 │ pull_request │ Main   │ NULL  │ 2020-08-05 16:15:44+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1012348253 │     1012348253 │ failure    │ {'tree_id': '3781a799e538f99a2e05399fea4237e5d06d5df2', 'timestamp': datetime.datetime(2020, 8, 5, 16, 12, 4, tzinfo=&lt;UTC&gt;), ... +4}  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196491206/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         21 │           1 │ 2020-08-05 15:10:13+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166/rerun │ MDEwOkNoZWNrU3VpdGUxMDExODM5NTE5 │ []                                                                               │ 196367166 │ MDExOldvcmtmbG93UnVuMTk2MzY3MTY2 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196367166 │ e88d621425c939857b3b9391794c5ddfd7615981 │ {'trees_url': 'https://api.github.com/repos/datapythonista/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/datapythonista/ibis/teams', ... +44} │ 2020-08-05 14:36:39+00:00 │ conda-windows │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166 │ pull_request │ Main   │ NULL  │ 2020-08-05 14:36:39+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1011839519 │     1011839519 │ failure    │ {'tree_id': 'e093ce6398be8a2fb5331d944d07ef0c5518cc84', 'timestamp': datetime.datetime(2020, 8, 5, 14, 36, 30, tzinfo=&lt;UTC&gt;), ... +4} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196367166/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         20 │           1 │ 2020-08-05 14:32:14+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939/rerun │ MDEwOkNoZWNrU3VpdGUxMDExNjUzNDY5 │ []                                                                               │ 196316939 │ MDExOldvcmtmbG93UnVuMTk2MzE2OTM5 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939/artifacts │ https://github.com/ibis-project/ibis/actions/runs/196316939 │ 702446a96a1b9e6b463084f2f09f2f2106fef8d4 │ {'trees_url': 'https://api.github.com/repos/datapythonista/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/datapythonista/ibis/teams', ... +44} │ 2020-08-05 14:01:24+00:00 │ conda-windows │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939 │ pull_request │ Main   │ NULL  │ 2020-08-05 14:01:24+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1011653469 │     1011653469 │ failure    │ {'tree_id': 'cdac62bce1914add5faceafd210f32965aa00fe7', 'timestamp': datetime.datetime(2020, 8, 5, 14, 1, 11, tzinfo=&lt;UTC&gt;), ... +4}  │ https://api.github.com/repos/ibis-project/ibis/actions/runs/196316939/logs │\n│ https://api.github.com/repos/ibis-project/ibis/actions/workflows/2100986 │     2100986 │ NULL             │         19 │           1 │ 2020-08-05 01:20:00+00:00 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439/cancel │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439/rerun │ MDEwOkNoZWNrU3VpdGUxMDA5MDQ3OTk5 │ []                                                                               │ 195537439 │ MDExOldvcmtmbG93UnVuMTk1NTM3NDM5 │ completed │ {'trees_url': 'https://api.github.com/repos/ibis-project/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/ibis-project/ibis/teams', ... +44} │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439/jobs │ NULL                 │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439/artifacts │ https://github.com/ibis-project/ibis/actions/runs/195537439 │ 2ab26f385b87f39b66cf51783d7ab8904fdb4677 │ {'trees_url': 'https://api.github.com/repos/datapythonista/ibis/git/trees{/sha}', 'teams_url': 'https://api.github.com/repos/datapythonista/ibis/teams', ... +44} │ 2020-08-05 00:48:17+00:00 │ conda-windows │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439 │ pull_request │ Main   │ NULL  │ 2020-08-05 00:48:17+00:00 │ https://api.github.com/repos/ibis-project/ibis/check-suites/1009047999 │     1009047999 │ failure    │ {'tree_id': '33ca23ad93f84344f03894d952d7ffeaf8fb5990', 'timestamp': datetime.datetime(2020, 8, 5, 0, 48, 8, tzinfo=&lt;UTC&gt;), ... +4}   │ https://api.github.com/repos/ibis-project/ibis/actions/runs/195537439/logs │\n│ …                                                                        │           … │ …                │          … │           … │ …                         │ …                                                                            │ …                                                                           │ …                                │ …                                                                                │         … │ …                                │ …         │ …                                                                                                                                                             │ …                                                                          │ …                    │ …                                                                               │ …                                                           │ …                                        │ …                                                                                                                                                                 │ …                         │ …             │ …                                                                     │ …            │ …      │ …     │ …                         │ …                                                                      │              … │ …          │ …                                                                                                                                     │ …                                                                          │\n└──────────────────────────────────────────────────────────────────────────┴─────────────┴──────────────────┴────────────┴─────────────┴───────────────────────────┴──────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────┴──────────────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┴───────────┴──────────────────────────────────┴───────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────┴──────────────────────┴─────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────┴──────────────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴───────────────────────────┴───────────────┴───────────────────────────────────────────────────────────────────────┴──────────────┴────────┴───────┴───────────────────────────┴────────────────────────────────────────────────────────────────────────┴────────────────┴────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────┘\n\n\n\nAgain we have a bunch of columns that aren’t so useful to us, so let’s see what else is there.\n\nworkflows.columns\n\n['workflow_url',\n 'workflow_id',\n 'triggering_actor',\n 'run_number',\n 'run_attempt',\n 'updated_at',\n 'cancel_url',\n 'rerun_url',\n 'check_suite_node_id',\n 'pull_requests',\n 'id',\n 'node_id',\n 'status',\n 'repository',\n 'jobs_url',\n 'previous_attempt_url',\n 'artifacts_url',\n 'html_url',\n 'head_sha',\n 'head_repository',\n 'run_started_at',\n 'head_branch',\n 'url',\n 'event',\n 'name',\n 'actor',\n 'created_at',\n 'check_suite_url',\n 'check_suite_id',\n 'conclusion',\n 'head_commit',\n 'logs_url']\n\n\nWe don’t care about many of these for the purposes of this analysis, however we need the id and a few values derived from the run_started_at column.\n\nid: the unique identifier of the workflow run\nrun_started_at: the time the workflow run started\n\nWe compute the date the run started at so we can later compare it to the dates where we added poetry and switched to the team plan.\n\nworkflows = workflows.select(\n    _.id, _.run_started_at, started_date=_.run_started_at.date()\n)\nworkflows\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ id        ┃ run_started_at            ┃ started_date ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ int64     │ timestamp('UTC')          │ date         │\n├───────────┼───────────────────────────┼──────────────┤\n│ 195478382 │ 2020-08-04 23:54:29+00:00 │ 2020-08-04   │\n│ 195476517 │ 2020-08-04 23:51:44+00:00 │ 2020-08-04   │\n│ 195475525 │ 2020-08-04 23:50:11+00:00 │ 2020-08-04   │\n│ 195468677 │ 2020-08-04 23:39:51+00:00 │ 2020-08-04   │\n│ 195465343 │ 2020-08-04 23:34:11+00:00 │ 2020-08-04   │\n│ 195460611 │ 2020-08-04 23:29:07+00:00 │ 2020-08-04   │\n│ 195452505 │ 2020-08-04 23:17:29+00:00 │ 2020-08-04   │\n│ 195447886 │ 2020-08-04 23:11:35+00:00 │ 2020-08-04   │\n│ 195435521 │ 2020-08-04 23:02:34+00:00 │ 2020-08-04   │\n│ 195433385 │ 2020-08-04 23:01:00+00:00 │ 2020-08-04   │\n│         … │ …                         │ …            │\n└───────────┴───────────────────────────┴──────────────┘\n\n\n\nWe need to associate jobs and workflows somehow, so let’s join them on the relevant key fields.\n\njoined = jobs.join(workflows, jobs.run_id == workflows.id)\njoined\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┓\n┃ run_id    ┃ job_duration ┃ last_job_started_at       ┃ last_job_completed_at     ┃ id        ┃ run_started_at            ┃ started_date ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━┩\n│ int64     │ int64        │ timestamp('UTC')          │ timestamp('UTC')          │ int64     │ timestamp('UTC')          │ date         │\n├───────────┼──────────────┼───────────────────────────┼───────────────────────────┼───────────┼───────────────────────────┼──────────────┤\n│ 647137270 │      1000000 │ 2021-03-12 18:59:42+00:00 │ 2021-03-12 18:59:43+00:00 │ 647137270 │ 2021-03-12 18:59:31+00:00 │ 2021-03-12   │\n│ 647113324 │    801000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    832000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    380000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    371000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    748000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    775000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    384000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    395000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│ 647113324 │    863000000 │ 2021-03-12 18:51:40+00:00 │ 2021-03-12 19:06:13+00:00 │ 647113324 │ 2021-03-12 18:51:26+00:00 │ 2021-03-12   │\n│         … │            … │ …                         │ …                         │         … │ …                         │ …            │\n└───────────┴──────────────┴───────────────────────────┴───────────────────────────┴───────────┴───────────────────────────┴──────────────┘\n\n\n\nSweet! Now we have workflow runs and job runs together in the same table, let’s start exploring summarization.\nLet’s encode our knowledge about when the poetry move happened and also when we moved to the team plan.\n\nfrom datetime import date\n\nPOETRY_MERGED_DATE = date(2021, 10, 15)\nTEAMIZATION_DATE = date(2022, 11, 28)\n\nLet’s compute some indicator variables indicating whether a given row contains data after poetry changes occurred, and do the same for the team plan.\nLet’s also compute queueing time and workflow duration.\n\nstats = joined.select(\n    _.started_date,\n    _.job_duration,\n    has_poetry=_.started_date &gt; POETRY_MERGED_DATE,\n    has_team=_.started_date &gt; TEAMIZATION_DATE,\n    queueing_time=_.last_job_started_at.delta(_.run_started_at, \"microsecond\"),\n    workflow_duration=_.last_job_completed_at.delta(_.run_started_at, \"microsecond\"),\n)\nstats\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ started_date ┃ job_duration ┃ has_poetry ┃ has_team ┃ queueing_time ┃ workflow_duration ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ date         │ int64        │ boolean    │ boolean  │ int64         │ int64             │\n├──────────────┼──────────────┼────────────┼──────────┼───────────────┼───────────────────┤\n│ 2021-07-25   │      1000000 │ False      │ False    │       7000000 │           8000000 │\n│ 2021-07-25   │            0 │ False      │ False    │      10000000 │          10000000 │\n│ 2021-07-25   │    389000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    482000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    451000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    519000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    733000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    758000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    388000000 │ False      │ False    │      11000000 │        1010000000 │\n│ 2021-07-25   │    403000000 │ False      │ False    │      11000000 │        1010000000 │\n│ …            │            … │ …          │ …        │             … │                 … │\n└──────────────┴──────────────┴────────────┴──────────┴───────────────┴───────────────────┘\n\n\n\nLet’s create a column ranging from 0 to 2 inclusive where:\n\n0: no improvements\n1: just poetry\n2: poetry and the team plan\n\nLet’s also give them some names that’ll look nice on our plots.\n\nstats = stats.mutate(\n    raw_improvements=_.has_poetry.cast(\"int\") + _.has_team.cast(\"int\")\n).mutate(\n    improvements=(\n        _.raw_improvements.case()\n        .when(0, \"None\")\n        .when(1, \"Poetry\")\n        .when(2, \"Poetry + Team Plan\")\n        .else_(\"NA\")\n        .end()\n    ),\n    team_plan=ibis.where(_.raw_improvements &gt; 1, \"Poetry + Team Plan\", \"None\"),\n)\nstats\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ started_date ┃ job_duration ┃ has_poetry ┃ has_team ┃ queueing_time ┃ workflow_duration ┃ raw_improvements ┃ improvements ┃ team_plan ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ date         │ int64        │ boolean    │ boolean  │ int64         │ int64             │ int64            │ string       │ string    │\n├──────────────┼──────────────┼────────────┼──────────┼───────────────┼───────────────────┼──────────────────┼──────────────┼───────────┤\n│ 2022-10-08   │     43000000 │ True       │ False    │       8000000 │          51000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │     15000000 │ True       │ False    │     458000000 │         473000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │            0 │ True       │ False    │      16000000 │          16000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │      8000000 │ True       │ False    │      16000000 │          24000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │            0 │ True       │ False    │      16000000 │          16000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │            0 │ True       │ False    │      27000000 │          27000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │      1000000 │ True       │ False    │      25000000 │          25000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │            0 │ True       │ False    │      25000000 │          25000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │      1000000 │ True       │ False    │      25000000 │          25000000 │                1 │ Poetry       │ None      │\n│ 2022-10-08   │      2000000 │ True       │ False    │      25000000 │          25000000 │                1 │ Poetry       │ None      │\n│ …            │            … │ …          │ …        │             … │                 … │                … │ …            │ …         │\n└──────────────┴──────────────┴────────────┴──────────┴───────────────┴───────────────────┴──────────────────┴──────────────┴───────────┘\n\n\n\nFinally, we can summarize by averaging the different durations, grouping on the variables of interest.\n\nUSECS_PER_MIN = 60_000_000\n\nagged = stats.group_by([_.started_date, _.improvements, _.team_plan]).agg(\n    job=_.job_duration.div(USECS_PER_MIN).mean(),\n    workflow=_.workflow_duration.div(USECS_PER_MIN).mean(),\n    queueing_time=_.queueing_time.div(USECS_PER_MIN).mean(),\n)\nagged\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ started_date ┃ improvements       ┃ team_plan          ┃ job       ┃ workflow  ┃ queueing_time ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ date         │ string             │ string             │ float64   │ float64   │ float64       │\n├──────────────┼────────────────────┼────────────────────┼───────────┼───────────┼───────────────┤\n│ 2022-08-10   │ Poetry             │ None               │  5.358063 │ 27.103921 │     22.845206 │\n│ 2020-10-02   │ None               │ None               │ 11.536410 │ 29.325641 │     28.941538 │\n│ 2022-12-29   │ Poetry + Team Plan │ Poetry + Team Plan │  2.612843 │  6.538934 │      5.663799 │\n│ 2022-05-25   │ Poetry             │ None               │  2.171620 │  9.641242 │      9.223480 │\n│ 2021-03-23   │ None               │ None               │  9.908273 │ 18.121004 │      1.824016 │\n│ 2022-10-21   │ Poetry             │ None               │  3.347873 │ 12.195076 │      9.156614 │\n│ 2021-01-31   │ None               │ None               │  0.000000 │  0.266667 │      0.266667 │\n│ 2021-07-27   │ None               │ None               │  0.016667 │  0.183333 │      0.166667 │\n│ 2022-03-15   │ Poetry             │ None               │  2.275418 │  9.091200 │      8.498640 │\n│ 2021-12-12   │ Poetry             │ None               │  4.245767 │ 15.027579 │     10.329464 │\n│ …            │ …                  │ …                  │         … │         … │             … │\n└──────────────┴────────────────────┴────────────────────┴───────────┴───────────┴───────────────┘\n\n\n\nIf at any point you want to inspect the SQL you’ll be running, ibis has you covered with ibis.to_sql.\n\nibis.to_sql(agged)\n\nWITH t0 AS (\n  SELECT\n    t6.*\n  FROM `ibis-gbq`.workflows.jobs AS t6\n  WHERE\n    t6.`started_at` &lt; '2023-01-09'\n), t1 AS (\n  SELECT\n    t6.`id`,\n    t6.`run_started_at`,\n    DATE(t6.`run_started_at`) AS `started_date`\n  FROM `ibis-gbq`.workflows.workflows AS t6\n), t2 AS (\n  SELECT\n    t0.`run_id`,\n    TIMESTAMP_DIFF(t0.`completed_at`, t0.`started_at`, MICROSECOND) AS `job_duration`,\n    MAX(t0.`started_at`) OVER (PARTITION BY t0.`run_id`) AS `last_job_started_at`,\n    MAX(t0.`completed_at`) OVER (PARTITION BY t0.`run_id`) AS `last_job_completed_at`\n  FROM t0\n), t3 AS (\n  SELECT\n    `started_date`,\n    `job_duration`,\n    `started_date` &gt; CAST('2021-10-15' AS DATE) AS `has_poetry`,\n    `started_date` &gt; CAST('2022-11-28' AS DATE) AS `has_team`,\n    TIMESTAMP_DIFF(`last_job_started_at`, `run_started_at`, MICROSECOND) AS `queueing_time`,\n    TIMESTAMP_DIFF(`last_job_completed_at`, `run_started_at`, MICROSECOND) AS `workflow_duration`\n  FROM t2\n  INNER JOIN t1\n    ON t2.`run_id` = t1.`id`\n), t4 AS (\n  SELECT\n    t3.*,\n    CAST(t3.`has_poetry` AS INT64) + CAST(t3.`has_team` AS INT64) AS `raw_improvements`\n  FROM t3\n)\nSELECT\n  t5.`started_date`,\n  t5.`improvements`,\n  t5.`team_plan`,\n  avg(IEEE_DIVIDE(t5.`job_duration`, 60000000)) AS `job`,\n  avg(IEEE_DIVIDE(t5.`workflow_duration`, 60000000)) AS `workflow`,\n  avg(IEEE_DIVIDE(t5.`queueing_time`, 60000000)) AS `queueing_time`\nFROM (\n  SELECT\n    t4.*,\n    CASE t4.`raw_improvements`\n      WHEN 0\n      THEN 'None'\n      WHEN 1\n      THEN 'Poetry'\n      WHEN 2\n      THEN 'Poetry + Team Plan'\n      ELSE 'NA'\n    END AS `improvements`,\n    IF(t4.`raw_improvements` &gt; 1, 'Poetry + Team Plan', 'None') AS `team_plan`\n  FROM t4\n) AS t5\nGROUP BY\n  1,\n  2,\n  3"
  },
  {
    "objectID": "posts/ci-analysis/index.html#result-1-job-duration",
    "href": "posts/ci-analysis/index.html#result-1-job-duration",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Result #1: Job Duration",
    "text": "Result #1: Job Duration\nThis result is pretty interesting.\nA few things pop out to me right away:\n\nThe move to poetry decreased the average job run duration by quite a bit. No, I’m not going to do any statistical tests.\nThe variability of job run durations also decreased by quite a bit after introducing poetry.\nMoving to the team plan had little to no effect on job run duration.\n\n\n(\n    ggplot(\n        df.loc[df.entity != \"job\"].reset_index(drop=True),\n        aes(x=\"started_date\", y=\"duration\", color=\"factor(improvements)\"),\n    )\n    + facet_wrap(\"entity\", ncol=1)\n    + geom_point()\n    + geom_vline(\n        xintercept=[TEAMIZATION_DATE, POETRY_MERGED_DATE],\n        linetype=\"dashed\",\n    )\n    + scale_color_brewer(\n        palette=7,\n        type='qual',\n        limits=[\"None\", \"Poetry\", \"Poetry + Team Plan\"],\n    )\n    + geom_text(x=POETRY_MERGED_DATE, label=poetry_label, y=75, color=\"blue\")\n    + geom_text(x=TEAMIZATION_DATE, label=team_label, y=50, color=\"blue\")\n    + stat_smooth(method=\"lm\")\n    + labs(x=\"Date\", y=\"Duration (minutes)\")\n    + ggtitle(\"Workflow Duration\")\n    + theme(\n        figure_size=(22, 13),\n        legend_position=(0.68, 0.75),\n        legend_direction=\"vertical\",\n    )\n)\n\n\n\n\n&lt;Figure Size: (2200 x 1300)&gt;"
  },
  {
    "objectID": "posts/ci-analysis/index.html#result-2-workflow-duration-and-queueing-time",
    "href": "posts/ci-analysis/index.html#result-2-workflow-duration-and-queueing-time",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Result #2: Workflow Duration and Queueing Time",
    "text": "Result #2: Workflow Duration and Queueing Time\nAnother interesting result.\n\nQueueing Time\n\nIt almost looks like moving to poetry made average queueing time worse. This is probably due to our perception that faster jobs means faster ci. As we see here that isn’t the case\nMoving to the team plan cut down the queueing time by quite a bit\n\n\n\nWorkflow Duration\n\nOverall workflow duration appears to be strongly influenced by moving to the team plan, which is almost certainly due to the drop in queueing time since we are no longer limited by slow job durations.\nPerhaps it’s obvious, but queueing time and workflow duration appear to be highly correlated.\n\nIn the next plot we’ll look at that correlation.\n\n(\n    ggplot(raw_df, aes(x=\"workflow\", y=\"queueing_time\"))\n    + geom_point()\n    + geom_rug()\n    + facet_grid(\". ~ team_plan\")\n    + labs(x=\"Workflow Duration (minutes)\", y=\"Queueing Time (minutes)\")\n    + ggtitle(\"Workflow Duration vs. Queueing Time\")\n    + theme(figure_size=(22, 6))\n)\n\n\n\n\n&lt;Figure Size: (2200 x 600)&gt;"
  },
  {
    "objectID": "posts/ci-analysis/index.html#result-3-workflow-duration-and-queueing-duration-are-correlated",
    "href": "posts/ci-analysis/index.html#result-3-workflow-duration-and-queueing-duration-are-correlated",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Result #3: Workflow Duration and Queueing Duration are correlated",
    "text": "Result #3: Workflow Duration and Queueing Duration are correlated\nIt also seems that moving to the team plan (though also the move to poetry might be related here) reduced the variability of both metrics.\nWe’re lacking data compared to the past so we should wait for more to come in."
  },
  {
    "objectID": "posts/ci-analysis/index.html#conclusions",
    "href": "posts/ci-analysis/index.html#conclusions",
    "title": "Analysis of Ibis’s CI performance",
    "section": "Conclusions",
    "text": "Conclusions\nIt appears that you need both a short queue time and fast individual jobs to minimize time spent in CI.\nIf you have a short queue time, but long job runs then you’ll be bottlenecked on individual jobs, and if you have more jobs than queue slots then you’ll be blocked on queueing time.\nI think we can sum this up nicely:\n\nslow jobs, slow queue: 🤷 blocked by jobs or queue\nslow jobs, fast queue: ❓ blocked by jobs, if jobs are slow enough\nfast jobs, slow queue: ❗ blocked by queue, with enough jobs\nfast jobs, fast queue: ✅"
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html",
    "href": "posts/pydata-performance-part2/index.html",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "",
    "text": "TL; DR: Ibis supports both Polars and DataFusion. Both backends are have about the same runtime performance, and lag far behind DuckDB on this workload. There’s negligible performance difference between Ibis and the backend native APIs."
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html#motivation",
    "href": "posts/pydata-performance-part2/index.html#motivation",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "Motivation",
    "text": "Motivation\nThis is part 2 of a series of posts showing performance across various backends that Ibis supports.\nCheck out part 1 if you haven’t already!\nIn this post, I’ll continue with the Polars and DataFusion backends.\nI show each tool using both the Ibis API and the tool’s native API. We’ll see that the performance difference between these approaches is negligible."
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html#setup",
    "href": "posts/pydata-performance-part2/index.html#setup",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "Setup",
    "text": "Setup\nI ran all of the code in this blog post on a machine with these specs.\n\n\n\nComponent\nSpecification\n\n\n\n\nCPU\nAMD EPYC 7B12 (64 threads)\n\n\nRAM\n94 GiB\n\n\nDisk\n1.5 TiB SSD\n\n\nOS\nNixOS (Linux 6.1.66)\n\n\n\n\nLibrary versions\nHere are the versions I used to run this experiment at the time of writing.\n\n\n\nDependency\nVersion\n\n\n\n\nPython\n3.10.13 (main, Aug 24 2023, 12:59:26) [GCC 12.3.0]\n\n\ndatafusion\n33.0.0\n\n\nibis\n2b54b9800\n\n\npandas\n2.1.4\n\n\npolars\n0.19.19\n\n\npyarrow\n14.0.1"
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html#running-the-query-across-backends",
    "href": "posts/pydata-performance-part2/index.html#running-the-query-across-backends",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "Running the query across backends",
    "text": "Running the query across backends\nHere are the different Ibis expressions for each backend as well as the same query with native APIs, along with timed executions of the query.\n\nDuckDB\nFirst, let’s run the Ibis + DuckDB version of the query from the original post:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(project_count=_.project_name.nunique())\n    .dropna(\"ext\")\n    .order_by([_.month.desc(), _.project_count.desc()])\n)\ndf = expr.to_pandas()\n\nduckdb_ibis_results = %timeit -n1 -r1 -o %run duckdb_ibis.py\ndf.head()\n\n33.9 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10\n\n\n\n\n\n\n\n\n\nDataFusion and Polars\n\nDataFusionPolars\n\n\n\nIbisDataFusion native\n\n\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _\n\nibis.set_backend(\"datafusion\")\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(project_count=_.project_name.nunique())\n    .dropna(\"ext\")\n    .order_by([_.month.desc(), _.project_count.desc()])\n)\ndf = expr.to_pandas()\n\ndatafusion_ibis_results = %timeit -n1 -r1 -o %run datafusion_ibis.py\ndf.head()\n\n8min 54s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10\n\n\n\n\n\n\n\n\n\n\n\nDataFusion SQL\n\nSELECT\n  month,\n  ext,\n  COUNT(DISTINCT project_name) AS project_count\nFROM (\n  SELECT\n    project_name,\n    DATE_TRUNC('month', uploaded_on) AS month,\n    NULLIF(\n      REPLACE(\n        REPLACE(\n          REPLACE(\n            REGEXP_REPLACE(\n              REGEXP_REPLACE(\n                REGEXP_MATCH(path, CONCAT('(', '\\.([a-z0-9]+)$', ')'))[2],\n                'cxx|cpp|cc|c|hpp|h',\n                'C/C++',\n                'g'\n              ),\n              '^f.*$',\n              'Fortran',\n              'g'\n            ),\n            'rs',\n            'Rust'\n          ),\n          'go',\n          'Go'\n        ),\n        'asm',\n        'Assembly'\n      ),\n      ''\n    ) AS ext\n  FROM pypi\n  WHERE COALESCE(\n      ARRAY_LENGTH(\n        REGEXP_MATCH(path, '\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$')\n      ) &gt; 0,\n      FALSE\n    )\n    AND NOT COALESCE(ARRAY_LENGTH(REGEXP_MATCH(path, '(^|/)test(|s|ing)')) &gt; 0, FALSE)\n    AND NOT STRPOS(path, '/site-packages/') &gt; 0\n)\nWHERE ext IS NOT NULL\nGROUP BY month, ext\nORDER BY month DESC, project_count DESC\n\nfrom __future__ import annotations\n\nimport datafusion\n\nwith open(\"./datafusion_native.sql\") as f:\n    query = f.read()\n\nctx = datafusion.SessionContext()\nctx.register_parquet(name=\"pypi\", path=\"/data/pypi-parquet/*.parquet\")\nexpr = ctx.sql(query)\n\ndf = expr.to_pandas()\n\ndatafusion_native_results = %timeit -n1 -r1 -o %run datafusion_native.py\ndf.head()\n\n9min 4s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\nIbisPolars native\n\n\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _\n\nibis.set_backend(\"polars\")\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(project_count=_.project_name.nunique())\n    .dropna(\"ext\")\n    .order_by([_.month.desc(), _.project_count.desc()])\n)\ndf = expr.to_pandas(streaming=True)\n\npolars_ibis_results = %timeit -n1 -r1 -o %run polars_ibis.py\ndf.head()\n\n6min 32s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10\n\n\n\n\n\n\n\n\n\nfrom __future__ import annotations\n\nimport polars as pl\n\nexpr = (\n    pl.scan_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            pl.col(\"path\").str.contains(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~pl.col(\"path\").str.contains(r\"(^|/)test(|s|ing)\"),\n            ~pl.col(\"path\").str.contains(\"/site-packages/\", literal=True),\n        ]\n    )\n    .with_columns(\n        month=pl.col(\"uploaded_on\").dt.truncate(\"1mo\"),\n        ext=pl.col(\"path\")\n        .str.extract(pattern=r\"\\.([a-z0-9]+)$\", group_index=1)\n        .str.replace_all(pattern=r\"cxx|cpp|cc|c|hpp|h\", value=\"C/C++\")\n        .str.replace_all(pattern=\"^f.*$\", value=\"Fortran\")\n        .str.replace(\"rs\", \"Rust\", literal=True)\n        .str.replace(\"go\", \"Go\", literal=True)\n        .str.replace(\"asm\", \"Assembly\", literal=True)\n        .replace({\"\": None}),\n    )\n    .group_by([\"month\", \"ext\"])\n    .agg(project_count=pl.col(\"project_name\").n_unique())\n    .drop_nulls([\"ext\"])\n    .sort([\"month\", \"project_count\"], descending=True)\n)\n\ndf = expr.collect(streaming=True).to_pandas()\n\npolars_native_results = %timeit -n1 -r1 -o %run polars_native.py\ndf.head()\n\n6min 54s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10"
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html#takeaways",
    "href": "posts/pydata-performance-part2/index.html#takeaways",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "Takeaways",
    "text": "Takeaways\nIbis + DuckDB is the only system tested that handles this workload well out of the box\n\nBoth Polars and DataFusion are much slower than DuckDB and Dask on this workload.\nPolars memory use fluctuates quite bit, while DataFusion’s memory profile is similar to DuckDB.\n\nLet’s recap the results with some numbers:\n\nNumbers\n\n\n\nToolset\nData size\nDuration\nThroughput\n\n\n\n\nIbis + DuckDB\n25,825 MiB\n34 seconds\n763 MiB/s\n\n\nIbis + Polars\n25,825 MiB\n393 seconds\n66 MiB/s\n\n\nPolars native API\n25,825 MiB\n415 seconds\n62 MiB/s\n\n\nIbis + DataFusion\n25,825 MiB\n535 seconds\n48 MiB/s\n\n\nDataFusion native API\n25,825 MiB\n545 seconds\n47 MiB/s\n\n\n\n\n\n\n\n\n\nThe Polars run durations were highly variable\n\n\n\nI couldn’t figure out how to get consistent run times."
  },
  {
    "objectID": "posts/pydata-performance-part2/index.html#conclusion",
    "href": "posts/pydata-performance-part2/index.html#conclusion",
    "title": "Ibis versus X: Performance across the ecosystem part 2",
    "section": "Conclusion",
    "text": "Conclusion\nIf you’re considering Polars for new code, give Ibis a try with the DuckDB backend.\nYou’ll get better performance than Polars on some workloads, and with a broader cross-backend API that helps you scale from development to production.\nIf you find that Polars has better performance than DuckDB on a particular workload you can always switch to the Polars backend for that workload.\nEveryone wins!\nIn the next post in this series we’ll cover the cloud backends: Snowflake, BigQuery, Trino and ClickHouse."
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html",
    "href": "posts/ibis-version-6.0.0-release/index.html",
    "title": "Ibis v6.0.0",
    "section": "",
    "text": "Ibis 6.0.0 adds the Oracle backend, revamped UDF support, and many new features. This release also includes a number of refactors, bug fixes, and performance improvements. You can view the full changelog in the release notes.\nIf you’re new to Ibis, see how to install and the getting started tutorial.\nTo follow along with this blog, ensure you’re on 'ibis-framework&gt;=6,&lt;7'. First, we’ll setup Ibis and fetch some sample data to use.\n\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\nibis.options.repr.interactive.max_rows = 3\n\nNow, fetch the penguins dataset.\n\nt = ibis.examples.penguins.fetch()\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#overview",
    "href": "posts/ibis-version-6.0.0-release/index.html#overview",
    "title": "Ibis v6.0.0",
    "section": "",
    "text": "Ibis 6.0.0 adds the Oracle backend, revamped UDF support, and many new features. This release also includes a number of refactors, bug fixes, and performance improvements. You can view the full changelog in the release notes.\nIf you’re new to Ibis, see how to install and the getting started tutorial.\nTo follow along with this blog, ensure you’re on 'ibis-framework&gt;=6,&lt;7'. First, we’ll setup Ibis and fetch some sample data to use.\n\nimport ibis\nimport ibis.selectors as s\n\nibis.options.interactive = True\nibis.options.repr.interactive.max_rows = 3\n\nNow, fetch the penguins dataset.\n\nt = ibis.examples.penguins.fetch()\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#breaking-changes",
    "href": "posts/ibis-version-6.0.0-release/index.html#breaking-changes",
    "title": "Ibis v6.0.0",
    "section": "Breaking changes",
    "text": "Breaking changes\n\nJoin duplicate column names\nPreviously when joining tables with duplicate column names, _x and _y suffixes would be appended by default to the left and right tables respectively. You could override this with the suffix argument, which is now removed in favor of lname and rname arguments. The default is changed to no suffix for the left table and _right for the right table.\n\nt.join(t, \"island\").select(s.startswith(\"species\"))\n\n┏━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ species ┃ species_right ┃\n┡━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string  │ string        │\n├─────────┼───────────────┤\n│ Adelie  │ Adelie        │\n│ Adelie  │ Adelie        │\n│ Adelie  │ Adelie        │\n│ …       │ …             │\n└─────────┴───────────────┘\n\n\n\nTo replicate the previous behavior:\n\nt.join(t, \"island\", lname=\"{name}_x\", rname=\"{name}_y\").select(\n    s.startswith(\"species\")\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species_x ┃ species_y ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Adelie    │\n│ Adelie    │ Adelie    │\n│ Adelie    │ Adelie    │\n│ …         │ …         │\n└───────────┴───────────┘\n\n\n\n\n\n.count() column names no longer named count automatically\nColumns created with the .count() aggregation are no longer automatically named count. This is to follow convention with other aggregations and reduce the likelihood of name collisions.\n\nt.group_by(\"species\").agg(ibis._.species.count())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species   ┃ Count(species) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string    │ int64          │\n├───────────┼────────────────┤\n│ Adelie    │            152 │\n│ Gentoo    │            124 │\n│ Chinstrap │             68 │\n└───────────┴────────────────┘\n\n\n\nTo reproduce the old behavior, you can rename the column to count with:\nt.group_by(\"species\").agg(count=ibis._.species.count())"
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#backends",
    "href": "posts/ibis-version-6.0.0-release/index.html#backends",
    "title": "Ibis v6.0.0",
    "section": "Backends",
    "text": "Backends\n\nOracle\nThe Oracle backend was added! See the Voltron Data blog for more details.\nibis.connect(f\"oracle://user:password@host\")\n\n\nDuckDB\nThere were various DuckDB improvements, but one notable new feature is the ability to attach to a SQLite database through DuckDB. This allows you to run OLAP queries via DuckDB significantly faster on source data from SQLite.\nFirst we’ll create a DuckDB connection and show it has no tables:\n\nduckdb_con = ibis.connect(\"duckdb://\")\nduckdb_con.list_tables()\n\n[]\n\n\nThen create a SQLite database with a table:\n\nsqlite_con = ibis.connect(\"sqlite://penguins.db\")\nsqlite_con.create_table(\"penguins\", t.to_pandas(), overwrite=True)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │             181.0 │      3750.0 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │             186.0 │      3800.0 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │             195.0 │      3250.0 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nAnd attach it:\n\nduckdb_con.attach_sqlite(\"./penguins.db\")\nduckdb_con.list_tables()\n\n['penguins']\n\n\n\nMotherDuck support!\nMotherDuck launched recently and is now supported in Ibis!\nSimply connect with the DuckDB backend using md: or motherduck: as the database.\nibis.connect(\"duckdb://md:\")\n\n\n\nPolars\nThe Polars backend received many improvements from community members @alexander-beedie and @mesejo, with plenty of operations now supported.\nSome additions in this version include:\n\nany and all reductions\nargmin and argmax\nidentical_to\ncorr\nsupport for .sql()\n\nGive it a try by setting your backend to Polars with ibis.set_backend(\"polars\")."
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#functionality",
    "href": "posts/ibis-version-6.0.0-release/index.html#functionality",
    "title": "Ibis v6.0.0",
    "section": "Functionality",
    "text": "Functionality\n\nUDFs\nUser-defined functions (UDFs) have been revamped with a new syntax and new backends added. To get started, import the decorator:\n\nfrom ibis import udf\n\nDefine a UDF:\n\n@udf.scalar.python\ndef num_vowels(s: str, include_y: bool = False) -&gt; int:\n    return sum(map(s.lower().count, \"aeiou\" + (\"y\" * include_y)))\n\nAnd call it:\n\nnum_vowels(t[:1].species.execute()[0])\n\n\n\n\n\n4\n\n\n\n\nt.group_by(num_vowels=num_vowels(t.species)).agg(\n    num_vowels_island_count=t.island.count()\n)\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ num_vowels ┃ num_vowels_island_count ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64      │ int64                   │\n├────────────┼─────────────────────────┤\n│          4 │                     152 │\n│          3 │                     124 │\n│          2 │                      68 │\n└────────────┴─────────────────────────┘\n\n\n\n\nt.filter(num_vowels(t.species) &lt; 4)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Gentoo  │ Biscoe │           46.1 │          13.2 │               211 │        4500 │ female │  2007 │\n│ Gentoo  │ Biscoe │           50.0 │          16.3 │               230 │        5700 │ male   │  2007 │\n│ Gentoo  │ Biscoe │           48.7 │          14.1 │               210 │        4450 │ female │  2007 │\n│ …       │ …      │              … │             … │                 … │           … │ …      │     … │\n└─────────┴────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nto_torch API\nA new to_torch output method was added. Combined with UDFs, this brings powerful ML capabilities into Ibis. See a complete example in the Ibis + DuckDB + PyTorch blog.\n\nimport torch\n\ntorch.set_printoptions(threshold=10)\n\n\nt.select(s.numeric()).to_torch()\n\n{'bill_length_mm': tensor([39.1000, 39.5000, 40.3000,  ..., 49.6000, 50.8000, 50.2000],\n        dtype=torch.float64),\n 'bill_depth_mm': tensor([18.7000, 17.4000, 18.0000,  ..., 18.2000, 19.0000, 18.7000],\n        dtype=torch.float64),\n 'flipper_length_mm': tensor([181, 186, 195,  ..., 193, 210, 198]),\n 'body_mass_g': tensor([3750, 3800, 3250,  ..., 3775, 4100, 3775]),\n 'year': tensor([2007, 2007, 2007,  ..., 2009, 2009, 2009])}\n\n\n\n\nArray zip support\nA new zip operation was added on array data types, allowing you to zip together multiple arrays.\n\narrays = ibis.memtable(\n    {\"numbers\": [[3, 2], [], None], \"strings\": [[\"a\", \"c\"], None, [\"e\"]]}\n)\narrays\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ numbers      ┃ strings       ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ array&lt;int64&gt; │ array&lt;string&gt; │\n├──────────────┼───────────────┤\n│ [3, 2]       │ ['a', 'c']    │\n│ []           │ NULL          │\n│ NULL         │ ['e']         │\n└──────────────┴───────────────┘\n\n\n\n\narrays.numbers.zip(arrays.strings)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;struct&lt;f1: int64, f2: string&gt;&gt; │\n├──────────────────────────────────────┤\n│ [{...}, {...}]                       │\n│ []                                   │\n│ [{...}]                              │\n└──────────────────────────────────────┘\n\n\n\n\narrays.numbers.zip(arrays.strings).unnest()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayZip()                    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ struct&lt;f1: int64, f2: string&gt; │\n├───────────────────────────────┤\n│ {'f1': 3, 'f2': 'a'}          │\n│ {'f1': 2, 'f2': 'c'}          │\n│ {'f1': None, 'f2': 'e'}       │\n└───────────────────────────────┘\n\n\n\n\n\nTry cast support\nA new try_cast() operation was added that allows you to cast a column to a type, but return null if the cast fails.\n\nibis.literal(\"a\").try_cast(\"int\")\n\n\n\n\n\nNone\n\n\n\nibis.literal(0).try_cast(\"float\")\n\n\n__dataframe__ support\nIbis now supports the dataframe interchange protocol, allowing Ibis expressions to be used in any framework that supports it. Adoption of the protocol is still in its early stages, but we expect this to enable Ibis to be used in many new places going forward.\n\nt.__dataframe__()\n\n&lt;ibis.expr.types.dataframe_interchange.IbisDataFrame at 0x7f7bf415d9c0&gt;\n\n\n\n\nStreamlit experimental connection interface\nA new experimental connection interface was added for Streamlit. See how-to write a Streamlit app with Ibis.\n\n\nSQL dialect parameter\nIn SQL methods, you can now pass the dialect parameter to specify the SQL dialect used. This leverages sqlglot under the hood.\n\nbigquery_sql = \"\"\"\nSELECT\n  t0.`species`,\n  COUNT(t0.`species`) AS `count`,\n  CAST(COUNT(DISTINCT t0.`island`) AS FLOAT64) AS `islands`\nFROM penguins AS t0\nGROUP BY\n  1\n\"\"\"\n\nduckdb_con.sql(bigquery_sql, dialect=\"bigquery\")\n\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━┓\n┃ species   ┃ count ┃ islands ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━┩\n│ string    │ int64 │ float64 │\n├───────────┼───────┼─────────┤\n│ Adelie    │   152 │     3.0 │\n│ Gentoo    │   124 │     1.0 │\n│ Chinstrap │    68 │     1.0 │\n└───────────┴───────┴─────────┘\n\n\n\n\n\nDelta Lake read/write support for some backends\nDelta Lake tables are supported through the deltalake package with read_delta() implemented for DuckDB, Polars, and DataFusion.\n\nt.to_delta(\"penguins.delta\", mode=\"overwrite\")\n\n\nt = ibis.read_delta(\"penguins.delta\")\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\n\n\nSelectors\nSome minor selectors improvements were added including the ability to use abstract type names and lists of strings.\n\nt.select(s.of_type(\"string\"))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island    ┃ sex    ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━┩\n│ string  │ string    │ string │\n├─────────┼───────────┼────────┤\n│ Adelie  │ Torgersen │ male   │\n│ Adelie  │ Torgersen │ female │\n│ Adelie  │ Torgersen │ female │\n│ …       │ …         │ …      │\n└─────────┴───────────┴────────┘\n\n\n\n\nt.agg(s.across([\"species\", \"island\"], ibis._.count()))\n\n┏━━━━━━━━━┳━━━━━━━━┓\n┃ species ┃ island ┃\n┡━━━━━━━━━╇━━━━━━━━┩\n│ int64   │ int64  │\n├─────────┼────────┤\n│     344 │    344 │\n└─────────┴────────┘"
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#refactors",
    "href": "posts/ibis-version-6.0.0-release/index.html#refactors",
    "title": "Ibis v6.0.0",
    "section": "Refactors",
    "text": "Refactors\nSeveral internal refactors that shouldn’t affect normal usage were made. See the release notes for more details."
  },
  {
    "objectID": "posts/ibis-version-6.0.0-release/index.html#wrapping-up",
    "href": "posts/ibis-version-6.0.0-release/index.html#wrapping-up",
    "title": "Ibis v6.0.0",
    "section": "Wrapping up",
    "text": "Wrapping up\nIbis v6.0.0 brings exciting new features that enable future support for ML and streaming workloads.\nAs always, try Ibis by installing and getting started.\nIf you run into any issues or find support is lacking for your backend, open an issue or discussion and let us know!"
  },
  {
    "objectID": "posts/ibis-duckdb-geospatial/index.html",
    "href": "posts/ibis-duckdb-geospatial/index.html",
    "title": "Ibis + DuckDB geospatial: a match made on Earth",
    "section": "",
    "text": "Ibis now has support for DuckDB geospatial functions!\nThis blogpost showcases some examples of the geospatial API for the DuckDB backend. The material is inspired by the “DuckDB: Spatial Relationships” lesson from Dr. Qiusheng Wu’s course “Spatial Data Management” from the Department of Geography & Sustainability at the University of Tennessee, Knoxville."
  },
  {
    "objectID": "posts/ibis-duckdb-geospatial/index.html#data",
    "href": "posts/ibis-duckdb-geospatial/index.html#data",
    "title": "Ibis + DuckDB geospatial: a match made on Earth",
    "section": "Data",
    "text": "Data\nWe are going to be working with data from New York City. The database contains multiple tables with information about subway stations, streets, neighborhood, census data and, homicides. The datasets in the database are in NAD83 / UTM zone 18N projection, EPSG:26918.\n\nfrom pathlib import Path\nfrom zipfile import ZipFile\nfrom urllib.request import urlretrieve\n\n# Download and unzip\nurl = \"https://open.gishub.org/data/duckdb/nyc_data.db.zip\"\nzip_path = Path(\"nyc_data.db.zip\")\ndb_path = Path(\"nyc_data.db\")\n\nif not zip_path.exists():\n    urlretrieve(url, zip_path)\n\nif not db_path.exists():\n    with ZipFile(zip_path) as zip_file:\n        zip_file.extract(\"nyc_data.db\")"
  },
  {
    "objectID": "posts/ibis-duckdb-geospatial/index.html#lets-get-started",
    "href": "posts/ibis-duckdb-geospatial/index.html#lets-get-started",
    "title": "Ibis + DuckDB geospatial: a match made on Earth",
    "section": "Let’s get started",
    "text": "Let’s get started\nThe beauty of spatial databases is that they allow us to both store and compute over geometries.\n\nimport ibis\nfrom ibis import _\n\nibis.options.interactive = True\n\ncon = ibis.duckdb.connect(\"nyc_data.db\")\ncon.list_tables()\n\n['nyc_census_blocks',\n 'nyc_homicides',\n 'nyc_neighborhoods',\n 'nyc_streets',\n 'nyc_subway_stations']\n\n\nWe have multiple tables with information about New York City. Following Dr. Wu’s class, we’ll take a look at some spatial relations.\nWe can start by taking a peek at the nyc_subway_stations table.\n\nsubway_stations = con.table(\"nyc_subway_stations\")\nsubway_stations\n\n┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ OBJECTID ┃ ID      ┃ NAME         ┃ ALT_NAME        ┃ CROSS_ST        ┃ LONG_NAME                               ┃ LABEL                             ┃ BOROUGH   ┃ NGHBHD ┃ ROUTES ┃ TRANSFERS ┃ COLOR        ┃ EXPRESS ┃ CLOSED ┃ geom                             ┃\n┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64  │ float64 │ string       │ string          │ string          │ string                                  │ string                            │ string    │ string │ string │ string    │ string       │ string  │ string │ geospatial:geometry              │\n├──────────┼─────────┼──────────────┼─────────────────┼─────────────────┼─────────────────────────────────────────┼───────────────────────────────────┼───────────┼────────┼────────┼───────────┼──────────────┼─────────┼────────┼──────────────────────────────────┤\n│      1.0 │   376.0 │ Cortlandt St │ NULL            │ Church St       │ Cortlandt St (R,W) Manhattan            │ Cortlandt St (R,W)                │ Manhattan │ NULL   │ R,W    │ R,W       │ YELLOW       │ NULL    │ NULL   │ &lt;POINT (583521.854 4507077.863)&gt; │\n│      2.0 │     2.0 │ Rector St    │ NULL            │ NULL            │ Rector St (1) Manhattan                 │ Rector St (1)                     │ Manhattan │ NULL   │ 1      │ 1         │ RED          │ NULL    │ NULL   │ &lt;POINT (583324.487 4506805.373)&gt; │\n│      3.0 │     1.0 │ South Ferry  │ NULL            │ NULL            │ South Ferry (1) Manhattan               │ South Ferry (1)                   │ Manhattan │ NULL   │ 1      │ 1         │ RED          │ NULL    │ NULL   │ &lt;POINT (583304.182 4506069.654)&gt; │\n│      4.0 │   125.0 │ 138th St     │ Grand Concourse │ Grand Concourse │ 138th St / Grand Concourse (4,5) Bronx  │ 138th St / Grand Concourse (4,5)  │ Bronx     │ NULL   │ 4,5    │ 4,5       │ GREEN        │ NULL    │ NULL   │ &lt;POINT (590250.106 4518558.02)&gt;  │\n│      5.0 │   126.0 │ 149th St     │ Grand Concourse │ Grand Concourse │ 149th St / Grand Concourse (4) Bronx    │ 149th St / Grand Concourse (4)    │ Bronx     │ NULL   │ 4      │ 2,4,5     │ GREEN        │ express │ NULL   │ &lt;POINT (590454.74 4519145.72)&gt;   │\n│      6.0 │    45.0 │ 149th St     │ Grand Concourse │ Grand Concourse │ 149th St / Grand Concourse (2,5) Bronx  │ 149th St / Grand Concourse (2,5)  │ Bronx     │ NULL   │ 2,5    │ 2,4,5     │ RED-GREEN    │ express │ NULL   │ &lt;POINT (590465.893 4519168.697)&gt; │\n│      7.0 │   127.0 │ 161st St     │ Yankee Stadium  │ River Ave       │ 161st St / Yankee Stadium (B,D,4) Bronx │ 161st St / Yankee Stadium (B,D,4) │ Bronx     │ NULL   │ B,D,4  │ B,D,4     │ GREEN-ORANGE │ NULL    │ NULL   │ &lt;POINT (590573.169 4520214.766)&gt; │\n│      8.0 │   208.0 │ 167th St     │ NULL            │ Grand Concourse │ 167th St (B,D) Bronx                    │ 167th St (B,D)                    │ Bronx     │ NULL   │ B,D    │ B,D       │ ORANGE       │ NULL    │ NULL   │ &lt;POINT (591252.831 4520950.353)&gt; │\n│      9.0 │   128.0 │ 167th St     │ NULL            │ River Ave       │ 167th St (4) Bronx                      │ 167th St (4)                      │ Bronx     │ NULL   │ 4      │ 4         │ GREEN        │ NULL    │ NULL   │ &lt;POINT (590946.397 4521077.319)&gt; │\n│     10.0 │   209.0 │ 170th St     │ NULL            │ Grand Concourse │ 170th St (B,D) Bronx                    │ 170th St (B,D)                    │ Bronx     │ NULL   │ B,D    │ B,D       │ ORANGE       │ NULL    │ NULL   │ &lt;POINT (591583.611 4521434.847)&gt; │\n│        … │       … │ …            │ …               │ …               │ …                                       │ …                                 │ …         │ …      │ …      │ …         │ …            │ …       │ …      │ …                                │\n└──────────┴─────────┴──────────────┴─────────────────┴─────────────────┴─────────────────────────────────────────┴───────────────────────────────────┴───────────┴────────┴────────┴───────────┴──────────────┴─────────┴────────┴──────────────────────────────────┘\n\n\n\nNotice that the last column has a geometry type, and in this case it contains points that represent the location of each subway station. Let’s grab the entry for the Broad St subway station.\n\nbroad_station = subway_stations.filter(subway_stations.NAME == \"Broad St\")\nbroad_station\n\n┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ OBJECTID ┃ ID      ┃ NAME     ┃ ALT_NAME ┃ CROSS_ST ┃ LONG_NAME                  ┃ LABEL            ┃ BOROUGH   ┃ NGHBHD ┃ ROUTES ┃ TRANSFERS ┃ COLOR  ┃ EXPRESS ┃ CLOSED ┃ geom                             ┃\n┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64  │ float64 │ string   │ string   │ string   │ string                     │ string           │ string    │ string │ string │ string    │ string │ string  │ string │ geospatial:geometry              │\n├──────────┼─────────┼──────────┼──────────┼──────────┼────────────────────────────┼──────────────────┼───────────┼────────┼────────┼───────────┼────────┼─────────┼────────┼──────────────────────────────────┤\n│    332.0 │   304.0 │ Broad St │ NULL     │ Wall St  │ Broad St (J,M,Z) Manhattan │ Broad St (J,M,Z) │ Manhattan │ NULL   │ J,M,Z  │ J,M,Z     │ BROWN  │ express │ NULL   │ &lt;POINT (583571.906 4506714.341)&gt; │\n└──────────┴─────────┴──────────┴──────────┴──────────┴────────────────────────────┴──────────────────┴───────────┴────────┴────────┴───────────┴────────┴─────────┴────────┴──────────────────────────────────┘\n\n\n\n\ngeo_equals (ST_Equals)\nIn DuckDB ST_Equals returns True if two geometries are topologically equal. This means that they have the same dimension and identical coordinate values, although the order of the vertices may be different.\nThe following is a bit redundant but we can check if our \"Broad St\" point matches only one point in our data using geo_equals\n\nsubway_stations.filter(subway_stations.geom.geo_equals(broad_station.geom))\n\n┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ OBJECTID ┃ ID      ┃ NAME     ┃ ALT_NAME ┃ CROSS_ST ┃ LONG_NAME                  ┃ LABEL            ┃ BOROUGH   ┃ NGHBHD ┃ ROUTES ┃ TRANSFERS ┃ COLOR  ┃ EXPRESS ┃ CLOSED ┃ geom                             ┃\n┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64  │ float64 │ string   │ string   │ string   │ string                     │ string           │ string    │ string │ string │ string    │ string │ string  │ string │ geospatial:geometry              │\n├──────────┼─────────┼──────────┼──────────┼──────────┼────────────────────────────┼──────────────────┼───────────┼────────┼────────┼───────────┼────────┼─────────┼────────┼──────────────────────────────────┤\n│    332.0 │   304.0 │ Broad St │ NULL     │ Wall St  │ Broad St (J,M,Z) Manhattan │ Broad St (J,M,Z) │ Manhattan │ NULL   │ J,M,Z  │ J,M,Z     │ BROWN  │ express │ NULL   │ &lt;POINT (583571.906 4506714.341)&gt; │\n└──────────┴─────────┴──────────┴──────────┴──────────┴────────────────────────────┴──────────────────┴───────────┴────────┴────────┴───────────┴────────┴─────────┴────────┴──────────────────────────────────┘\n\n\n\nWe can also write this query without using broad_station as a variable, and with the help of the deferred expressions API, also known as the underscore API.\n\nsubway_stations.filter(_.geom.geo_equals(_.filter(_.NAME == \"Broad St\").geom))\n\n┏━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ OBJECTID ┃ ID      ┃ NAME     ┃ ALT_NAME ┃ CROSS_ST ┃ LONG_NAME                  ┃ LABEL            ┃ BOROUGH   ┃ NGHBHD ┃ ROUTES ┃ TRANSFERS ┃ COLOR  ┃ EXPRESS ┃ CLOSED ┃ geom                             ┃\n┡━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64  │ float64 │ string   │ string   │ string   │ string                     │ string           │ string    │ string │ string │ string    │ string │ string  │ string │ geospatial:geometry              │\n├──────────┼─────────┼──────────┼──────────┼──────────┼────────────────────────────┼──────────────────┼───────────┼────────┼────────┼───────────┼────────┼─────────┼────────┼──────────────────────────────────┤\n│    332.0 │   304.0 │ Broad St │ NULL     │ Wall St  │ Broad St (J,M,Z) Manhattan │ Broad St (J,M,Z) │ Manhattan │ NULL   │ J,M,Z  │ J,M,Z     │ BROWN  │ express │ NULL   │ &lt;POINT (583571.906 4506714.341)&gt; │\n└──────────┴─────────┴──────────┴──────────┴──────────┴────────────────────────────┴──────────────────┴───────────┴────────┴────────┴───────────┴────────┴─────────┴────────┴──────────────────────────────────┘\n\n\n\n\n\nintersect (ST_Intersect)\nLet’s locate the neighborhood of the “Broad Street” subway station using the geospatial intersect function. The intersect function returns True if two geometries have any points in common.\n\nboroughs = con.table(\"nyc_neighborhoods\")\nboroughs\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ BORONAME      ┃ NAME                     ┃ geom                                                                             ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string        │ string                   │ geospatial:geometry                                                              │\n├───────────────┼──────────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n│ Brooklyn      │ Bensonhurst              │ &lt;MULTIPOLYGON (((582771.426 4495167.427, 584651.294 4497541.643, 585422.281 ...&gt; │\n│ Manhattan     │ East Village             │ &lt;MULTIPOLYGON (((585508.753 4509691.267, 586826.357 4508984.188, 586726.827 ...&gt; │\n│ Manhattan     │ West Village             │ &lt;MULTIPOLYGON (((583263.278 4509242.626, 583276.82 4509378.825, 583473.971 4...&gt; │\n│ The Bronx     │ Throggs Neck             │ &lt;MULTIPOLYGON (((597640.009 4520272.72, 597647.746 4520617.824, 597805.462 4...&gt; │\n│ The Bronx     │ Wakefield-Williamsbridge │ &lt;MULTIPOLYGON (((595285.205 4525938.798, 595348.545 4526158.777, 595672 4527...&gt; │\n│ Queens        │ Auburndale               │ &lt;MULTIPOLYGON (((600973.009 4510338.857, 601002.162 4510743.044, 601131.315 ...&gt; │\n│ Manhattan     │ Battery Park             │ &lt;MULTIPOLYGON (((583408.101 4508093.111, 583356.048 4507665.145, 583260.947 ...&gt; │\n│ Manhattan     │ Carnegie Hill            │ &lt;MULTIPOLYGON (((588501.208 4515525.88, 588125.03 4514806.77, 587702.963 451...&gt; │\n│ Staten Island │ Mariners Harbor          │ &lt;MULTIPOLYGON (((570300.108 4497031.156, 570393.836 4497227.426, 570478.075 ...&gt; │\n│ Staten Island │ Rossville                │ &lt;MULTIPOLYGON (((564664.957 4489358.427, 564771.457 4489415.865, 564783.746 ...&gt; │\n│ …             │ …                        │ …                                                                                │\n└───────────────┴──────────────────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\nboroughs.filter(boroughs.geom.intersects(broad_station.select(broad_station.geom).to_array()))\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ BORONAME  ┃ NAME               ┃ geom                                                                             ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string             │ geospatial:geometry                                                              │\n├───────────┼────────────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n│ Manhattan │ Financial District │ &lt;MULTIPOLYGON (((583356.048 4507665.145, 583505.038 4507562.36, 583828.604 4...&gt; │\n└───────────┴────────────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\nd_within (ST_DWithin)\nWe can also find the streets near (say, within 10 meters) the Broad St subway station using the d_within function. The d_within function returns True if the geometries are within a given distance.\n\nstreets = con.table(\"nyc_streets\")\nstreets\n\n┏━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ID    ┃ NAME            ┃ ONEWAY ┃ TYPE          ┃ geom                                                                             ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int32 │ string          │ string │ string        │ geospatial:geometry                                                              │\n├───────┼─────────────────┼────────┼───────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n│     1 │ Shore Pky S     │ NULL   │ residential   │ &lt;MULTILINESTRING ((586785.477 4492901.001, 586898.232 4492943.725, 587118.98...&gt; │\n│     2 │ NULL            │ NULL   │ footway       │ &lt;MULTILINESTRING ((586645.007 4504977.75, 586664.225 4504988.544, 586672.151...&gt; │\n│     3 │ Avenue O        │ NULL   │ residential   │ &lt;MULTILINESTRING ((586750.302 4496109.722, 586837.373 4496123.393, 586929.08...&gt; │\n│     4 │ Walsh Ct        │ NULL   │ residential   │ &lt;MULTILINESTRING ((586728.696 4497971.053, 586886.358 4498000.536))&gt;             │\n│     5 │ NULL            │ NULL   │ motorway_link │ &lt;MULTILINESTRING ((586587.053 4510088.25, 586641.734 4510156.835))&gt;              │\n│     6 │ Avenue Z        │ NULL   │ residential   │ &lt;MULTILINESTRING ((586792.159 4493279.322, 586978.534 4493308.473, 587056.76...&gt; │\n│     7 │ Dank Ct         │ NULL   │ residential   │ &lt;MULTILINESTRING ((586794.754 4493361.729, 586966.095 4493387.928))&gt;             │\n│     8 │ Cumberland Walk │ NULL   │ footway       │ &lt;MULTILINESTRING ((586657.468 4505324.904, 586692.489 4505320.983, 586707.76...&gt; │\n│     9 │ Cumberland Walk │ NULL   │ footway       │ &lt;MULTILINESTRING ((586670.712 4505521.567, 586667.915 4505500.551, 586657.46...&gt; │\n│    10 │ NULL            │ NULL   │ residential   │ &lt;MULTILINESTRING ((586598.326 4510424.446, 586602.314 4510430.044, 586604.94...&gt; │\n│     … │ …               │ …      │ …             │ …                                                                                │\n└───────┴─────────────────┴────────┴───────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\nUsing the deferred API, we can check which streets are within d=10 meters of distance.\n\nsts_near_broad = streets.filter(_.geom.d_within(broad_station.select(_.geom).to_array(), 10))\nsts_near_broad\n\n┏━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ID    ┃ NAME      ┃ ONEWAY ┃ TYPE         ┃ geom                                                                             ┃\n┡━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int32 │ string    │ string │ string       │ geospatial:geometry                                                              │\n├───────┼───────────┼────────┼──────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n│ 17394 │ Wall St   │ NULL   │ unclassified │ &lt;MULTILINESTRING ((583483.954 4506785.646, 583522.11 4506758.431, 583571.509...&gt; │\n│ 17399 │ Broad St  │ NULL   │ unclassified │ &lt;MULTILINESTRING ((583571.509 4506715.578, 583529.136 4506622.066, 583509.85...&gt; │\n│ 17445 │ Nassau St │ NULL   │ unclassified │ &lt;MULTILINESTRING ((583571.509 4506715.578, 583610.912 4506780.181, 583641.80...&gt; │\n└───────┴───────────┴────────┴──────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn the previous query, streets and broad_station are different tables. We use to_array() to generate a scalar subquery from a table with a single column (whose shape is scalar).\n\n\nTo visualize the findings, we will convert the tables to GeoPandas DataFrames.\n\nbroad_station_gdf = broad_station.to_pandas()\nbroad_station_gdf.crs = \"EPSG:26918\"\n\nsts_near_broad_gdf = sts_near_broad.to_pandas()\nsts_near_broad_gdf.crs = \"EPSG:26918\"\n\nstreets_gdf = streets.to_pandas()\nstreets_gdf.crs = \"EPSG:26918\"\n\n\n1import leafmap.deckgl as leafmap\n\n\n1\n\nleafmap.deckgl allows us to visualize multiple layers\n\n\n\n\n\nm = leafmap.Map()\n\nm.add_vector(broad_station_gdf, get_fill_color=\"blue\")\nm.add_vector(sts_near_broad_gdf, get_color=\"red\", opacity=0.5)\nm.add_vector(streets_gdf, get_color=\"grey\", zoom_to_layer=False, opacity=0.3)\nm\n\n\n\n\nYou can zoom in and out, and hover over the map to check on the street names.\n\n\nbuffer (ST_Buffer)\nNext, we’ll take a look at the homicides table and showcase some additional functionality related to polygon handling.\n\nhomicides = con.table(\"nyc_homicides\")\nhomicides\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ INCIDENT_D ┃ BORONAME      ┃ NUM_VICTIM ┃ PRIMARY_MO ┃ ID    ┃ WEAPON ┃ LIGHT_DARK ┃ YEAR  ┃ geom                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ date       │ string        │ string     │ string     │ int32 │ string │ string     │ int32 │ geospatial:geometry              │\n├────────────┼───────────────┼────────────┼────────────┼───────┼────────┼────────────┼───────┼──────────────────────────────────┤\n│ 2008-01-01 │ Brooklyn      │ 1          │ NULL       │     7 │ gun    │ D          │  2008 │ &lt;POINT (592158.666 4502210.892)&gt; │\n│ 2008-01-04 │ Manhattan     │ 1          │ NULL       │    14 │ gun    │ D          │  2008 │ &lt;POINT (588654.952 4517855.383)&gt; │\n│ 2008-01-05 │ Queens        │ 1          │ NULL       │    15 │ gun    │ D          │  2008 │ &lt;POINT (605800.815 4505730.608)&gt; │\n│ 2008-01-04 │ Queens        │ 1          │ NULL       │    16 │ knife  │ D          │  2008 │ &lt;POINT (594255.157 4512250.378)&gt; │\n│ 2008-01-05 │ Queens        │ 1          │ NULL       │    18 │ gun    │ D          │  2008 │ &lt;POINT (605498.135 4496052.64)&gt;  │\n│ 2008-01-07 │ Brooklyn      │ 1          │ NULL       │    20 │ gun    │ D          │  2008 │ &lt;POINT (592020.999 4505733.647)&gt; │\n│ 2008-01-10 │ Manhattan     │ 1          │ NULL       │    22 │ gun    │ D          │  2008 │ &lt;POINT (584055.518 4511774.724)&gt; │\n│ 2008-01-10 │ Manhattan     │ 1          │ NULL       │    23 │ gun    │ D          │  2008 │ &lt;POINT (587283.748 4516908.39)&gt;  │\n│ 2008-01-13 │ Staten Island │ 1          │ NULL       │    25 │ gun    │ D          │  2008 │ &lt;POINT (570593.125 4498222.78)&gt;  │\n│ 2008-01-16 │ Queens        │ 1          │ NULL       │    27 │ gun    │ D          │  2008 │ &lt;POINT (607385.969 4501506.717)&gt; │\n│ …          │ …             │ …          │ …          │     … │ …      │ …          │     … │ …                                │\n└────────────┴───────────────┴────────────┴────────────┴───────┴────────┴────────────┴───────┴──────────────────────────────────┘\n\n\n\nLet’s use the buffer method to find homicides near our \"Broad St\" station point.\nThe buffer method computes a polygon or multipolygon that represents all points whose distance from a geometry is less than or equal to a given distance.\n\nbroad_station.geom.buffer(200)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ GeoBuffer(geom, 200.0)                                                           ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ geospatial:geometry                                                              │\n├──────────────────────────────────────────────────────────────────────────────────┤\n│ &lt;POLYGON ((583771.906 4506714.341, 583768.063 4506675.323, 583756.682 450663...&gt; │\n└──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\nWe can check the area using the area (ST_Area) function, and see that is \\(~ \\pi r^{2}=125664\\)\n\nbroad_station.geom.buffer(200).area()\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ GeoArea(GeoBuffer(geom, 200.0)) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ float64                         │\n├─────────────────────────────────┤\n│                    124857.80609 │\n└─────────────────────────────────┘\n\n\n\nTo find if there were any homicides in that area, we can find where the polygon resulting from adding the 200 meters buffer to our “Broad St” station point intersects with the geometry column in our homicides table.\n\nh_near_broad = homicides.filter(_.geom.intersects(broad_station.select(_.geom.buffer(200)).to_array()))\nh_near_broad\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ INCIDENT_D ┃ BORONAME  ┃ NUM_VICTIM ┃ PRIMARY_MO ┃ ID    ┃ WEAPON ┃ LIGHT_DARK ┃ YEAR  ┃ geom                             ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ date       │ string    │ string     │ string     │ int32 │ string │ string     │ int32 │ geospatial:geometry              │\n├────────────┼───────────┼────────────┼────────────┼───────┼────────┼────────────┼───────┼──────────────────────────────────┤\n│ 2009-07-07 │ Manhattan │ 1          │ NULL       │  3544 │ NULL   │ NULL       │  2009 │ &lt;POINT (583443.249 4506757.877)&gt; │\n└────────────┴───────────┴────────────┴────────────┴───────┴────────┴────────────┴───────┴──────────────────────────────────┘\n\n\n\nIt looks like there was one homicide within 200 meters from the “Broad St” station, but from this data we can’t tell the street near which it happened. However, we can check if the homicide point is within a small distance of a street.\n\nh_street = streets.filter(_.geom.d_within(h_near_broad.select(_.geom).to_array(), 2))\nh_street\n\n┏━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ID    ┃ NAME      ┃ ONEWAY ┃ TYPE         ┃ geom                                                                             ┃\n┡━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int32 │ string    │ string │ string       │ geospatial:geometry                                                              │\n├───────┼───────────┼────────┼──────────────┼──────────────────────────────────────────────────────────────────────────────────┤\n│ 17296 │ Rector St │ yes    │ unclassified │ &lt;MULTILINESTRING ((583184.691 4506868.803, 583257.066 4506835.054, 583324.11...&gt; │\n└───────┴───────────┴────────┴──────────────┴──────────────────────────────────────────────────────────────────────────────────┘\n\n\n\nLet’s plot this:\n\nbroad_station_zone = broad_station.mutate(geom=broad_station.geom.buffer(200))\nbroad_station_zone = broad_station_zone.to_pandas()\nbroad_station_zone.crs = \"EPSG:26918\"\n\nh_near_broad_gdf = h_near_broad.to_pandas()\nh_near_broad_gdf.crs = \"EPSG:26918\"\n\nh_street_gdf = h_street.to_pandas()\nh_street_gdf.crs = \"EPSG:26918\"\n\n\nmh = leafmap.Map()\nmh.add_vector(broad_station_gdf, get_fill_color=\"orange\")\nmh.add_vector(broad_station_zone, get_fill_color=\"orange\", opacity=0.1)\nmh.add_vector(h_near_broad_gdf, get_fill_color=\"red\", opacity=0.5)\nmh.add_vector(h_street_gdf, get_color=\"blue\", opacity=0.3)\nmh.add_vector(streets_gdf, get_color=\"grey\", zoom_to_layer=False, opacity=0.2)\n\nmh"
  },
  {
    "objectID": "posts/ibis-duckdb-geospatial/index.html#functions-supported-and-next-steps",
    "href": "posts/ibis-duckdb-geospatial/index.html#functions-supported-and-next-steps",
    "title": "Ibis + DuckDB geospatial: a match made on Earth",
    "section": "Functions supported and next steps",
    "text": "Functions supported and next steps\nAt the moment in Ibis we have support for around thirty geospatial functions in DuckDB and we will add some more (see list here).\nWe also support reading multiple geospatial formats via read_geo().\nHere are some resources to learn more about Ibis:\n\nIbis Docs\nIbis GitHub\n\nChat with us on Zulip:\n\nIbis Zulip Chat"
  },
  {
    "objectID": "posts/ibis-to-file/index.html",
    "href": "posts/ibis-to-file/index.html",
    "title": "Ibis sneak peek: writing to files",
    "section": "",
    "text": "Ibis 5.0 is coming soon and will offer new functionality and fixes to users. To enhance clarity around this process, we’re sharing a sneak peek into what we’re working on.\nIn Ibis 4.0, we added the ability to read CSVs and Parquet via the Ibis interface. We felt this was important because, well, the ability to read files is simply necessary, be it on a local scale, legacy data, data not yet in a database, and so on. However, for a user, the natural next question was “can I go ahead and write when I’m done?” The answer was no. We didn’t like that, especially since we do care about file-based use cases.\nSo, we’ve gone ahead and fixed that for Ibis 5.0."
  },
  {
    "objectID": "posts/ibis-to-file/index.html#files-in-files-out",
    "href": "posts/ibis-to-file/index.html#files-in-files-out",
    "title": "Ibis sneak peek: writing to files",
    "section": "Files in, Files out",
    "text": "Files in, Files out\nBefore we can write a file, we need data — so let’s read in a file, to start this off:\n\nimport ibis\n\nibis.options.interactive = True\n\nt = ibis.read_csv(\n    \"https://storage.googleapis.com/ibis-examples/data/penguins.csv.gz\"\n)\nt\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │            nan │           nan │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nOf course, we could just write out, but let’s do an operation first — how about using selectors, which you can read more about here? Self-promotion aside, here’s an operation:\n\nfrom ibis import _\nimport ibis.selectors as s\n\nexpr = (\n    t.group_by(\"species\")\n     .mutate(s.across(s.numeric() & ~s.c(\"year\"), (_ - _.mean()) / _.std()))\n)\nexpr\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ float64           │ float64     │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Biscoe    │      -1.423513 │     -0.366874 │          0.312925 │    0.053074 │ female │  2009 │\n│ Adelie  │ Torgersen │       0.979426 │      0.126283 │          1.842104 │    0.380180 │ male   │  2009 │\n│ Adelie  │ Torgersen │       1.542615 │     -0.613453 │          0.924596 │    2.179266 │ male   │  2008 │\n│ Adelie  │ Biscoe    │       0.641513 │     -0.366874 │         -0.451665 │   -1.091799 │ female │  2007 │\n│ Adelie  │ Biscoe    │      -0.222043 │      1.359177 │          0.007089 │    0.434698 │ male   │  2009 │\n│ Adelie  │ Torgersen │       1.392432 │      1.934527 │          1.077514 │    1.743124 │ male   │  2007 │\n│ Adelie  │ Torgersen │       1.129610 │      0.866019 │          1.230432 │    1.634089 │ male   │  2008 │\n│ Adelie  │ Dream     │      -0.747686 │      0.126283 │          0.465843 │   -0.437586 │ female │  2009 │\n│ Adelie  │ Dream     │      -0.860324 │     -0.284681 │         -1.216254 │   -1.200835 │ female │  2007 │\n│ Adelie  │ Dream     │       0.754151 │      0.044090 │          0.771678 │    0.434698 │ male   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nNow, finally, time to do the exciting part:\n\nexpr.to_parquet(\"normalized.parquet\")\n\nLike many things in Ibis, this is as simple and plain-looking as it is important. Being able to create files from Ibis instead of redirecting into other libraries first enables operation at larger scales and fewer steps. Where desired, you can address a backend directly to use its native export functionality — we want to make sure you have the flexibility to use Ibis or the backend as you see fit."
  },
  {
    "objectID": "posts/ibis-to-file/index.html#wrapping-up",
    "href": "posts/ibis-to-file/index.html#wrapping-up",
    "title": "Ibis sneak peek: writing to files",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIbis is an interface tool for analytical engines that can reach scales far beyond a laptop. Files are important to Ibis because:\n\nIbis also supports local execution, where files are the standard unit of data — we want to support all our users.\nFiles are useful for moving between platforms, and long-term storage that isn’t tied to a particular backend.\nFiles can move more easily between our backends than database files, so we think this adds some convenience for the multi-backend use case.\n\nWe’re excited to release this functionality in Ibis 5.0.\nInterested in Ibis? Docs are available on this very website, at:\n\nIbis Docs\n\nand the repo is always at:\n\nIbis GitHub\n\nPlease feel free to reach out on GitHub!"
  },
  {
    "objectID": "posts/pydata-performance/index.html",
    "href": "posts/pydata-performance/index.html",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "",
    "text": "TL; DR: Ibis has a lot of great backends. They’re all good at different things. For working with local data, it’s hard to beat DuckDB on feature set and performance.\nBuckle up, it’s going to be a long one."
  },
  {
    "objectID": "posts/pydata-performance/index.html#motivation",
    "href": "posts/pydata-performance/index.html#motivation",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Motivation",
    "text": "Motivation\nIbis maintainer Gil Forsyth recently wrote a post on our blog replicating another blog post but using Ibis instead of raw SQL.\nI thought it would be interesting to see how other tools compare to this setup, so I decided I’d try to do the same workflow on the same machine using a few tools from across the ecosystem.\nI chose two incumbents–pandas and dask–to see how they compare to Ibis + DuckDB on this workload. In part 2 of this series I will compare two newer engines–Polars and DataFusion–to Ibis + DuckDB.\nI’ve worked on both pandas and Dask in the past but it’s been such a long time since I’ve used these tools for data analysis that I consider myself rather naive about how to best use them today.\nInitially I was interested in API comparisons since usability is really where Ibis shines, but as I started to explore things, I was unable to complete my analysis in some cases due to running out of memory.\n\n\n\n\n\n\nThis is not a forum to trash the work of others.\n\n\n\nI’m not interested in tearing down other tools.\nIbis has backends for each of these tools and it’s in everyone’s best interest that all of the tools discussed here work to their full potential.\n\n\nI show each tool using its native API, in an attempt to compare ease-of-use out of the box and maximize each library’s ability to complete the workload.\nLet’s dig in."
  },
  {
    "objectID": "posts/pydata-performance/index.html#setup",
    "href": "posts/pydata-performance/index.html#setup",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Setup",
    "text": "Setup\nI ran all of the code in this blog post on a machine with these specs.\nAll OS caches were cleared before running this document with\n$ sudo sysctl -w vm.drop_caches=3\n\n\n\n\n\n\nClearing operating system caches does not represent a realistic usage scenario\n\n\n\nIt is a method for putting the tools here on more equal footing. When you’re in the thick of an analysis you’re not going to artificially limit any OS optimizations.\n\n\n\n\n\nComponent\nSpecification\n\n\n\n\nCPU\nAMD EPYC 7B12 (64 threads)\n\n\nRAM\n94 GiB\n\n\nDisk\n1.5 TiB SSD\n\n\nOS\nNixOS (Linux 6.1.68)\n\n\n\n\nSoft constraints\nI’ll introduce some soft UX constraints on the problem, that I think help convey the perspective of someone who wants to get started quickly with a data set:\n\nI don’t want to get another computer to run this workload.\nI want to use the data as is, that is, without altering the files I already have.\nI’d like to run this computation with the default configuration. Ideally configuration isn’t required to complete this workload out of the box.\n\n\n\nLibrary versions\nHere are the versions I used to run this experiment at the time of writing.\n\n\n\nDependency\nVersion\n\n\n\n\nPython\n3.10.13 (main, Aug 24 2023, 12:59:26) [GCC 12.3.0]\n\n\ndask\n2023.12.0\n\n\ndistributed\n2023.12.0\n\n\nduckdb\n0.9.2\n\n\nibis\n2d37ae816\n\n\npandas\n2.1.4\n\n\npyarrow\n14.0.1\n\n\n\n\n\nData\nI used the files here in this link to run my experiment.\nHere’s a summary of the data set’s file sizes:\n$ du -h /data/pypi-parquet/*.parquet\n1.8G    /data/pypi-parquet/index-12.parquet\n1.7G    /data/pypi-parquet/index-10.parquet\n1.9G    /data/pypi-parquet/index-2.parquet\n1.9G    /data/pypi-parquet/index-0.parquet\n1.8G    /data/pypi-parquet/index-5.parquet\n1.7G    /data/pypi-parquet/index-13.parquet\n1.7G    /data/pypi-parquet/index-9.parquet\n1.8G    /data/pypi-parquet/index-6.parquet\n1.7G    /data/pypi-parquet/index-7.parquet\n1.7G    /data/pypi-parquet/index-8.parquet\n800M    /data/pypi-parquet/index-14.parquet\n1.8G    /data/pypi-parquet/index-4.parquet\n1.8G    /data/pypi-parquet/index-11.parquet\n1.9G    /data/pypi-parquet/index-3.parquet\n1.9G    /data/pypi-parquet/index-1.parquet"
  },
  {
    "objectID": "posts/pydata-performance/index.html#recapping-the-original-ibis-post",
    "href": "posts/pydata-performance/index.html#recapping-the-original-ibis-post",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Recapping the original Ibis post",
    "text": "Recapping the original Ibis post\nCheck out the original blog post if you haven’t already!\nHere’s the Ibis + DuckDB code, along with a timed execution of the query:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _, udf\n\n\n@udf.scalar.builtin\n1def flatten(x: list[list[str]]) -&gt; list[str]:\n    ...\n\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .order_by(_.month.desc())\n    .mutate(\n        ext=_.ext.re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .group_by([\"month\", \"ext\"])\n    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n    .dropna(\"ext\")\n2    .order_by([_.month.desc(), _.project_count.desc()])\n)\n\n1\n\nWe’ve since implemented a flatten method on array expressions so it’s no longer necessary to define a UDF here. I’ll leave this code unchanged for this post. This has no effect on the performance of the query. In both cases the generated code contains a DuckDB-native call to its flatten function.\n\n2\n\nThis is a small change from the original query that adds a final sort key to make the results deterministic.\n\n\n\n%time df = expr.to_pandas()\ndf\n\n\n\n\nCPU times: user 20min 46s, sys: 1min 10s, total: 21min 56s\nWall time: 28.5 s\n\n\n\n\n\n\n\n\n\nmonth\next\nproject_count\n\n\n\n\n0\n2023-11-01\nC/C++\n836\n\n\n1\n2023-11-01\nRust\n190\n\n\n2\n2023-11-01\nFortran\n48\n\n\n3\n2023-11-01\nGo\n33\n\n\n4\n2023-11-01\nAssembly\n10\n\n\n...\n...\n...\n...\n\n\n794\n2005-08-01\nC/C++\n7\n\n\n795\n2005-07-01\nC/C++\n4\n\n\n796\n2005-05-01\nC/C++\n1\n\n\n797\n2005-04-01\nC/C++\n1\n\n\n798\n2005-03-01\nC/C++\n1\n\n\n\n\n799 rows × 3 columns\n\n\n\nLet’s show peak memory usage in GB as reported by the resource module:\n\nimport resource\n\nrss_kb = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\nrss_mb = rss_kb / 1e3\nrss_gb = rss_mb / 1e3\n\nprint(round(rss_gb, 1), \"GB\")\n\n8.6 GB"
  },
  {
    "objectID": "posts/pydata-performance/index.html#pandas",
    "href": "posts/pydata-performance/index.html#pandas",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Pandas",
    "text": "Pandas\nLet’s try to replicate this workflow using pandas.\nI started with this code:\n\nimport pandas as pd\n\ndf = pd.read_parquet(\"/data/pypi-parquet/*.parquet\")\n\nFileNotFoundError: [Errno 2] No such file or directory: '/data/pypi-parquet/*.parquet'\n\n\nLooks like pandas doesn’t support globs. That’s fine, we can use the builtin glob module.\nimport glob\n\ndf = pd.read_parquet(glob.glob(\"/data/pypi-parquet/*.parquet\"))\nThis eventually triggers the Linux OOM killer after some minutes, so I can’t run the code.\nLet’s try again with just a single file. I’ll pick the smallest file, to avoid any potential issues with memory and give pandas the best possible shot.\n\nimport os\n\nsmallest_file = min(glob.glob(\"/data/pypi-parquet/*.parquet\"), key=os.path.getsize)\n\nThe smallest file is 799 MiB on disk.\n\n%time df = pd.read_parquet(smallest_file)\ndf\n\nCPU times: user 26 s, sys: 13.3 s, total: 39.3 s\nWall time: 27 s\n\n\n\n\n\n\n\n\n\nproject_name\nproject_version\nproject_release\nuploaded_on\npath\narchive_path\nsize\nhash\nskip_reason\nlines\nrepository\n\n\n\n\n0\nzyte-spider-templates\n0.1.0\nzyte_spider_templates-0.1.0-py3-none-any.whl\n2023-10-26 07:29:49.894\npackages/zyte-spider-templates/zyte_spider_tem...\nzyte_spider_templates/spiders/ecommerce.py\n5748\nb'\\xe0\\xa6\\x9bd\\xc0+\\xe0\\xf8$J2\\xb3\\xf8\\x8c\\x9...\n\n160\n237\n\n\n1\nzyte-spider-templates\n0.1.0\nzyte_spider_templates-0.1.0-py3-none-any.whl\n2023-10-26 07:29:49.894\npackages/zyte-spider-templates/zyte_spider_tem...\nzyte_spider_templates/spiders/base.py\n4160\nb'\\x1ck\\xd46={\\x7f`\\xbe\\xfaIg*&\\x977T\\xdb\\x8fJ'\n\n122\n237\n\n\n2\nzyte-spider-templates\n0.1.0\nzyte_spider_templates-0.1.0-py3-none-any.whl\n2023-10-26 07:29:49.894\npackages/zyte-spider-templates/zyte_spider_tem...\nzyte_spider_templates/spiders/__init__.py\n0\nb'\\xe6\\x9d\\xe2\\x9b\\xb2\\xd1\\xd6CK\\x8b)\\xaewZ\\xd...\nempty\n0\n237\n\n\n3\nzyte-spider-templates\n0.1.0\nzyte_spider_templates-0.1.0-py3-none-any.whl\n2023-10-26 07:29:49.894\npackages/zyte-spider-templates/zyte_spider_tem...\nzyte_spider_templates/page_objects/product_nav...\n3528\nb'\\xcd\\xc9\\xfc[\\xda\\xcf!\\x94\\x1b\\x92\\xffbJC\\xf...\n\n106\n237\n\n\n4\nzyte-spider-templates\n0.1.0\nzyte_spider_templates-0.1.0-py3-none-any.whl\n2023-10-26 07:29:49.894\npackages/zyte-spider-templates/zyte_spider_tem...\nzyte_spider_templates/page_objects/__init__.py\n75\nb'r\\xb9\\xc1\\xcf2\\xa7\\xdc?\\xd1\\xa8\\xfcc+`\\xf3\\x...\n\n1\n237\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n35468828\n1AH22CS174\n1.81.11\n1AH22CS174-1.81.11.tar.gz\n2023-11-19 13:30:00.113\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n1AH22CS174-1.81.11/1AH22CS174.egg-info/top_lev...\n1\nb\"\\x8b\\x13x\\x91y\\x1f\\xe9i'\\xadx\\xe6K\\n\\xad{\\xd...\n\n1\n242\n\n\n35468829\n1AH22CS174\n1.81.11\n1AH22CS174-1.81.11.tar.gz\n2023-11-19 13:30:00.113\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n1AH22CS174-1.81.11/1AH22CS174.egg-info/require...\n16\nb\"qG\\xad\\xc3:.'q\\xde\\xaa\\xac\\x91\\x89\\xf7S\\xcb\\...\n\n2\n242\n\n\n35468830\n1AH22CS174\n1.81.11\n1AH22CS174-1.81.11.tar.gz\n2023-11-19 13:30:00.113\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n1AH22CS174-1.81.11/1AH22CS174.egg-info/depende...\n1\nb\"\\x8b\\x13x\\x91y\\x1f\\xe9i'\\xadx\\xe6K\\n\\xad{\\xd...\n\n1\n242\n\n\n35468831\n1AH22CS174\n1.81.11\n1AH22CS174-1.81.11.tar.gz\n2023-11-19 13:30:00.113\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n1AH22CS174-1.81.11/1AH22CS174.egg-info/SOURCES...\n187\nb'\\xa2O$4|X\\x15,\\xb0\\x9a\\x07\\xe6\\x81[\\x15\\x1f|...\n\n7\n242\n\n\n35468832\n1AH22CS174\n1.81.11\n1AH22CS174-1.81.11.tar.gz\n2023-11-19 13:30:00.113\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n1AH22CS174-1.81.11/1AH22CS174.egg-info/PKG-INFO\n509\nb'\\xee\\xbe\\xbaoh*\\xacA\\xb0\\x8a}\\xb5\\x00\\xcbpz\\...\n\n16\n242\n\n\n\n\n35468833 rows × 11 columns\n\n\n\nLoading the smallest file from the dataset is already pretty close to the time it took Ibis and DuckDB to execute the entire query.\nLet’s give pandas a leg up and tell it what columns to use to avoid reading in a bunch of data we’re not going to use.\nWe can determine what these columns are by inspecting the Ibis code above.\n\ncolumns = [\"path\", \"uploaded_on\", \"project_name\"]\n\n%time df = pd.read_parquet(smallest_file, columns=columns)\ndf\n\nCPU times: user 13.9 s, sys: 7.09 s, total: 21 s\nWall time: 16.9 s\n\n\n\n\n\n\n\n\n\npath\nuploaded_on\nproject_name\n\n\n\n\n0\npackages/zyte-spider-templates/zyte_spider_tem...\n2023-10-26 07:29:49.894\nzyte-spider-templates\n\n\n1\npackages/zyte-spider-templates/zyte_spider_tem...\n2023-10-26 07:29:49.894\nzyte-spider-templates\n\n\n2\npackages/zyte-spider-templates/zyte_spider_tem...\n2023-10-26 07:29:49.894\nzyte-spider-templates\n\n\n3\npackages/zyte-spider-templates/zyte_spider_tem...\n2023-10-26 07:29:49.894\nzyte-spider-templates\n\n\n4\npackages/zyte-spider-templates/zyte_spider_tem...\n2023-10-26 07:29:49.894\nzyte-spider-templates\n\n\n...\n...\n...\n...\n\n\n35468828\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n2023-11-19 13:30:00.113\n1AH22CS174\n\n\n35468829\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n2023-11-19 13:30:00.113\n1AH22CS174\n\n\n35468830\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n2023-11-19 13:30:00.113\n1AH22CS174\n\n\n35468831\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n2023-11-19 13:30:00.113\n1AH22CS174\n\n\n35468832\npackages/1AH22CS174/1AH22CS174-1.81.11.tar.gz/...\n2023-11-19 13:30:00.113\n1AH22CS174\n\n\n\n\n35468833 rows × 3 columns\n\n\n\nSweet, read times improved!\nLet’s peek at the memory usage of the DataFrame.\n\nprint(round(df.memory_usage(deep=True).sum() / (1 &lt;&lt; 30), 1), \"GiB\")\n\n8.7 GiB\n\n\nI still have plenty of space to do my analysis, nice!\nFirst, filter the data:\n\n%%time\ndf = df[\n    (\n        df.path.str.contains(r\"\\.(?:asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\")\n1        & ~df.path.str.contains(r\"(?:^|/)test(?:|s|ing)|/site-packages/\")\n    )\n]\ndf\n\n\n1\n\nI altered the original query here to avoid creating an unnecessary intermediate Series object.\n\n\n\n\nCPU times: user 2min 21s, sys: 297 ms, total: 2min 22s\nWall time: 2min 21s\n\n\n\n\n\n\n\n\n\npath\nuploaded_on\nproject_name\n\n\n\n\n1462\npackages/zipline-tej/zipline_tej-0.0.50-cp38-c...\n2023-10-27 02:23:07.153\nzipline-tej\n\n\n1470\npackages/zipline-tej/zipline_tej-0.0.50-cp38-c...\n2023-10-27 02:23:07.153\nzipline-tej\n\n\n1477\npackages/zipline-tej/zipline_tej-0.0.50-cp38-c...\n2023-10-27 02:23:07.153\nzipline-tej\n\n\n1481\npackages/zipline-tej/zipline_tej-0.0.50-cp38-c...\n2023-10-27 02:23:07.153\nzipline-tej\n\n\n1485\npackages/zipline-tej/zipline_tej-0.0.50-cp38-c...\n2023-10-27 02:23:07.153\nzipline-tej\n\n\n...\n...\n...\n...\n\n\n35460320\npackages/atomicshop/atomicshop-2.5.12-py3-none...\n2023-11-19 14:29:22.109\natomicshop\n\n\n35460515\npackages/atomicshop/atomicshop-2.5.11-py3-none...\n2023-11-19 11:58:09.589\natomicshop\n\n\n35460710\npackages/atomicshop/atomicshop-2.5.10-py3-none...\n2023-11-19 11:48:16.980\natomicshop\n\n\n35463761\npackages/ai-flow-nightly/ai_flow_nightly-2023....\n2023-11-19 16:06:36.819\nai-flow-nightly\n\n\n35464036\npackages/ai-flow-nightly/ai_flow_nightly-2023....\n2023-11-19 16:06:33.327\nai-flow-nightly\n\n\n\n\n7166291 rows × 3 columns\n\n\n\nWe’ve blown way past our Ibis + DuckDB latency budget.\nLet’s keep going!\nNext, group by and aggregate:\n\n%%time\ndf = (\n    df.groupby(\n        [\n            df.uploaded_on.dt.floor(\"M\").rename(\"month\"),\n            df.path.str.extract(r\"\\.([a-z0-9]+)$\", 0, expand=False).rename(\"ext\"),\n        ]\n    )\n    .agg({\"project_name\": lambda s: list(set(s))})\n    .sort_index(level=\"month\", ascending=False)\n)\ndf\n\nValueError: &lt;MonthEnd&gt; is a non-fixed frequency\n\n\nHere we hit the first API issue going back to an old pandas issue: we can’t truncate a timestamp column to month frequency.\nLet’s try the solution recommended in that issue.\n\n%%time\ndf = (\n    df.groupby(\n        [\n            df.uploaded_on.dt.to_period(\"M\").dt.to_timestamp().rename(\"month\"),\n            df.path.str.extract(r\"\\.([a-z0-9]+)$\", 0, expand=False).rename(\"ext\"),\n        ]\n    )\n    .agg({\"project_name\": lambda s: list(set(s))})\n    .rename(columns={\"project_name\": \"projects\"})\n    .sort_index(level=\"month\", ascending=False)\n)\ndf\n\nCPU times: user 8.14 s, sys: 189 ms, total: 8.32 s\nWall time: 8.33 s\n\n\n\n\n\n\n\n\n\n\nprojects\n\n\nmonth\next\n\n\n\n\n\n2023-11-01\nrs\n[qoqo-for-braket-devices, rouge-rs, h3ronpy, g...\n\n\nhpp\n[isotree, boutpp-nightly, fdasrsf, PlotPy, mod...\n\n\nh\n[numina, pyppmd, pantab, liftover, jupyter-cpp...\n\n\ngo\n[cppyy-cling, ascend-deployer, c2cciutils, aws...\n\n\nfor\n[iricore]\n\n\nf95\n[PyGeopack, easychem, dioptas, scikit-digital-...\n\n\nf90\n[iricore, pdfo, cosmosis, mkl-include, c4p, ml...\n\n\nf03\n[mkl-include]\n\n\nf\n[fastjet, pdfo, pyspharm-syl, PyAstronomy, PyG...\n\n\ncxx\n[cppyy-cling, teca, boutpp-nightly, aplr, CPyC...\n\n\ncpp\n[numina, liftover, jupyter-cpp-kernel, cuda-qu...\n\n\ncc\n[numina, boutpp-nightly, tiledb, pyxai, arcae,...\n\n\nc\n[cytimes, assemblyline, pyppmd, pantab, liftov...\n\n\nasm\n[couchbase, hrm-interpreter, cmeel-assimp, aws...\n\n\n2023-10-01\nrs\n[ruff, xvc, qarray-rust-core, del-msh, uniffi-...\n\n\nhpp\n[pycaracal, pylibrb, cripser, icupy, cylp, mai...\n\n\nh\n[pycaracal, icupy, memprocfs, mindspore-dev, w...\n\n\ngo\n[cryptography, c2cciutils, awscrt, odoo14-addo...\n\n\nf90\n[pypestutils, petitRADTRANS, alpaqa, molalignl...\n\n\nf\n[alpaqa, pestifer, gnssrefl, LightSim2Grid, od...\n\n\ncxx\n[cars, wxPython-zombie, AnalysisG, petsc, fift...\n\n\ncpp\n[pycaracal, roboflex.util.png, reynir, ParmEd,...\n\n\ncc\n[tf-nightly-cpu-aws, cornflakes, trajgenpy, tf...\n\n\nc\n[pycaracal, cytimes, fibers-ddtest, assemblyli...\n\n\nasm\n[fibers-ddtest, maud-metabolic-models, chipsec...\n\n\n\n\n\n\n\nSort the values, add a new column and do the final aggregation:\n\n%%time\ndf = (\n    df.reset_index()\n    .assign(\n        ext=lambda t: t.ext.str.replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\", regex=True)\n        .str.replace(\"^f.*$\", \"Fortran\", regex=True)\n        .str.replace(\"rs\", \"Rust\")\n        .str.replace(\"go\", \"Go\")\n        .str.replace(\"asm\", \"Assembly\")\n        .replace(\"\", None)\n    )\n    .groupby([\"month\", \"ext\"])\n    .agg({\"projects\": lambda s: len(set(sum(s, [])))})\n)\ndf\n\nCPU times: user 4.96 ms, sys: 0 ns, total: 4.96 ms\nWall time: 4.81 ms\n\n\n\n\n\n\n\n\n\n\nprojects\n\n\nmonth\next\n\n\n\n\n\n2023-10-01\nAssembly\n14\n\n\nC/C++\n484\n\n\nFortran\n23\n\n\nGo\n25\n\n\nRust\n99\n\n\n2023-11-01\nAssembly\n10\n\n\nC/C++\n836\n\n\nFortran\n48\n\n\nGo\n33\n\n\nRust\n190\n\n\n\n\n\n\n\nRemember, all of the previous code is executing on a single file and still takes minutes to run.\n\nConclusion\nIf I only have pandas at my disposal, I’m unsure of how I can avoid getting a bigger computer to run this query over the entire data set.\n\n\nRewriting the query to be fair\nAt this point I wondered whether this was a fair query to run with pandas.\nAfter all, the downsides of pandas’ use of object arrays to hold nested data structures like lists are well-known.\nThe original query uses a lot of nested array types, which are very performant in DuckDB, but in this case we’re throwing away all of our arrays and we don’t need to use them.\nAdditionally, I’m using lambda functions instead of taking advantage of pandas’ fast built-in methods like count, nunique and others.\nLet’s see if we can alter the original query to give pandas a leg up.\n\nA story of two GROUP BYs\nHere’s the first Ibis expression:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _, udf\n\n\n@udf.scalar.builtin\ndef flatten(x: list[list[str]]) -&gt; list[str]:\n    ...\n\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .order_by(_.month.desc())\n    .mutate(\n        ext=_.ext.re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .group_by([\"month\", \"ext\"])\n    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n    .dropna(\"ext\")\n    .order_by([_.month.desc(), _.project_count.desc()])\n)\nIt looks like we can remove the double group_by by moving the second mutate expression directly into the first group_by call.\nApplying these changes:\n--- step0.py    2023-12-12 05:20:01.712513949 -0500\n+++ step1.py    2023-12-12 05:20:01.712513949 -0500\n@@ -5,7 +5,7 @@\n \n \n @udf.scalar.builtin\n-def flatten(x: list[list[str]]) -&gt; list[str]:  # &lt;1&gt;\n+def flatten(x: list[list[str]]) -&gt; list[str]:\n     ...\n \n \n@@ -22,20 +22,16 @@\n     )\n     .group_by(\n         month=_.uploaded_on.truncate(\"M\"),\n-        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1),\n-    )\n-    .aggregate(projects=_.project_name.collect().unique())\n-    .order_by(_.month.desc())\n-    .mutate(\n-        ext=_.ext.re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n+        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n+        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n         .re_replace(\"^f.*$\", \"Fortran\")\n         .replace(\"rs\", \"Rust\")\n         .replace(\"go\", \"Go\")\n         .replace(\"asm\", \"Assembly\")\n         .nullif(\"\"),\n     )\n+    .aggregate(projects=_.project_name.collect().unique())\n+    .order_by(_.month.desc())\n     .group_by([\"month\", \"ext\"])\n     .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n-    .dropna(\"ext\")\n-    .order_by([_.month.desc(), _.project_count.desc()])  # &lt;2&gt;\n )\nWe get:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _, udf\n\n\n@udf.scalar.builtin\ndef flatten(x: list[list[str]]) -&gt; list[str]:\n    ...\n\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .order_by(_.month.desc())\n    .group_by([\"month\", \"ext\"])\n    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n)\n\n\nDon’t sort unnecessarily\nNotice this order_by call just before a group_by call. Ordering before grouping is somewhat useless here; we should probably sort after we’ve reduced our data. Let’s stick the ordering at the end of the query.\nApplying these changes:\n--- step1.py    2023-12-12 05:20:01.712513949 -0500\n+++ step2.py    2023-12-12 05:20:01.712513949 -0500\n@@ -31,7 +31,7 @@\n         .nullif(\"\"),\n     )\n     .aggregate(projects=_.project_name.collect().unique())\n-    .order_by(_.month.desc())\n     .group_by([\"month\", \"ext\"])\n     .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n+    .order_by(_.month.desc())\n )\nWe get:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _, udf\n\n\n@udf.scalar.builtin\ndef flatten(x: list[list[str]]) -&gt; list[str]:\n    ...\n\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(projects=_.project_name.collect().unique())\n    .group_by([\"month\", \"ext\"])\n    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n    .order_by(_.month.desc())\n)\n\n\nDon’t repeat yourself\nNotice that we are now:\n\ngrouping\naggregating\ngrouping again by the same keys\naggregating\n\nThis is less optimal than it could be. We are also flattening an array, computing its distinct values and then computing its length.\nWe are computing the grouped number of distinct values, and we likely don’t need to collect values into an array to do that.\nLet’s try using a COUNT(DISTINCT ...) query instead, to avoid wasting cycles collecting arrays.\nWe’ll remove the second group by and then call nunique() to get the final query.\nApplying these changes:\n--- step2.py    2023-12-12 05:20:01.712513949 -0500\n+++ step3.py    2023-12-12 05:20:01.712513949 -0500\n@@ -1,13 +1,7 @@\n from __future__ import annotations\n \n import ibis\n-from ibis import _, udf\n-\n-\n-@udf.scalar.builtin\n-def flatten(x: list[list[str]]) -&gt; list[str]:\n-    ...\n-\n+from ibis import _\n \n expr = (\n     ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n@@ -30,8 +24,7 @@\n         .replace(\"asm\", \"Assembly\")\n         .nullif(\"\"),\n     )\n-    .aggregate(projects=_.project_name.collect().unique())\n-    .group_by([\"month\", \"ext\"])\n-    .aggregate(project_count=flatten(_.projects.collect()).unique().length())\n-    .order_by(_.month.desc())\n+    .aggregate(project_count=_.project_name.nunique())\n+    .dropna(\"ext\")\n+    .order_by([_.month.desc(), _.project_count.desc()])  # &lt;1&gt;\n )\nWe get:\nfrom __future__ import annotations\n\nimport ibis\nfrom ibis import _\n\nexpr = (\n    ibis.read_parquet(\"/data/pypi-parquet/*.parquet\")\n    .filter(\n        [\n            _.path.re_search(\n                r\"\\.(asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n            ),\n            ~_.path.re_search(r\"(^|/)test(|s|ing)\"),\n            ~_.path.contains(\"/site-packages/\"),\n        ]\n    )\n    .group_by(\n        month=_.uploaded_on.truncate(\"M\"),\n        ext=_.path.re_extract(r\"\\.([a-z0-9]+)$\", 1)\n        .re_replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\")\n        .re_replace(\"^f.*$\", \"Fortran\")\n        .replace(\"rs\", \"Rust\")\n        .replace(\"go\", \"Go\")\n        .replace(\"asm\", \"Assembly\")\n        .nullif(\"\"),\n    )\n    .aggregate(project_count=_.project_name.nunique())\n    .dropna(\"ext\")\n1    .order_by([_.month.desc(), _.project_count.desc()])\n)\n\n1\n\nI added a second sort key (project_count) for deterministic output.\n\n\nLet’s run it to make sure the results are as expected:\n\nduckdb_results = %timeit -n1 -r1 -o expr.to_pandas()\n\n\n\n\n31.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nIt looks like the new query might be a bit slower even though we’re ostensibly doing less computation. Since we’re still pretty close to the original duration, let’s keep going.\n\n\n\nFinal pandas run with the new query\nRewriting the pandas code we get:\nfrom __future__ import annotations\n\nimport glob\nimport os\n\nimport pandas as pd\n\ndf = pd.read_parquet(\n    min(glob.glob(\"/data/pypi-parquet/*.parquet\"), key=os.path.getsize),\n    columns=[\"path\", \"uploaded_on\", \"project_name\"],\n)\ndf = df[\n    df.path.str.contains(r\"\\.(?:asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\")\n    & ~df.path.str.contains(r\"(?:(?:^|/)test(?:|s|ing)|/site-packages/)\")\n]\nprint(\n    df.assign(\n        month=df.uploaded_on.dt.to_period(\"M\").dt.to_timestamp(),\n        ext=df.path.str.extract(r\"\\.([a-z0-9]+)$\", 0)\n        .iloc[:, 0]\n        .str.replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\", regex=True)\n        .str.replace(\"^f.*$\", \"Fortran\", regex=True)\n        .str.replace(\"rs\", \"Rust\")\n        .str.replace(\"go\", \"Go\")\n        .str.replace(\"asm\", \"Assembly\"),\n    )\n    .groupby([\"month\", \"ext\"])\n    .project_name.nunique()\n    .rename(\"project_count\")\n    .reset_index()\n    .sort_values([\"month\", \"project_count\"], ascending=False)\n)\nRunning it we get:\n\npandas_results = %timeit -n1 -r1 -o %run pandas_impl.py\n\n       month       ext  project_count\n6 2023-11-01     C/C++            836\n9 2023-11-01      Rust            190\n7 2023-11-01   Fortran             48\n8 2023-11-01        Go             33\n5 2023-11-01  Assembly             10\n1 2023-10-01     C/C++            484\n4 2023-10-01      Rust             99\n3 2023-10-01        Go             25\n2 2023-10-01   Fortran             23\n0 2023-10-01  Assembly             14\n3min 2s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\n\n\n\n\n\n\nRemember, this is the time it took pandas to run the query for a single file.\n\n\n\nDuckDB runs the query over the entire dataset about 4x faster than that!\n\n\nLet’s try a tool that nominally scales to our problem: Dask."
  },
  {
    "objectID": "posts/pydata-performance/index.html#dask",
    "href": "posts/pydata-performance/index.html#dask",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Dask",
    "text": "Dask\nOne really nice component of Dask is dask.dataframe.\nDask DataFrame implements a good chunk of the pandas API and can be a drop-in replacement for pandas.\nI am happy that this turned out to be the case here.\nMy first attempt was somewhat naive and was effectively a one line change from import pandas as pd to import dask.dataframe as pd.\nThis worked and the workload completed. However, after talking to Dask expert and Ibis contributor Naty Clementi she suggested I try a few things:\n\nUse the distributed scheduler.\nEnsure that pyarrow string arrays are used instead of NumPy object arrays. This required no changes to my Dask code because PyArrow strings have been the default since version 2023.7.1, hooray!\nExplore some of the options to read_parquet. It turned that without setting split_row_groups=True I ran out of memory.\n\nLet’s look at the Dask implementation:\nfrom __future__ import annotations\n\nimport logging\n\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\nif __name__ == \"__main__\":\n    client = Client(silence_logs=logging.ERROR)\n    df = dd.read_parquet(\n        \"/data/pypi-parquet/*.parquet\",\n        columns=[\"path\", \"uploaded_on\", \"project_name\"],\n        split_row_groups=True,\n    )\n    df = df[\n        df.path.str.contains(\n            r\"\\.(?:asm|c|cc|cpp|cxx|h|hpp|rs|[Ff][0-9]{0,2}(?:or)?|go)$\"\n        )\n        & ~df.path.str.contains(r\"(?:^|/)test(?:|s|ing)\")\n        & ~df.path.str.contains(\"/site-packages/\")\n    ]\n    print(\n        df.assign(\n            month=df.uploaded_on.dt.to_period(\"M\").dt.to_timestamp(),\n            ext=df.path.str.extract(r\"\\.([a-z0-9]+)$\", 0, expand=False)\n            .str.replace(r\"cxx|cpp|cc|c|hpp|h\", \"C/C++\", regex=True)\n            .str.replace(\"^f.*$\", \"Fortran\", regex=True)\n            .str.replace(\"rs\", \"Rust\")\n            .str.replace(\"go\", \"Go\")\n            .str.replace(\"asm\", \"Assembly\"),\n        )\n        .groupby([\"month\", \"ext\"])\n        .project_name.nunique()\n        .rename(\"project_count\")\n        .compute()\n        .reset_index()\n        .sort_values([\"month\", \"project_count\"], ascending=False)\n    )\n    client.shutdown()\nLet’s run the code:\n\ndask_results = %timeit -n1 -r1 -o %run dask_impl.py\n\n         month       ext  project_count\n794 2023-11-01     C/C++            836\n796 2023-11-01      Rust            190\n797 2023-11-01   Fortran             48\n795 2023-11-01        Go             33\n798 2023-11-01  Assembly             10\n..         ...       ...            ...\n2   2005-08-01     C/C++              7\n1   2005-07-01     C/C++              4\n83  2005-05-01     C/C++              1\n82  2005-04-01     C/C++              1\n0   2005-03-01     C/C++              1\n\n[799 rows x 3 columns]\n52.6 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n\n\nThat’s a great improvement over pandas: we finished the workload and our running time is pretty close to DuckDB."
  },
  {
    "objectID": "posts/pydata-performance/index.html#takeaways",
    "href": "posts/pydata-performance/index.html#takeaways",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Takeaways",
    "text": "Takeaways\nIbis + DuckDB is the only system tested that handles this workload well out of the box\n\nPandas couldn’t handle the workload due to memory constraints.\nDask required its recommended distributed scheduler to achieve maximum performance and still used a lot of memory.\n\nLet’s recap the results with some numbers:\n\nNumbers\n\n\n\nToolset\nData size\nDuration\nThroughput\n\n\n\n\nIbis + DuckDB\n25,825 MiB\n31 seconds\n823 MiB/s\n\n\nDask + Distributed\n25,825 MiB\n53 seconds\n491 MiB/s\n\n\n\nWith Ibis + DuckDB, I was able to write the query the way I wanted to without running out of memory, using the default configuration provided by Ibis.\nI was able run this computation around 188x faster than you can expect with pandas using this hardware setup.\nIn contrast, pandas ran out of memory on a single file without some hand holding and while Dask didn’t cause my program to run out of memory it still used quite a bit more than DuckDB.\n\n\nPandas is untenable for this workload\nPandas requires me to load everything into memory, and my machine doesn’t have enough memory to do that.\nGiven that Ibis + DuckDB runs this workload on my machine it doesn’t seem worth the effort to write any additional code to make pandas scale to the whole dataset.\n\n\nDask finishes in a similar amount of time as Ibis + DuckDB (within 2x)\nOut of the box I had quite a bit of difficulty figuring out how to maximize performance and not run out of memory.\nPlease get in touch if you think my Dask code can be improved!\nI know the Dask community is hard at work building dask-expr which might improve the performance of this workload when it lands."
  },
  {
    "objectID": "posts/pydata-performance/index.html#next-steps",
    "href": "posts/pydata-performance/index.html#next-steps",
    "title": "Ibis versus X: Performance across the ecosystem part 1",
    "section": "Next steps",
    "text": "Next steps\n\nPlease get in touch!\nIf you have ideas about how to speed up my use of the tools I’ve discussed here please get in touch by opening a GitHub discussion!\nWe would love it if more backends handled this workload!\n\n\nLook out for part 2\nIn part 2 of this series we’ll explore how Polars and DataFusion perform on this query. Stay tuned!"
  },
  {
    "objectID": "contribute/03_style.html",
    "href": "contribute/03_style.html",
    "title": "Style and formatting",
    "section": "",
    "text": "Ibis uses several code linters, like ruff, shellcheck, statix, nixpkgs-fmt and others, that are enforced by CI. Developers should run them locally before submitting a PR.\n\nInstall pre-commit\n\npip install pre-commit\n\nRun\n\npre-commit run --all-files\n\n\n\n\n\n\nNote\n\n\n\nSome of the packages needed to run the pre-commit linting can not be installed automatically (e.g. prettier, actionlint, shellcheck), and they need to be installed through a system package manager.\n\n\nOptionally, you may want to setup the pre-commit hooks to run automatically when making a git commit. To do this, run the following from the root of the Ibis repository:\npre-commit install\nThis will run the code linters automatically when you make a git commit. If you want to skip these checks, do git commit --no-verify\n\n\n\n\n\n\nTip\n\n\n\nIf you use nix-shell, all of these are already setup for you and ready to use, and you don’t need to do anything to install these tools.\n\n\n\n\n\nWe use numpydoc as our standard format for docstrings."
  },
  {
    "objectID": "contribute/03_style.html#code-style",
    "href": "contribute/03_style.html#code-style",
    "title": "Style and formatting",
    "section": "",
    "text": "Ibis uses several code linters, like ruff, shellcheck, statix, nixpkgs-fmt and others, that are enforced by CI. Developers should run them locally before submitting a PR.\n\nInstall pre-commit\n\npip install pre-commit\n\nRun\n\npre-commit run --all-files\n\n\n\n\n\n\nNote\n\n\n\nSome of the packages needed to run the pre-commit linting can not be installed automatically (e.g. prettier, actionlint, shellcheck), and they need to be installed through a system package manager.\n\n\nOptionally, you may want to setup the pre-commit hooks to run automatically when making a git commit. To do this, run the following from the root of the Ibis repository:\npre-commit install\nThis will run the code linters automatically when you make a git commit. If you want to skip these checks, do git commit --no-verify\n\n\n\n\n\n\nTip\n\n\n\nIf you use nix-shell, all of these are already setup for you and ready to use, and you don’t need to do anything to install these tools."
  },
  {
    "objectID": "contribute/03_style.html#docstrings",
    "href": "contribute/03_style.html#docstrings",
    "title": "Style and formatting",
    "section": "",
    "text": "We use numpydoc as our standard format for docstrings."
  },
  {
    "objectID": "contribute/01_environment.html",
    "href": "contribute/01_environment.html",
    "title": "Setting up a development environment",
    "section": "",
    "text": "git\n\n\npipCondaNix\n\n\n\n\n\n\n\n\nWarning\n\n\n\npip will not handle installation of system dependencies\npip will not install system dependencies needed for some packages such as psycopg2 and kerberos.\nFor a better development experience see the conda or nix setup instructions.\n\n\n\nInstall gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nChange directory into ibis:\ncd ibis\nInstall development dependencies\npip install 'poetry==1.7.1'\npip install -r requirements-dev.txt\nInstall ibis in development mode\npip install -e '.[all]'\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome optional dependencies for Windows are not available through conda/mamba\n\n\n\n\n\n\n\nPython Version\nPython 3.9\nPython 3.10\nPython 3.11\n\n\n\n\nOperating System\n\n\n\n\n\nLinux\n1\n\n\n\n\nmacOS (x86_64)\n\n\n\n\n\nmacOS (aarch64)\n\n\n\n\n\nWindows\n\n\n\n\n\n\n\ncondamamba\n\n\n\nInstall Miniconda\nInstall gh\nconda install -c conda-forge gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nCreate a Conda environment from a lock file in the repo:\n\n\nLinuxmacOS (x86_64)macOS (aarch64)Windows\n\n\n# Create a dev environment for linux-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/linux-64/3.10.lock\n\n\n# Create a dev environment for osx-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/osx-64/3.10.lock\n\n\n# Create a dev environment for osx-arm64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/osx-arm64/3.10.lock\n\n\n# Create a dev environment for win-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/win-64/3.10.lock\n\n\n\n\nActivate the environment\nconda activate ibis-dev\nInstall your local copy of ibis into the Conda environment\ncd ibis\npip install -e '.[all]'\n\n\n\n\nInstall Mamba\nInstall gh\nmamba install -c conda-forge gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nCreate a Conda environment from a lock file in the repo:\n\n\nLinuxmacOS (x86_64)macOS (aarch64)Windows\n\n\n# Create a dev environment for linux-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/linux-64/3.10.lock\n\n\n# Create a dev environment for osx-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/osx-64/3.10.lock\n\n\n# Create a dev environment for osx-arm64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/osx-arm64/3.10.lock\n\n\n# Create a dev environment for win-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/win-64/3.10.lock\n\n\n\n\nActivate the environment\nmamba activate ibis-dev\nInstall your local copy of ibis into the Conda environment\ncd ibis\npip install -e '.[all]'\n\n\n\n\n\n\n\n\n\n\n\n\nPython Version \nPython 3.9\nPython 3.10\nPython 3.11\n\n\n\n\nOperating System \n\n\n\n\n\nLinux\n 2\n\n\n\n\nmacOS (x86_64)\n\n\n\n\n\nmacOS (aarch64)\n3\n\n\n\n\nWindows\n4\n\n\n\n\n\n\nInstall nix\nConfigure nix\nEdit/create your nix.conf file ($XDG_CONFIG_HOME/nix/nix.conf) and add the line\nexperimental-features = nix-command flakes\nInstall gh:\n\nnix-shellnix-env\n\n\nnix-shell -p gh\n\n\nnix-env -iA gh\n\n\n\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nSet up the public ibis Cachix cache to pull pre-built dependencies:\nnix-shell -p cachix --run 'cachix use ibis'\nRun nix develop in the checkout directory:\ncd ibis\nnix develop\nThis will launch a bash shell with all of the required dependencies installed. This may take a while due to artifact download from the cache."
  },
  {
    "objectID": "contribute/01_environment.html#required-dependencies",
    "href": "contribute/01_environment.html#required-dependencies",
    "title": "Setting up a development environment",
    "section": "",
    "text": "git\n\n\npipCondaNix\n\n\n\n\n\n\n\n\nWarning\n\n\n\npip will not handle installation of system dependencies\npip will not install system dependencies needed for some packages such as psycopg2 and kerberos.\nFor a better development experience see the conda or nix setup instructions.\n\n\n\nInstall gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nChange directory into ibis:\ncd ibis\nInstall development dependencies\npip install 'poetry==1.7.1'\npip install -r requirements-dev.txt\nInstall ibis in development mode\npip install -e '.[all]'\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome optional dependencies for Windows are not available through conda/mamba\n\n\n\n\n\n\n\nPython Version\nPython 3.9\nPython 3.10\nPython 3.11\n\n\n\n\nOperating System\n\n\n\n\n\nLinux\n1\n\n\n\n\nmacOS (x86_64)\n\n\n\n\n\nmacOS (aarch64)\n\n\n\n\n\nWindows\n\n\n\n\n\n\n\ncondamamba\n\n\n\nInstall Miniconda\nInstall gh\nconda install -c conda-forge gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nCreate a Conda environment from a lock file in the repo:\n\n\nLinuxmacOS (x86_64)macOS (aarch64)Windows\n\n\n# Create a dev environment for linux-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/linux-64/3.10.lock\n\n\n# Create a dev environment for osx-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/osx-64/3.10.lock\n\n\n# Create a dev environment for osx-arm64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/osx-arm64/3.10.lock\n\n\n# Create a dev environment for win-64\ncd ibis\nconda create -n ibis-dev --file=ci/conda-lock/win-64/3.10.lock\n\n\n\n\nActivate the environment\nconda activate ibis-dev\nInstall your local copy of ibis into the Conda environment\ncd ibis\npip install -e '.[all]'\n\n\n\n\nInstall Mamba\nInstall gh\nmamba install -c conda-forge gh\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nCreate a Conda environment from a lock file in the repo:\n\n\nLinuxmacOS (x86_64)macOS (aarch64)Windows\n\n\n# Create a dev environment for linux-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/linux-64/3.10.lock\n\n\n# Create a dev environment for osx-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/osx-64/3.10.lock\n\n\n# Create a dev environment for osx-arm64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/osx-arm64/3.10.lock\n\n\n# Create a dev environment for win-64\ncd ibis\nmamba create -n ibis-dev --file=ci/conda-lock/win-64/3.10.lock\n\n\n\n\nActivate the environment\nmamba activate ibis-dev\nInstall your local copy of ibis into the Conda environment\ncd ibis\npip install -e '.[all]'\n\n\n\n\n\n\n\n\n\n\n\n\nPython Version \nPython 3.9\nPython 3.10\nPython 3.11\n\n\n\n\nOperating System \n\n\n\n\n\nLinux\n 2\n\n\n\n\nmacOS (x86_64)\n\n\n\n\n\nmacOS (aarch64)\n3\n\n\n\n\nWindows\n4\n\n\n\n\n\n\nInstall nix\nConfigure nix\nEdit/create your nix.conf file ($XDG_CONFIG_HOME/nix/nix.conf) and add the line\nexperimental-features = nix-command flakes\nInstall gh:\n\nnix-shellnix-env\n\n\nnix-shell -p gh\n\n\nnix-env -iA gh\n\n\n\nFork and clone the ibis repository:\ngh repo fork --clone --remote ibis-project/ibis\nSet up the public ibis Cachix cache to pull pre-built dependencies:\nnix-shell -p cachix --run 'cachix use ibis'\nRun nix develop in the checkout directory:\ncd ibis\nnix develop\nThis will launch a bash shell with all of the required dependencies installed. This may take a while due to artifact download from the cache."
  },
  {
    "objectID": "contribute/01_environment.html#building-the-docs",
    "href": "contribute/01_environment.html#building-the-docs",
    "title": "Setting up a development environment",
    "section": "Building the docs",
    "text": "Building the docs\nInstall just and run\njust docs-preview\nto build and serve the documentation."
  },
  {
    "objectID": "contribute/04_maintainers_guide.html",
    "href": "contribute/04_maintainers_guide.html",
    "title": "Maintaining the codebase",
    "section": "",
    "text": "Ibis maintainers are expected to handle the following tasks as they arise:\n\nReviewing and merging pull requests\nTriaging new issues\n\n\n\nA number of tasks that are typically associated with maintenance are partially or fully automated.\n\nWhiteSource Renovate (Python library dependencies and GitHub Actions)\nCustom GitHub Action (Nix dependencies)\n\n\n\nOccasionally you may need to lock poetry dependencies. Edit pyproject.toml as needed, then run:\npoetry lock --no-update\n\n\n\n\nIf you’re not a maintainer, please open an issue asking us to add your example.\n\n\nYou need the ability to write to the gs://ibis-examples GCS bucket to add an example.\n\n\n\nMake sure you’re in the root of the ibis git repository.\nAssuming your file is called example.csv:\n\nAdd a gzip-compressed CSV file with the path ibis/examples/data/example.csv.gz.\nAdd a file named ibis/examples/descriptions/example that contains a description of your example. One line is best, but not necessary.\nRun one of the following from the git root of an ibis clone:\n\npython ibis/examples/gen_registry.py (doesn’t include R dependencies)\nnix run '.#gen-examples' (includes R dependencies)\n\n\n\n\n\n\nIbis is released on PyPI and Conda Forge.\n\nPyPIconda-forge\n\n\nReleases to PyPI are handled automatically using semantic release.\nTo trigger a release use the Release GitHub Action.\n\n\nThe conda-forge package is maintained as a conda-forge feedstock.\nAfter a release to PyPI, the conda-forge bot automatically updates the ibis package."
  },
  {
    "objectID": "contribute/04_maintainers_guide.html#dependencies",
    "href": "contribute/04_maintainers_guide.html#dependencies",
    "title": "Maintaining the codebase",
    "section": "",
    "text": "A number of tasks that are typically associated with maintenance are partially or fully automated.\n\nWhiteSource Renovate (Python library dependencies and GitHub Actions)\nCustom GitHub Action (Nix dependencies)\n\n\n\nOccasionally you may need to lock poetry dependencies. Edit pyproject.toml as needed, then run:\npoetry lock --no-update"
  },
  {
    "objectID": "contribute/04_maintainers_guide.html#adding-examples",
    "href": "contribute/04_maintainers_guide.html#adding-examples",
    "title": "Maintaining the codebase",
    "section": "",
    "text": "If you’re not a maintainer, please open an issue asking us to add your example.\n\n\nYou need the ability to write to the gs://ibis-examples GCS bucket to add an example.\n\n\n\nMake sure you’re in the root of the ibis git repository.\nAssuming your file is called example.csv:\n\nAdd a gzip-compressed CSV file with the path ibis/examples/data/example.csv.gz.\nAdd a file named ibis/examples/descriptions/example that contains a description of your example. One line is best, but not necessary.\nRun one of the following from the git root of an ibis clone:\n\npython ibis/examples/gen_registry.py (doesn’t include R dependencies)\nnix run '.#gen-examples' (includes R dependencies)"
  },
  {
    "objectID": "contribute/04_maintainers_guide.html#release",
    "href": "contribute/04_maintainers_guide.html#release",
    "title": "Maintaining the codebase",
    "section": "",
    "text": "Ibis is released on PyPI and Conda Forge.\n\nPyPIconda-forge\n\n\nReleases to PyPI are handled automatically using semantic release.\nTo trigger a release use the Release GitHub Action.\n\n\nThe conda-forge package is maintained as a conda-forge feedstock.\nAfter a release to PyPI, the conda-forge bot automatically updates the ibis package."
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html",
    "title": "Basic operations",
    "section": "",
    "text": "In this tutorial, we will perform basic operations on demo data in Starburst Galaxy."
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#prerequisites",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#prerequisites",
    "title": "Basic operations",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis tutorial assumes you have completed the setup and connected to a database with the astronauts and missions demo data, including setup of a Python environment with Ibis and the Trino backend installed.\n\n\nCode\n1import os\nimport ibis\nfrom dotenv import load_dotenv\n\n2ibis.options.interactive = True\n\n3load_dotenv()\n\n4user = os.getenv(\"USERNAME\")\npassword = os.getenv(\"PASSWORD\")\nhost = os.getenv(\"HOSTNAME\")\nport = os.getenv(\"PORTNUMBER\")\n5catalog = \"sample\"\nschema = \"demo\"\n\n6con = ibis.trino.connect(\n    user=user, password=password, host=host, port=port, database=catalog, schema=schema\n)\n7con\n\n\n\n1\n\nImport necessary libraries.\n\n2\n\nUse Ibis in interactive mode.\n\n3\n\nLoad environment variables.\n\n4\n\nLoad secrets from environment variables.\n\n5\n\nUse the sample demo data.\n\n6\n\nConnect to Starburst Galaxy.\n\n7\n\nDisplay the connection object.\n\n\n\n\n&lt;ibis.backends.trino.Backend at 0x15508ecd0&gt;"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#load-tables",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#load-tables",
    "title": "Basic operations",
    "section": "Load tables",
    "text": "Load tables\nOnce you have a connection, you can assign tables to variables.\n\n1astronauts = con.table(\"astronauts\")\n2missions = con.table(\"missions\")\n\n\n1\n\nCreate astonauts variable.\n\n2\n\nCreate missions variable.\n\n\n\n\nYou can display slices of data:\n\n1astronauts[0:5]\n\n\n1\n\nDisplay the first 5 rows of the astronauts table.\n\n\n\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ id    ┃ number ┃ nationwide_number ┃ name                ┃ original_name           ┃ sex    ┃ year_of_birth ┃ nationality    ┃ military_civilian ┃ selection              ┃ year_of_selection ┃ mission_number ┃ total_number_of_missions ┃ occupation ┃ year_of_mission ┃ mission_title   ┃ ascend_shuttle  ┃ in_orbit        ┃ descend_shuttle ┃ hours_mission ┃ total_hrs_sum ┃ field21 ┃ eva_hrs_mission ┃ total_eva_hrs ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ int32 │ int32  │ int32             │ string              │ string                  │ string │ int32         │ string         │ string            │ string                 │ int32             │ int32          │ int32                    │ string     │ int32           │ string          │ string          │ string          │ string          │ float64       │ float64       │ int32   │ float64         │ float64       │\n├───────┼────────┼───────────────────┼─────────────────────┼─────────────────────────┼────────┼───────────────┼────────────────┼───────────────────┼────────────────────────┼───────────────────┼────────────────┼──────────────────────────┼────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼─────────────────┼───────────────┼───────────────┼─────────┼─────────────────┼───────────────┤\n│     1 │      1 │                 1 │ Gagarin, Yuri       │ ГАГАРИН Юрий Алексеевич │ male   │          1934 │ U.S.S.R/Russia │ military          │ TsPK-1                 │              1960 │              1 │                        1 │ pilot      │            1961 │ Vostok 1        │ Vostok 1        │ Vostok 2        │ Vostok 3        │          1.77 │          1.77 │       0 │             0.0 │           0.0 │\n│     2 │      2 │                 2 │ Titov, Gherman      │ ТИТОВ Герман Степанович │ male   │          1935 │ U.S.S.R/Russia │ military          │ TsPK-1                 │              1960 │              1 │                        1 │ pilot      │            1961 │ Vostok 2        │ Vostok 2        │ Vostok 2        │ Vostok 2        │         25.00 │         25.30 │       0 │             0.0 │           0.0 │\n│     3 │      3 │                 1 │ Glenn, John H., Jr. │ Glenn, John H., Jr.     │ male   │          1921 │ U.S.           │ military          │ NASA Astronaut Group 1 │              1959 │              1 │                        2 │ pilot      │            1962 │ MA-6            │ MA-6            │ MA-6            │ MA-6            │          5.00 │        218.00 │       0 │             0.0 │           0.0 │\n│     4 │      3 │                 1 │ Glenn, John H., Jr. │ Glenn, John H., Jr.     │ male   │          1921 │ U.S.           │ military          │ NASA Astronaut Group 2 │              1959 │              2 │                        2 │ PSP        │            1998 │ STS-95          │ STS-95          │ STS-95          │ STS-95          │        213.00 │        218.00 │       0 │             0.0 │           0.0 │\n│     5 │      4 │                 2 │ Carpenter, M. Scott │ Carpenter, M. Scott     │ male   │          1925 │ U.S.           │ military          │ NASA- 1                │              1959 │              1 │                        1 │ Pilot      │            1962 │ Mercury-Atlas 7 │ Mercury-Atlas 7 │ Mercury-Atlas 7 │ Mercury-Atlas 7 │          5.00 │          5.00 │       0 │             0.0 │           0.0 │\n└───────┴────────┴───────────────────┴─────────────────────┴─────────────────────────┴────────┴───────────────┴────────────────┴───────────────────┴────────────────────────┴───────────────────┴────────────────┴──────────────────────────┴────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴───────────────┴─────────┴─────────────────┴───────────────┘\n\n\n\n\n1missions[0:5]\n\n\n1\n\nDisplay the first 5 rows of the missions table.\n\n\n\n\n┏━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ id    ┃ company_name ┃ location                                                  ┃ date                       ┃ detail                                       ┃ status_rocket ┃ cost    ┃ status_mission ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int32 │ string       │ string                                                    │ string                     │ string                                       │ string        │ float64 │ string         │\n├───────┼──────────────┼───────────────────────────────────────────────────────────┼────────────────────────────┼──────────────────────────────────────────────┼───────────────┼─────────┼────────────────┤\n│     0 │ SpaceX       │ LC-39A, Kennedy Space Center, Florida, USA                │ Fri Aug 07, 2020 05:12 UTC │ Falcon 9 Block 5 | Starlink V1 L9 & BlackSky │ StatusActive  │   50.00 │ Success        │\n│     1 │ CASC         │ Site 9401 (SLS-2), Jiuquan Satellite Launch Center, China │ Thu Aug 06, 2020 04:01 UTC │ Long March 2D | Gaofen-9 04 & Q-SAT          │ StatusActive  │   29.75 │ Success        │\n│     2 │ SpaceX       │ Pad A, Boca Chica, Texas, USA                             │ Tue Aug 04, 2020 23:57 UTC │ Starship Prototype | 150 Meter Hop           │ StatusActive  │     nan │ Success        │\n│     3 │ Roscosmos    │ Site 200/39, Baikonur Cosmodrome, Kazakhstan              │ Thu Jul 30, 2020 21:25 UTC │ Proton-M/Briz-M | Ekspress-80 & Ekspress-103 │ StatusActive  │   65.00 │ Success        │\n│     4 │ ULA          │ SLC-41, Cape Canaveral AFS, Florida, USA                  │ Thu Jul 30, 2020 11:50 UTC │ Atlas V 541 | Perseverance                   │ StatusActive  │  145.00 │ Success        │\n└───────┴──────────────┴───────────────────────────────────────────────────────────┴────────────────────────────┴──────────────────────────────────────────────┴───────────────┴─────────┴────────────────┘"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#table-schemas",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#table-schemas",
    "title": "Basic operations",
    "section": "Table schemas",
    "text": "Table schemas\nYou can view the schemas of the tables:\n\n1astronauts.schema()\n\n\n1\n\nDisplay the schema of the astronauts table.\n\n\n\n\nibis.Schema {\n  id                        int32\n  number                    int32\n  nationwide_number         int32\n  name                      string\n  original_name             string\n  sex                       string\n  year_of_birth             int32\n  nationality               string\n  military_civilian         string\n  selection                 string\n  year_of_selection         int32\n  mission_number            int32\n  total_number_of_missions  int32\n  occupation                string\n  year_of_mission           int32\n  mission_title             string\n  ascend_shuttle            string\n  in_orbit                  string\n  descend_shuttle           string\n  hours_mission             float64\n  total_hrs_sum             float64\n  field21                   int32\n  eva_hrs_mission           float64\n  total_eva_hrs             float64\n}\n\n\n\n1missions.schema()\n\n\n1\n\nDisplay the schema of the missions table.\n\n\n\n\nibis.Schema {\n  id              int32\n  company_name    string\n  location        string\n  date            string\n  detail          string\n  status_rocket   string\n  cost            float64\n  status_mission  string\n}"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#selecting-columns",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#selecting-columns",
    "title": "Basic operations",
    "section": "Selecting columns",
    "text": "Selecting columns\nWith Ibis, you can run SQL-like queries on your tables. For example, you can select specific columns from a table:\n\n1t = astronauts.select(\"name\", \"nationality\", \"mission_title\", \"mission_number\", \"hours_mission\")\n2t.head(3)\n\n\n1\n\nSelect specific columns from the astronauts table.\n\n2\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ name                ┃ nationality    ┃ mission_title ┃ mission_number ┃ hours_mission ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string              │ string         │ string        │ int32          │ float64       │\n├─────────────────────┼────────────────┼───────────────┼────────────────┼───────────────┤\n│ Gagarin, Yuri       │ U.S.S.R/Russia │ Vostok 1      │              1 │          1.77 │\n│ Titov, Gherman      │ U.S.S.R/Russia │ Vostok 2      │              1 │         25.00 │\n│ Glenn, John H., Jr. │ U.S.           │ MA-6          │              1 │          5.00 │\n└─────────────────────┴────────────────┴───────────────┴────────────────┴───────────────┘\n\n\n\nAnd from the missions table:\n\n1t = missions.select(\"company_name\", \"status_rocket\", \"cost\", \"status_mission\")\n2t.head(3)\n\n\n1\n\nSelect specific columns from the missions table.\n\n2\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ company_name ┃ status_rocket ┃ cost    ┃ status_mission ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string       │ string        │ float64 │ string         │\n├──────────────┼───────────────┼─────────┼────────────────┤\n│ SpaceX       │ StatusActive  │   50.00 │ Success        │\n│ CASC         │ StatusActive  │   29.75 │ Success        │\n│ SpaceX       │ StatusActive  │     nan │ Success        │\n└──────────────┴───────────────┴─────────┴────────────────┘\n\n\n\nYou can also apply filters to your queries:\n\n1t = astronauts.filter(~astronauts[\"nationality\"].like(\"U.S.%\"))\n2t.head(3)\n\n\n1\n\nFilter astronauts table by nationality.\n\n2\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ id    ┃ number ┃ nationwide_number ┃ name                      ┃ original_name            ┃ sex    ┃ year_of_birth ┃ nationality ┃ military_civilian ┃ selection              ┃ year_of_selection ┃ mission_number ┃ total_number_of_missions ┃ occupation      ┃ year_of_mission ┃ mission_title ┃ ascend_shuttle ┃ in_orbit ┃ descend_shuttle ┃ hours_mission ┃ total_hrs_sum ┃ field21 ┃ eva_hrs_mission ┃ total_eva_hrs ┃\n┡━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ int32 │ int32  │ int32             │ string                    │ string                   │ string │ int32         │ string      │ string            │ string                 │ int32             │ int32          │ int32                    │ string          │ int32           │ string        │ string         │ string   │ string          │ float64       │ float64       │ int32   │ float64         │ float64       │\n├───────┼────────┼───────────────────┼───────────────────────────┼──────────────────────────┼────────┼───────────────┼─────────────┼───────────────────┼────────────────────────┼───────────────────┼────────────────┼──────────────────────────┼─────────────────┼─────────────────┼───────────────┼────────────────┼──────────┼─────────────────┼───────────────┼───────────────┼─────────┼─────────────────┼───────────────┤\n│    68 │    101 │                 1 │ Jugderdemidiin Gurragchaa │ Жүгдэрдэмидийн Гүррагчаа │ male   │          1947 │ Mongolia    │ civilian          │ 1978 Intercosmos Group │              1978 │              1 │                        1 │ flight engineer │            1981 │ Soyuz 39      │ Soyuz 39       │ Soyuz 39 │ Soyuz 39        │         188.7 │        188.70 │       0 │             0.0 │          0.00 │\n│    73 │    103 │                 1 │ Dumitru Prunariu          │ Dumitru-Dorin Prunariu   │ male   │          1952 │ Romania     │ civilian          │ 1978 Intercosmos Group │              1978 │              1 │                        1 │ MSP             │            1981 │ Soyuz 40      │ Soyuz 40       │ Soyuz 40 │ Soyuz 40        │         188.7 │        188.70 │       0 │             0.0 │          0.00 │\n│    81 │    108 │                 1 │ Chrétien, Jean-Loup       │ Chrétien, Jean-Loup      │ male   │          1938 │ France      │ military          │ CNES-1                 │              1980 │              1 │                        3 │ MSP             │            1982 │ Salyut 7      │ Soyuz T-6      │ Salyut 7 │ Soyuz T-6       │         190.0 │       1043.32 │       0 │             0.0 │          5.95 │\n└───────┴────────┴───────────────────┴───────────────────────────┴──────────────────────────┴────────┴───────────────┴─────────────┴───────────────────┴────────────────────────┴───────────────────┴────────────────┴──────────────────────────┴─────────────────┴─────────────────┴───────────────┴────────────────┴──────────┴─────────────────┴───────────────┴───────────────┴─────────┴─────────────────┴───────────────┘\n\n\n\nAnd in the missions table:\n\n1t = missions.filter(missions[\"status_mission\"] == \"Failure\")\n2t.head(3)\n\n\n1\n\nFilter missions table by mission status.\n\n2\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ id    ┃ company_name ┃ location                                                ┃ date                       ┃ detail                                      ┃ status_rocket ┃ cost    ┃ status_mission ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int32 │ string       │ string                                                  │ string                     │ string                                      │ string        │ float64 │ string         │\n├───────┼──────────────┼─────────────────────────────────────────────────────────┼────────────────────────────┼─────────────────────────────────────────────┼───────────────┼─────────┼────────────────┤\n│    11 │ ExPace       │ Site 95, Jiuquan Satellite Launch Center, China         │ Fri Jul 10, 2020 04:17 UTC │ Kuaizhou 11 | Jilin-1 02E, CentiSpace-1 S2  │ StatusActive  │    28.3 │ Failure        │\n│    15 │ Rocket Lab   │ Rocket Lab LC-1A, M?\\x81hia Peninsula, New Zealand      │ Sat Jul 04, 2020 21:19 UTC │ Electron/Curie | Pics Or It Didn??¦t Happen │ StatusActive  │     7.5 │ Failure        │\n│    27 │ Virgin Orbit │ Cosmic Girl, Mojave Air and Space Port, California, USA │ Mon May 25, 2020 19:50 UTC │ LauncherOne | Demo Flight                   │ StatusActive  │    12.0 │ Failure        │\n└───────┴──────────────┴─────────────────────────────────────────────────────────┴────────────────────────────┴─────────────────────────────────────────────┴───────────────┴─────────┴────────────────┘"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#mutating-columns",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#mutating-columns",
    "title": "Basic operations",
    "section": "Mutating columns",
    "text": "Mutating columns\n\n1t = missions.mutate(date=ibis.coalesce(ibis._[\"date\"], None))\n2t = t.order_by(t[\"date\"].asc())\n3t.head(3)\n\n\n1\n\nMutate the date column.\n\n2\n\nOrder the results by the date column.\n\n3\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ id    ┃ company_name ┃ location                                          ┃ date                       ┃ detail                       ┃ status_rocket ┃ cost    ┃ status_mission ┃\n┡━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ int32 │ string       │ string                                            │ string                     │ string                       │ string        │ float64 │ string         │\n├───────┼──────────────┼───────────────────────────────────────────────────┼────────────────────────────┼──────────────────────────────┼───────────────┼─────────┼────────────────┤\n│  4268 │ US Air Force │ SLC-17A, Cape Canaveral AFS, Florida, USA         │ Fri Apr 01, 1960 11:40 UTC │ Thor DM-18 Able-II | TIROS-1 │ StatusRetired │     nan │ Success        │\n│  3366 │ RVSN USSR    │ Site 43/3, Plesetsk Cosmodrome, Russia            │ Fri Apr 02, 1971 08:20 UTC │ Voskhod | Cosmos 403         │ StatusRetired │     nan │ Success        │\n│  1262 │ Arianespace  │ ELA-2, Guiana Space Centre, French Guiana, France │ Fri Apr 02, 1999 22:03 UTC │ Ariane 42P | Insat-2E        │ StatusRetired │     nan │ Success        │\n└───────┴──────────────┴───────────────────────────────────────────────────┴────────────────────────────┴──────────────────────────────┴───────────────┴─────────┴────────────────┘"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#aggregating-and-grouping-results",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#aggregating-and-grouping-results",
    "title": "Basic operations",
    "section": "Aggregating and grouping results",
    "text": "Aggregating and grouping results\nIbis also supports aggregate functions and grouping. For example, you can count the number of rows in a table and group the results by a specific column:\n\n1t = astronauts.filter(~astronauts[\"nationality\"].like(\"U.S.%\")).agg(\n    [\n2        ibis._.count().name(\"number_trips\"),\n        ibis._[\"hours_mission\"].max().name(\"longest_time\"),\n        ibis._[\"hours_mission\"].min().name(\"shortest_time\"),\n    ]\n)\n3t.head(3)\n\n\n1\n\nFilter the astronauts table.\n\n2\n\nAggregate the results.\n\n3\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ number_trips ┃ longest_time ┃ shortest_time ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ int64        │ float64      │ float64       │\n├──────────────┼──────────────┼───────────────┤\n│          149 │      6902.35 │          21.0 │\n└──────────────┴──────────────┴───────────────┘\n\n\n\nYou can add a group by:\n\nt = (\n1    astronauts.filter(~astronauts[\"nationality\"].like(\"U.S.%\"))\n2    .group_by(\"nationality\")\n3    .agg(\n        [\n            ibis._.count().name(\"number_trips\"),\n            ibis._[\"hours_mission\"].max().name(\"longest_time\"),\n            ibis._[\"hours_mission\"].min().name(\"shortest_time\"),\n        ]\n    )\n)\n4t.head(3)\n\n\n1\n\nFilter the astronauts table.\n\n2\n\nGroup by nationality.\n\n3\n\nAggregate the results.\n\n4\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ nationality ┃ number_trips ┃ longest_time ┃ shortest_time ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string      │ int64        │ float64      │ float64       │\n├─────────────┼──────────────┼──────────────┼───────────────┤\n│ Mongolia    │            1 │       188.70 │         188.7 │\n│ Romania     │            1 │       188.70 │         188.7 │\n│ France      │           18 │      4721.83 │         118.8 │\n└─────────────┴──────────────┴──────────────┴───────────────┘\n\n\n\nAnd order the results by ‘number_trips’ and ‘longest_time’ in descending order:\n\nt = (\n1    astronauts.filter(~astronauts[\"nationality\"].like(\"U.S.%\"))\n2    .group_by(\"nationality\")\n3    .agg(\n        [\n            ibis._.count().name(\"number_trips\"),\n            ibis._[\"hours_mission\"].max().name(\"longest_time\"),\n            ibis._[\"hours_mission\"].min().name(\"shortest_time\"),\n        ]\n    )\n4    .order_by([ibis.desc(\"number_trips\"), ibis.desc(\"longest_time\")])\n)\n5t.head(3)\n\n\n1\n\nFilter the astronauts table.\n\n2\n\nGroup by nationality.\n\n3\n\nAggregate the results.\n\n4\n\nOrder the result.\n\n5\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ nationality ┃ number_trips ┃ longest_time ┃ shortest_time ┃\n┡━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string      │ int64        │ float64      │ float64       │\n├─────────────┼──────────────┼──────────────┼───────────────┤\n│ Japan       │           20 │      6902.35 │        189.90 │\n│ Canada      │           18 │      4887.00 │        193.77 │\n│ France      │           18 │      4721.83 │        118.80 │\n└─────────────┴──────────────┴──────────────┴───────────────┘\n\n\n\nFor the missions table, you can group by ‘company_name’ and ‘status_rocket’, and then sum the ‘cost’:\n\nt = (\n1    missions.filter(missions[\"status_mission\"] == \"Failure\")\n2    .group_by([\"company_name\", \"status_rocket\"])\n3    .agg(ibis._[\"cost\"].sum().name(\"cost\"))\n4    .order_by(ibis.desc(\"cost\"))\n)\n5t.head(3)\n\n\n1\n\nFilter the missions table.\n\n2\n\nGroup by company_name and status_rocket.\n\n3\n\nAggregate the results.\n\n4\n\nOrder the results.\n\n5\n\nDisplay the results.\n\n\n\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ company_name ┃ status_rocket ┃ cost    ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n│ string       │ string        │ float64 │\n├──────────────┼───────────────┼─────────┤\n│ NASA         │ StatusRetired │   900.0 │\n│ Northrop     │ StatusActive  │   255.0 │\n│ Arianespace  │ StatusActive  │   237.0 │\n└──────────────┴───────────────┴─────────┘"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#writing-tables",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#writing-tables",
    "title": "Basic operations",
    "section": "Writing tables",
    "text": "Writing tables\nFinally, let’s write a table back to Starburst Galaxy.\n\n\n\n\n\n\nWarning\n\n\n\nYou cannot write to the sample catalog; uncomment the code and write to a catalog you have write access to.\n\n\n\n#con.create_table(\"t\", t, overwrite=True)"
  },
  {
    "objectID": "tutorials/data-platforms/starburst-galaxy/1_basics.html#next-steps",
    "href": "tutorials/data-platforms/starburst-galaxy/1_basics.html#next-steps",
    "title": "Basic operations",
    "section": "Next steps",
    "text": "Next steps\nNow that you’ve connected to Starburst Galaxy and learned the basics, you can query your own data. See the rest of the Ibis documentation or Starburst Galaxy documentation. You can open an issue if you run into one!"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html",
    "href": "tutorials/ibis-for-dplyr-users.html",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "R users familiar with dplyr, tidyr, and other packages in the Tidyverse are likely to find Ibis familiar.\nIn fact, some Ibis features were even inspired by similar features in the Tidyverse.\nHowever, due to differences between Python and R and the design and goals of Ibis itself, you may notice some big differences right away:\n\nNo pipe: The handy magrittr pipe (%&gt;%) or R’s newer native pipe (|&gt;) don’t exist in Python so you instead have to chain sequences of operations together with a period (.). The . in Python is analogous to R’s $ which lets you access attributes and methods on objects.\nNo unquoted column names: Non-standard evaluation is common in R but not present in Python. To reference a column in Ibis, you can pass a string, property on a table (e.g., tbl.some_column), or you can make use of selectors.\nIbis is lazy by default: Similar to dbplyr and its collect() method, Ibis does not evaluate our queries until we call .to_pandas(). For the purposes of this document, we set ibis.options.interactive = True which limits results to 10 rows, executes automatically, and prints a nicely-formatted table.\n\nUsing the same example data and similar operations as in Introduction to dplyr, below you will find some examples of the more common dplyr and tidyr operations and their Ibis equivalents.\n\n\nTo start using dplyr in R we would run:\n\nlibrary(dplyr)\n\nTo load Ibis:\n\nimport ibis\n\nAnd then also load and alias some helpers to make our code more concise:\n\nimport ibis.selectors as s\nfrom ibis import _\n\nLast, as mentioned above, to get Ibis to automatically execute our queries and show the results in a nicely-formatted table, we run:\n\nibis.options.interactive = True\n\n\n\n\nIn R, datasets are typically lazily loaded with packages. For instance, the starwars dataset is packaged with dplyr, but is not loaded in memory before you start using it. Ibis provides many datasets in the ibis.examples module. So to be able to use the starwars dataset, you can use:\n\nstarwars = ibis.examples.starwars.fetch()\n\nSimilar to dplyr, if we evaluate the name of a table, we get a nicely-formatted table:\n\nstarwars\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string        │ string      │ … │\n├────────────────────┼────────┼─────────┼───────────────┼─────────────┼───┤\n│ Luke Skywalker     │    172 │    77.0 │ blond         │ fair        │ … │\n│ C-3PO              │    167 │    75.0 │ NULL          │ gold        │ … │\n│ R2-D2              │     96 │    32.0 │ NULL          │ white, blue │ … │\n│ Darth Vader        │    202 │   136.0 │ none          │ white       │ … │\n│ Leia Organa        │    150 │    49.0 │ brown         │ light       │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey   │ light       │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown         │ light       │ … │\n│ R5-D4              │     97 │    32.0 │ NULL          │ white, red  │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black         │ light       │ … │\n│ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ fair        │ … │\n│ …                  │      … │       … │ …             │ …           │ … │\n└────────────────────┴────────┴─────────┴───────────────┴─────────────┴───┘\n\n\nIn addition to printing a nicely-formatted table and automatically executing, setting ibis.options.interactive to True also causes our query to be limited to 10 rows. To get Ibis to give us all rows, we can directly call to_pandas and save the result as a pandas DataFrame:\n\nstarwars_df = starwars.to_pandas()\n\nWhich then gives us all of the data as a pandas DataFrame:\n\nstarwars_df\n\n              name  height   mass hair_color  ... species films  vehicles starships\n0   Luke Skywalker   172.0   77.0      blond  ...   Human  None      None      None\n1            C-3PO   167.0   75.0       None  ...   Droid  None      None      None\n2            R2-D2    96.0   32.0       None  ...   Droid  None      None      None\n3      Darth Vader   202.0  136.0       none  ...   Human  None      None      None\n4      Leia Organa   150.0   49.0      brown  ...   Human  None      None      None\n..             ...     ...    ...        ...  ...     ...   ...       ...       ...\n82             Rey     NaN    NaN      brown  ...   Human  None      None      None\n83     Poe Dameron     NaN    NaN      brown  ...   Human  None      None      None\n84             BB8     NaN    NaN       none  ...   Droid  None      None      None\n85  Captain Phasma     NaN    NaN    unknown  ...    None  None      None      None\n86   Padmé Amidala   165.0   45.0      brown  ...   Human  None      None      None\n\n[87 rows x 14 columns]\n\n\nDirectly calling to_pandas and saving the result to a variable is useful for passing the results of Ibis table expressions to other packages (e.g., matplotlib.\n\n\n\nJust like in R, you can use head() to inspect the beginning of a dataset. You can also specify the number of rows you want to get back by using the parameter n (default n = 5).\nIn R:\n\nhead(starwars) # or starwars |&gt; head()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n4 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n5 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n6 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nWith Ibis:\n\nstarwars.head(6)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name           ┃ height ┃ mass    ┃ hair_color  ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string         │ int64  │ float64 │ string      │ string      │ … │\n├────────────────┼────────┼─────────┼─────────────┼─────────────┼───┤\n│ Luke Skywalker │    172 │    77.0 │ blond       │ fair        │ … │\n│ C-3PO          │    167 │    75.0 │ NULL        │ gold        │ … │\n│ R2-D2          │     96 │    32.0 │ NULL        │ white, blue │ … │\n│ Darth Vader    │    202 │   136.0 │ none        │ white       │ … │\n│ Leia Organa    │    150 │    49.0 │ brown       │ light       │ … │\n│ Owen Lars      │    178 │   120.0 │ brown, grey │ light       │ … │\n└────────────────┴────────┴─────────┴─────────────┴─────────────┴───┘\n\n\nThere is no tail() in Ibis because most databases do not support this operation.\nAnother method you can use to limit the number of rows returned by a query is limit() which also takes the n parameter.\n\nstarwars.limit(3)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ name           ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ eye_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string         │ int64  │ float64 │ string     │ string      │ string    │ … │\n├────────────────┼────────┼─────────┼────────────┼─────────────┼───────────┼───┤\n│ Luke Skywalker │    172 │    77.0 │ blond      │ fair        │ blue      │ … │\n│ C-3PO          │    167 │    75.0 │ NULL       │ gold        │ yellow    │ … │\n│ R2-D2          │     96 │    32.0 │ NULL       │ white, blue │ red       │ … │\n└────────────────┴────────┴─────────┴────────────┴─────────────┴───────────┴───┘\n\n\n\n\n\nIbis, like dplyr, has filter to select rows based on conditions.\nWith dplyr:\n\nstarwars |&gt;\n  filter(skin_color == \"light\")\n\n# A tibble: 11 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Leia Or…    150    49 brown      light      brown             19 fema… femin…\n 2 Owen La…    178   120 brown, gr… light      blue              52 male  mascu…\n 3 Beru Wh…    165    75 brown      light      blue              47 fema… femin…\n 4 Biggs D…    183    84 black      light      brown             24 male  mascu…\n 5 Lobot       175    79 none       light      blue              37 male  mascu…\n 6 Cordé       157    NA brown      light      brown             NA fema… femin…\n 7 Dormé       165    NA brown      light      brown             NA fema… femin…\n 8 Raymus …    188    79 brown      light      brown             NA male  mascu…\n 9 Rey          NA    NA brown      light      hazel             NA fema… femin…\n10 Poe Dam…     NA    NA brown      light      brown             NA male  mascu…\n11 Padmé A…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis:\n\nstarwars.filter(_.skin_color == \"light\")\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color  ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string      │ string     │ … │\n├────────────────────┼────────┼─────────┼─────────────┼────────────┼───┤\n│ Leia Organa        │    150 │    49.0 │ brown       │ light      │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey │ light      │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown       │ light      │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black       │ light      │ … │\n│ Lobot              │    175 │    79.0 │ none        │ light      │ … │\n│ Cordé              │    157 │     nan │ brown       │ light      │ … │\n│ Dormé              │    165 │     nan │ brown       │ light      │ … │\n│ Raymus Antilles    │    188 │    79.0 │ brown       │ light      │ … │\n│ Rey                │   NULL │     nan │ brown       │ light      │ … │\n│ Poe Dameron        │   NULL │     nan │ brown       │ light      │ … │\n│ …                  │      … │       … │ …           │ …          │ … │\n└────────────────────┴────────┴─────────┴─────────────┴────────────┴───┘\n\n\nIn dplyr, you can specify multiple conditions separated with , that are then combined with the & operator:\n\nstarwars |&gt;\n  filter(skin_color == \"light\", eye_color == \"brown\")\n\n# A tibble: 7 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Biggs Da…    183    84 black      light      brown             24 male  mascu…\n3 Cordé        157    NA brown      light      brown             NA fema… femin…\n4 Dormé        165    NA brown      light      brown             NA fema… femin…\n5 Raymus A…    188    79 brown      light      brown             NA male  mascu…\n6 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n7 Padmé Am…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, you can do the same by putting multiple conditions in a list:\n\nstarwars.filter([_.skin_color == \"light\", _.eye_color == \"brown\"])\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string     │ … │\n├───────────────────┼────────┼─────────┼────────────┼────────────┼───┤\n│ Leia Organa       │    150 │    49.0 │ brown      │ light      │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light      │ … │\n│ Cordé             │    157 │     nan │ brown      │ light      │ … │\n│ Dormé             │    165 │     nan │ brown      │ light      │ … │\n│ Raymus Antilles   │    188 │    79.0 │ brown      │ light      │ … │\n│ Poe Dameron       │   NULL │     nan │ brown      │ light      │ … │\n│ Padmé Amidala     │    165 │    45.0 │ brown      │ light      │ … │\n└───────────────────┴────────┴─────────┴────────────┴────────────┴───┘\n\n\nIn previous code, we used the _ helper we imported earlier. The _ is shorthand for the table returned by the previous step in the chained sequence of operations (in this case, starwars). We could have also written the more verbose form,\n\nstarwars.filter([starwars.skin_color == \"light\", starwars.eye_color == \"brown\"])\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string     │ … │\n├───────────────────┼────────┼─────────┼────────────┼────────────┼───┤\n│ Leia Organa       │    150 │    49.0 │ brown      │ light      │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light      │ … │\n│ Cordé             │    157 │     nan │ brown      │ light      │ … │\n│ Dormé             │    165 │     nan │ brown      │ light      │ … │\n│ Raymus Antilles   │    188 │    79.0 │ brown      │ light      │ … │\n│ Poe Dameron       │   NULL │     nan │ brown      │ light      │ … │\n│ Padmé Amidala     │    165 │    45.0 │ brown      │ light      │ … │\n└───────────────────┴────────┴─────────┴────────────┴────────────┴───┘\n\n\nIf you want to combine multiple conditions, in dplyr, you could do:\n\nstarwars |&gt;\n  filter(\n      (skin_color == \"light\" & eye_color == \"brown\") |\n       species == \"Droid\"\n  )\n\n# A tibble: 13 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 C-3PO       167    75 &lt;NA&gt;       gold       yellow           112 none  mascu…\n 2 R2-D2        96    32 &lt;NA&gt;       white, bl… red               33 none  mascu…\n 3 Leia Or…    150    49 brown      light      brown             19 fema… femin…\n 4 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu…\n 5 Biggs D…    183    84 black      light      brown             24 male  mascu…\n 6 IG-88       200   140 none       metal      red               15 none  mascu…\n 7 Cordé       157    NA brown      light      brown             NA fema… femin…\n 8 Dormé       165    NA brown      light      brown             NA fema… femin…\n 9 R4-P17       96    NA none       silver, r… red, blue         NA none  femin…\n10 Raymus …    188    79 brown      light      brown             NA male  mascu…\n11 Poe Dam…     NA    NA brown      light      brown             NA male  mascu…\n12 BB8          NA    NA none       none       black             NA none  mascu…\n13 Padmé A…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, this would be:\n\nstarwars.filter(\n    ((_.skin_color == \"light\") & (_.eye_color == \"brown\")) |\n    (_.species == \"Droid\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ C-3PO             │    167 │    75.0 │ NULL       │ gold        │ … │\n│ R2-D2             │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R5-D4             │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ IG-88             │    200 │   140.0 │ none       │ metal       │ … │\n│ Leia Organa       │    150 │    49.0 │ brown      │ light       │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light       │ … │\n│ Cordé             │    157 │     nan │ brown      │ light       │ … │\n│ Dormé             │    165 │     nan │ brown      │ light       │ … │\n│ R4-P17            │     96 │     nan │ none       │ silver, red │ … │\n│ BB8               │   NULL │     nan │ none       │ none        │ … │\n│ …                 │      … │       … │ …          │ …           │ … │\n└───────────────────┴────────┴─────────┴────────────┴─────────────┴───┘\n\n\n\n\n\nTo sort a column, dplyr has the verb arrange. For instance, to sort the column height using dplyr:\n\nstarwars |&gt;\n   arrange(height)\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yoda         66    17 white      green      brown            896 male  mascu…\n 2 Ratts T…     79    15 none       grey, blue unknown           NA male  mascu…\n 3 Wicket …     88    20 brown      brown      brown              8 male  mascu…\n 4 Dud Bolt     94    45 none       blue, grey yellow            NA male  mascu…\n 5 R2-D2        96    32 &lt;NA&gt;       white, bl… red               33 none  mascu…\n 6 R4-P17       96    NA none       silver, r… red, blue         NA none  femin…\n 7 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu…\n 8 Sebulba     112    40 none       grey, red  orange            NA male  mascu…\n 9 Gasgano     122    NA none       white, bl… black             NA male  mascu…\n10 Watto       137    NA black      blue, grey yellow            NA male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIbis has the order_by method, so to perform the same operation:\n\nstarwars.order_by(_.height)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name                  ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string                │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ Yoda                  │     66 │    17.0 │ white      │ green       │ … │\n│ Ratts Tyerell         │     79 │    15.0 │ none       │ grey, blue  │ … │\n│ Wicket Systri Warrick │     88 │    20.0 │ brown      │ brown       │ … │\n│ Dud Bolt              │     94 │    45.0 │ none       │ blue, grey  │ … │\n│ R2-D2                 │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R4-P17                │     96 │     nan │ none       │ silver, red │ … │\n│ R5-D4                 │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ Sebulba               │    112 │    40.0 │ none       │ grey, red   │ … │\n│ Gasgano               │    122 │     nan │ none       │ white, blue │ … │\n│ Watto                 │    137 │     nan │ black      │ blue, grey  │ … │\n│ …                     │      … │       … │ …          │ …           │ … │\n└───────────────────────┴────────┴─────────┴────────────┴─────────────┴───┘\n\n\nYou might notice that while dplyr puts missing values at the end, Ibis places them at the top. This behavior can actually vary from backend to backend and is something to be aware of when using Ibis.\nIf you want to order using multiple variables, you can pass them as a list:\n\nstarwars.order_by([_.height, _.mass]) # or starwars.order_by([\"height\", \"mass\"])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name                  ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string                │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ Yoda                  │     66 │    17.0 │ white      │ green       │ … │\n│ Ratts Tyerell         │     79 │    15.0 │ none       │ grey, blue  │ … │\n│ Wicket Systri Warrick │     88 │    20.0 │ brown      │ brown       │ … │\n│ Dud Bolt              │     94 │    45.0 │ none       │ blue, grey  │ … │\n│ R2-D2                 │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R4-P17                │     96 │     nan │ none       │ silver, red │ … │\n│ R5-D4                 │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ Sebulba               │    112 │    40.0 │ none       │ grey, red   │ … │\n│ Gasgano               │    122 │     nan │ none       │ white, blue │ … │\n│ Watto                 │    137 │     nan │ black      │ blue, grey  │ … │\n│ …                     │      … │       … │ …          │ …           │ … │\n└───────────────────────┴────────┴─────────┴────────────┴─────────────┴───┘\n\n\nTo order a column in descending order, there are two ways to do it. Note that missing values remain at the top.\n\nstarwars.order_by(_.height.desc()) # or: starwars.order_by(ibis.desc(\"height\"))\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━┓\n┃ name         ┃ height ┃ mass    ┃ hair_color ┃ skin_color   ┃ … ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━┩\n│ string       │ int64  │ float64 │ string     │ string       │ … │\n├──────────────┼────────┼─────────┼────────────┼──────────────┼───┤\n│ Yarael Poof  │    264 │     nan │ none       │ white        │ … │\n│ Tarfful      │    234 │   136.0 │ brown      │ brown        │ … │\n│ Lama Su      │    229 │    88.0 │ none       │ grey         │ … │\n│ Chewbacca    │    228 │   112.0 │ brown      │ unknown      │ … │\n│ Roos Tarpals │    224 │    82.0 │ none       │ grey         │ … │\n│ Grievous     │    216 │   159.0 │ none       │ brown, white │ … │\n│ Taun We      │    213 │     nan │ none       │ grey         │ … │\n│ Rugor Nass   │    206 │     nan │ none       │ green        │ … │\n│ Tion Medon   │    206 │    80.0 │ none       │ grey         │ … │\n│ Darth Vader  │    202 │   136.0 │ none       │ white        │ … │\n│ …            │      … │       … │ …          │ …            │ … │\n└──────────────┴────────┴─────────┴────────────┴──────────────┴───┘\n\n\n\n\n\nIbis, like dplyr, has a select method to include or exclude columns:\nWith dplyr:\n\nstarwars |&gt;\n    select(hair_color)\n\n# A tibble: 87 × 1\n   hair_color   \n   &lt;chr&gt;        \n 1 blond        \n 2 &lt;NA&gt;         \n 3 &lt;NA&gt;         \n 4 none         \n 5 brown        \n 6 brown, grey  \n 7 brown        \n 8 &lt;NA&gt;         \n 9 black        \n10 auburn, white\n# ℹ 77 more rows\n\n\nIn Ibis:\n\nstarwars.select(_.hair_color)\n\n┏━━━━━━━━━━━━━━━┓\n┃ hair_color    ┃\n┡━━━━━━━━━━━━━━━┩\n│ string        │\n├───────────────┤\n│ blond         │\n│ NULL          │\n│ NULL          │\n│ none          │\n│ brown         │\n│ brown, grey   │\n│ brown         │\n│ NULL          │\n│ black         │\n│ auburn, white │\n│ …             │\n└───────────────┘\n\n\nNote: A common pitfall to be aware of when referencing column names in Ibis is when column names collide with built-in methods on the Ibis Table object, such as count. In this situation, you will have to reference count like table[\"count\"] or _[\"count\"].\ndplyr also allows selecting more than one column at a time:\n\nstarwars |&gt;\n    select(hair_color, skin_color, eye_color)\n\n# A tibble: 87 × 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# ℹ 77 more rows\n\n\nIn Ibis, we can either quote the names:\n\nstarwars.select(\"hair_color\", \"skin_color\", \"eye_color\")\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\nOr use the _ helper:\n\nstarwars.select(_.hair_color, _.skin_color, _.eye_color)\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\nTo select columns by name based on a condition, dplyr has helpers such as:\n\nstarts_with(): Starts with a prefix.\nends_with(): Ends with a suffix.\ncontains(): Contains a literal string.\n\nThese and many more selectors are available in Ibis as well, with slightly different names:\n\nstarwars.select(s.startswith(\"h\"))\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ height ┃ hair_color    ┃ homeworld ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64  │ string        │ string    │\n├────────┼───────────────┼───────────┤\n│    172 │ blond         │ Tatooine  │\n│    167 │ NULL          │ Tatooine  │\n│     96 │ NULL          │ Naboo     │\n│    202 │ none          │ Tatooine  │\n│    150 │ brown         │ Alderaan  │\n│    178 │ brown, grey   │ Tatooine  │\n│    165 │ brown         │ Tatooine  │\n│     97 │ NULL          │ Tatooine  │\n│    183 │ black         │ Tatooine  │\n│    182 │ auburn, white │ Stewjon   │\n│      … │ …             │ …         │\n└────────┴───────────────┴───────────┘\n\n\n\nstarwars.select(s.endswith(\"color\"))\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\n\nstarwars.select(s.contains(\"world\"))\n\n┏━━━━━━━━━━━┓\n┃ homeworld ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ Tatooine  │\n│ Tatooine  │\n│ Naboo     │\n│ Tatooine  │\n│ Alderaan  │\n│ Tatooine  │\n│ Tatooine  │\n│ Tatooine  │\n│ Tatooine  │\n│ Stewjon   │\n│ …         │\n└───────────┘\n\n\nSee the Ibis Column Selectors documentation for the full list of selectors in Ibis.\n\n\n\nIbis allows you to rename columns using rename() which provides similar functionality to rename() in dplyr.\nIn dplyr:\n\nstarwars |&gt;\n    rename(\"home_world\" = \"homeworld\")\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: home_world &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, use rename and pass a dict of name mappings:\n\nstarwars.rename(home_world=\"homeworld\")\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string        │ string      │ … │\n├────────────────────┼────────┼─────────┼───────────────┼─────────────┼───┤\n│ Luke Skywalker     │    172 │    77.0 │ blond         │ fair        │ … │\n│ C-3PO              │    167 │    75.0 │ NULL          │ gold        │ … │\n│ R2-D2              │     96 │    32.0 │ NULL          │ white, blue │ … │\n│ Darth Vader        │    202 │   136.0 │ none          │ white       │ … │\n│ Leia Organa        │    150 │    49.0 │ brown         │ light       │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey   │ light       │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown         │ light       │ … │\n│ R5-D4              │     97 │    32.0 │ NULL          │ white, red  │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black         │ light       │ … │\n│ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ fair        │ … │\n│ …                  │      … │       … │ …             │ …           │ … │\n└────────────────────┴────────┴─────────┴───────────────┴─────────────┴───┘\n\n\n\n\n\nIbis, like dplyr, uses the mutate verb to add columns.\nIn dplyr,\n\nstarwars |&gt;\n    mutate(height_m = height / 100) |&gt;\n    select(name, height_m)\n\n# A tibble: 87 × 2\n   name               height_m\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Luke Skywalker         1.72\n 2 C-3PO                  1.67\n 3 R2-D2                  0.96\n 4 Darth Vader            2.02\n 5 Leia Organa            1.5 \n 6 Owen Lars              1.78\n 7 Beru Whitesun lars     1.65\n 8 R5-D4                  0.97\n 9 Biggs Darklighter      1.83\n10 Obi-Wan Kenobi         1.82\n# ℹ 77 more rows\n\n\nIn Ibis:\n\n(\n    starwars\n        .mutate(height_m = _.height / 100)\n        .select(\"name\", \"height_m\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ name               ┃ height_m ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string             │ float64  │\n├────────────────────┼──────────┤\n│ Luke Skywalker     │     1.72 │\n│ C-3PO              │     1.67 │\n│ R2-D2              │     0.96 │\n│ Darth Vader        │     2.02 │\n│ Leia Organa        │     1.50 │\n│ Owen Lars          │     1.78 │\n│ Beru Whitesun lars │     1.65 │\n│ R5-D4              │     0.97 │\n│ Biggs Darklighter  │     1.83 │\n│ Obi-Wan Kenobi     │     1.82 │\n│ …                  │        … │\n└────────────────────┴──────────┘\n\n\nA big difference between dplyr’s mutate and Ibis’ mutate is that, in Ibis, you have to chain separate mutate calls together when you reference newly-created columns in the same mutate whereas in dplyr, you can put them all in the same call. This makes Ibis’ mutate more similar to transform in base R.\nIn dplyr, we only need one mutate call:\n\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\n\n# A tibble: 87 × 16\n     BMI name      height  mass hair_color skin_color eye_color birth_year sex  \n   &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n 1  26.0 Luke Sky…    172    77 blond      fair       blue            19   male \n 2  26.9 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none \n 3  34.7 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none \n 4  33.3 Darth Va…    202   136 none       white      yellow          41.9 male \n 5  21.8 Leia Org…    150    49 brown      light      brown           19   fema…\n 6  37.9 Owen Lars    178   120 brown, gr… light      blue            52   male \n 7  27.5 Beru Whi…    165    75 brown      light      blue            47   fema…\n 8  34.0 R5-D4         97    32 &lt;NA&gt;       white, red red             NA   none \n 9  25.1 Biggs Da…    183    84 black      light      brown           24   male \n10  23.2 Obi-Wan …    182    77 auburn, w… fair       blue-gray       57   male \n# ℹ 77 more rows\n# ℹ 7 more variables: gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;,\n#   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;, height_m &lt;dbl&gt;\n\n\nIn Ibis, for BMI to reference height_m, it needs to be in a separate mutate call:\n\n(starwars\n    .mutate(\n        height_m = _.height / 100\n    )\n    .mutate(\n        BMI = _.mass / (_.height_m**2)\n    )\n    .select(\"BMI\", ~s.matches(\"BMI\"))\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━┓\n┃ BMI       ┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━┩\n│ float64   │ string             │ int64  │ float64 │ string        │ … │\n├───────────┼────────────────────┼────────┼─────────┼───────────────┼───┤\n│ 26.027582 │ Luke Skywalker     │    172 │    77.0 │ blond         │ … │\n│ 26.892323 │ C-3PO              │    167 │    75.0 │ NULL          │ … │\n│ 34.722222 │ R2-D2              │     96 │    32.0 │ NULL          │ … │\n│ 33.330066 │ Darth Vader        │    202 │   136.0 │ none          │ … │\n│ 21.777778 │ Leia Organa        │    150 │    49.0 │ brown         │ … │\n│ 37.874006 │ Owen Lars          │    178 │   120.0 │ brown, grey   │ … │\n│ 27.548209 │ Beru Whitesun lars │    165 │    75.0 │ brown         │ … │\n│ 34.009990 │ R5-D4              │     97 │    32.0 │ NULL          │ … │\n│ 25.082863 │ Biggs Darklighter  │    183 │    84.0 │ black         │ … │\n│ 23.245985 │ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ … │\n│         … │ …                  │      … │       … │ …             │ … │\n└───────────┴────────────────────┴────────┴─────────┴───────────────┴───┘\n\n\n\n\n\nTo summarize tables, dplyr has the verbs summarise/summarize:\nIn dplyr:\n\nstarwars %&gt;%\n    summarise(height = mean(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  height\n   &lt;dbl&gt;\n1   174.\n\n\nIn Ibis, the corresponding verb is aggregate:\n\nstarwars.aggregate(height = _.height.mean())\n\n┏━━━━━━━━━━━━┓\n┃ height     ┃\n┡━━━━━━━━━━━━┩\n│ float64    │\n├────────────┤\n│ 174.358025 │\n└────────────┘\n\n\nNote: Throughout this guide, where dplyr uses R generics, Ibis uses Python methods. In the previous code cell, aggregate is a method on a table and mean is a method on a column. If you want to perform aggregations on multiple columns, you can call the method that you want on the column you want to apply it to.\n\n\n\nTo demonstrate how to do joins with Ibis, we’ll load two more example datasets that also come from the example datasets included in dplyr:\n\nband_members = ibis.examples.band_members.fetch()\nband_instruments = ibis.examples.band_instruments.fetch()\n\nIn dplyr, we can perform a left join of these two tables like:\n\nband_members |&gt;\n    left_join(band_instruments)\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nIn Ibis:\n\nband_members.left_join(band_instruments, \"name\")\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n┃ name   ┃ band    ┃ name_right ┃ plays  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ string     │ string │\n├────────┼─────────┼────────────┼────────┤\n│ John   │ Beatles │ John       │ guitar │\n│ Paul   │ Beatles │ Paul       │ bass   │\n│ Mick   │ Stones  │ NULL       │ NULL   │\n└────────┴─────────┴────────────┴────────┘\n\n\nThere are two main differences between Ibis and dplyr here:\n\nIbis requires us to explicitly specify our join key (“name”, in this example) whereas in dplyr, if the join key is missing, we get the natural join of the two tables which joins across all shared column names\nIbis keeps columns for join keys from each table whereas dplyr does not by default\n\nTo replicate the result we’d get by default in dplyr but using Ibis, we need to incorporate two other verbs we’ve already seen in this tutorial:\n\n(\n    band_members\n        .left_join(band_instruments, \"name\")\n        .select(~s.contains(\"_right\"))\n)\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ name   ┃ band    ┃ plays  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ string │\n├────────┼─────────┼────────┤\n│ John   │ Beatles │ guitar │\n│ Paul   │ Beatles │ bass   │\n│ Mick   │ Stones  │ NULL   │\n└────────┴─────────┴────────┘\n\n\n\n\n\ndplyr users are likely to be familiar with the pivot_wider and pivot_longer functions from the tidyr package which convert tables between wide and long formats, respectively.\npivot_longer in dplyr + tidyr:\n\nlibrary(tidyr)\n\nstarwars_colors &lt;-\n    starwars |&gt;\n        select(name, matches(\"color\")) |&gt;\n        pivot_longer(matches(\"color\"), names_to = \"attribute\", values_to = \"color\")\n\nIn Ibis:\n\nstarwars_colors = (\n    starwars\n        .select(\"name\", s.matches(\"color\"))\n        .pivot_longer(s.matches(\"color\"), names_to=\"attribute\", values_to=\"color\")\n)\n\nstarwars_colors\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ name           ┃ attribute  ┃ color       ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string         │ string     │ string      │\n├────────────────┼────────────┼─────────────┤\n│ Luke Skywalker │ hair_color │ blond       │\n│ Luke Skywalker │ skin_color │ fair        │\n│ Luke Skywalker │ eye_color  │ blue        │\n│ C-3PO          │ hair_color │ NULL        │\n│ C-3PO          │ skin_color │ gold        │\n│ C-3PO          │ eye_color  │ yellow      │\n│ R2-D2          │ hair_color │ NULL        │\n│ R2-D2          │ skin_color │ white, blue │\n│ R2-D2          │ eye_color  │ red         │\n│ Darth Vader    │ hair_color │ none        │\n│ …              │ …          │ …           │\n└────────────────┴────────────┴─────────────┘\n\n\nAnd pivot_wider:\n\nstarwars_colors |&gt;\n    pivot_wider(names_from = \"attribute\", values_from = \"color\")\n\n# A tibble: 87 × 4\n   name               hair_color    skin_color  eye_color\n   &lt;chr&gt;              &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 Luke Skywalker     blond         fair        blue     \n 2 C-3PO              &lt;NA&gt;          gold        yellow   \n 3 R2-D2              &lt;NA&gt;          white, blue red      \n 4 Darth Vader        none          white       yellow   \n 5 Leia Organa        brown         light       brown    \n 6 Owen Lars          brown, grey   light       blue     \n 7 Beru Whitesun lars brown         light       blue     \n 8 R5-D4              &lt;NA&gt;          white, red  red      \n 9 Biggs Darklighter  black         light       brown    \n10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n# ℹ 77 more rows\n\n\nIn Ibis:\n\n(\n    starwars_colors.\n        pivot_wider(names_from=\"attribute\", values_from=\"color\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ name               ┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string             │ string        │ string      │ string    │\n├────────────────────┼───────────────┼─────────────┼───────────┤\n│ Luke Skywalker     │ blond         │ fair        │ blue      │\n│ C-3PO              │ NULL          │ gold        │ yellow    │\n│ R2-D2              │ NULL          │ white, blue │ red       │\n│ Darth Vader        │ none          │ white       │ yellow    │\n│ Leia Organa        │ brown         │ light       │ brown     │\n│ Owen Lars          │ brown, grey   │ light       │ blue      │\n│ Beru Whitesun lars │ brown         │ light       │ blue      │\n│ R5-D4              │ NULL          │ white, red  │ red       │\n│ Biggs Darklighter  │ black         │ light       │ brown     │\n│ Obi-Wan Kenobi     │ auburn, white │ fair        │ blue-gray │\n│ …                  │ …             │ …           │ …         │\n└────────────────────┴───────────────┴─────────────┴───────────┘\n\n\n\n\n\nNow that you’ve gotten an introduction to the common differences between dplyr and Ibis, head over to Getting started with ibis for a full introduction. If you’re familiar with SQL, check out Ibis for SQL users. If you’re familiar with pandas, take a look at Ibis for pandas users"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#loading-ibis",
    "href": "tutorials/ibis-for-dplyr-users.html#loading-ibis",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "To start using dplyr in R we would run:\n\nlibrary(dplyr)\n\nTo load Ibis:\n\nimport ibis\n\nAnd then also load and alias some helpers to make our code more concise:\n\nimport ibis.selectors as s\nfrom ibis import _\n\nLast, as mentioned above, to get Ibis to automatically execute our queries and show the results in a nicely-formatted table, we run:\n\nibis.options.interactive = True"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#loading-example-data",
    "href": "tutorials/ibis-for-dplyr-users.html#loading-example-data",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "In R, datasets are typically lazily loaded with packages. For instance, the starwars dataset is packaged with dplyr, but is not loaded in memory before you start using it. Ibis provides many datasets in the ibis.examples module. So to be able to use the starwars dataset, you can use:\n\nstarwars = ibis.examples.starwars.fetch()\n\nSimilar to dplyr, if we evaluate the name of a table, we get a nicely-formatted table:\n\nstarwars\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string        │ string      │ … │\n├────────────────────┼────────┼─────────┼───────────────┼─────────────┼───┤\n│ Luke Skywalker     │    172 │    77.0 │ blond         │ fair        │ … │\n│ C-3PO              │    167 │    75.0 │ NULL          │ gold        │ … │\n│ R2-D2              │     96 │    32.0 │ NULL          │ white, blue │ … │\n│ Darth Vader        │    202 │   136.0 │ none          │ white       │ … │\n│ Leia Organa        │    150 │    49.0 │ brown         │ light       │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey   │ light       │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown         │ light       │ … │\n│ R5-D4              │     97 │    32.0 │ NULL          │ white, red  │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black         │ light       │ … │\n│ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ fair        │ … │\n│ …                  │      … │       … │ …             │ …           │ … │\n└────────────────────┴────────┴─────────┴───────────────┴─────────────┴───┘\n\n\nIn addition to printing a nicely-formatted table and automatically executing, setting ibis.options.interactive to True also causes our query to be limited to 10 rows. To get Ibis to give us all rows, we can directly call to_pandas and save the result as a pandas DataFrame:\n\nstarwars_df = starwars.to_pandas()\n\nWhich then gives us all of the data as a pandas DataFrame:\n\nstarwars_df\n\n              name  height   mass hair_color  ... species films  vehicles starships\n0   Luke Skywalker   172.0   77.0      blond  ...   Human  None      None      None\n1            C-3PO   167.0   75.0       None  ...   Droid  None      None      None\n2            R2-D2    96.0   32.0       None  ...   Droid  None      None      None\n3      Darth Vader   202.0  136.0       none  ...   Human  None      None      None\n4      Leia Organa   150.0   49.0      brown  ...   Human  None      None      None\n..             ...     ...    ...        ...  ...     ...   ...       ...       ...\n82             Rey     NaN    NaN      brown  ...   Human  None      None      None\n83     Poe Dameron     NaN    NaN      brown  ...   Human  None      None      None\n84             BB8     NaN    NaN       none  ...   Droid  None      None      None\n85  Captain Phasma     NaN    NaN    unknown  ...    None  None      None      None\n86   Padmé Amidala   165.0   45.0      brown  ...   Human  None      None      None\n\n[87 rows x 14 columns]\n\n\nDirectly calling to_pandas and saving the result to a variable is useful for passing the results of Ibis table expressions to other packages (e.g., matplotlib."
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#inspecting-the-dataset-with-head",
    "href": "tutorials/ibis-for-dplyr-users.html#inspecting-the-dataset-with-head",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Just like in R, you can use head() to inspect the beginning of a dataset. You can also specify the number of rows you want to get back by using the parameter n (default n = 5).\nIn R:\n\nhead(starwars) # or starwars |&gt; head()\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Luke Sky…    172    77 blond      fair       blue            19   male  mascu…\n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n4 Darth Va…    202   136 none       white      yellow          41.9 male  mascu…\n5 Leia Org…    150    49 brown      light      brown           19   fema… femin…\n6 Owen Lars    178   120 brown, gr… light      blue            52   male  mascu…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nWith Ibis:\n\nstarwars.head(6)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name           ┃ height ┃ mass    ┃ hair_color  ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string         │ int64  │ float64 │ string      │ string      │ … │\n├────────────────┼────────┼─────────┼─────────────┼─────────────┼───┤\n│ Luke Skywalker │    172 │    77.0 │ blond       │ fair        │ … │\n│ C-3PO          │    167 │    75.0 │ NULL        │ gold        │ … │\n│ R2-D2          │     96 │    32.0 │ NULL        │ white, blue │ … │\n│ Darth Vader    │    202 │   136.0 │ none        │ white       │ … │\n│ Leia Organa    │    150 │    49.0 │ brown       │ light       │ … │\n│ Owen Lars      │    178 │   120.0 │ brown, grey │ light       │ … │\n└────────────────┴────────┴─────────┴─────────────┴─────────────┴───┘\n\n\nThere is no tail() in Ibis because most databases do not support this operation.\nAnother method you can use to limit the number of rows returned by a query is limit() which also takes the n parameter.\n\nstarwars.limit(3)\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━┓\n┃ name           ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ eye_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━┩\n│ string         │ int64  │ float64 │ string     │ string      │ string    │ … │\n├────────────────┼────────┼─────────┼────────────┼─────────────┼───────────┼───┤\n│ Luke Skywalker │    172 │    77.0 │ blond      │ fair        │ blue      │ … │\n│ C-3PO          │    167 │    75.0 │ NULL       │ gold        │ yellow    │ … │\n│ R2-D2          │     96 │    32.0 │ NULL       │ white, blue │ red       │ … │\n└────────────────┴────────┴─────────┴────────────┴─────────────┴───────────┴───┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#filtering-rows-with-filter",
    "href": "tutorials/ibis-for-dplyr-users.html#filtering-rows-with-filter",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Ibis, like dplyr, has filter to select rows based on conditions.\nWith dplyr:\n\nstarwars |&gt;\n  filter(skin_color == \"light\")\n\n# A tibble: 11 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Leia Or…    150    49 brown      light      brown             19 fema… femin…\n 2 Owen La…    178   120 brown, gr… light      blue              52 male  mascu…\n 3 Beru Wh…    165    75 brown      light      blue              47 fema… femin…\n 4 Biggs D…    183    84 black      light      brown             24 male  mascu…\n 5 Lobot       175    79 none       light      blue              37 male  mascu…\n 6 Cordé       157    NA brown      light      brown             NA fema… femin…\n 7 Dormé       165    NA brown      light      brown             NA fema… femin…\n 8 Raymus …    188    79 brown      light      brown             NA male  mascu…\n 9 Rey          NA    NA brown      light      hazel             NA fema… femin…\n10 Poe Dam…     NA    NA brown      light      brown             NA male  mascu…\n11 Padmé A…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis:\n\nstarwars.filter(_.skin_color == \"light\")\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color  ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string      │ string     │ … │\n├────────────────────┼────────┼─────────┼─────────────┼────────────┼───┤\n│ Leia Organa        │    150 │    49.0 │ brown       │ light      │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey │ light      │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown       │ light      │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black       │ light      │ … │\n│ Lobot              │    175 │    79.0 │ none        │ light      │ … │\n│ Cordé              │    157 │     nan │ brown       │ light      │ … │\n│ Dormé              │    165 │     nan │ brown       │ light      │ … │\n│ Raymus Antilles    │    188 │    79.0 │ brown       │ light      │ … │\n│ Rey                │   NULL │     nan │ brown       │ light      │ … │\n│ Poe Dameron        │   NULL │     nan │ brown       │ light      │ … │\n│ …                  │      … │       … │ …           │ …          │ … │\n└────────────────────┴────────┴─────────┴─────────────┴────────────┴───┘\n\n\nIn dplyr, you can specify multiple conditions separated with , that are then combined with the & operator:\n\nstarwars |&gt;\n  filter(skin_color == \"light\", eye_color == \"brown\")\n\n# A tibble: 7 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Biggs Da…    183    84 black      light      brown             24 male  mascu…\n3 Cordé        157    NA brown      light      brown             NA fema… femin…\n4 Dormé        165    NA brown      light      brown             NA fema… femin…\n5 Raymus A…    188    79 brown      light      brown             NA male  mascu…\n6 Poe Dame…     NA    NA brown      light      brown             NA male  mascu…\n7 Padmé Am…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, you can do the same by putting multiple conditions in a list:\n\nstarwars.filter([_.skin_color == \"light\", _.eye_color == \"brown\"])\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string     │ … │\n├───────────────────┼────────┼─────────┼────────────┼────────────┼───┤\n│ Leia Organa       │    150 │    49.0 │ brown      │ light      │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light      │ … │\n│ Cordé             │    157 │     nan │ brown      │ light      │ … │\n│ Dormé             │    165 │     nan │ brown      │ light      │ … │\n│ Raymus Antilles   │    188 │    79.0 │ brown      │ light      │ … │\n│ Poe Dameron       │   NULL │     nan │ brown      │ light      │ … │\n│ Padmé Amidala     │    165 │    45.0 │ brown      │ light      │ … │\n└───────────────────┴────────┴─────────┴────────────┴────────────┴───┘\n\n\nIn previous code, we used the _ helper we imported earlier. The _ is shorthand for the table returned by the previous step in the chained sequence of operations (in this case, starwars). We could have also written the more verbose form,\n\nstarwars.filter([starwars.skin_color == \"light\", starwars.eye_color == \"brown\"])\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string     │ … │\n├───────────────────┼────────┼─────────┼────────────┼────────────┼───┤\n│ Leia Organa       │    150 │    49.0 │ brown      │ light      │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light      │ … │\n│ Cordé             │    157 │     nan │ brown      │ light      │ … │\n│ Dormé             │    165 │     nan │ brown      │ light      │ … │\n│ Raymus Antilles   │    188 │    79.0 │ brown      │ light      │ … │\n│ Poe Dameron       │   NULL │     nan │ brown      │ light      │ … │\n│ Padmé Amidala     │    165 │    45.0 │ brown      │ light      │ … │\n└───────────────────┴────────┴─────────┴────────────┴────────────┴───┘\n\n\nIf you want to combine multiple conditions, in dplyr, you could do:\n\nstarwars |&gt;\n  filter(\n      (skin_color == \"light\" & eye_color == \"brown\") |\n       species == \"Droid\"\n  )\n\n# A tibble: 13 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 C-3PO       167    75 &lt;NA&gt;       gold       yellow           112 none  mascu…\n 2 R2-D2        96    32 &lt;NA&gt;       white, bl… red               33 none  mascu…\n 3 Leia Or…    150    49 brown      light      brown             19 fema… femin…\n 4 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu…\n 5 Biggs D…    183    84 black      light      brown             24 male  mascu…\n 6 IG-88       200   140 none       metal      red               15 none  mascu…\n 7 Cordé       157    NA brown      light      brown             NA fema… femin…\n 8 Dormé       165    NA brown      light      brown             NA fema… femin…\n 9 R4-P17       96    NA none       silver, r… red, blue         NA none  femin…\n10 Raymus …    188    79 brown      light      brown             NA male  mascu…\n11 Poe Dam…     NA    NA brown      light      brown             NA male  mascu…\n12 BB8          NA    NA none       none       black             NA none  mascu…\n13 Padmé A…    165    45 brown      light      brown             46 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, this would be:\n\nstarwars.filter(\n    ((_.skin_color == \"light\") & (_.eye_color == \"brown\")) |\n    (_.species == \"Droid\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name              ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string            │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ C-3PO             │    167 │    75.0 │ NULL       │ gold        │ … │\n│ R2-D2             │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R5-D4             │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ IG-88             │    200 │   140.0 │ none       │ metal       │ … │\n│ Leia Organa       │    150 │    49.0 │ brown      │ light       │ … │\n│ Biggs Darklighter │    183 │    84.0 │ black      │ light       │ … │\n│ Cordé             │    157 │     nan │ brown      │ light       │ … │\n│ Dormé             │    165 │     nan │ brown      │ light       │ … │\n│ R4-P17            │     96 │     nan │ none       │ silver, red │ … │\n│ BB8               │   NULL │     nan │ none       │ none        │ … │\n│ …                 │      … │       … │ …          │ …           │ … │\n└───────────────────┴────────┴─────────┴────────────┴─────────────┴───┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#sorting-your-data-with-order_by",
    "href": "tutorials/ibis-for-dplyr-users.html#sorting-your-data-with-order_by",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "To sort a column, dplyr has the verb arrange. For instance, to sort the column height using dplyr:\n\nstarwars |&gt;\n   arrange(height)\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Yoda         66    17 white      green      brown            896 male  mascu…\n 2 Ratts T…     79    15 none       grey, blue unknown           NA male  mascu…\n 3 Wicket …     88    20 brown      brown      brown              8 male  mascu…\n 4 Dud Bolt     94    45 none       blue, grey yellow            NA male  mascu…\n 5 R2-D2        96    32 &lt;NA&gt;       white, bl… red               33 none  mascu…\n 6 R4-P17       96    NA none       silver, r… red, blue         NA none  femin…\n 7 R5-D4        97    32 &lt;NA&gt;       white, red red               NA none  mascu…\n 8 Sebulba     112    40 none       grey, red  orange            NA male  mascu…\n 9 Gasgano     122    NA none       white, bl… black             NA male  mascu…\n10 Watto       137    NA black      blue, grey yellow            NA male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIbis has the order_by method, so to perform the same operation:\n\nstarwars.order_by(_.height)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name                  ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string                │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ Yoda                  │     66 │    17.0 │ white      │ green       │ … │\n│ Ratts Tyerell         │     79 │    15.0 │ none       │ grey, blue  │ … │\n│ Wicket Systri Warrick │     88 │    20.0 │ brown      │ brown       │ … │\n│ Dud Bolt              │     94 │    45.0 │ none       │ blue, grey  │ … │\n│ R2-D2                 │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R4-P17                │     96 │     nan │ none       │ silver, red │ … │\n│ R5-D4                 │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ Sebulba               │    112 │    40.0 │ none       │ grey, red   │ … │\n│ Gasgano               │    122 │     nan │ none       │ white, blue │ … │\n│ Watto                 │    137 │     nan │ black      │ blue, grey  │ … │\n│ …                     │      … │       … │ …          │ …           │ … │\n└───────────────────────┴────────┴─────────┴────────────┴─────────────┴───┘\n\n\nYou might notice that while dplyr puts missing values at the end, Ibis places them at the top. This behavior can actually vary from backend to backend and is something to be aware of when using Ibis.\nIf you want to order using multiple variables, you can pass them as a list:\n\nstarwars.order_by([_.height, _.mass]) # or starwars.order_by([\"height\", \"mass\"])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name                  ┃ height ┃ mass    ┃ hair_color ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string                │ int64  │ float64 │ string     │ string      │ … │\n├───────────────────────┼────────┼─────────┼────────────┼─────────────┼───┤\n│ Yoda                  │     66 │    17.0 │ white      │ green       │ … │\n│ Ratts Tyerell         │     79 │    15.0 │ none       │ grey, blue  │ … │\n│ Wicket Systri Warrick │     88 │    20.0 │ brown      │ brown       │ … │\n│ Dud Bolt              │     94 │    45.0 │ none       │ blue, grey  │ … │\n│ R2-D2                 │     96 │    32.0 │ NULL       │ white, blue │ … │\n│ R4-P17                │     96 │     nan │ none       │ silver, red │ … │\n│ R5-D4                 │     97 │    32.0 │ NULL       │ white, red  │ … │\n│ Sebulba               │    112 │    40.0 │ none       │ grey, red   │ … │\n│ Gasgano               │    122 │     nan │ none       │ white, blue │ … │\n│ Watto                 │    137 │     nan │ black      │ blue, grey  │ … │\n│ …                     │      … │       … │ …          │ …           │ … │\n└───────────────────────┴────────┴─────────┴────────────┴─────────────┴───┘\n\n\nTo order a column in descending order, there are two ways to do it. Note that missing values remain at the top.\n\nstarwars.order_by(_.height.desc()) # or: starwars.order_by(ibis.desc(\"height\"))\n\n┏━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━┓\n┃ name         ┃ height ┃ mass    ┃ hair_color ┃ skin_color   ┃ … ┃\n┡━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━┩\n│ string       │ int64  │ float64 │ string     │ string       │ … │\n├──────────────┼────────┼─────────┼────────────┼──────────────┼───┤\n│ Yarael Poof  │    264 │     nan │ none       │ white        │ … │\n│ Tarfful      │    234 │   136.0 │ brown      │ brown        │ … │\n│ Lama Su      │    229 │    88.0 │ none       │ grey         │ … │\n│ Chewbacca    │    228 │   112.0 │ brown      │ unknown      │ … │\n│ Roos Tarpals │    224 │    82.0 │ none       │ grey         │ … │\n│ Grievous     │    216 │   159.0 │ none       │ brown, white │ … │\n│ Taun We      │    213 │     nan │ none       │ grey         │ … │\n│ Rugor Nass   │    206 │     nan │ none       │ green        │ … │\n│ Tion Medon   │    206 │    80.0 │ none       │ grey         │ … │\n│ Darth Vader  │    202 │   136.0 │ none       │ white        │ … │\n│ …            │      … │       … │ …          │ …            │ … │\n└──────────────┴────────┴─────────┴────────────┴──────────────┴───┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#selecting-columns-with-select",
    "href": "tutorials/ibis-for-dplyr-users.html#selecting-columns-with-select",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Ibis, like dplyr, has a select method to include or exclude columns:\nWith dplyr:\n\nstarwars |&gt;\n    select(hair_color)\n\n# A tibble: 87 × 1\n   hair_color   \n   &lt;chr&gt;        \n 1 blond        \n 2 &lt;NA&gt;         \n 3 &lt;NA&gt;         \n 4 none         \n 5 brown        \n 6 brown, grey  \n 7 brown        \n 8 &lt;NA&gt;         \n 9 black        \n10 auburn, white\n# ℹ 77 more rows\n\n\nIn Ibis:\n\nstarwars.select(_.hair_color)\n\n┏━━━━━━━━━━━━━━━┓\n┃ hair_color    ┃\n┡━━━━━━━━━━━━━━━┩\n│ string        │\n├───────────────┤\n│ blond         │\n│ NULL          │\n│ NULL          │\n│ none          │\n│ brown         │\n│ brown, grey   │\n│ brown         │\n│ NULL          │\n│ black         │\n│ auburn, white │\n│ …             │\n└───────────────┘\n\n\nNote: A common pitfall to be aware of when referencing column names in Ibis is when column names collide with built-in methods on the Ibis Table object, such as count. In this situation, you will have to reference count like table[\"count\"] or _[\"count\"].\ndplyr also allows selecting more than one column at a time:\n\nstarwars |&gt;\n    select(hair_color, skin_color, eye_color)\n\n# A tibble: 87 × 3\n   hair_color    skin_color  eye_color\n   &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 blond         fair        blue     \n 2 &lt;NA&gt;          gold        yellow   \n 3 &lt;NA&gt;          white, blue red      \n 4 none          white       yellow   \n 5 brown         light       brown    \n 6 brown, grey   light       blue     \n 7 brown         light       blue     \n 8 &lt;NA&gt;          white, red  red      \n 9 black         light       brown    \n10 auburn, white fair        blue-gray\n# ℹ 77 more rows\n\n\nIn Ibis, we can either quote the names:\n\nstarwars.select(\"hair_color\", \"skin_color\", \"eye_color\")\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\nOr use the _ helper:\n\nstarwars.select(_.hair_color, _.skin_color, _.eye_color)\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\nTo select columns by name based on a condition, dplyr has helpers such as:\n\nstarts_with(): Starts with a prefix.\nends_with(): Ends with a suffix.\ncontains(): Contains a literal string.\n\nThese and many more selectors are available in Ibis as well, with slightly different names:\n\nstarwars.select(s.startswith(\"h\"))\n\n┏━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ height ┃ hair_color    ┃ homeworld ┃\n┡━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ int64  │ string        │ string    │\n├────────┼───────────────┼───────────┤\n│    172 │ blond         │ Tatooine  │\n│    167 │ NULL          │ Tatooine  │\n│     96 │ NULL          │ Naboo     │\n│    202 │ none          │ Tatooine  │\n│    150 │ brown         │ Alderaan  │\n│    178 │ brown, grey   │ Tatooine  │\n│    165 │ brown         │ Tatooine  │\n│     97 │ NULL          │ Tatooine  │\n│    183 │ black         │ Tatooine  │\n│    182 │ auburn, white │ Stewjon   │\n│      … │ …             │ …         │\n└────────┴───────────────┴───────────┘\n\n\n\nstarwars.select(s.endswith(\"color\"))\n\n┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string        │ string      │ string    │\n├───────────────┼─────────────┼───────────┤\n│ blond         │ fair        │ blue      │\n│ NULL          │ gold        │ yellow    │\n│ NULL          │ white, blue │ red       │\n│ none          │ white       │ yellow    │\n│ brown         │ light       │ brown     │\n│ brown, grey   │ light       │ blue      │\n│ brown         │ light       │ blue      │\n│ NULL          │ white, red  │ red       │\n│ black         │ light       │ brown     │\n│ auburn, white │ fair        │ blue-gray │\n│ …             │ …           │ …         │\n└───────────────┴─────────────┴───────────┘\n\n\n\nstarwars.select(s.contains(\"world\"))\n\n┏━━━━━━━━━━━┓\n┃ homeworld ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ Tatooine  │\n│ Tatooine  │\n│ Naboo     │\n│ Tatooine  │\n│ Alderaan  │\n│ Tatooine  │\n│ Tatooine  │\n│ Tatooine  │\n│ Tatooine  │\n│ Stewjon   │\n│ …         │\n└───────────┘\n\n\nSee the Ibis Column Selectors documentation for the full list of selectors in Ibis."
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#renaming-columns-with-rename",
    "href": "tutorials/ibis-for-dplyr-users.html#renaming-columns-with-rename",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Ibis allows you to rename columns using rename() which provides similar functionality to rename() in dplyr.\nIn dplyr:\n\nstarwars |&gt;\n    rename(\"home_world\" = \"homeworld\")\n\n# A tibble: 87 × 14\n   name     height  mass hair_color skin_color eye_color birth_year sex   gender\n   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n 1 Luke Sk…    172    77 blond      fair       blue            19   male  mascu…\n 2 C-3PO       167    75 &lt;NA&gt;       gold       yellow         112   none  mascu…\n 3 R2-D2        96    32 &lt;NA&gt;       white, bl… red             33   none  mascu…\n 4 Darth V…    202   136 none       white      yellow          41.9 male  mascu…\n 5 Leia Or…    150    49 brown      light      brown           19   fema… femin…\n 6 Owen La…    178   120 brown, gr… light      blue            52   male  mascu…\n 7 Beru Wh…    165    75 brown      light      blue            47   fema… femin…\n 8 R5-D4        97    32 &lt;NA&gt;       white, red red             NA   none  mascu…\n 9 Biggs D…    183    84 black      light      brown           24   male  mascu…\n10 Obi-Wan…    182    77 auburn, w… fair       blue-gray       57   male  mascu…\n# ℹ 77 more rows\n# ℹ 5 more variables: home_world &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n\n\nIn Ibis, use rename and pass a dict of name mappings:\n\nstarwars.rename(home_world=\"homeworld\")\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━┓\n┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ skin_color  ┃ … ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━┩\n│ string             │ int64  │ float64 │ string        │ string      │ … │\n├────────────────────┼────────┼─────────┼───────────────┼─────────────┼───┤\n│ Luke Skywalker     │    172 │    77.0 │ blond         │ fair        │ … │\n│ C-3PO              │    167 │    75.0 │ NULL          │ gold        │ … │\n│ R2-D2              │     96 │    32.0 │ NULL          │ white, blue │ … │\n│ Darth Vader        │    202 │   136.0 │ none          │ white       │ … │\n│ Leia Organa        │    150 │    49.0 │ brown         │ light       │ … │\n│ Owen Lars          │    178 │   120.0 │ brown, grey   │ light       │ … │\n│ Beru Whitesun lars │    165 │    75.0 │ brown         │ light       │ … │\n│ R5-D4              │     97 │    32.0 │ NULL          │ white, red  │ … │\n│ Biggs Darklighter  │    183 │    84.0 │ black         │ light       │ … │\n│ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ fair        │ … │\n│ …                  │      … │       … │ …             │ …           │ … │\n└────────────────────┴────────┴─────────┴───────────────┴─────────────┴───┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#add-new-columns-with-mutate",
    "href": "tutorials/ibis-for-dplyr-users.html#add-new-columns-with-mutate",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Ibis, like dplyr, uses the mutate verb to add columns.\nIn dplyr,\n\nstarwars |&gt;\n    mutate(height_m = height / 100) |&gt;\n    select(name, height_m)\n\n# A tibble: 87 × 2\n   name               height_m\n   &lt;chr&gt;                 &lt;dbl&gt;\n 1 Luke Skywalker         1.72\n 2 C-3PO                  1.67\n 3 R2-D2                  0.96\n 4 Darth Vader            2.02\n 5 Leia Organa            1.5 \n 6 Owen Lars              1.78\n 7 Beru Whitesun lars     1.65\n 8 R5-D4                  0.97\n 9 Biggs Darklighter      1.83\n10 Obi-Wan Kenobi         1.82\n# ℹ 77 more rows\n\n\nIn Ibis:\n\n(\n    starwars\n        .mutate(height_m = _.height / 100)\n        .select(\"name\", \"height_m\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ name               ┃ height_m ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ string             │ float64  │\n├────────────────────┼──────────┤\n│ Luke Skywalker     │     1.72 │\n│ C-3PO              │     1.67 │\n│ R2-D2              │     0.96 │\n│ Darth Vader        │     2.02 │\n│ Leia Organa        │     1.50 │\n│ Owen Lars          │     1.78 │\n│ Beru Whitesun lars │     1.65 │\n│ R5-D4              │     0.97 │\n│ Biggs Darklighter  │     1.83 │\n│ Obi-Wan Kenobi     │     1.82 │\n│ …                  │        … │\n└────────────────────┴──────────┘\n\n\nA big difference between dplyr’s mutate and Ibis’ mutate is that, in Ibis, you have to chain separate mutate calls together when you reference newly-created columns in the same mutate whereas in dplyr, you can put them all in the same call. This makes Ibis’ mutate more similar to transform in base R.\nIn dplyr, we only need one mutate call:\n\nstarwars %&gt;%\n  mutate(\n    height_m = height / 100,\n    BMI = mass / (height_m^2)\n  ) %&gt;%\n  select(BMI, everything())\n\n# A tibble: 87 × 16\n     BMI name      height  mass hair_color skin_color eye_color birth_year sex  \n   &lt;dbl&gt; &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;\n 1  26.0 Luke Sky…    172    77 blond      fair       blue            19   male \n 2  26.9 C-3PO        167    75 &lt;NA&gt;       gold       yellow         112   none \n 3  34.7 R2-D2         96    32 &lt;NA&gt;       white, bl… red             33   none \n 4  33.3 Darth Va…    202   136 none       white      yellow          41.9 male \n 5  21.8 Leia Org…    150    49 brown      light      brown           19   fema…\n 6  37.9 Owen Lars    178   120 brown, gr… light      blue            52   male \n 7  27.5 Beru Whi…    165    75 brown      light      blue            47   fema…\n 8  34.0 R5-D4         97    32 &lt;NA&gt;       white, red red             NA   none \n 9  25.1 Biggs Da…    183    84 black      light      brown           24   male \n10  23.2 Obi-Wan …    182    77 auburn, w… fair       blue-gray       57   male \n# ℹ 77 more rows\n# ℹ 7 more variables: gender &lt;chr&gt;, homeworld &lt;chr&gt;, species &lt;chr&gt;,\n#   films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;, height_m &lt;dbl&gt;\n\n\nIn Ibis, for BMI to reference height_m, it needs to be in a separate mutate call:\n\n(starwars\n    .mutate(\n        height_m = _.height / 100\n    )\n    .mutate(\n        BMI = _.mass / (_.height_m**2)\n    )\n    .select(\"BMI\", ~s.matches(\"BMI\"))\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━┓\n┃ BMI       ┃ name               ┃ height ┃ mass    ┃ hair_color    ┃ … ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━┩\n│ float64   │ string             │ int64  │ float64 │ string        │ … │\n├───────────┼────────────────────┼────────┼─────────┼───────────────┼───┤\n│ 26.027582 │ Luke Skywalker     │    172 │    77.0 │ blond         │ … │\n│ 26.892323 │ C-3PO              │    167 │    75.0 │ NULL          │ … │\n│ 34.722222 │ R2-D2              │     96 │    32.0 │ NULL          │ … │\n│ 33.330066 │ Darth Vader        │    202 │   136.0 │ none          │ … │\n│ 21.777778 │ Leia Organa        │    150 │    49.0 │ brown         │ … │\n│ 37.874006 │ Owen Lars          │    178 │   120.0 │ brown, grey   │ … │\n│ 27.548209 │ Beru Whitesun lars │    165 │    75.0 │ brown         │ … │\n│ 34.009990 │ R5-D4              │     97 │    32.0 │ NULL          │ … │\n│ 25.082863 │ Biggs Darklighter  │    183 │    84.0 │ black         │ … │\n│ 23.245985 │ Obi-Wan Kenobi     │    182 │    77.0 │ auburn, white │ … │\n│         … │ …                  │      … │       … │ …             │ … │\n└───────────┴────────────────────┴────────┴─────────┴───────────────┴───┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#summarize-values-with-aggregate",
    "href": "tutorials/ibis-for-dplyr-users.html#summarize-values-with-aggregate",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "To summarize tables, dplyr has the verbs summarise/summarize:\nIn dplyr:\n\nstarwars %&gt;%\n    summarise(height = mean(height, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  height\n   &lt;dbl&gt;\n1   174.\n\n\nIn Ibis, the corresponding verb is aggregate:\n\nstarwars.aggregate(height = _.height.mean())\n\n┏━━━━━━━━━━━━┓\n┃ height     ┃\n┡━━━━━━━━━━━━┩\n│ float64    │\n├────────────┤\n│ 174.358025 │\n└────────────┘\n\n\nNote: Throughout this guide, where dplyr uses R generics, Ibis uses Python methods. In the previous code cell, aggregate is a method on a table and mean is a method on a column. If you want to perform aggregations on multiple columns, you can call the method that you want on the column you want to apply it to."
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#join-tables-with-left_join",
    "href": "tutorials/ibis-for-dplyr-users.html#join-tables-with-left_join",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "To demonstrate how to do joins with Ibis, we’ll load two more example datasets that also come from the example datasets included in dplyr:\n\nband_members = ibis.examples.band_members.fetch()\nband_instruments = ibis.examples.band_instruments.fetch()\n\nIn dplyr, we can perform a left join of these two tables like:\n\nband_members |&gt;\n    left_join(band_instruments)\n\n# A tibble: 3 × 3\n  name  band    plays \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt; \n1 Mick  Stones  &lt;NA&gt;  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\n\nIn Ibis:\n\nband_members.left_join(band_instruments, \"name\")\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n┃ name   ┃ band    ┃ name_right ┃ plays  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ string     │ string │\n├────────┼─────────┼────────────┼────────┤\n│ John   │ Beatles │ John       │ guitar │\n│ Paul   │ Beatles │ Paul       │ bass   │\n│ Mick   │ Stones  │ NULL       │ NULL   │\n└────────┴─────────┴────────────┴────────┘\n\n\nThere are two main differences between Ibis and dplyr here:\n\nIbis requires us to explicitly specify our join key (“name”, in this example) whereas in dplyr, if the join key is missing, we get the natural join of the two tables which joins across all shared column names\nIbis keeps columns for join keys from each table whereas dplyr does not by default\n\nTo replicate the result we’d get by default in dplyr but using Ibis, we need to incorporate two other verbs we’ve already seen in this tutorial:\n\n(\n    band_members\n        .left_join(band_instruments, \"name\")\n        .select(~s.contains(\"_right\"))\n)\n\n┏━━━━━━━━┳━━━━━━━━━┳━━━━━━━━┓\n┃ name   ┃ band    ┃ plays  ┃\n┡━━━━━━━━╇━━━━━━━━━╇━━━━━━━━┩\n│ string │ string  │ string │\n├────────┼─────────┼────────┤\n│ John   │ Beatles │ guitar │\n│ Paul   │ Beatles │ bass   │\n│ Mick   │ Stones  │ NULL   │\n└────────┴─────────┴────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#pivot-data-with-pivot_wider-and-pivot_longer",
    "href": "tutorials/ibis-for-dplyr-users.html#pivot-data-with-pivot_wider-and-pivot_longer",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "dplyr users are likely to be familiar with the pivot_wider and pivot_longer functions from the tidyr package which convert tables between wide and long formats, respectively.\npivot_longer in dplyr + tidyr:\n\nlibrary(tidyr)\n\nstarwars_colors &lt;-\n    starwars |&gt;\n        select(name, matches(\"color\")) |&gt;\n        pivot_longer(matches(\"color\"), names_to = \"attribute\", values_to = \"color\")\n\nIn Ibis:\n\nstarwars_colors = (\n    starwars\n        .select(\"name\", s.matches(\"color\"))\n        .pivot_longer(s.matches(\"color\"), names_to=\"attribute\", values_to=\"color\")\n)\n\nstarwars_colors\n\n┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━┓\n┃ name           ┃ attribute  ┃ color       ┃\n┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━┩\n│ string         │ string     │ string      │\n├────────────────┼────────────┼─────────────┤\n│ Luke Skywalker │ hair_color │ blond       │\n│ Luke Skywalker │ skin_color │ fair        │\n│ Luke Skywalker │ eye_color  │ blue        │\n│ C-3PO          │ hair_color │ NULL        │\n│ C-3PO          │ skin_color │ gold        │\n│ C-3PO          │ eye_color  │ yellow      │\n│ R2-D2          │ hair_color │ NULL        │\n│ R2-D2          │ skin_color │ white, blue │\n│ R2-D2          │ eye_color  │ red         │\n│ Darth Vader    │ hair_color │ none        │\n│ …              │ …          │ …           │\n└────────────────┴────────────┴─────────────┘\n\n\nAnd pivot_wider:\n\nstarwars_colors |&gt;\n    pivot_wider(names_from = \"attribute\", values_from = \"color\")\n\n# A tibble: 87 × 4\n   name               hair_color    skin_color  eye_color\n   &lt;chr&gt;              &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;    \n 1 Luke Skywalker     blond         fair        blue     \n 2 C-3PO              &lt;NA&gt;          gold        yellow   \n 3 R2-D2              &lt;NA&gt;          white, blue red      \n 4 Darth Vader        none          white       yellow   \n 5 Leia Organa        brown         light       brown    \n 6 Owen Lars          brown, grey   light       blue     \n 7 Beru Whitesun lars brown         light       blue     \n 8 R5-D4              &lt;NA&gt;          white, red  red      \n 9 Biggs Darklighter  black         light       brown    \n10 Obi-Wan Kenobi     auburn, white fair        blue-gray\n# ℹ 77 more rows\n\n\nIn Ibis:\n\n(\n    starwars_colors.\n        pivot_wider(names_from=\"attribute\", values_from=\"color\")\n)\n\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ name               ┃ hair_color    ┃ skin_color  ┃ eye_color ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string             │ string        │ string      │ string    │\n├────────────────────┼───────────────┼─────────────┼───────────┤\n│ Luke Skywalker     │ blond         │ fair        │ blue      │\n│ C-3PO              │ NULL          │ gold        │ yellow    │\n│ R2-D2              │ NULL          │ white, blue │ red       │\n│ Darth Vader        │ none          │ white       │ yellow    │\n│ Leia Organa        │ brown         │ light       │ brown     │\n│ Owen Lars          │ brown, grey   │ light       │ blue      │\n│ Beru Whitesun lars │ brown         │ light       │ blue      │\n│ R5-D4              │ NULL          │ white, red  │ red       │\n│ Biggs Darklighter  │ black         │ light       │ brown     │\n│ Obi-Wan Kenobi     │ auburn, white │ fair        │ blue-gray │\n│ …                  │ …             │ …           │ …         │\n└────────────────────┴───────────────┴─────────────┴───────────┘"
  },
  {
    "objectID": "tutorials/ibis-for-dplyr-users.html#next-steps",
    "href": "tutorials/ibis-for-dplyr-users.html#next-steps",
    "title": "Tutorial: Ibis for dplyr users",
    "section": "",
    "text": "Now that you’ve gotten an introduction to the common differences between dplyr and Ibis, head over to Getting started with ibis for a full introduction. If you’re familiar with SQL, check out Ibis for SQL users. If you’re familiar with pandas, take a look at Ibis for pandas users"
  },
  {
    "objectID": "tutorials/getting_started.html",
    "href": "tutorials/getting_started.html",
    "title": "Tutorial: getting started",
    "section": "",
    "text": "This is a quick tour of some basic commands and usage patterns, just to get your flippers wet."
  },
  {
    "objectID": "tutorials/getting_started.html#install-ibis",
    "href": "tutorials/getting_started.html#install-ibis",
    "title": "Tutorial: getting started",
    "section": "Install Ibis",
    "text": "Install Ibis\nWe recommend starting with the default (DuckDB) backend for a performant, fully-featured local experience. You can install Ibis with pip, conda, mamba, or pixi.\n\nUsing pipUsing condaUsing mambaUsing pixi\n\n\npip install 'ibis-framework[duckdb]'\n\n\n\n\n\n\nWarning\n\n\n\nNote that the ibis-framework package is not the same as the ibis package in PyPI. These two libraries cannot coexist in the same Python environment, as they are both imported with the ibis module name.\n\n\n\n\nconda install -c conda-forge ibis-duckdb\n\n\nmamba install -c conda-forge ibis-duckdb\n\n\npixi add ibis-duckdb"
  },
  {
    "objectID": "tutorials/getting_started.html#create-a-database-file",
    "href": "tutorials/getting_started.html#create-a-database-file",
    "title": "Tutorial: getting started",
    "section": "Create a database file",
    "text": "Create a database file\nIbis can work with several file types, but at its core it connects to existing databases and interacts with the data there. We’ll analyze the Palmer penguins 1 dataset with DuckDB to get the hang of this.\n\nimport ibis\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")\ncon.create_table(\n    \"penguins\", ibis.examples.penguins.fetch().to_pyarrow(), overwrite=True\n)\n\nDatabaseTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\n\n\nYou can now see the example dataset copied over to the database:\n\ncon = ibis.connect(\"duckdb://penguins.ddb\")\ncon.list_tables()\n\n['penguins']\n\n\nThere’s one table, called penguins. We can ask Ibis to give us an object that we can interact with.\n\npenguins = con.table(\"penguins\")\npenguins\n\nDatabaseTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\n\n\nIbis is lazily evaluated, so instead of seeing the data, we see the schema of the table, instead. To peek at the data, we can call head and then to_pandas to get the first few rows of the table as a pandas DataFrame.\n\npenguins.head().to_pandas()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNone\n2007\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nfemale\n2007\n\n\n\n\n\n\n\nto_pandas takes the existing lazy table expression and evaluates it. If we leave it off, you’ll see the Ibis representation of the table expression that to_pandas will evaluate (when you’re ready!).\n\npenguins.head()\n\nr0 := DatabaseTable: penguins\n  species           string\n  island            string\n  bill_length_mm    float64\n  bill_depth_mm     float64\n  flipper_length_mm int64\n  body_mass_g       int64\n  sex               string\n  year              int64\n\nLimit[r0, n=5]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIbis returns results as a pandas DataFrame using to_pandas, but isn’t using pandas to perform any of the computation. The query is executed by the backend (DuckDB in this case). Only when to_pandas is called does Ibis then pull back the results and convert them into a DataFrame."
  },
  {
    "objectID": "tutorials/getting_started.html#interactive-mode",
    "href": "tutorials/getting_started.html#interactive-mode",
    "title": "Tutorial: getting started",
    "section": "Interactive mode",
    "text": "Interactive mode\nFor the rest of this intro, we’ll turn on interactive mode, which partially executes queries to give users a preview of the results. There is a small difference in the way the output is formatted, but otherwise this is the same as calling to_pandas on the table expression with a limit of 10 result rows returned.\n\nibis.options.interactive = True\npenguins.head()\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘"
  },
  {
    "objectID": "tutorials/getting_started.html#common-operations",
    "href": "tutorials/getting_started.html#common-operations",
    "title": "Tutorial: getting started",
    "section": "Common operations",
    "text": "Common operations\nIbis has a collection of useful table methods to manipulate and query the data in a table (or tables).\n\nfilter\nfilter allows you to select rows based on a condition or set of conditions.\nWe can filter so we only have penguins of the species Adelie:\n\npenguins.filter(penguins.species == \"Adelie\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nOr filter for Adelie penguins that reside on the island of Torgersen:\n\npenguins.filter((penguins.island == \"Torgersen\") & (penguins.species == \"Adelie\"))\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┘\n\n\n\nYou can use any boolean comparison in a filter (although if you try to do something like use &lt; on a string, Ibis will yell at you).\n\n\nselect\nYour data analysis might not require all the columns present in a given table. select lets you pick out only those columns that you want to work with.\nTo select a column you can use the name of the column as a string:\n\npenguins.select(\"species\", \"island\", \"year\")\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\n\n\n\nOr you can use column objects directly (this can be convenient when paired with tab-completion):\n\npenguins.select(penguins.species, penguins.island, penguins.year)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\n\n\n\nOr you can mix-and-match:\n\npenguins.select(\"species\", \"island\", penguins.year)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━┓\n┃ species ┃ island    ┃ year  ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━┩\n│ string  │ string    │ int64 │\n├─────────┼───────────┼───────┤\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ Adelie  │ Torgersen │  2007 │\n│ …       │ …         │     … │\n└─────────┴───────────┴───────┘\n\n\n\n\n\nmutate\nmutate lets you add new columns to your table, derived from the values of existing columns.\n\npenguins.mutate(bill_length_cm=penguins.bill_length_mm / 10)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64        │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │           39.1 │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │           39.5 │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │           40.3 │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │           NULL │          NULL │              NULL │        NULL │ NULL   │  2007 │           NULL │\n│ Adelie  │ Torgersen │           36.7 │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │           39.3 │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │           38.9 │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │           39.2 │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │           34.1 │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │           42.0 │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │              … │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\n\n\n\nNotice that the table is a little too wide to display all the columns now (depending on your screen-size). bill_length is now present in millimeters AND centimeters. Use a select to trim down the number of columns we’re looking at.\n\npenguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n    \"species\",\n    \"island\",\n    \"bill_depth_mm\",\n    \"flipper_length_mm\",\n    \"body_mass_g\",\n    \"sex\",\n    \"year\",\n    \"bill_length_cm\",\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │          NULL │              NULL │        NULL │ NULL   │  2007 │           NULL │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\n\n\n\n\n\nselectors\nTyping out ALL of the column names except one is a little annoying. Instead of doing that again, we can use a selector to quickly select or deselect groups of columns.\n\nimport ibis.selectors as s\n\npenguins.mutate(bill_length_cm=penguins.bill_length_mm / 10).select(\n    ~s.matches(\"bill_length_mm\")\n    # match every column except `bill_length_mm`\n)\n\n┏━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃ species ┃ island    ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ sex    ┃ year  ┃ bill_length_cm ┃\n┡━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│ string  │ string    │ float64       │ int64             │ int64       │ string │ int64 │ float64        │\n├─────────┼───────────┼───────────────┼───────────────────┼─────────────┼────────┼───────┼────────────────┤\n│ Adelie  │ Torgersen │          18.7 │               181 │        3750 │ male   │  2007 │           3.91 │\n│ Adelie  │ Torgersen │          17.4 │               186 │        3800 │ female │  2007 │           3.95 │\n│ Adelie  │ Torgersen │          18.0 │               195 │        3250 │ female │  2007 │           4.03 │\n│ Adelie  │ Torgersen │          NULL │              NULL │        NULL │ NULL   │  2007 │           NULL │\n│ Adelie  │ Torgersen │          19.3 │               193 │        3450 │ female │  2007 │           3.67 │\n│ Adelie  │ Torgersen │          20.6 │               190 │        3650 │ male   │  2007 │           3.93 │\n│ Adelie  │ Torgersen │          17.8 │               181 │        3625 │ female │  2007 │           3.89 │\n│ Adelie  │ Torgersen │          19.6 │               195 │        4675 │ male   │  2007 │           3.92 │\n│ Adelie  │ Torgersen │          18.1 │               193 │        3475 │ NULL   │  2007 │           3.41 │\n│ Adelie  │ Torgersen │          20.2 │               190 │        4250 │ NULL   │  2007 │           4.20 │\n│ …       │ …         │             … │                 … │           … │ …      │     … │              … │\n└─────────┴───────────┴───────────────┴───────────────────┴─────────────┴────────┴───────┴────────────────┘\n\n\n\nYou can also use a selector alongside a column name.\n\npenguins.select(\"island\", s.numeric())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━┓\n┃ island    ┃ bill_length_mm ┃ bill_depth_mm ┃ flipper_length_mm ┃ body_mass_g ┃ year  ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━┩\n│ string    │ float64        │ float64       │ int64             │ int64       │ int64 │\n├───────────┼────────────────┼───────────────┼───────────────────┼─────────────┼───────┤\n│ Torgersen │           39.1 │          18.7 │               181 │        3750 │  2007 │\n│ Torgersen │           39.5 │          17.4 │               186 │        3800 │  2007 │\n│ Torgersen │           40.3 │          18.0 │               195 │        3250 │  2007 │\n│ Torgersen │           NULL │          NULL │              NULL │        NULL │  2007 │\n│ Torgersen │           36.7 │          19.3 │               193 │        3450 │  2007 │\n│ Torgersen │           39.3 │          20.6 │               190 │        3650 │  2007 │\n│ Torgersen │           38.9 │          17.8 │               181 │        3625 │  2007 │\n│ Torgersen │           39.2 │          19.6 │               195 │        4675 │  2007 │\n│ Torgersen │           34.1 │          18.1 │               193 │        3475 │  2007 │\n│ Torgersen │           42.0 │          20.2 │               190 │        4250 │  2007 │\n│ …         │              … │             … │                 … │           … │     … │\n└───────────┴────────────────┴───────────────┴───────────────────┴─────────────┴───────┘\n\n\n\nYou can read more about selectors in the docs!\n\n\norder_by\norder_by arranges the values of one or more columns in ascending or descending order.\nBy default, ibis sorts in ascending order:\n\npenguins.order_by(penguins.flipper_length_mm).select(\n    \"species\", \"island\", \"flipper_length_mm\"\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ flipper_length_mm ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ int64             │\n├───────────┼───────────┼───────────────────┤\n│ Adelie    │ Biscoe    │               172 │\n│ Adelie    │ Biscoe    │               174 │\n│ Adelie    │ Torgersen │               176 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Adelie    │ Dream     │               178 │\n│ Chinstrap │ Dream     │               178 │\n│ Adelie    │ Dream     │               179 │\n│ Adelie    │ Torgersen │               180 │\n│ Adelie    │ Biscoe    │               180 │\n│ …         │ …         │                 … │\n└───────────┴───────────┴───────────────────┘\n\n\n\nYou can sort in descending order using the desc method of a column:\n\npenguins.order_by(penguins.flipper_length_mm.desc()).select(\n    \"species\", \"island\", \"flipper_length_mm\"\n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string │ int64             │\n├─────────┼────────┼───────────────────┤\n│ Gentoo  │ Biscoe │               231 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               229 │\n│ Gentoo  │ Biscoe │               229 │\n│ …       │ …      │                 … │\n└─────────┴────────┴───────────────────┘\n\n\n\nOr you can use ibis.desc\n\npenguins.order_by(ibis.desc(\"flipper_length_mm\")).select(\n    \"species\", \"island\", \"flipper_length_mm\"\n)\n\n┏━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃ species ┃ island ┃ flipper_length_mm ┃\n┡━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ string  │ string │ int64             │\n├─────────┼────────┼───────────────────┤\n│ Gentoo  │ Biscoe │               231 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               230 │\n│ Gentoo  │ Biscoe │               229 │\n│ Gentoo  │ Biscoe │               229 │\n│ …       │ …      │                 … │\n└─────────┴────────┴───────────────────┘\n\n\n\n\n\naggregates\nIbis has several aggregate functions available to help summarize data.\nmean, max, min, count, sum (the list goes on).\nTo aggregate an entire column, call the corresponding method on that column.\n\npenguins.flipper_length_mm.mean()\n\n\n\n\n\n200.91520467836258\n\n\n\nYou can compute multiple aggregates at once using the aggregate method:\n\npenguins.aggregate([penguins.flipper_length_mm.mean(), penguins.bill_depth_mm.max()])\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓\n┃ Mean(flipper_length_mm) ┃ Max(bill_depth_mm) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩\n│ float64                 │ float64            │\n├─────────────────────────┼────────────────────┤\n│              200.915205 │               21.5 │\n└─────────────────────────┴────────────────────┘\n\n\n\nBut aggregate really shines when it’s paired with group_by.\n\n\ngroup_by\ngroup_by creates groupings of rows that have the same value for one or more columns.\nBut it doesn’t do much on its own – you can pair it with aggregate to get a result.\n\npenguins.group_by(\"species\").aggregate()\n\n┏━━━━━━━━━━━┓\n┃ species   ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ Chinstrap │\n│ Gentoo    │\n│ Adelie    │\n└───────────┘\n\n\n\nWe grouped by the species column and handed it an “empty” aggregate command. The result of that is a column of the unique values in the species column.\nIf we add a second column to the group_by, we’ll get each unique pairing of the values in those columns.\n\npenguins.group_by([\"species\", \"island\"]).aggregate()\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┓\n┃ species   ┃ island    ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━┩\n│ string    │ string    │\n├───────────┼───────────┤\n│ Adelie    │ Torgersen │\n│ Adelie    │ Dream     │\n│ Adelie    │ Biscoe    │\n│ Chinstrap │ Dream     │\n│ Gentoo    │ Biscoe    │\n└───────────┴───────────┘\n\n\n\nNow, if we add an aggregation function to that, we start to really open things up.\n\npenguins.group_by([\"species\", \"island\"]).aggregate(penguins.bill_length_mm.mean())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │\n├───────────┼───────────┼──────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │\n│ Adelie    │ Dream     │            38.501786 │\n│ Gentoo    │ Biscoe    │            47.504878 │\n│ Adelie    │ Biscoe    │            38.975000 │\n│ Chinstrap │ Dream     │            48.833824 │\n└───────────┴───────────┴──────────────────────┘\n\n\n\nBy adding that mean to the aggregate, we now have a concise way to calculate aggregates over each of the distinct groups in the group_by. And we can calculate as many aggregates as we need.\n\npenguins.group_by([\"species\", \"island\"]).aggregate(\n    [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ float64              │ int64                  │\n├───────────┼───────────┼──────────────────────┼────────────────────────┤\n│ Adelie    │ Torgersen │            38.950980 │                    210 │\n│ Adelie    │ Dream     │            38.501786 │                    208 │\n│ Gentoo    │ Biscoe    │            47.504878 │                    231 │\n│ Adelie    │ Biscoe    │            38.975000 │                    203 │\n│ Chinstrap │ Dream     │            48.833824 │                    212 │\n└───────────┴───────────┴──────────────────────┴────────────────────────┘\n\n\n\nIf we need more specific groups, we can add to the group_by.\n\npenguins.group_by([\"species\", \"island\", \"sex\"]).aggregate(\n    [penguins.bill_length_mm.mean(), penguins.flipper_length_mm.max()]\n)\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ species   ┃ island    ┃ sex    ┃ Mean(bill_length_mm) ┃ Max(flipper_length_mm) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string    │ string    │ string │ float64              │ int64                  │\n├───────────┼───────────┼────────┼──────────────────────┼────────────────────────┤\n│ Adelie    │ Biscoe    │ female │            37.359091 │                    199 │\n│ Gentoo    │ Biscoe    │ female │            45.563793 │                    222 │\n│ Chinstrap │ Dream     │ female │            46.573529 │                    202 │\n│ Adelie    │ Torgersen │ NULL   │            37.925000 │                    193 │\n│ Adelie    │ Dream     │ NULL   │            37.500000 │                    179 │\n│ Gentoo    │ Biscoe    │ male   │            49.473770 │                    231 │\n│ Chinstrap │ Dream     │ male   │            51.094118 │                    212 │\n│ Adelie    │ Torgersen │ female │            37.554167 │                    196 │\n│ Adelie    │ Dream     │ female │            36.911111 │                    202 │\n│ Adelie    │ Biscoe    │ male   │            40.590909 │                    203 │\n│ …         │ …         │ …      │                    … │                      … │\n└───────────┴───────────┴────────┴──────────────────────┴────────────────────────┘"
  },
  {
    "objectID": "tutorials/getting_started.html#chaining-it-all-together",
    "href": "tutorials/getting_started.html#chaining-it-all-together",
    "title": "Tutorial: getting started",
    "section": "Chaining it all together",
    "text": "Chaining it all together\nWe’ve already chained some Ibis calls together. We used mutate to create a new column and then select to only view a subset of the new table. We were just chaining group_by with aggregate.\nThere’s nothing stopping us from putting all of these concepts together to ask questions of the data.\nHow about:\n\nWhat was the largest female penguin (by body mass) on each island in the year 2008?\n\n\npenguins.filter((penguins.sex == \"female\") & (penguins.year == 2008)).group_by(\n    [\"island\"]\n).aggregate(penguins.body_mass_g.max())\n\n┏━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┓\n┃ island    ┃ Max(body_mass_g) ┃\n┡━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━┩\n│ string    │ int64            │\n├───────────┼──────────────────┤\n│ Torgersen │             3800 │\n│ Dream     │             3900 │\n│ Biscoe    │             5200 │\n└───────────┴──────────────────┘\n\n\n\n\nWhat about the largest male penguin (by body mass) on each island for each year of data collection?\n\n\npenguins.filter(penguins.sex == \"male\").group_by([\"island\", \"year\"]).aggregate(\n    penguins.body_mass_g.max().name(\"max_body_mass\")\n).order_by([\"year\", \"max_body_mass\"])\n\n┏━━━━━━━━━━━┳━━━━━━━┳━━━━━━━━━━━━━━━┓\n┃ island    ┃ year  ┃ max_body_mass ┃\n┡━━━━━━━━━━━╇━━━━━━━╇━━━━━━━━━━━━━━━┩\n│ string    │ int64 │ int64         │\n├───────────┼───────┼───────────────┤\n│ Dream     │  2007 │          4650 │\n│ Torgersen │  2007 │          4675 │\n│ Biscoe    │  2007 │          6300 │\n│ Torgersen │  2008 │          4700 │\n│ Dream     │  2008 │          4800 │\n│ Biscoe    │  2008 │          6000 │\n│ Torgersen │  2009 │          4300 │\n│ Dream     │  2009 │          4475 │\n│ Biscoe    │  2009 │          6000 │\n└───────────┴───────┴───────────────┘"
  },
  {
    "objectID": "tutorials/getting_started.html#learn-more",
    "href": "tutorials/getting_started.html#learn-more",
    "title": "Tutorial: getting started",
    "section": "Learn more",
    "text": "Learn more\nThat’s all for this quick-start guide. If you want to learn more, check out the examples repository."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Ibis presentations are linked here.\n\n\n\n Back to top"
  },
  {
    "objectID": "backends/trino.html",
    "href": "backends/trino.html",
    "title": "Trino",
    "section": "",
    "text": "https://trino.io"
  },
  {
    "objectID": "backends/trino.html#install",
    "href": "backends/trino.html#install",
    "title": "Trino",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Trino backend:\n\npipcondamamba\n\n\nInstall with the trino extra:\npip install 'ibis-framework[trino]'\nAnd connect:\nimport ibis\n\n1con = ibis.trino.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Trino:\nconda install -c conda-forge ibis-trino\nAnd connect:\nimport ibis\n\n1con = ibis.trino.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Trino:\nmamba install -c conda-forge ibis-trino\nAnd connect:\nimport ibis\n\n1con = ibis.trino.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/trino.html#connect",
    "href": "backends/trino.html#connect",
    "title": "Trino",
    "section": "Connect",
    "text": "Connect\n\nibis.trino.connect\ncon = ibis.trino.connect(\n    user=\"user\",\n    password=\"password\",\n    port=8080,\n    database=\"database\",\n    schema=\"default\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.trino.connect is a thin wrapper around ibis.backends.trino.Backend.do_connect.”\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, user='user', password=None, host='localhost', port=8080, database=None, schema=None, source=None, **connect_args)\nConnect to Trino.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUsername to connect with\n'user'\n\n\npassword\nstr | None\nPassword to connect with\nNone\n\n\nhost\nstr\nHostname of the Trino server\n'localhost'\n\n\nport\nint\nPort of the Trino server\n8080\n\n\ndatabase\nstr | None\nCatalog to use on the Trino server\nNone\n\n\nschema\nstr | None\nSchema to use on the Trino server\nNone\n\n\nsource\nstr | None\nApplication name passed to Trino\nNone\n\n\nconnect_args\n\nAdditional keyword arguments passed directly to SQLAlchemy’s create_engine\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; catalog = \"hive\"\n&gt;&gt;&gt; schema = \"default\"\nConnect using a URL, with the default user, password, host and port\n&gt;&gt;&gt; con = ibis.connect(f\"trino:///{catalog}/{schema}\")\nConnect using a URL\n&gt;&gt;&gt; con = ibis.connect(f\"trino://user:password@host:port/{catalog}/{schema}\")\nConnect using keyword arguments\n&gt;&gt;&gt; con = ibis.trino.connect(database=catalog, schema=schema)\n&gt;&gt;&gt; con = ibis.trino.connect(database=catalog, schema=schema, source=\"my-app\")\n\n\n\n\nAuthenticating with SSO\nIbis supports connecting to SSO-enabled Trino clusters using the OAuth2Authentication helper from the trino library.\nimport ibis\nfrom trino.auth import OAuth2Authentication\n\ncon = ibis.trino.connect(\n  user=\"user\",\n  host=\"hostname\",\n  port=443,\n  database=\"database\",\n  schema=\"default\",\n  auth=OAuth2Authentication(),\n  http_scheme=\"https\"\n)\n\n\nConnecting to Starburst managed Trino instances\nStarburst makes use of role-based access controls. When connecting to a Starburst Trino cluster, if you encounter issues listing or connecting to tables, ensure that a role is specified using the roles keyword.\nimport ibis\n\ncon = ibis.trino.connect(\n    user=\"user\",\n    host=\"hostname\",\n    port=443,\n    database=\"sample\",\n    schema=\"demo\",\n1    roles=\"defaultrolewithtableaccess\",\n    http_scheme=\"https\"\n)\n\n1\n\nRole names will be visible in the Starburst Galaxy dashboard.\n\n\n\nFinding your Starburst host\nLog into Starburst Galaxy and select Clusters from the left-hand-side menu:\n\nSelect Connection info for the cluster you wish to connect to – the username and hostname displayed can be copied directly into the Ibis connect call."
  },
  {
    "objectID": "backends/trino.html#ibis.backends.trino.Backend",
    "href": "backends/trino.html#ibis.backends.trino.Backend",
    "title": "Trino",
    "section": "trino.Backend",
    "text": "trino.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False, comment=None, properties=None)\nCreate a table in Trino.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to create\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nThe data with which to populate the table; optional, but one of obj or schema must be specified\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema of the table to create; optional, but one of obj or schema must be specified\nNone\n\n\ndatabase\nstr | None\nNot yet implemented.\nNone\n\n\ntemp\nbool\nThis parameter is not yet supported in the Trino backend, because Trino doesn’t implement temporary tables\nFalse\n\n\noverwrite\nbool\nIf True, replace the table if it already exists, otherwise fail if the table exists\nFalse\n\n\ncomment\nstr | None\nAdd a comment to the table\nNone\n\n\nproperties\ncollections.abc.Mapping[str, typing.Any] | None\nTable properties to set on creation\nNone\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, database=None, force=False)\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList the tables in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern to use for listing tables.\nNone\n\n\ndatabase\nstr | None\nThe database (catalog) to perform the list against.\nNone\n\n\nschema\nstr | None\nThe schema inside database to perform the list against. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/pyspark.html",
    "href": "backends/pyspark.html",
    "title": "PySpark",
    "section": "",
    "text": "https://spark.apache.org/docs/latest/api/python"
  },
  {
    "objectID": "backends/pyspark.html#install",
    "href": "backends/pyspark.html#install",
    "title": "PySpark",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the PySpark backend:\n\npipcondamamba\n\n\nInstall with the pyspark extra:\npip install 'ibis-framework[pyspark]'\nAnd connect:\nimport ibis\n\n1con = ibis.pyspark.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for PySpark:\nconda install -c conda-forge ibis-pyspark\nAnd connect:\nimport ibis\n\n1con = ibis.pyspark.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for PySpark:\nmamba install -c conda-forge ibis-pyspark\nAnd connect:\nimport ibis\n\n1con = ibis.pyspark.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/pyspark.html#connect",
    "href": "backends/pyspark.html#connect",
    "title": "PySpark",
    "section": "Connect",
    "text": "Connect\n\nibis.pyspark.connect\ncon = ibis.pyspark.connect(session=session)\n\n\n\n\n\n\nNote\n\n\n\nibis.pyspark.connect is a thin wrapper around ibis.backends.pyspark.Backend.do_connect.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe pyspark backend does not create SparkSession objects, you must create a SparkSession and pass that to ibis.pyspark.connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, session)\nCreate a PySpark Backend for use with Ibis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsession\npyspark.sql.SparkSession\nA SparkSession instance\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from pyspark.sql import SparkSession\n&gt;&gt;&gt; session = SparkSession.builder.getOrCreate()\n&gt;&gt;&gt; ibis.pyspark.connect(session)\n&lt;ibis.backends.pyspark.Backend at 0x...&gt;"
  },
  {
    "objectID": "backends/pyspark.html#ibis.backends.pyspark.Backend",
    "href": "backends/pyspark.html#ibis.backends.pyspark.Backend",
    "title": "PySpark",
    "section": "pyspark.Backend",
    "text": "pyspark.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nclose\nclose(self)\nClose Spark connection and drop any temporary objects.\n\n\ncompile\ncompile(self, expr, timecontext=None, params=None, *args, **kwargs)\nCompile an ibis expression to a PySpark DataFrame object.\n\n\ncompute_stats\ncompute_stats(self, name, database=None, noscan=False)\nIssue a COMPUTE STATISTICS command for a given table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\nnoscan\nbool\nIf True, collect only basic statistics for the table (number of rows, size in bytes).\nFalse\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_database\ncreate_database(self, name, path=None, force=False)\nCreate a new Spark database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase name\nrequired\n\n\npath\nstr | pathlib.Path | None\nPath where to store the database data; otherwise uses Spark default\nNone\n\n\nforce\nbool\nWhether to append IF NOT EXISTS to the database creation SQL\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False, format='parquet')\nCreate a new table in Spark.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\nibis.ibis.Table | pandas.pandas.DataFrame | pyarrow.pyarrow.Table | None\nIf passed, creates table from SELECT statement results\nNone\n\n\nschema\nibis.ibis.Schema | None\nMutually exclusive with obj, creates an empty table with a schema\nNone\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\ntemp\nbool | None\nWhether the new table is temporary\nNone\n\n\noverwrite\nbool\nIf True, overwrite existing data\nFalse\n\n\nformat\nstr\nFormat of the table on disk\n'parquet'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe newly created table.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con.create_table(\"new_table_name\", table_expr)  # quartodoc: +SKIP\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\nCreate a Spark view from a table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nView name\nrequired\n\n\nobj\nibis.ibis.Table\nExpression to use for the view\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\noverwrite\nbool\nReplace an existing view of the same name if it exists\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe created view\n\n\n\n\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop a Spark database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase name\nrequired\n\n\nforce\nbool\nIf False, Spark throws exception if database is not empty or database does not exist\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\n\ndrop_table_or_view\ndrop_table_or_view(self, name, *, database=None, force=False)\nDrop a Spark table or view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable or view name\nrequired\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\nforce\nbool\nDatabase may throw exception if table does not exist\nFalse\n\n\n\n\n\nExamples\n&gt;&gt;&gt; table = \"my_table\"\n&gt;&gt;&gt; db = \"operations\"\n&gt;&gt;&gt; con.drop_table_or_view(table, db, force=True)  # quartodoc: +SKIP\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\nDrop a view.\n\n\nexecute\nexecute(self, expr, **kwargs)\nExecute an expression.\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\nFetch data from cursor.\n\n\nget_schema\nget_schema(self, table_name, database=None)\nReturn a Schema object for the indicated table and database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nTable name. May be fully qualified\nrequired\n\n\ndatabase\nstr | None\nSpark does not have a database argument for its table() method, so this must be None\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nAn ibis schema\n\n\n\n\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj=None, database=None, overwrite=False, values=None, validate=True)\nInsert data into an existing table.\n\nExamples\n&gt;&gt;&gt; table = \"my_table\"\n&gt;&gt;&gt; con.insert(table, table_expr)  # quartodoc: +SKIP"
  },
  {
    "objectID": "backends/druid.html",
    "href": "backends/druid.html",
    "title": "Druid",
    "section": "",
    "text": "https://druid.apache.org"
  },
  {
    "objectID": "backends/druid.html#install",
    "href": "backends/druid.html#install",
    "title": "Druid",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Druid backend:\n\npipcondamamba\n\n\nInstall with the druid extra:\npip install 'ibis-framework[druid]'\nAnd connect:\nimport ibis\n\n1con = ibis.druid.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Druid:\nconda install -c conda-forge ibis-druid\nAnd connect:\nimport ibis\n\n1con = ibis.druid.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Druid:\nmamba install -c conda-forge ibis-druid\nAnd connect:\nimport ibis\n\n1con = ibis.druid.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/druid.html#connect",
    "href": "backends/druid.html#connect",
    "title": "Druid",
    "section": "Connect",
    "text": "Connect\n\nibis.druid.connect\ncon = ibis.druid.connect(\n    host=\"hostname\",\n    port=8082,\n    database=\"druid/v2/sql\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.druid.connect is a thin wrapper around ibis.backends.druid.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, host='localhost', port=8082, database='druid/v2/sql', **_)\nCreate an Ibis client using the passed connection parameters.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhost\nstr\nHostname\n'localhost'\n\n\nport\nint\nPort\n8082\n\n\ndatabase\nstr | None\nDatabase to connect to\n'druid/v2/sql'\n\n\n\n\n\n\n\nibis.connect URL format\nIn addition to ibis.druid.connect, you can also connect to Druid by passing a properly formatted Druid connection URL to ibis.connect\ncon = ibis.connect(\"druid://localhost:8082/druid/v2/sql\")"
  },
  {
    "objectID": "backends/druid.html#ibis.backends.druid.Backend",
    "href": "backends/druid.html#ibis.backends.druid.Backend",
    "title": "Druid",
    "section": "druid.Backend",
    "text": "druid.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/exasol.html",
    "href": "backends/exasol.html",
    "title": "Exasol",
    "section": "",
    "text": "https://www.exasol.com"
  },
  {
    "objectID": "backends/exasol.html#install",
    "href": "backends/exasol.html#install",
    "title": "Exasol",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Exasol backend:\n\npipcondamamba\n\n\nInstall with the exasol extra:\npip install 'ibis-framework[exasol]'\nAnd connect:\nimport ibis\n\n1con = ibis.exasol.connect(...)\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Exasol:\nconda install -c conda-forge ibis-exasol\nAnd connect:\nimport ibis\n\n1con = ibis.exasol.connect(...)\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Exasol:\nmamba install -c conda-forge ibis-exasol\nAnd connect:\nimport ibis\n\n1con = ibis.exasol.connect(...)\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/exasol.html#connect",
    "href": "backends/exasol.html#connect",
    "title": "Exasol",
    "section": "Connect",
    "text": "Connect\n\nibis.exasol.connect\ncon = ibis.exasol.connect(\n    user = \"username\",\n    password = \"password\",\n    host = \"localhost\",\n    port = 8563,\n    schema = None,\n    encryption = True,\n    certificate_validation = True,\n    encoding = \"en_US.UTF-8\"\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.exasol.connect is a thin wrapper around ibis.backends.exasol.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, user, password, host='localhost', port=8563, schema=None, encryption=True, certificate_validation=True, encoding='en_US.UTF-8')\nCreate an Ibis client connected to an Exasol database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuser\nstr\nUsername used for authentication.\nrequired\n\n\npassword\nstr\nPassword used for authentication.\nrequired\n\n\nhost\nstr\nHostname to connect to (default: “localhost”).\n'localhost'\n\n\nport\nint\nPort number to connect to (default: 8563)\n8563\n\n\nschema\nstr | None\nDatabase schema to open, if None, no schema will be opened.\nNone\n\n\nencryption\nbool\nEnables/disables transport layer encryption (default: True).\nTrue\n\n\ncertificate_validation\nbool\nEnables/disables certificate validation (default: True).\nTrue\n\n\nencoding\nstr\nThe encoding format (default: “en_US.UTF-8”).\n'en_US.UTF-8'"
  },
  {
    "objectID": "backends/dask.html",
    "href": "backends/dask.html",
    "title": "Dask",
    "section": "",
    "text": "https://www.dask.org"
  },
  {
    "objectID": "backends/dask.html#install",
    "href": "backends/dask.html#install",
    "title": "Dask",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Dask backend:\n\npipcondamamba\n\n\nInstall with the dask extra:\npip install 'ibis-framework[dask]'\nAnd connect:\nimport ibis\n\n1con = ibis.dask.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Dask:\nconda install -c conda-forge ibis-dask\nAnd connect:\nimport ibis\n\n1con = ibis.dask.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Dask:\nmamba install -c conda-forge ibis-dask\nAnd connect:\nimport ibis\n\n1con = ibis.dask.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/dask.html#ibis.backends.dask.Backend",
    "href": "backends/dask.html#ibis.backends.dask.Backend",
    "title": "Dask",
    "section": "dask.Backend",
    "text": "dask.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\ncompile\ncompile(self, query, params=None, **kwargs)\nCompile expr.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndask.dask.dataframe.dask.dataframe.core.dask.dataframe.core.DataFrame | dask.dask.dataframe.dask.dataframe.core.dask.dataframe.core.Series | dask.dask.dataframe.dask.dataframe.core.dask.dataframe.core.Scalar\nDask graph.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False)\nCreate a table.\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, force=False)\n\n\ndrop_view\ndrop_view(self, name, *, force=False)\n\n\nexecute\nexecute(self, query, params=None, limit='default', **kwargs)\nExecute an expression.\n\n\nfrom_dataframe\nfrom_dataframe(self, df, name='df', client=None)\nConstruct an ibis table from a pandas DataFrame.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npandas.pandas.DataFrame\nA pandas DataFrame\nrequired\n\n\nname\nstr\nThe name of the pandas DataFrame\n'df'\n\n\nclient\nibis.backends.pandas.BasePandasBackend | None\nClient dictionary will be mutated with the name of the DataFrame, if not provided a new client is created\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\n\nget_schema\nget_schema(self, table_name, database=None)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\n\n\nread_csv\nread_csv(self, source, table_name=None, **kwargs)\nRegister a CSV file as a table in the current session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.pathlib.Path\nThe data source. Can be a local or remote file, pathlike objects also accepted.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Pandas loading function. See https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, source, table_name=None, **kwargs)\nRegister a parquet file as a table in the current session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.pathlib.Path\nThe data source(s). May be a path to a file, an iterable of files, or directory of parquet files.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Pandas loading function. See https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\ntable\ntable(self, name, schema=None)\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, params=None, limit=None, **kwargs)\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name."
  },
  {
    "objectID": "backends/impala.quarto.html",
    "href": "backends/impala.quarto.html",
    "title": "Impala",
    "section": "",
    "text": "https://impala.apache.org"
  },
  {
    "objectID": "backends/impala.quarto.html#install",
    "href": "backends/impala.quarto.html#install",
    "title": "Impala",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Impala backend:\n\npipcondamamba\n\n\nInstall with the impala extra:\npip install 'ibis-framework[impala]'\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Impala:\nconda install -c conda-forge ibis-impala\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Impala:\nmamba install -c conda-forge ibis-impala\nAnd connect:\nimport ibis\n\n1con = ibis.impala.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/impala.quarto.html#database-methods",
    "href": "backends/impala.quarto.html#database-methods",
    "title": "Impala",
    "section": "Database methods",
    "text": "Database methods\n\ncreate_database\ncreate_database(self, name, path=None, force=False)\nCreate a new Impala database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nDatabase name\nrequired\n\n\npath\n\nPath where to store the database data; otherwise uses the Impala default\nNone\n\n\nforce\n\nForcibly create the database\nFalse\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop an Impala database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\n\nDatabase name\nrequired\n\n\nforce\n\nIf False and there are any tables in this database, raises an IntegrityError\nFalse\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)"
  },
  {
    "objectID": "backends/impala.quarto.html#table-methods",
    "href": "backends/impala.quarto.html#table-methods",
    "title": "Impala",
    "section": "Table methods",
    "text": "Table methods\nThe Backend object itself has many helper utility methods. You’ll find the most methods on ImpalaTable.\n\nbackend.resolved_bases\n\n[Class('SQLBackend', 69, 664),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66),\n Class('BaseBackend', 754, 1267),\n Class('_DatabaseSchemaHandler', 29, 66)]\n\n\nKeyError: 'sql'\nThe best way to interact with a single table is through the ImpalaTable object you get back from Backend.table.\n\ndrop\ndrop(self)\nDrop the table from the database.\n\n\ninsert\ninsert(self, obj=None, overwrite=False, partition=None, values=None, validate=True)\nInsert into an Impala table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nobj\n\nTable expression or DataFrame\nNone\n\n\noverwrite\n\nIf True, will replace existing contents of table\nFalse\n\n\npartition\n\nFor partitioned tables, indicate the partition that’s being inserted into, either with an ordered list of partition keys or a dict of partition field name to value. For example for the partition (year=2007, month=7), this can be either (2007, 7) or {‘year’: 2007, ‘month’: 7}.\nNone\n\n\nvalues\n\nUnsupported and unused\nNone\n\n\nvalidate\n\nIf True, do more rigorous validation that schema of table being inserted is compatible with the existing table\nTrue\n\n\n\n\n\nExamples\nAppend to an existing table\n&gt;&gt;&gt; t.insert(table_expr)  # quartodoc: +SKIP\nCompletely overwrite contents\n&gt;&gt;&gt; t.insert(table_expr, overwrite=True)  # quartodoc: +SKIP\n\n\n\ndescribe_formatted"
  },
  {
    "objectID": "backends/impala.quarto.html#creating-views",
    "href": "backends/impala.quarto.html#creating-views",
    "title": "Impala",
    "section": "Creating views",
    "text": "Creating views\n\ndrop_table_or_view\ndrop_table_or_view(self, name, *, database=None, force=False)\nDrop view or table.\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nir.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created."
  },
  {
    "objectID": "backends/impala.quarto.html#accessing-data",
    "href": "backends/impala.quarto.html#accessing-data",
    "title": "Impala",
    "section": "Accessing data",
    "text": "Accessing data\n\ndelimited_file\ndelimited_file(self, directory, schema, name=None, database=None, delimiter=',', na_rep=None, escapechar=None, lineterminator=None, external=True)\nInterpret delimited text files as an Ibis table expression.\nSee the parquet_file method for more details on what happens under the hood.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nServer directory containing delimited text files\nrequired\n\n\nschema\n\nIbis schema\nrequired\n\n\nname\n\nName for the table; otherwise random names are generated\nNone\n\n\ndatabase\n\nDatabase to create the table in\nNone\n\n\ndelimiter\n\nCharacter used to delimit columns\n','\n\n\nna_rep\n\nCharacter used to represent NULL values\nNone\n\n\nescapechar\n\nCharacter used to escape special characters\nNone\n\n\nlineterminator\n\nCharacter used to delimit lines\nNone\n\n\nexternal\n\nCreate table as EXTERNAL (data will not be deleted on drop).\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nImpalaTable\nImpala table expression\n\n\n\n\n\n\nparquet_file\nparquet_file(self, directory, schema=None, name=None, database=None, external=True, like_file=None, like_table=None)\nCreate an Ibis table from the passed directory of Parquet files.\nThe table can be optionally named, otherwise a unique name will be generated.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\nstr | Path\nPath\nrequired\n\n\nschema\nsch.Schema | None\nIf no schema provided, and neither of the like_* argument is passed, one will be inferred from one of the parquet files in the directory.\nNone\n\n\nlike_file\nstr | Path | None\nAbsolute path to Parquet file on the server to use for schema definitions. An alternative to having to supply an explicit schema\nNone\n\n\nlike_table\nstr | None\nFully scoped and escaped string to an Impala table whose schema we will use for the newly created table.\nNone\n\n\nname\nstr | None\nRandom unique name generated otherwise\nNone\n\n\ndatabase\nstr | None\nDatabase to create the (possibly temporary) table in\nNone\n\n\nexternal\nbool\nIf a table is external, the referenced data will not be deleted when the table is dropped in Impala. Otherwise Impala takes ownership of the Parquet file.\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nImpalaTable\nImpala table expression\n\n\n\n\n\n\navro_file\navro_file(self, directory, avro_schema, name=None, database=None, external=True)\nCreate a table to read a collection of Avro data.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndirectory\n\nServer path to directory containing avro files\nrequired\n\n\navro_schema\n\nThe Avro schema for the data as a Python dict\nrequired\n\n\nname\n\nTable name\nNone\n\n\ndatabase\n\nDatabase name\nNone\n\n\nexternal\n\nWhether the table is external\nTrue\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nImpalaTable\nImpala table expression"
  },
  {
    "objectID": "backends/impala.quarto.html#the-impala-client-object",
    "href": "backends/impala.quarto.html#the-impala-client-object",
    "title": "Impala",
    "section": "The Impala client object",
    "text": "The Impala client object\nTo use Ibis with Impala, you first must connect to a cluster using the ibis.impala.connect function:\nimport ibis\n\nclient = ibis.impala.connect(host=impala_host, port=impala_port)\nBy default binary transport mode is used, however it is also possible to use HTTP. Depending on your configuration, additional connection arguments may need to be provided. For the full list of possible connection arguments please refer to the impyla documentation.\nimport ibis\n\nclient = ibis.impala.connect(\n    host=impala_host,\n    port=impala_port,\n    username=username,\n    password=password,\n    use_ssl=True,\n    auth_mechanism='LDAP',\n    use_http_transport=True,\n    http_path='cliservice',\n)\nAll examples here use the following block of code to connect to impala using docker:\nimport ibis\n\nclient = ibis.impala.connect(host=host)\nYou can accomplish many tasks directly through the client object, but we additionally provide APIs to streamline tasks involving a single Impala table or database."
  },
  {
    "objectID": "backends/impala.quarto.html#table-objects",
    "href": "backends/impala.quarto.html#table-objects",
    "title": "Impala",
    "section": "Table objects",
    "text": "Table objects\n\ntable\ntable(self, name, schema=None, database=None)\nConstruct a table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\nschema\nstr | None\n[deprecated] Schema name\nNone\n\n\ndatabase\ntuple[str, str] | str | None\nDatabase name\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\nThe client’s table method allows you to create an Ibis table expression referencing a physical Impala table:\ntable = client.table('functional_alltypes', database='ibis_testing')\nImpalaTable is a Python subclass of the more general Ibis Table that has additional Impala-specific methods. So you can use it interchangeably with any code expecting a Table.\nWhile the client has a drop_table method you can use to drop tables, the table itself has a method drop that you can use:\ntable.drop()"
  },
  {
    "objectID": "backends/impala.quarto.html#expression-execution",
    "href": "backends/impala.quarto.html#expression-execution",
    "title": "Impala",
    "section": "Expression execution",
    "text": "Expression execution\nIbis expressions have execution methods like to_pandas that compile and run the expressions on Impala or whichever backend is being referenced.\nFor example:\n&gt;&gt;&gt; fa = db.functional_alltypes\n&gt;&gt;&gt; expr = fa.double_col.sum()\n&gt;&gt;&gt; expr.to_pandas()\n331785.00000000006\nFor longer-running queries, Ibis will attempt to cancel the query in progress if an interrupt is received."
  },
  {
    "objectID": "backends/impala.quarto.html#creating-tables",
    "href": "backends/impala.quarto.html#creating-tables",
    "title": "Impala",
    "section": "Creating tables",
    "text": "Creating tables\nThere are several ways to create new Impala tables:\n\nFrom an Ibis table expression\nEmpty, from a declared schema\nEmpty and partitioned\n\nIn all cases, you should use the create_table method either on the top-level client connection or a database object.\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=None, overwrite=False, external=False, format='parquet', location=None, partition=None, like_parquet=None)\nCreate a new table using an Ibis table expression or in-memory data.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\nobj\nir.Table | pd.DataFrame | pa.Table | pl.DataFrame | pl.LazyFrame | None\nIf passed, creates table from select statement results\nNone\n\n\nschema\n\nMutually exclusive with obj, creates an empty table with a particular schema\nNone\n\n\ndatabase\n\nDatabase name\nNone\n\n\ntemp\nbool | None\nWhether a table is temporary\nNone\n\n\noverwrite\nbool\nDo not create table if table with indicated name already exists\nFalse\n\n\nexternal\nbool\nCreate an external table; Impala will not delete the underlying data when the table is dropped\nFalse\n\n\nformat\n\nFile format\n'parquet'\n\n\nlocation\n\nSpecify the directory location where Impala reads and writes files for the table\nNone\n\n\npartition\n\nMust pass a schema to use this. Cannot partition from an expression.\nNone\n\n\nlike_parquet\n\nCan specify instead of a schema\nNone\n\n\n\n\n\n\nCreating tables from a table expression\nIf you pass an Ibis expression to create_table, Ibis issues a CREATE TABLE ... AS SELECT (CTAS) statement:\n&gt;&gt;&gt; table = db.table('functional_alltypes')\n&gt;&gt;&gt; expr = table.group_by('string_col').size()\n&gt;&gt;&gt; db.create_table('string_freqs', expr, format='parquet')\n\n&gt;&gt;&gt; freqs = db.table('string_freqs')\n&gt;&gt;&gt; freqs.to_pandas()\n  string_col  count\n0          9    730\n1          3    730\n2          6    730\n3          4    730\n4          1    730\n5          8    730\n6          2    730\n7          7    730\n8          5    730\n9          0    730\n\n&gt;&gt;&gt; files = freqs.files()\n&gt;&gt;&gt; files\n                                                Path  Size Partition\n0  hdfs://impala:8020/user/hive/warehouse/ibis_te...  584B\n\n&gt;&gt;&gt; freqs.drop()\nYou can also choose to create an empty table and use insert (see below).\n\n\nCreating an empty table\nTo create an empty table, you must declare an Ibis schema that will be translated to the appropriate Impala schema and data types.\nAs Ibis types are simplified compared with Impala types, this may expand in the future to include a more fine-grained schema declaration.\nYou can use the create_table method either on a database or client object.\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema)\nBy default, this stores the data files in the database default location. You can force a particular path with the location option.\nfrom getpass import getuser\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\nlocation = '/home/{}/new-table-data'.format(getuser())\ndb.create_table(name, schema=schema, location=location)\nIf the schema matches a known table schema, you can always use the schema method to get a schema object:\n&gt;&gt;&gt; t = db.table('functional_alltypes')\n&gt;&gt;&gt; t.schema()\nibis.Schema {\n  id               int32\n  bool_col         boolean\n  tinyint_col      int8\n  smallint_col     int16\n  int_col          int32\n  bigint_col       int64\n  float_col        float32\n  double_col       float64\n  date_string_col  string\n  string_col       string\n  timestamp_col    timestamp\n  year             int32\n  month            int32\n}\n\n\nCreating a partitioned table\nTo create an empty partitioned table, include a list of columns to be used as the partition keys.\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema, partition=['year', 'month'])"
  },
  {
    "objectID": "backends/impala.quarto.html#partitioned-tables",
    "href": "backends/impala.quarto.html#partitioned-tables",
    "title": "Impala",
    "section": "Partitioned tables",
    "text": "Partitioned tables\nIbis enables you to manage partitioned tables in various ways. Since each partition behaves as its own \"subtable\" sharing a common schema, each partition can have its own file format, directory path, serialization properties, and so forth.\nThere are a handful of table methods for adding and removing partitions and getting information about the partition schema and any existing partition data:\n\nadd_partition\nadd_partition(self, spec, location=None)\nAdd a new table partition.\nPartition parameters can be set in a single DDL statement or you can use alter_partition to set them after the fact.\n\n\ndrop_partition\ndrop_partition(self, spec)\nDrop an existing table partition.\n\n\nis_partitioned\nTrue if the table is partitioned.\n\n\npartition_schema\npartition_schema(self)\nReturn the schema for the partition columns.\n\n\npartitions\npartitions(self)\nReturn information about the table’s partitions.\nRaises an exception if the table is not partitioned.\nTo address a specific partition in any method that is partition specific, you can either use a dict with the partition key names and values, or pass a list of the partition values:\nschema = ibis.schema(dict(foo='string', year='int32', month='int16'))\nname = 'new_table'\ndb.create_table(name, schema=schema, partition=['year', 'month'])\n\ntable = db.table(name)\n\ntable.add_partition({'year': 2007, 'month', 4})\ntable.add_partition([2007, 5])\ntable.add_partition([2007, 6])\n\ntable.drop_partition([2007, 6])\nWe’ll cover partition metadata management and data loading below."
  },
  {
    "objectID": "backends/impala.quarto.html#inserting-data-into-tables",
    "href": "backends/impala.quarto.html#inserting-data-into-tables",
    "title": "Impala",
    "section": "Inserting data into tables",
    "text": "Inserting data into tables\nIf the schemas are compatible, you can insert into a table directly from an Ibis table expression:\n&gt;&gt;&gt; t = db.functional_alltypes\n&gt;&gt;&gt; db.create_table('insert_test', schema=t.schema())\n&gt;&gt;&gt; target = db.table('insert_test')\n\n&gt;&gt;&gt; target.insert(t[:3])\n&gt;&gt;&gt; target.insert(t[:3])\n&gt;&gt;&gt; target.insert(t[:3])\n\n&gt;&gt;&gt; target.to_pandas()\n     id  bool_col  tinyint_col  ...           timestamp_col  year  month\n0  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n1  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n2  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n3  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n4  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n5  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n6  5770      True            0  ... 2010-08-01 00:00:00.000  2010      8\n7  5771     False            1  ... 2010-08-01 00:01:00.000  2010      8\n8  5772      True            2  ... 2010-08-01 00:02:00.100  2010      8\n\n[9 rows x 13 columns]\n\n&gt;&gt;&gt; target.drop()\nIf the table is partitioned, you must indicate the partition you are inserting into:\npart = {'year': 2007, 'month': 4}\ntable.insert(expr, partition=part)"
  },
  {
    "objectID": "backends/impala.quarto.html#managing-table-metadata",
    "href": "backends/impala.quarto.html#managing-table-metadata",
    "title": "Impala",
    "section": "Managing table metadata",
    "text": "Managing table metadata\nIbis has functions that wrap many of the DDL commands for Impala table metadata.\n\nDetailed table metadata: DESCRIBE FORMATTED\nTo get a handy wrangled version of DESCRIBE FORMATTED use the metadata method.\n\nmetadata\nmetadata(self)\nReturn results of DESCRIBE FORMATTED statement.\n&gt;&gt;&gt; t = client.table('ibis_testing.functional_alltypes')\n&gt;&gt;&gt; meta = t.metadata()\n&gt;&gt;&gt; meta\n&lt;class 'ibis.backends.impala.metadata.TableMetadata'&gt;\n{'info': {'CreateTime': datetime.datetime(2021, 1, 14, 21, 23, 8),\n          'Database': 'ibis_testing',\n          'LastAccessTime': 'UNKNOWN',\n          'Location': 'hdfs://impala:8020/__ibis/ibis-testing-data/parquet/functional_alltypes',\n          'Owner': 'root',\n          'Protect Mode': 'None',\n          'Retention': 0,\n          'Table Parameters': {'COLUMN_STATS_ACCURATE': False,\n                               'EXTERNAL': True,\n                               'STATS_GENERATED_VIA_STATS_TASK': True,\n                               'numFiles': 3,\n                               'numRows': 7300,\n                               'rawDataSize': '-1',\n                               'totalSize': 106278,\n                               'transient_lastDdlTime': datetime.datetime(2021, 1, 14, 21, 23, 17)},\n          'Table Type': 'EXTERNAL_TABLE'},\n 'schema': [('id', 'int'),\n            ('bool_col', 'boolean'),\n            ('tinyint_col', 'tinyint'),\n            ('smallint_col', 'smallint'),\n            ('int_col', 'int'),\n            ('bigint_col', 'bigint'),\n            ('float_col', 'float'),\n            ('double_col', 'double'),\n            ('date_string_col', 'string'),\n            ('string_col', 'string'),\n            ('timestamp_col', 'timestamp'),\n            ('year', 'int'),\n            ('month', 'int')],\n 'storage info': {'Bucket Columns': '[]',\n                  'Compressed': False,\n                  'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\n                  'Num Buckets': 0,\n                  'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\n                  'SerDe Library': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\n                  'Sort Columns': '[]'}}\n\n&gt;&gt;&gt; meta.location\n'hdfs://impala:8020/__ibis/ibis-testing-data/parquet/functional_alltypes'\n\n&gt;&gt;&gt; meta.create_time\ndatetime.datetime(2021, 1, 14, 21, 23, 8)\nThe files function is also available to see all of the physical HDFS data files backing a table:\n\n\nfiles\nfiles(self)\nReturn results of SHOW FILES statement.\n&gt;&gt;&gt; ss = c.table('tpcds_parquet.store_sales')\n\n&gt;&gt;&gt; ss.files()[:5]\n                                                path      size  \\\n0  hdfs://localhost:20500/test-warehouse/tpcds.st...  160.61KB\n1  hdfs://localhost:20500/test-warehouse/tpcds.st...  123.88KB\n2  hdfs://localhost:20500/test-warehouse/tpcds.st...  139.28KB\n3  hdfs://localhost:20500/test-warehouse/tpcds.st...  139.60KB\n4  hdfs://localhost:20500/test-warehouse/tpcds.st...   62.84KB\n\n                 partition\n0  ss_sold_date_sk=2451803\n1  ss_sold_date_sk=2451819\n2  ss_sold_date_sk=2451772\n3  ss_sold_date_sk=2451789\n4  ss_sold_date_sk=2451741\n\n\n\nModifying table metadata\nFor unpartitioned tables, you can use the alter method to change its location, file format, and other properties. For partitioned tables, to change partition-specific metadata use alter_partition.\n\nalter\nalter(self, location=None, format=None, tbl_properties=None, serde_properties=None)\nChange settings and parameters of the table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlocation\n\nFor partitioned tables, you may want the alter_partition function\nNone\n\n\nformat\n\nTable format\nNone\n\n\ntbl_properties\n\nTable properties\nNone\n\n\nserde_properties\n\nSerialization/deserialization properties\nNone\n\n\n\n\n\n\nalter_partition\nalter_partition(self, spec, location=None, format=None, tbl_properties=None, serde_properties=None)\nChange settings and parameters of an existing partition.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nspec\n\nThe partition keys for the partition being modified\nrequired\n\n\nlocation\n\nLocation of the partition\nNone\n\n\nformat\n\nTable format\nNone\n\n\ntbl_properties\n\nTable properties\nNone\n\n\nserde_properties\n\nSerialization/deserialization properties\nNone\n\n\n\nFor example, if you wanted to \"point\" an existing table at a directory of CSV files, you could run the following command:\nfrom getpass import getuser\n\ncsv_props = {\n    'serialization.format': ',',\n    'field.delim': ',',\n}\ndata_dir = '/home/{}/my-csv-files'.format(getuser())\n\ntable.alter(location=data_dir, format='text', serde_properties=csv_props)\nIf the table is partitioned, you can modify only the properties of a particular partition:\ntable.alter_partition(\n    {'year': 2007, 'month': 5},\n    location=data_dir,\n    format='text',\n    serde_properties=csv_props\n)"
  },
  {
    "objectID": "backends/impala.quarto.html#table-statistics",
    "href": "backends/impala.quarto.html#table-statistics",
    "title": "Impala",
    "section": "Table statistics",
    "text": "Table statistics\n\nComputing table and partition statistics\n\ncompute_stats\ncompute_stats(self, incremental=False)\nInvoke Impala COMPUTE STATS command on the table.\nImpala-backed physical tables have a method compute_stats that computes table, column, and partition-level statistics to assist with query planning and optimization. It is standard practice to invoke this after creating a table or loading new data:\ntable.compute_stats()\nIf you are using a recent version of Impala, you can also access the COMPUTE INCREMENTAL STATS DDL command:\ntable.compute_stats(incremental=True)\n\n\n\nSeeing table and column statistics\n\ncolumn_stats\ncolumn_stats(self)\nReturn results of SHOW COLUMN STATS.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\nColumn statistics\n\n\n\n\n\n\nstats\nstats(self)\nReturn results of SHOW TABLE STATS.\nIf not partitioned, contains only one row.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDataFrame\nTable statistics\n\n\n\nThe compute_stats and stats functions return the results of SHOW COLUMN STATS and SHOW TABLE STATS, respectively, and their output will depend, of course, on the last COMPUTE STATS call.\n&gt;&gt;&gt; ss = c.table('tpcds_parquet.store_sales')\n&gt;&gt;&gt; ss.compute_stats(incremental=True)\n&gt;&gt;&gt; stats = ss.stats()\n&gt;&gt;&gt; stats[:5]\n  ss_sold_date_sk  #Rows  #Files     Size Bytes Cached Cache Replication  \\\n0         2450829   1071       1  78.34KB   NOT CACHED        NOT CACHED\n1         2450846    839       1  61.83KB   NOT CACHED        NOT CACHED\n2         2450860    747       1  54.86KB   NOT CACHED        NOT CACHED\n3         2450874    922       1  66.74KB   NOT CACHED        NOT CACHED\n4         2450888    856       1  63.33KB   NOT CACHED        NOT CACHED\n\n    Format Incremental stats  \\\n0  PARQUET              true\n1  PARQUET              true\n2  PARQUET              true\n3  PARQUET              true\n4  PARQUET              true\n\n                                            Location\n0  hdfs://localhost:20500/test-warehouse/tpcds.st...\n1  hdfs://localhost:20500/test-warehouse/tpcds.st...\n2  hdfs://localhost:20500/test-warehouse/tpcds.st...\n3  hdfs://localhost:20500/test-warehouse/tpcds.st...\n4  hdfs://localhost:20500/test-warehouse/tpcds.st...\n\n&gt;&gt;&gt; cstats = ss.column_stats()\n&gt;&gt;&gt; cstats\n                   Column          Type  #Distinct Values  #Nulls  Max Size  Avg Size\n0         ss_sold_time_sk        BIGINT             13879      -1       NaN         8\n1              ss_item_sk        BIGINT             17925      -1       NaN         8\n2          ss_customer_sk        BIGINT             15207      -1       NaN         8\n3             ss_cdemo_sk        BIGINT             16968      -1       NaN         8\n4             ss_hdemo_sk        BIGINT              6220      -1       NaN         8\n5              ss_addr_sk        BIGINT             14077      -1       NaN         8\n6             ss_store_sk        BIGINT                 6      -1       NaN         8\n7             ss_promo_sk        BIGINT               298      -1       NaN         8\n8        ss_ticket_number           INT             15006      -1       NaN         4\n9             ss_quantity           INT                99      -1       NaN         4\n10      ss_wholesale_cost  DECIMAL(7,2)             10196      -1       NaN         4\n11          ss_list_price  DECIMAL(7,2)             19393      -1       NaN         4\n12         ss_sales_price  DECIMAL(7,2)             15594      -1       NaN         4\n13    ss_ext_discount_amt  DECIMAL(7,2)             29772      -1       NaN         4\n14     ss_ext_sales_price  DECIMAL(7,2)            102758      -1       NaN         4\n15  ss_ext_wholesale_cost  DECIMAL(7,2)            125448      -1       NaN         4\n16      ss_ext_list_price  DECIMAL(7,2)            141419      -1       NaN         4\n17             ss_ext_tax  DECIMAL(7,2)             33837      -1       NaN         4\n18          ss_coupon_amt  DECIMAL(7,2)             29772      -1       NaN         4\n19            ss_net_paid  DECIMAL(7,2)            109981      -1       NaN         4\n20    ss_net_paid_inc_tax  DECIMAL(7,2)            132286      -1       NaN         4\n21          ss_net_profit  DECIMAL(7,2)            122436      -1       NaN         4\n22        ss_sold_date_sk        BIGINT               120       0       NaN         8\n\n\n\n\nREFRESH and INVALIDATE METADATA\nThese DDL commands are available as table-level and client-level methods:\n\ninvalidate_metadata\ninvalidate_metadata(self, name=None, database=None)\nIssue an INVALIDATE METADATA command.\nOptionally this applies to a specific table. See Impala documentation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nTable name. Can be fully qualified (with database)\nNone\n\n\ndatabase\nstr | None\nDatabase name\nNone\n\n\n\n\n\n\ninvalidate_metadata\ninvalidate_metadata(self)\n\n\nrefresh\nrefresh(self)\nYou can invalidate the cached metadata for a single table or for all tables using invalidate_metadata, and similarly invoke REFRESH db_name.table_name using the refresh method.\nclient.invalidate_metadata()\n\ntable = db.table(table_name)\ntable.invalidate_metadata()\n\ntable.refresh()\nThese methods are often used in conjunction with the LOAD DATA commands and COMPUTE STATS. See the Impala documentation for full details."
  },
  {
    "objectID": "backends/impala.quarto.html#parquet-and-other-session-options",
    "href": "backends/impala.quarto.html#parquet-and-other-session-options",
    "title": "Impala",
    "section": "Parquet and other session options",
    "text": "Parquet and other session options\nIbis gives you access to Impala session-level variables that affect query execution:\n\nget_options\nget_options(self)\nReturn current query options for the Impala session.\n\n\nset_options\nset_options(self, options)\n\n\nset_compression_codec\nset_compression_codec(self, codec)\nFor example:\n&gt;&gt;&gt; client.get_options()\n{'ABORT_ON_ERROR': '0',\n 'APPX_COUNT_DISTINCT': '0',\n 'BUFFER_POOL_LIMIT': '',\n 'COMPRESSION_CODEC': '',\n 'COMPUTE_STATS_MIN_SAMPLE_SIZE': '1073741824',\n 'DEFAULT_JOIN_DISTRIBUTION_MODE': '0',\n 'DEFAULT_SPILLABLE_BUFFER_SIZE': '2097152',\n 'DISABLE_ROW_RUNTIME_FILTERING': '0',\n 'DISABLE_STREAMING_PREAGGREGATIONS': '0',\n 'DISABLE_UNSAFE_SPILLS': '0',\n 'ENABLE_EXPR_REWRITES': '1',\n 'EXEC_SINGLE_NODE_ROWS_THRESHOLD': '100',\n 'EXEC_TIME_LIMIT_S': '0',\n 'EXPLAIN_LEVEL': '1',\n 'HBASE_CACHE_BLOCKS': '0',\n 'HBASE_CACHING': '0',\n 'IDLE_SESSION_TIMEOUT': '0',\n 'MAX_ERRORS': '100',\n 'MAX_NUM_RUNTIME_FILTERS': '10',\n 'MAX_ROW_SIZE': '524288',\n 'MEM_LIMIT': '0',\n 'MIN_SPILLABLE_BUFFER_SIZE': '65536',\n 'MT_DOP': '',\n 'NUM_SCANNER_THREADS': '0',\n 'OPTIMIZE_PARTITION_KEY_SCANS': '0',\n 'PARQUET_ANNOTATE_STRINGS_UTF8': '0',\n 'PARQUET_ARRAY_RESOLUTION': '2',\n 'PARQUET_DICTIONARY_FILTERING': '1',\n 'PARQUET_FALLBACK_SCHEMA_RESOLUTION': '0',\n 'PARQUET_FILE_SIZE': '0',\n 'PARQUET_READ_STATISTICS': '1',\n 'PREFETCH_MODE': '1',\n 'QUERY_TIMEOUT_S': '0',\n 'REPLICA_PREFERENCE': '0',\n 'REQUEST_POOL': '',\n 'RUNTIME_BLOOM_FILTER_SIZE': '1048576',\n 'RUNTIME_FILTER_MAX_SIZE': '16777216',\n 'RUNTIME_FILTER_MIN_SIZE': '1048576',\n 'RUNTIME_FILTER_MODE': '2',\n 'RUNTIME_FILTER_WAIT_TIME_MS': '0',\n 'S3_SKIP_INSERT_STAGING': '1',\n 'SCHEDULE_RANDOM_REPLICA': '0',\n 'SCRATCH_LIMIT': '-1',\n 'SEQ_COMPRESSION_MODE': '',\n 'SYNC_DDL': '0'}\nTo enable Snappy compression for Parquet files, you could do either of:\n&gt;&gt;&gt; client.set_options({'COMPRESSION_CODEC': 'snappy'})\n&gt;&gt;&gt; client.set_compression_codec('snappy')\n\n&gt;&gt;&gt; client.get_options()['COMPRESSION_CODEC']\n'SNAPPY'"
  },
  {
    "objectID": "backends/impala.quarto.html#ingesting-data-from-pandas",
    "href": "backends/impala.quarto.html#ingesting-data-from-pandas",
    "title": "Impala",
    "section": "Ingesting data from pandas",
    "text": "Ingesting data from pandas\nOverall interoperability between the Hadoop / Spark ecosystems and pandas / the PyData stack is poor, but it will improve in time (this is a major part of the Ibis roadmap).\nIbis’s Impala tools currently interoperate with pandas in these ways:\n\nIbis expressions return pandas objects (i.e. DataFrame or Series) for non-scalar expressions when calling their to_pandas method\nThe create_table and insert methods can accept pandas objects. This includes inserting into partitioned tables. It currently uses CSV as the ingest route.\n\nFor example:\n&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; data = pd.DataFrame({'foo': [1, 2, 3, 4], 'bar': ['a', 'b', 'c', 'd']})\n\n&gt;&gt;&gt; db.create_table('pandas_table', data)\n&gt;&gt;&gt; t = db.pandas_table\n&gt;&gt;&gt; t.to_pandas()\n  bar  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\n&gt;&gt;&gt; t.drop()\n\n&gt;&gt;&gt; db.create_table('empty_for_insert', schema=t.schema())\n\n&gt;&gt;&gt; to_insert = db.empty_for_insert\n&gt;&gt;&gt; to_insert.insert(data)\n&gt;&gt;&gt; to_insert.to_pandas()\n  bar  foo\n0   a    1\n1   b    2\n2   c    3\n3   d    4\n\n&gt;&gt;&gt; to_insert.drop()\n&gt;&gt;&gt; import pandas as pd\n\n&gt;&gt;&gt; data = pd.DataFrame({'foo': [1, 2, 3, 4], 'bar': ['a', 'b', 'c', 'd']})\n\n&gt;&gt;&gt; db.create_table('pandas_table', data)\n&gt;&gt;&gt; t = db.pandas_table\n&gt;&gt;&gt; t.to_pandas()\n   foo bar\n0    1   a\n1    2   b\n2    3   c\n3    4   d\n\n&gt;&gt;&gt; t.drop()\n&gt;&gt;&gt; db.create_table('empty_for_insert', schema=t.schema())\n&gt;&gt;&gt; to_insert = db.empty_for_insert\n&gt;&gt;&gt; to_insert.insert(data)\n&gt;&gt;&gt; to_insert.to_pandas()\n   foo bar\n0    1   a\n1    2   b\n2    3   c\n3    4   d\n\n&gt;&gt;&gt; to_insert.drop()"
  },
  {
    "objectID": "backends/impala.quarto.html#queries-on-parquet-avro-and-delimited-files",
    "href": "backends/impala.quarto.html#queries-on-parquet-avro-and-delimited-files",
    "title": "Impala",
    "section": "Queries on Parquet, Avro, and Delimited files",
    "text": "Queries on Parquet, Avro, and Delimited files\nIbis can easily create temporary or persistent Impala tables that reference data in the following formats:\n\nParquet (parquet_file)\nAvro (avro_file)\nDelimited text formats (CSV, TSV, etc.) (delimited_file)\n\nParquet is the easiest because the schema can be read from the data files:\n&gt;&gt;&gt; path = '/__ibis/ibis-testing-data/parquet/tpch_lineitem'\n&gt;&gt;&gt; lineitem = con.parquet_file(path)\n&gt;&gt;&gt; lineitem.limit(2)\n   l_orderkey  l_partkey  l_suppkey  l_linenumber l_quantity l_extendedprice  \\\n0           1     155190       7706             1      17.00        21168.23\n1           1      67310       7311             2      36.00        45983.16\n\n  l_discount l_tax l_returnflag l_linestatus  l_shipdate l_commitdate  \\\n0       0.04  0.02            N            O  1996-03-13   1996-02-12\n1       0.09  0.06            N            O  1996-04-12   1996-02-28\n\n  l_receiptdate     l_shipinstruct l_shipmode  \\\n0    1996-03-22  DELIVER IN PERSON      TRUCK\n1    1996-04-20   TAKE BACK RETURN       MAIL\n\n                            l_comment\n0             egular courts above the\n1  ly final dependencies: slyly bold\n&gt;&gt;&gt; lineitem.l_extendedprice.sum()\nDecimal('229577310901.20')\nIf you want to query a Parquet file and also create a table in Impala that remains after your session, you can pass more information to parquet_file:\n&gt;&gt;&gt; table = con.parquet_file(path, name='my_parquet_table',\n...                          database='ibis_testing',\n...                          persist=True)\n&gt;&gt;&gt; table.l_extendedprice.sum()\nDecimal('229577310901.20')\n&gt;&gt;&gt; con.table('my_parquet_table').l_extendedprice.sum()\nDecimal('229577310901.20')\n&gt;&gt;&gt; con.drop_table('my_parquet_table')\nTo query delimited files, you need to write down an Ibis schema.\n&gt;&gt;&gt; schema = ibis.schema(dict(foo='string', bar='double', baz='int32'))\n&gt;&gt;&gt; table = con.delimited_file('/__ibis/ibis-testing-data/csv', schema)\n&gt;&gt;&gt; table.limit(10)\n          foo       bar  baz\n0  63IEbRheTh  0.679389    6\n1  mG4hlqnjeG  2.807106   15\n2  JTPdX9SZH5 -0.155126   55\n3  2jcl6FypOl  1.037878   21\n4  k3TbJLaadQ -1.401908   23\n5  rP5J4xvinM -0.442093   22\n6  WniUylixYt -0.863748   27\n7  znsDuKOB1n -0.566030   47\n8  4SRP9jlo1M  0.331460   88\n9  KsfjPyDf5e -0.578931   70\n&gt;&gt;&gt; table.bar.summary()\n   count  nulls       min       max       sum    mean  approx_nunique\n0    100      0 -1.401908  2.807106  8.479978  0.0848              10\nFor functions like parquet_file and delimited_file, a directory must be passed and the directory must contain files all having the same schema."
  },
  {
    "objectID": "backends/impala.quarto.html#other-helper-functions-for-interacting-with-the-database",
    "href": "backends/impala.quarto.html#other-helper-functions-for-interacting-with-the-database",
    "title": "Impala",
    "section": "Other helper functions for interacting with the database",
    "text": "Other helper functions for interacting with the database\nWe’re adding a growing list of useful utility functions for interacting with an Impala cluster on the client object. The idea is that you should be able to do any database-admin-type work with Ibis and not have to switch over to the Impala SQL shell. Any ways we can make this more pleasant, please let us know.\nHere’s some of the features, which we’ll give examples for:\n\nListing and searching for available databases and tables\nCreating and dropping databases\nGetting table schemas\n\n&gt;&gt;&gt; con.list_databases(like='ibis*')\n['ibis_testing', 'ibis_testing_tmp_db']\n&gt;&gt;&gt; con.list_tables(database='ibis_testing', like='tpch*')\n['tpch_customer',\n 'tpch_lineitem',\n 'tpch_nation',\n 'tpch_orders',\n 'tpch_part',\n 'tpch_partsupp',\n 'tpch_region',\n 'tpch_region_avro',\n 'tpch_supplier']\n&gt;&gt;&gt; schema = con.get_schema('functional_alltypes')\n&gt;&gt;&gt; schema\nibis.Schema {\n  id               int32\n  bool_col         boolean\n  tinyint_col      int8\n  smallint_col     int16\n  int_col          int32\n  bigint_col       int64\n  float_col        float32\n  double_col       float64\n  date_string_col  string\n  string_col       string\n  timestamp_col    timestamp\n  year             int32\n  month            int32\n}\nDatabases can be created, too, and you can set the storage path in HDFS you want for the data files\n&gt;&gt;&gt; db = 'ibis_testing2'\n&gt;&gt;&gt; con.create_database(db, force=True)\n&gt;&gt;&gt; con.create_table('example_table', con.table('functional_alltypes'),\n...                  database=db, force=True)\nTo drop a database, including all tables in it, you can use drop_database with force=True:\n&gt;&gt;&gt; con.drop_database(db, force=True)"
  },
  {
    "objectID": "backends/impala.quarto.html#user-defined-functions-udf",
    "href": "backends/impala.quarto.html#user-defined-functions-udf",
    "title": "Impala",
    "section": "User Defined functions (UDF)",
    "text": "User Defined functions (UDF)\nImpala currently supports user-defined scalar functions (known henceforth as UDFs) and aggregate functions (respectively UDAs) via a C++ extension API.\nInitial support for using C++ UDFs in Ibis came in version 0.4.0.\n\nUsing scalar functions (UDFs)\nLet’s take an example to illustrate how to make a C++ UDF available to Ibis. Here is a function that computes an approximate equality between floating point values:\n#include \"impala_udf/udf.h\"\n\n#include &lt;cctype&gt;\n#include &lt;cmath&gt;\n\nBooleanVal FuzzyEquals(FunctionContext* ctx, const DoubleVal& x, const DoubleVal& y) {\n  const double EPSILON = 0.000001f;\n  if (x.is_null || y.is_null) return BooleanVal::null();\n  double delta = fabs(x.val - y.val);\n  return BooleanVal(delta &lt; EPSILON);\n}\nYou can compile this to either a shared library (a .so file) or to LLVM bitcode with clang (a .ll file). Skipping that step for now (will add some more detailed instructions here later, promise).\nTo make this function callable, we use ibis.impala.wrap_udf:\nlibrary = '/ibis/udfs/udftest.ll'\ninputs = ['double', 'double']\noutput = 'boolean'\nsymbol = 'FuzzyEquals'\nudf_db = 'ibis_testing'\nudf_name = 'fuzzy_equals'\n\nfuzzy_equals = ibis.impala.wrap_udf(\n    library, inputs, output, symbol, name=udf_name\n)\nIn typical workflows, you will set up a UDF in Impala once then use it thenceforth. So the first time you do this, you need to create the UDF in Impala:\nclient.create_function(fuzzy_equals, database=udf_db)\nNow, we must register this function as a new Impala operation in Ibis. This must take place each time you load your Ibis session.\nfunc.register(fuzzy_equals.name, udf_db)\nThe object fuzzy_equals is callable and works with Ibis expressions:\n&gt;&gt;&gt; t = con.tables.functional_alltypes\n\n&gt;&gt;&gt; expr = fuzzy_equals(t.float_col, t.double_col / 10)\n\n&gt;&gt;&gt; expr.to_pandas()[:10]\n0     True\n1    False\n2    False\n3    False\n4    False\n5    False\n6    False\n7    False\n8    False\n9    False\nName: tmp, dtype: bool\nNote that the call to register on the UDF object must happen each time you use Ibis. If you have a lot of UDFs, I suggest you create a file with all of your wrapper declarations and user APIs that you load with your Ibis session to plug in all your own functions."
  },
  {
    "objectID": "backends/impala.quarto.html#working-with-secure-clusters-kerberos",
    "href": "backends/impala.quarto.html#working-with-secure-clusters-kerberos",
    "title": "Impala",
    "section": "Working with secure clusters (Kerberos)",
    "text": "Working with secure clusters (Kerberos)\nIbis is compatible with Hadoop clusters that are secured with Kerberos (as well as SSL and LDAP). Note that to enable this support, you’ll also need to install the kerberos package.\n$ pip install kerberos\nJust like the Impala shell and ODBC/JDBC connectors, Ibis connects to Impala through the HiveServer2 interface (using the impyla client). Therefore, the connection semantics are similar to the other access methods for working with secure clusters.\nSpecifically, after authenticating yourself against Kerberos (e.g., by issuing the appropriate kinit command), pass auth_mechanism='GSSAPI' or auth_mechanism='LDAP' (and set kerberos_service_name if necessary along with user and password if necessary) to the ibis.impala_connect(...) method. This method also takes arguments to configure SSL (use_ssl, ca_cert). See the documentation for the Impala shell for more details."
  },
  {
    "objectID": "backends/duckdb.html",
    "href": "backends/duckdb.html",
    "title": "DuckDB",
    "section": "",
    "text": "https://duckdb.org"
  },
  {
    "objectID": "backends/duckdb.html#install",
    "href": "backends/duckdb.html#install",
    "title": "DuckDB",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the DuckDB backend:\n\npipcondamamba\n\n\nInstall with the duckdb extra:\npip install 'ibis-framework[duckdb]'\nAnd connect:\nimport ibis\n\n1con = ibis.duckdb.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for DuckDB:\nconda install -c conda-forge ibis-duckdb\nAnd connect:\nimport ibis\n\n1con = ibis.duckdb.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for DuckDB:\nmamba install -c conda-forge ibis-duckdb\nAnd connect:\nimport ibis\n\n1con = ibis.duckdb.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/duckdb.html#connect",
    "href": "backends/duckdb.html#connect",
    "title": "DuckDB",
    "section": "Connect",
    "text": "Connect\n\nibis.duckdb.connect\nConnect to an in-memory database:\ncon = ibis.duckdb.connect()\nConnect to, or create, a local DuckDB file\ncon = ibis.duckdb.connect(\"mydb.duckdb\")\n\n\n\n\n\n\nNote\n\n\n\nibis.duckdb.connect is a thin wrapper around ibis.backends.duckdb.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, database=':memory:', read_only=False, temp_directory=None, extensions=None, **config)\nCreate an Ibis client connected to a DuckDB database.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndatabase\nstr | pathlib.Path\nPath to a duckdb database.\n':memory:'\n\n\nread_only\nbool\nWhether the database is read-only.\nFalse\n\n\ntemp_directory\nstr | pathlib.Path | None\nDirectory to use for spilling to disk. Only set by default for in-memory connections.\nNone\n\n\nextensions\ncollections.abc.Sequence[str] | None\nA list of duckdb extensions to install/load upon connection.\nNone\n\n\nconfig\ntyping.Any\nDuckDB configuration parameters. See the DuckDB configuration documentation for possible configuration values.\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.duckdb.connect(\"database.ddb\", threads=4, memory_limit=\"1GB\")\n&lt;ibis.backends.duckdb.Backend object at ...&gt;\n\n\n\n\nibis.connect URL format\nIn addition to ibis.duckdb.connect, you can also connect to DuckDB by passing a properly formatted DuckDB connection URL to ibis.connect\n\nimport ibis\n\ncon = ibis.connect(\"duckdb://local.ddb\")\n\nWithout an empty path, ibis.connect will connect to an ephemeral, in-memory database.\n\ncon = ibis.connect(\"duckdb://\")"
  },
  {
    "objectID": "backends/duckdb.html#motherduck",
    "href": "backends/duckdb.html#motherduck",
    "title": "DuckDB",
    "section": "MotherDuck",
    "text": "MotherDuck\nThe DuckDB backend supports MotherDuck. If you have an account, you can connect to MotherDuck by passing in the string md: or motherduck:. ibis will trigger the authentication prompt in-browser.\nimport ibis\n\ncon = ibis.duckdb.connect(\"md:\")\n\n\n\n\n\n\nNote\n\n\n\nAuthentication to MotherDuck will trigger on the first call that requires retrieving information (in this case list_tables)\n\n\ncon.list_tables()\nAttempting to automatically open the SSO authorization page in your default browser.\n1. Please open this link to login into your account: https://auth.motherduck.com/activate\n2. Enter the following code: ZSRQ-GJQS\n\n\nToken successfully retrieved ✅\nYou can store it as an environment variable to avoid having to log in again:\n  $ export motherduck_token='****************'\n\n['penguins']"
  },
  {
    "objectID": "backends/duckdb.html#ibis.backends.duckdb.Backend",
    "href": "backends/duckdb.html#ibis.backends.duckdb.Backend",
    "title": "DuckDB",
    "section": "duckdb.Backend",
    "text": "duckdb.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nattach\nattach(self, path, name=None, read_only=False)\nAttach another DuckDB database to the current DuckDB session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nPath to the database to attach.\nrequired\n\n\nname\nstr | None\nName to attach the database as. Defaults to the basename of path.\nNone\n\n\nread_only\nbool\nWhether to attach the database as read-only.\nFalse\n\n\n\n\n\n\nattach_sqlite\nattach_sqlite(self, path, overwrite=False, all_varchar=False)\nAttach a SQLite database to the current DuckDB session.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe path to the SQLite database.\nrequired\n\n\noverwrite\nbool\nAllow overwriting any tables or views that already exist in your current session with the contents of the SQLite database.\nFalse\n\n\nall_varchar\nbool\nSet all SQLite columns to type VARCHAR to avoid type errors on ingestion.\nFalse\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import sqlite3\n&gt;&gt;&gt; with sqlite3.connect(\"/tmp/attach_sqlite.db\") as con:\n...     con.execute(\"DROP TABLE IF EXISTS t\")\n...     con.execute(\"CREATE TABLE t (a INT, b TEXT)\")\n...     con.execute(\n...         \"INSERT INTO t VALUES (1, 'a'), (2, 'b'), (3, 'c')\"\n...     )\n&lt;...&gt;\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; con.list_tables()\n[]\n&gt;&gt;&gt; con.attach_sqlite(\"/tmp/attach_sqlite.db\")\n&gt;&gt;&gt; con.list_tables()\n['t']\n\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndetach\ndetach(self, name)\nDetach a database from the current DuckDB session.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the database to detach.\nrequired\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, database=None, force=False)\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\nList existing schemas in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned schema names.\nNone\n\n\ndatabase\nstr | None\nThe database to list schemas from. If None, the current database is searched.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe schema names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\nList tables and views.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nRegex to filter by table/view name.\nNone\n\n\ndatabase\nstr | None\nDatabase name. If not passed, uses the current database. Only supported with MotherDuck.\nNone\n\n\nschema\nstr | None\nSchema name. If not passed, uses the current schema.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nList of table and view names.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; con = ibis.duckdb.connect()\n&gt;&gt;&gt; foo = con.create_table(\"foo\", schema=ibis.schema(dict(a=\"int\")))\n&gt;&gt;&gt; con.list_tables()\n['foo']\n&gt;&gt;&gt; bar = con.create_view(\"bar\", foo)\n&gt;&gt;&gt; con.list_tables()\n['bar', 'foo']\n&gt;&gt;&gt; con.create_schema(\"my_schema\")\n&gt;&gt;&gt; con.list_tables(schema=\"my_schema\")\n[]\n&gt;&gt;&gt; with con.begin() as c:\n...     c.exec_driver_sql(\"CREATE TABLE my_schema.baz (a INTEGER)\")\n&lt;...&gt;\n&gt;&gt;&gt; con.list_tables(schema=\"my_schema\")\n['baz']\n\n\n\nload_extension\nload_extension(self, extension, force_install=False)\nInstall and load a duckdb extension by name or path.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nextension\nstr\nThe extension name or path.\nrequired\n\n\nforce_install\nbool\nForce reinstallation of the extension.\nFalse\n\n\n\n\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, source_list, table_name=None, **kwargs)\nRegister a CSV file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_list\nstr | list[str] | tuple[str]\nThe data source(s). May be a path to a file or directory of CSV files, or an iterable of CSV files.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to DuckDB loading function. See https://duckdb.org/docs/data/csv for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source_table, table_name=None, **kwargs)\nRegister a Delta Lake table as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_table\nstr\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.DeltaTable.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_geo\nread_geo(self, source, table_name=None, **kwargs)\nRegister a GEO file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr\nThe data source(s). Path to a file of geospatial files supported by duckdb. See https://duckdb.org/docs/extensions/spatial.html#st_read—read-spatial-data-from-files\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to DuckDB loading function. See https://duckdb.org/docs/extensions/spatial.html#st_read—read-spatial-data-from-files for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_in_memory\nread_in_memory(self, source, table_name=None)\nRegister a Pandas DataFrame or pyarrow object as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | pyarrow.pyarrow.RecordBatchReader\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_json\nread_json(self, source_list, table_name=None, **kwargs)\nRead newline-delimited JSON into an ibis table.\n\n\n\n\n\n\nThis feature requires duckdb&gt;=0.7.0\n\n\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_list\nstr | list[str] | tuple[str]\nFile or list of files\nrequired\n\n\ntable_name\nstr | None\nOptional table name\nNone\n\n\n**kwargs\n\nAdditional keyword arguments passed to DuckDB’s read_json_auto function\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nAn ibis table expression\n\n\n\n\n\n\nread_parquet\nread_parquet(self, source_list, table_name=None, **kwargs)\nRegister a parquet file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_list\nstr | collections.abc.Iterable[str]\nThe data source(s). May be a path to a file, an iterable of files, or directory of parquet files.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to DuckDB loading function. See https://duckdb.org/docs/data/parquet for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_postgres\nread_postgres(self, uri, table_name=None, schema='public')\nRegister a table from a postgres instance into a DuckDB table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nuri\nstr\nA postgres URI of the form postgres://user:password@host:port\nrequired\n\n\ntable_name\nstr | None\nThe table to read\nNone\n\n\nschema\nstr\nPostgreSQL schema where table_name resides\n'public'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_sqlite\nread_sqlite(self, path, table_name=None)\nRegister a table from a SQLite database into a DuckDB table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe path to the SQLite database\nrequired\n\n\ntable_name\nstr | None\nThe table to read\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import sqlite3\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; with sqlite3.connect(\"/tmp/sqlite.db\") as con:\n...     con.execute(\"DROP TABLE IF EXISTS t\")\n...     con.execute(\"CREATE TABLE t (a INT, b TEXT)\")\n...     con.execute(\n...         \"INSERT INTO t VALUES (1, 'a'), (2, 'b'), (3, 'c')\"\n...     )\n&lt;...&gt;\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; t = con.read_sqlite(\"/tmp/sqlite.db\", table_name=\"t\")\n&gt;&gt;&gt; t\n┏━━━━━━━┳━━━━━━━━┓\n┃ a     ┃ b      ┃\n┡━━━━━━━╇━━━━━━━━┩\n│ int64 │ string │\n├───────┼────────┤\n│     1 │ a      │\n│     2 │ b      │\n│     3 │ c      │\n└───────┴────────┘\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister\nregister(self, source, table_name=None, **kwargs)\nRegister a data source as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path | typing.Any\nThe data source(s). May be a path to a file or directory of parquet/csv files, an iterable of parquet or CSV files, a pandas dataframe, a pyarrow table or dataset, or a postgres URI.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to DuckDB loading functions for CSV or parquet. See https://duckdb.org/docs/data/csv and https://duckdb.org/docs/data/parquet for more information.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nregister_filesystem\nregister_filesystem(self, filesystem)\nRegister an fsspec filesystem object with DuckDB.\nThis allow a user to read from any fsspec compatible filesystem using read_csv, read_parquet, read_json, etc.\n\n\n\n\n\n\nNote\n\n\n\nCreating an fsspec filesystem requires that the corresponding backend-specific fsspec helper library is installed.\ne.g. to connect to Google Cloud Storage, gcsfs must be installed.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilesystem\nfsspec.AbstractFileSystem\nThe fsspec filesystem object to register with DuckDB. See https://duckdb.org/docs/guides/python/filesystems for details.\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import fsspec\n&gt;&gt;&gt; gcs = fsspec.filesystem(\"gcs\")\n&gt;&gt;&gt; con = ibis.duckdb.connect()\n&gt;&gt;&gt; con.register_filesystem(gcs)\n&gt;&gt;&gt; t = con.read_csv(\n...     \"gcs://ibis-examples/data/band_members.csv.gz\",\n...     table_name=\"band_members\",\n... )\nDatabaseTable: band_members\n  name string\n  band string\n\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, header=True, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nheader\nbool\nWhether to write the column names as the first line of the CSV file.\nTrue\n\n\n**kwargs\ntyping.Any\nDuckDB CSV writer arguments. https://duckdb.org/docs/data/csv.html#parameters\n{}\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nDuckDB Parquet writer arguments. See https://duckdb.org/docs/data/parquet#writing-to-parquet-files for details\n{}\n\n\n\n\n\nExamples\nWrite out an expression to a single parquet file.\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; con = ibis.get_backend(penguins)\n&gt;&gt;&gt; con.to_parquet(penguins, \"/tmp/penguins.parquet\")\nWrite out an expression to a hive-partitioned parquet file.\n&gt;&gt;&gt; import tempfile\n&gt;&gt;&gt; penguins = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; con = ibis.get_backend(penguins)\nPartition on a single column.\n&gt;&gt;&gt; con.to_parquet(penguins, tempfile.mkdtemp(), partition_by=\"year\")\nPartition on multiple columns.\n&gt;&gt;&gt; con.to_parquet(penguins, tempfile.mkdtemp(), partition_by=(\"year\", \"island\"))\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **_)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nReturn a stream of record batches.\nThe returned RecordBatchReader contains a cursor with an unbounded lifetime.\nFor analytics use cases this is usually nothing to fret about. In some cases you may need to explicit release the cursor.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nBound parameters\nNone\n\n\nlimit\nint | str | None\nLimit the result to this number of rows\nNone\n\n\nchunk_size\nint\n::: {.callout-warning} ## DuckDB returns 1024 size batches regardless of what argument is passed. :::\n1000000\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/badges.html",
    "href": "backends/badges.html",
    "title": "Ibis",
    "section": "",
    "text": "{% if imports %} [filebadge](https://img.shields.io/badge/Reads-{{ “%20|%20”.join(sorted(imports)) }}-blue?style=flat-square) {% endif %}\n{% if exports %} [exportbadge](https://img.shields.io/badge/Exports-{{ “%20|%20”.join(sorted(exports)) }}-orange?style=flat-square) {% endif %}\n\n\n\n Back to top"
  },
  {
    "objectID": "backends/mssql.html",
    "href": "backends/mssql.html",
    "title": "MSSQL",
    "section": "",
    "text": "https://www.microsoft.com/sql-server"
  },
  {
    "objectID": "backends/mssql.html#install",
    "href": "backends/mssql.html#install",
    "title": "MSSQL",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the MSSQL backend:\n\npipcondamamba\n\n\nInstall with the mssql extra:\npip install 'ibis-framework[mssql]'\nAnd connect:\nimport ibis\n\n1con = ibis.mssql.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for MSSQL:\nconda install -c conda-forge ibis-mssql\nAnd connect:\nimport ibis\n\n1con = ibis.mssql.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for MSSQL:\nmamba install -c conda-forge ibis-mssql\nAnd connect:\nimport ibis\n\n1con = ibis.mssql.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/mssql.html#connect",
    "href": "backends/mssql.html#connect",
    "title": "MSSQL",
    "section": "Connect",
    "text": "Connect\n\nibis.mssql.connect\ncon = ibis.mssql.connect(\n    user=\"username\",\n    password=\"password\",\n    host=\"hostname\",\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.mssql.connect is a thin wrapper around ibis.backends.mssql.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, host='localhost', user=None, password=None, port=1433, database=None, url=None, driver='pymssql')\n\n\n\nibis.connect URL format\nIn addition to ibis.mssql.connect, you can also connect to MSSQL by passing a properly formatted MSSQL connection URL to ibis.connect\ncon = ibis.connect(f\"mssql://{user}:{password}@{host}:{port}\")"
  },
  {
    "objectID": "backends/mssql.html#ibis.backends.mssql.Backend",
    "href": "backends/mssql.html#ibis.backends.mssql.Backend",
    "title": "MSSQL",
    "section": "mssql.Backend",
    "text": "mssql.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\nbegin\nbegin(self)\n\n\ncompile\ncompile(self, expr, limit=None, params=None, timecontext=None)\nCompile an Ibis expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr | None\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Expr, typing.Any] | None\nNamed unbound parameters\nNone\n\n\ntimecontext\ntuple[pandas.pandas.Timestamp, pandas.pandas.Timestamp] | None\nAdditional information about data source time boundaries\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ntyping.Any\nThe output of compilation. The type of this value depends on the backend.\n\n\n\n\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_database\ncreate_database(self, name, force=False)\nCreate a new database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new database.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database already exists.\nFalse\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, name, obj=None, *, schema=None, database=None, temp=False, overwrite=False)\nCreate a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nShould the table be temporary for the session.\nFalse\n\n\noverwrite\nbool\nClobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, name, obj, *, database=None, overwrite=False)\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop a database with name name.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase to drop.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database does not exist.\nFalse\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, name, *, database=None, force=False)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable to drop\nrequired\n\n\ndatabase\nstr | None\nDatabase to drop table from\nNone\n\n\nforce\nbool\nCheck for existence before dropping\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, name, *, database=None, force=False)\n\n\nexecute\nexecute(self, expr, params=None, limit='default', **kwargs)\nCompile and execute an Ibis expression.\nCompile and execute Ibis expression using this backend client interface, returning results in-memory in the appropriate object type\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression\nrequired\n\n\nlimit\nstr\nFor expressions yielding result sets; retrieve at most this number of values/rows. Overrides any limit already set on the expression.\n'default'\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nNamed unbound parameters\nNone\n\n\nkwargs\ntyping.Any\nBackend specific arguments. For example, the clickhouse backend uses this to receive external_tables as a dictionary of pandas DataFrames.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nDataFrame | Series | Scalar\n* Table: pandas.DataFrame * Column: pandas.Series * Scalar: Python scalar value\n\n\n\n\n\n\nexplain\nexplain(self, expr, params=None)\nExplain an expression.\nReturn the query plan associated with the indicated expression or SQL query.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nQuery plan\n\n\n\n\n\n\nfetch_from_cursor\nfetch_from_cursor(self, cursor, schema)\n\n\nhas_operation\nhas_operation(cls, operation)\n\n\ninsert\ninsert(self, table_name, obj, database=None, overwrite=False)\nInsert data into a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ntable_name\nstr\nThe name of the table to which data needs will be inserted\nrequired\n\n\nobj\npandas.pandas.DataFrame | ibis.ibis.Table | list | dict\nThe source data or expression to insert\nrequired\n\n\ndatabase\nstr | None\nName of the attached database that the table is located in.\nNone\n\n\noverwrite\nbool\nIf True then replace existing contents of table\nFalse\n\n\n\n\n\nRaises\n\n\n\nType\nDescription\n\n\n\n\nNotImplementedError\nIf inserting data from a different database\n\n\nValueError\nIf the type of obj isn’t supported\n\n\n\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\n\n\nlist_tables\nlist_tables(self, like=None, database=None, schema=None)\n\n\nraw_sql\nraw_sql(self, query)\nExecute a query and return the cursor used for execution.\n\n\n\n\n\n\nConsider using .sql instead\n\n\n\nIf your query is a SELECT statement you can use the backend .sql method to avoid having to manually release the cursor returned from this method.\n\n\n\n\n\n\nThe cursor returned from this method must be manually released\n\n\n\nYou do not need to call .close() on the cursor when running DDL or DML statements like CREATE, INSERT or DROP, only when using SELECT statements.\nTo release a cursor, call the close method on the returned cursor object.\nYou can close the cursor by explicitly calling its close method:\ncursor = con.raw_sql(\"SELECT ...\")\ncursor.close()\nOr you can use a context manager:\nwith con.raw_sql(\"SELECT ...\") as cursor:\n    ...\n\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlalchemy.sqlalchemy.sql.sqlalchemy.sql.ClauseElement\nSQL query or SQLAlchemy expression to execute\nrequired\n\n\n\n\n\nExamples\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; with con.raw_sql(\"SELECT 1\") as cursor:\n...     result = cursor.fetchall()\n&gt;&gt;&gt; result\n[(1,)]\n&gt;&gt;&gt; cursor.closed\nTrue\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source, table_name=None, **kwargs)\nRegister a Delta Lake table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the underlying backend or library.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table.\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\nschema\nschema(self, name)\nGet an ibis schema from the current database for the table name.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nThe ibis schema of name\n\n\n\n\n\n\nsql\nsql(self, query, schema=None, dialect=None)\nConvert a SQL query to an Ibis table expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr\nSQL string\nrequired\n\n\nschema\nibis.ibis.Schema | None\nThe expected schema for this query. If not provided, will be inferred automatically if possible.\nNone\n\n\ndialect\nstr | None\nOptional string indicating the dialect of query. The default value of None will use the backend’s native dialect.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\ntable\ntable(self, name, database=None, schema=None)\nCreate a table expression from a table in the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nTable name\nrequired\n\n\ndatabase\nstr | None\nThe database the table resides in\nNone\n\n\nschema\nstr | None\nThe schema inside database where the table resides. ::: {.callout-warning} ## schema refers to database hierarchy The schema parameter does not refer to the column names and types of table. :::\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nTable expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, *, params=None, limit=None, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **_)\nExecute expression and return an iterator of pyarrow record batches.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nRecordBatchReader\nCollection of pyarrow RecordBatchs.\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name.\n\n\n\n\n\n\ntruncate_table\ntruncate_table(self, name, database=None)"
  },
  {
    "objectID": "backends/datafusion.html",
    "href": "backends/datafusion.html",
    "title": "DataFusion",
    "section": "",
    "text": "https://arrow.apache.org/datafusion"
  },
  {
    "objectID": "backends/datafusion.html#install",
    "href": "backends/datafusion.html#install",
    "title": "DataFusion",
    "section": "Install",
    "text": "Install\nInstall Ibis and dependencies for the Apache DataFusion backend:\n\npipcondamamba\n\n\nInstall with the Apache datafusion extra:\npip install 'ibis-framework[datafusion]'\nAnd connect:\nimport ibis\n\n1con = ibis.datafusion.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Apache DataFusion:\nconda install -c conda-forge ibis-datafusion\nAnd connect:\nimport ibis\n\n1con = ibis.datafusion.connect()\n\n1\n\nAdjust connection parameters as needed.\n\n\n\n\nInstall for Apache DataFusion:\nmamba install -c conda-forge ibis-datafusion\nAnd connect:\nimport ibis\n\n1con = ibis.datafusion.connect()\n\n1\n\nAdjust connection parameters as needed."
  },
  {
    "objectID": "backends/datafusion.html#connect",
    "href": "backends/datafusion.html#connect",
    "title": "DataFusion",
    "section": "Connect",
    "text": "Connect\n\nibis.datafusion.connect\ncon = ibis.datafusion.connect()\ncon = ibis.datafusion.connect(\n    config={\"table1\": \"path/to/file.parquet\", \"table2\": \"path/to/file.csv\"}\n)\n\n\n\n\n\n\nNote\n\n\n\nibis.datafusion.connect is a thin wrapper around ibis.backends.datafusion.Backend.do_connect.\n\n\n\n\nConnection Parameters\n\ndo_connect\ndo_connect(self, config=None)\nCreate a Datafusion backend for use with Ibis.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nconfig\ncollections.abc.Mapping[str, str | pathlib.Path] | datafusion.SessionContext | None\nMapping of table names to files.\nNone\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; config = {\"t\": \"path/to/file.parquet\", \"s\": \"path/to/file.csv\"}\n&gt;&gt;&gt; ibis.datafusion.connect(config)"
  },
  {
    "objectID": "backends/datafusion.html#ibis.backends.datafusion.Backend",
    "href": "backends/datafusion.html#ibis.backends.datafusion.Backend",
    "title": "DataFusion",
    "section": "datafusion.Backend",
    "text": "datafusion.Backend\n\nadd_operation\nadd_operation(self, operation)\nAdd a translation function to the backend for a specific operation.\nOperations are defined in ibis.expr.operations, and a translation function receives the translator object and an expression as parameters, and returns a value depending on the backend.\n\n\ncompile\ncompile(self, expr, limit=None, params=None, **kwargs)\nCompile an Ibis expression to a DataFusion SQL string.\n\n\nconnect\nconnect(self, *args, **kwargs)\nConnect to the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\n*args\n\nMandatory connection parameters, see the docstring of do_connect for details.\n()\n\n\n**kwargs\n\nExtra connection parameters, see the docstring of do_connect for details.\n{}\n\n\n\n\n\nNotes\nThis creates a new backend instance with saved args and kwargs, then calls reconnect and finally returns the newly created and connected backend instance.\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.BaseBackend\nAn instance of the backend\n\n\n\n\n\n\ncreate_database\ncreate_database(self, name, force=False)\nCreate a new database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new database.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database already exists.\nFalse\n\n\n\n\n\n\ncreate_schema\ncreate_schema(self, name, database=None, force=False)\nCreate a schema named name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to create.\nrequired\n\n\ndatabase\nstr | None\nName of the database in which to create the schema. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema exists.\nFalse\n\n\n\n\n\n\ncreate_table\ncreate_table(self, *_, **__)\nCreate a new table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new table.\nrequired\n\n\nobj\npandas.pandas.DataFrame | pyarrow.pyarrow.Table | ibis.ibis.Table | None\nAn Ibis table expression or pandas table that will be used to extract the schema and the data of the new table. If not provided, schema must be given.\nNone\n\n\nschema\nibis.ibis.Schema | None\nThe schema for the new table. Only one of schema or obj can be provided.\nNone\n\n\ndatabase\nstr | None\nName of the database where the table will be created, if not the default.\nNone\n\n\ntemp\nbool\nWhether a table is temporary or not\nFalse\n\n\noverwrite\nbool\nWhether to clobber existing data\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe table that was created.\n\n\n\n\n\n\ncreate_view\ncreate_view(self, *_, **__)\nCreate a new view from an expression.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the new view.\nrequired\n\n\nobj\nibis.ibis.Table\nAn Ibis table expression that will be used to create the view.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view will be created, if not provided the database’s default is used.\nNone\n\n\noverwrite\nbool\nWhether to clobber an existing view with the same name\nFalse\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nThe view that was created.\n\n\n\n\n\n\ndatabase\ndatabase(self, name=None)\nReturn a Database object for the name database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr | None\nName of the database to return the object for.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nibis.backends.base.Database\nA database object for the specified database.\n\n\n\n\n\n\ndrop_database\ndrop_database(self, name, force=False)\nDrop a database with name name.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nDatabase to drop.\nrequired\n\n\nforce\nbool\nIf False, an exception is raised if the database does not exist.\nFalse\n\n\n\n\n\n\ndrop_schema\ndrop_schema(self, name, database=None, force=False)\nDrop the schema with name in database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the schema to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database to drop the schema from. If None, the current database is used.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the schema does not exist.\nFalse\n\n\n\n\n\n\ndrop_table\ndrop_table(self, *_, **__)\nDrop a table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the table to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the table exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the table does not exist.\nFalse\n\n\n\n\n\n\ndrop_view\ndrop_view(self, *_, **__)\nDrop a view.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nName of the view to drop.\nrequired\n\n\ndatabase\nstr | None\nName of the database where the view exists, if not the default.\nNone\n\n\nforce\nbool\nIf False, an exception is raised if the view does not exist.\nFalse\n\n\n\n\n\n\nexecute\nexecute(self, expr, **kwargs)\nExecute an expression.\n\n\nhas_operation\nhas_operation(cls, operation)\nReturn whether the backend implements support for operation.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\noperation\ntype[ibis.ibis.Value]\nA class corresponding to an operation.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nbool\nWhether the backend implements the operation.\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import ibis.expr.operations as ops\n&gt;&gt;&gt; ibis.sqlite.has_operation(ops.ArrayIndex)\nFalse\n&gt;&gt;&gt; ibis.postgres.has_operation(ops.ArrayIndex)\nTrue\n\n\n\nlist_databases\nlist_databases(self, like=None)\nList existing databases in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned database names.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe database names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_schemas\nlist_schemas(self, like=None, database=None)\nList existing schemas in the current connection.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlike\nstr | None\nA pattern in Python’s regex format to filter returned schema names.\nNone\n\n\ndatabase\nstr | None\nThe database to list schemas from. If None, the current database is searched.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nlist[str]\nThe schema names that exist in the current connection, that match the like pattern if provided.\n\n\n\n\n\n\nlist_tables\nlist_tables(self, like=None, database=None)\nList the available tables.\n\n\nraw_sql\nraw_sql(self, query)\nExecute a SQL string query against the database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nquery\nstr | sqlglot.sqlglot.exp.sqlglot.exp.Expression\nRaw SQL string\nrequired\n\n\nkwargs\n\nBackend specific query arguments\nrequired\n\n\n\n\n\n\nread_csv\nread_csv(self, path, table_name=None, **kwargs)\nRegister a CSV file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Datafusion loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_delta\nread_delta(self, source_table, table_name=None, **kwargs)\nRegister a Delta Lake table as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource_table\nstr | pathlib.Path\nThe data source. Must be a directory containing a Delta Lake table.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.DeltaTable.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_json\nread_json(self, path, table_name=None, **kwargs)\nRegister a JSON file as a table in the current backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the JSON file.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to the backend loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nread_parquet\nread_parquet(self, path, table_name=None, **kwargs)\nRegister a parquet file as a table in the current database.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npath\nstr | pathlib.Path\nThe data source.\nrequired\n\n\ntable_name\nstr | None\nAn optional name to use for the created table. This defaults to a sequentially generated name.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to Datafusion loading function.\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nibis.ibis.Table\nThe just-registered table\n\n\n\n\n\n\nreconnect\nreconnect(self)\nReconnect to the database already configured with connect.\n\n\nregister\nregister(self, source, table_name=None, **kwargs)\nRegister a data set with table_name located at source.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsource\nstr | pathlib.Path | pyarrow.pyarrow.Table | pyarrow.pyarrow.RecordBatch | pyarrow.pyarrow.Dataset | pandas.pandas.DataFrame\nThe data source(s). May be a path to a file or directory of parquet/csv files, a pandas dataframe, or a pyarrow table, dataset or record batch.\nrequired\n\n\ntable_name\nstr | None\nThe name of the table\nNone\n\n\nkwargs\ntyping.Any\nDatafusion-specific keyword arguments\n{}\n\n\n\n\n\nExamples\nRegister a csv:\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; conn = ibis.datafusion.connect(config)\n&gt;&gt;&gt; conn.register(\"path/to/data.csv\", \"my_table\")\n&gt;&gt;&gt; conn.table(\"my_table\")\nRegister a PyArrow table:\n&gt;&gt;&gt; import pyarrow as pa\n&gt;&gt;&gt; tab = pa.table({\"x\": [1, 2, 3]})\n&gt;&gt;&gt; conn.register(tab, \"my_table\")\n&gt;&gt;&gt; conn.table(\"my_table\")\nRegister a PyArrow dataset:\n&gt;&gt;&gt; import pyarrow.dataset as ds\n&gt;&gt;&gt; dataset = ds.dataset(\"path/to/table\")\n&gt;&gt;&gt; conn.register(dataset, \"my_table\")\n&gt;&gt;&gt; conn.table(\"my_table\")\n\n\n\nregister_options\nregister_options(cls)\nRegister custom backend options.\n\n\nrename_table\nrename_table(self, old_name, new_name)\nRename an existing table.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nold_name\nstr\nThe old name of the table.\nrequired\n\n\nnew_name\nstr\nThe new name of the table.\nrequired\n\n\n\n\n\n\ntable\ntable(self, name, schema=None)\nGet an ibis expression representing a DataFusion table.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nname\nstr\nThe name of the table to retrieve\nrequired\n\n\nschema\nibis.ibis.Schema | None\nAn optional schema for the table\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTable\nA table expression\n\n\n\n\n\n\nto_csv\nto_csv(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a CSV file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to CSV.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the CSV file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.csv.CSVWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_delta\nto_delta(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a Delta Lake table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to Delta Lake table.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the Delta Lake table.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nkwargs\ntyping.Any\nAdditional keyword arguments passed to deltalake.writer.write_deltalake method\n{}\n\n\n\n\n\n\nto_pandas\nto_pandas(self, expr, *, params=None, limit=None, **kwargs)\nExecute an Ibis expression and return a pandas DataFrame, Series, or scalar.\n\n\n\n\n\n\nNote\n\n\n\nThis method is a wrapper around execute.\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\n\nto_pandas_batches\nto_pandas_batches(self, expr, *, params=None, limit=None, chunk_size=1000000, **kwargs)\nExecute an Ibis expression and return an iterator of pandas DataFrames.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned DataFrame batch. This may have no effect depending on the backend.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ncollections.abc.Iterator[pandas.pandas.DataFrame]\nAn iterator of pandas DataFrames.\n\n\n\n\n\n\nto_parquet\nto_parquet(self, expr, path, *, params=None, **kwargs)\nWrite the results of executing the given expression to a parquet file.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Table\nThe ibis expression to execute and persist to parquet.\nrequired\n\n\npath\nstr | pathlib.Path\nThe data source. A string or Path to the parquet file.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\n**kwargs\ntyping.Any\nAdditional keyword arguments passed to pyarrow.parquet.ParquetWriter\n{}\n\n\nhttps\n\n\nrequired\n\n\n\n\n\n\nto_pyarrow\nto_pyarrow(self, expr, **kwargs)\nExecute expression and return results in as a pyarrow table.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTable\nA pyarrow table holding the results of the executed expression.\n\n\n\n\n\n\nto_pyarrow_batches\nto_pyarrow_batches(self, expr, *, chunk_size=1000000, **kwargs)\nExecute expression and return a RecordBatchReader.\nThis method is eager and will execute the associated expression immediately.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to export to pyarrow\nrequired\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means “no limit”. The default is in ibis/config.py.\nNone\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nMapping of scalar parameter expressions to value.\nNone\n\n\nchunk_size\nint\nMaximum number of rows in each returned record batch.\n1000000\n\n\nkwargs\ntyping.Any\nKeyword arguments\n{}\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nresults\nRecordBatchReader\n\n\n\n\n\n\nto_torch\nto_torch(self, expr, *, params=None, limit=None, **kwargs)\nExecute an expression and return results as a dictionary of torch tensors.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nexpr\nibis.ibis.Expr\nIbis expression to execute.\nrequired\n\n\nparams\ncollections.abc.Mapping[ibis.ibis.Scalar, typing.Any] | None\nParameters to substitute into the expression.\nNone\n\n\nlimit\nint | str | None\nAn integer to effect a specific row limit. A value of None means no limit.\nNone\n\n\nkwargs\ntyping.Any\nKeyword arguments passed into the backend’s to_torch implementation.\n{}\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\ndict[str, torch.torch.Tensor]\nA dictionary of torch tensors, keyed by column name."
  },
  {
    "objectID": "reference/scalar-udfs.html",
    "href": "reference/scalar-udfs.html",
    "title": "Scalar UDFs",
    "section": "",
    "text": "Scalar user-defined function APIs"
  },
  {
    "objectID": "reference/scalar-udfs.html#methods",
    "href": "reference/scalar-udfs.html#methods",
    "title": "Scalar UDFs",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbuiltin\nConstruct a scalar user-defined function that is built-in to the backend.\n\n\npandas\nConstruct a vectorized scalar user-defined function that accepts pandas Series’ as inputs.\n\n\npyarrow\nConstruct a vectorized scalar user-defined function that accepts PyArrow Arrays as input.\n\n\npython\nConstruct a non-vectorized scalar user-defined function that accepts Python scalar values as inputs.\n\n\n\n\nbuiltin\nbuiltin(fn=None, *, name=None, schema=None, signature=None, **kwargs)\nConstruct a scalar user-defined function that is built-in to the backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\nschema\n\nThe schema in which the builtin function resides.\nNone\n\n\nsignature\n\nAn optional signature to use for the UDF. If present, should be a tuple containing a tuple of argument types and a return type. For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the argument types will be derived from the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; @ibis.udf.scalar.builtin\n... def hamming(a: str, b: str) -&gt; int:\n...     '''Compute the Hamming distance between two strings.'''\n&gt;&gt;&gt; expr = hamming(\"duck\", \"luck\")\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; con.execute(expr)\n\n1\n\n\n\n\n\npandas\npandas(fn=None, *, name=None, schema=None, signature=None, **kwargs)\nConstruct a vectorized scalar user-defined function that accepts pandas Series’ as inputs.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\nschema\n\nThe schema in which to create the UDF.\nNone\n\n\nsignature\n\nAn optional signature to use for the UDF. If present, should be a tuple containing a tuple of argument types and a return type. For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the argument types will be derived from the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; @ibis.udf.scalar.pandas\n... def add_one(x: int) -&gt; int:\n...     return x + 1\n&gt;&gt;&gt; expr = add_one(2)\n&gt;&gt;&gt; con = ibis.connect(os.environ[\"SNOWFLAKE_URL\"])  # doctest: +SKIP\n&gt;&gt;&gt; con.execute(expr)  # doctest: +SKIP\n3\n\n\nSee Also\n\npython\npyarrow\n\n\n\n\npyarrow\npyarrow(fn=None, *, name=None, schema=None, signature=None, **kwargs)\nConstruct a vectorized scalar user-defined function that accepts PyArrow Arrays as input.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\nschema\n\nThe schema in which to create the UDF.\nNone\n\n\nsignature\n\nAn optional signature to use for the UDF. If present, should be a tuple containing a tuple of argument types and a return type. For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the argument types will be derived from the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow.compute as pc\n&gt;&gt;&gt; @ibis.udf.scalar.pyarrow\n... def add_one(x: int) -&gt; int:\n...     return pc.add(x, 1)\n&gt;&gt;&gt; expr = add_one(2)\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; con.execute(expr)\n\n3\n\n\n\n\nSee Also\n\npython\npandas\n\n\n\n\npython\npython(fn=None, *, name=None, schema=None, signature=None, **kwargs)\nConstruct a non-vectorized scalar user-defined function that accepts Python scalar values as inputs.\n\n\n\n\n\n\npython UDFs are likely to be slow\n\n\n\n\n\npython UDFs are not vectorized: they are executed row by row with one Python function call per row\nThis calling pattern tends to be much slower than pandas or pyarrow-based vectorized UDFs.\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\nschema\n\nThe schema in which to create the UDF.\nNone\n\n\nsignature\n\nAn optional signature to use for the UDF. If present, should be a tuple containing a tuple of argument types and a return type. For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the argument types will be derived from the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; @ibis.udf.scalar.python\n... def add_one(x: int) -&gt; int:\n...     return x + 1\n&gt;&gt;&gt; expr = add_one(2)\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; con.execute(expr)\n\n3\n\n\n\n\nSee Also\n\npandas\npyarrow"
  },
  {
    "objectID": "reference/expression-strings.html",
    "href": "reference/expression-strings.html",
    "title": "String expressions",
    "section": "",
    "text": "All string operations are valid for both scalars and columns."
  },
  {
    "objectID": "reference/expression-strings.html#methods",
    "href": "reference/expression-strings.html#methods",
    "title": "String expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nascii_str\nReturn the numeric ASCII code of the first character of a string.\n\n\nauthority\nParse a URL and extract authority.\n\n\ncapitalize\nCapitalize the input string.\n\n\nconcat\nConcatenate strings.\n\n\ncontains\nReturn whether the expression contains substr.\n\n\nconvert_base\nConvert a string representing an integer from one base to another.\n\n\nendswith\nDetermine if self ends with end.\n\n\nfile\nParse a URL and extract file.\n\n\nfind\nReturn the position of the first occurrence of substring.\n\n\nfind_in_set\nFind the first occurrence of str_list within a list of strings.\n\n\nfragment\nParse a URL and extract fragment identifier.\n\n\nhashbytes\nCompute the binary hash value of the input.\n\n\nhost\nParse a URL and extract host.\n\n\nilike\nMatch patterns against self, case-insensitive.\n\n\njoin\nJoin a list of strings using self as the separator.\n\n\nleft\nReturn the nchars left-most characters.\n\n\nlength\nCompute the length of a string.\n\n\nlevenshtein\nReturn the Levenshtein distance between two strings.\n\n\nlike\nMatch patterns against self, case-sensitive.\n\n\nlower\nConvert string to all lowercase.\n\n\nlpad\nPad arg by truncating on the right or padding on the left.\n\n\nlstrip\nRemove whitespace from the left side of string.\n\n\npath\nParse a URL and extract path.\n\n\nprotocol\nParse a URL and extract protocol.\n\n\nquery\nParse a URL and returns query strring or query string parameter.\n\n\nre_extract\nReturn the specified match at index from a regex pattern.\n\n\nre_replace\nReplace all matches found by regex pattern with replacement.\n\n\nre_search\nReturn whether the values match pattern.\n\n\nrepeat\nRepeat a string n times.\n\n\nreplace\nReplace each exact match of pattern with replacement.\n\n\nreverse\nReverse the characters of a string.\n\n\nright\nReturn up to nchars from the end of each string.\n\n\nrpad\nPad self by truncating or padding on the right.\n\n\nrstrip\nRemove whitespace from the right side of string.\n\n\nsplit\nSplit as string on delimiter.\n\n\nstartswith\nDetermine whether self starts with end.\n\n\nstrip\nRemove whitespace from left and right sides of a string.\n\n\nsubstr\nExtract a substring.\n\n\nto_timestamp\nParse a string and return a timestamp.\n\n\ntranslate\nReplace from_str characters in self characters in to_str.\n\n\nupper\nConvert string to all uppercase.\n\n\nuserinfo\nParse a URL and extract user info.\n\n\n\n\nascii_str\nascii_str()\nReturn the numeric ASCII code of the first character of a string.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nASCII code of the first character of the input\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t.s.ascii_str()\n\n┏━━━━━━━━━━━━━━━━┓\n┃ StringAscii(s) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ int32          │\n├────────────────┤\n│             97 │\n│            100 │\n│            103 │\n└────────────────┘\n\n\n\n\n\n\nauthority\nauthority()\nParse a URL and extract authority.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.authority()  # user:pass@example.com:80\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\ncapitalize\ncapitalize()\nCapitalize the input string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCapitalized string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t.s.capitalize()\n\n┏━━━━━━━━━━━━━━━┓\n┃ Capitalize(s) ┃\n┡━━━━━━━━━━━━━━━┩\n│ string        │\n├───────────────┤\n│ Abc           │\n│ Def           │\n│ Ghi           │\n└───────────────┘\n\n\n\n\n\n\nconcat\nconcat(other, *args)\nConcatenate strings.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nstr | StringValue\nString to concatenate\nrequired\n\n\nargs\nstr | StringValue\nAdditional strings to concatenate\n()\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nAll strings concatenated\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.concat(\"xyz\")\n\n┏━━━━━━━━━━━━━━━━┓\n┃ StringConcat() ┃\n┡━━━━━━━━━━━━━━━━┩\n│ string         │\n├────────────────┤\n│ abcxyz         │\n│ bacxyz         │\n│ bcaxyz         │\n└────────────────┘\n\n\n\n\n\n\ncontains\ncontains(substr)\nReturn whether the expression contains substr.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubstr\nstr | StringValue\nSubstring for which to check\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating the presence of substr in the expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"bab\", \"ddd\", \"eaf\"]})\n&gt;&gt;&gt; t.s.contains(\"a\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringContains(s, 'a') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ False                  │\n│ True                   │\n└────────────────────────┘\n\n\n\n\n\n\nconvert_base\nconvert_base(from_base, to_base)\nConvert a string representing an integer from one base to another.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_base\nint | ir.IntegerValue\nNumeric base of the expression\nrequired\n\n\nto_base\nint | ir.IntegerValue\nNew base\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nConverted expression\n\n\n\n\n\n\nendswith\nendswith(end)\nDetermine if self ends with end.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nend\nstr | StringValue\nSuffix to check for\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating whether self ends with end\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.endswith(\"project\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ EndsWith(s, 'project') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                │\n├────────────────────────┤\n│ True                   │\n│ False                  │\n└────────────────────────┘\n\n\n\n\n\n\nfile\nfile()\nParse a URL and extract file.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\n...     \"https://example.com:80/docs/books/tutorial/index.html?name=networking\"\n... )\n&gt;&gt;&gt; result = url.file()  # docs/books/tutorial/index.html?name=networking\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nfind\nfind(substr, start=None, end=None)\nReturn the position of the first occurrence of substring.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nsubstr\nstr | StringValue\nSubstring to search for\nrequired\n\n\nstart\nint | ir.IntegerValue | None\nZero based index of where to start the search\nNone\n\n\nend\nint | ir.IntegerValue | None\nZero based index of where to stop the search. Currently not implemented.\nNone\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nPosition of substr in arg starting from start\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.find(\"a\")\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ StringFind(s, 'a') ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                  0 │\n│                  1 │\n│                  2 │\n└────────────────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.find(\"z\")\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ StringFind(s, 'z') ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ int64              │\n├────────────────────┤\n│                 -1 │\n│                 -1 │\n│                 -1 │\n└────────────────────┘\n\n\n\n\n\n\nfind_in_set\nfind_in_set(str_list)\nFind the first occurrence of str_list within a list of strings.\nNo string in str_list can have a comma.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstr_list\nSequence[str]\nSequence of strings\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nPosition of str_list in self. Returns -1 if self isn’t found or if self contains ','.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; table = ibis.table(dict(string_col=\"string\"))\n&gt;&gt;&gt; result = table.string_col.find_in_set([\"a\", \"b\"])\n\n\n\n\nfragment\nfragment()\nParse a URL and extract fragment identifier.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://example.com:80/docs/#DOWNLOADING\")\n&gt;&gt;&gt; result = url.fragment()  # DOWNLOADING\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nhashbytes\nhashbytes(how='sha256')\nCompute the binary hash value of the input.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nhow\nLiteral[‘md5’, ‘sha1’, ‘sha256’, ‘sha512’]\nHash algorithm to use\n'sha256'\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBinaryValue\nBinary expression\n\n\n\n\n\n\nhost\nhost()\nParse a URL and extract host.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.host()  # example.com\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nilike\nilike(patterns)\nMatch patterns against self, case-insensitive.\nThis function is modeled after SQL’s ILIKE directive. Use % as a multiple-character wildcard or _ as a single-character wildcard.\nUse re_search or rlike for regular expression-based matching.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npatterns\nstr | StringValue | Iterable[str | StringValue]\nIf pattern is a list, then if any pattern matches the input then the corresponding row in the output is True.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nColumn indicating matches\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.ilike(\"%PROJect\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSQLILike(s, '%PROJect') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                       │\n├───────────────────────────────┤\n│ True                          │\n│ False                         │\n└───────────────────────────────┘\n\n\n\n\n\n\njoin\njoin(strings)\nJoin a list of strings using self as the separator.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstrings\nSequence[str | StringValue] | ir.ArrayValue\nStrings to join with arg\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nJoined string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"arr\": [[\"a\", \"b\", \"c\"], None, [], [\"b\", None]]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ arr                ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;      │\n├────────────────────┤\n│ ['a', 'b', ... +1] │\n│ NULL               │\n│ []                 │\n│ ['b', None]        │\n└────────────────────┘\n\n\n\n\n&gt;&gt;&gt; ibis.literal(\"|\").join(t.arr)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ ArrayStringJoin('|', arr) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                    │\n├───────────────────────────┤\n│ a|b|c                     │\n│ NULL                      │\n│ NULL                      │\n│ b                         │\n└───────────────────────────┘\n\n\n\n\n\nSee Also\nArrayValue.join\n\n\n\nleft\nleft(nchars)\nReturn the nchars left-most characters.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnchars\nint | ir.IntegerValue\nMaximum number of characters to return\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCharacters from the start\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.left(2)\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ Substring(s, 0, 2) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ string             │\n├────────────────────┤\n│ ab                 │\n│ de                 │\n│ hi                 │\n└────────────────────┘\n\n\n\n\n\n\nlength\nlength()\nCompute the length of a string.\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe length of each string in the expression\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"aaa\", \"a\", \"aa\"]})\n&gt;&gt;&gt; t.s.length()\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ StringLength(s) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ int32           │\n├─────────────────┤\n│               3 │\n│               1 │\n│               2 │\n└─────────────────┘\n\n\n\n\n\n\nlevenshtein\nlevenshtein(other)\nReturn the Levenshtein distance between two strings.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nStringValue\nString to compare to\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe edit distance between the two strings\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; s = ibis.literal(\"kitten\")\n&gt;&gt;&gt; s.levenshtein(\"sitting\")\n\n\n\n\n\n3\n\n\n\n\n\n\nlike\nlike(patterns)\nMatch patterns against self, case-sensitive.\nThis function is modeled after the SQL LIKE directive. Use % as a multiple-character wildcard or _ as a single-character wildcard.\nUse re_search or rlike for regular expression-based matching.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npatterns\nstr | StringValue | Iterable[str | StringValue]\nIf pattern is a list, then if any pattern matches the input then the corresponding row in the output is True.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nColumn indicating matches\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.like(\"%project\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSQLLike(s, '%project') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                      │\n├──────────────────────────────┤\n│ True                         │\n│ False                        │\n└──────────────────────────────┘\n\n\n\n\n\n\nlower\nlower()\nConvert string to all lowercase.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLowercase string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"AAA\", \"a\", \"AA\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ AAA    │\n│ a      │\n│ AA     │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.lower()\n\n┏━━━━━━━━━━━━━━┓\n┃ Lowercase(s) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ aaa          │\n│ a            │\n│ aa           │\n└──────────────┘\n\n\n\n\n\n\nlpad\nlpad(length, pad=' ')\nPad arg by truncating on the right or padding on the left.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlength\nint | ir.IntegerValue\nLength of output string\nrequired\n\n\npad\nstr | StringValue\nPad character\n' '\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLeft-padded string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghij\"]})\n&gt;&gt;&gt; t.s.lpad(5, \"-\")\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ LPad(s, 5, '-') ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ --abc           │\n│ --def           │\n│ -ghij           │\n└─────────────────┘\n\n\n\n\n\n\nlstrip\nlstrip()\nRemove whitespace from the left side of string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nLeft-stripped string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.lstrip()\n\n┏━━━━━━━━━━━┓\n┃ LStrip(s) ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ a\\t       │\n│ b\\n       │\n│ c\\t       │\n└───────────┘\n\n\n\n\n\n\npath\npath()\nParse a URL and extract path.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\n...     \"https://example.com:80/docs/books/tutorial/index.html?name=networking\"\n... )\n&gt;&gt;&gt; result = url.path()  # docs/books/tutorial/index.html\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nprotocol\nprotocol()\nParse a URL and extract protocol.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.protocol()  # https\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nquery\nquery(key=None)\nParse a URL and returns query strring or query string parameter.\nIf key is passed, return the value of the query string parameter named. If key is absent, return the query string.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nkey\nstr | StringValue | None\nQuery component to extract\nNone\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\n...     \"https://example.com:80/docs/books/tutorial/index.html?name=networking\"\n... )\n&gt;&gt;&gt; result = url.query()  # name=networking\n&gt;&gt;&gt; query_name = url.query(\"name\")  # networking\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value\n\n\n\n\n\n\nre_extract\nre_extract(pattern, index)\nReturn the specified match at index from a regex pattern.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | StringValue\nRegular expression pattern string\nrequired\n\n\nindex\nint | ir.IntegerValue\nThe index of the match group to return. The behavior of this function follows the behavior of Python’s match objects: when index is zero and there’s a match, return the entire match, otherwise return the content of the index-th match group.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted match or whole string if index is zero\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n\nExtract a specific group\n\n&gt;&gt;&gt; t.s.re_extract(r\"^(a)bc\", 1)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexExtract(s, '^(a)bc', 1) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ a                            │\n│ ~                            │\n│ ~                            │\n└──────────────────────────────┘\n\n\n\nExtract the entire match\n\n&gt;&gt;&gt; t.s.re_extract(r\"^(a)bc\", 0)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexExtract(s, '^(a)bc', 0) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ abc                          │\n│ ~                            │\n│ ~                            │\n└──────────────────────────────┘\n\n\n\n\n\n\nre_replace\nre_replace(pattern, replacement)\nReplace all matches found by regex pattern with replacement.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | StringValue\nRegular expression string\nrequired\n\n\nreplacement\nstr | StringValue\nReplacement string or regular expression\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nModified string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     {\"s\": [\"abc\", \"bac\", \"bca\", \"this has  multi \\t whitespace\"]}\n... )\n&gt;&gt;&gt; s = t.s\n\nReplace all “a”s that are at the beginning of the string with “b”:\n\n&gt;&gt;&gt; s.re_replace(\"^a\", \"b\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexReplace(s, '^a', 'b')    ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                        │\n├───────────────────────────────┤\n│ bbc                           │\n│ bac                           │\n│ bca                           │\n│ this has  multi \\t whitespace │\n└───────────────────────────────┘\n\n\n\nDouble up any “a”s or “b”s, using capture groups and backreferences:\n\n&gt;&gt;&gt; s.re_replace(\"([ab])\", r\"\\0\\0\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexReplace(s, '()', '\\\\0\\\\0')     ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                              │\n├─────────────────────────────────────┤\n│ aabbc                               │\n│ bbaac                               │\n│ bbcaa                               │\n│ this haas  multi \\t whitespaace     │\n└─────────────────────────────────────┘\n\n\n\nNormalize all whitespace to a single space:\n\n&gt;&gt;&gt; s.re_replace(\"\\s+\", \" \")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexReplace(s, '\\\\s+', ' ') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                       │\n├──────────────────────────────┤\n│ abc                          │\n│ bac                          │\n│ bca                          │\n│ this has multi whitespace    │\n└──────────────────────────────┘\n\n\n\n\n\n\nre_search\nre_search(pattern)\nReturn whether the values match pattern.\nReturns True if the regex matches a string and False otherwise.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nstr | StringValue\nRegular expression use for searching\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nIndicator of matches\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.re_search(\".+Hub\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ RegexSearch(s, '.+Hub') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean                 │\n├─────────────────────────┤\n│ False                   │\n│ True                    │\n└─────────────────────────┘\n\n\n\n\n\n\nrepeat\nrepeat(n)\nRepeat a string n times.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nn\nint | ir.IntegerValue\nNumber of repetitions\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRepeated string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"a\", \"bb\", \"c\"]})\n&gt;&gt;&gt; t.s.repeat(5)\n\n┏━━━━━━━━━━━━━━┓\n┃ Repeat(s, 5) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ aaaaa        │\n│ bbbbbbbbbb   │\n│ ccccc        │\n└──────────────┘\n\n\n\n\n\n\nreplace\nreplace(pattern, replacement)\nReplace each exact match of pattern with replacement.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npattern\nStringValue\nString pattern\nrequired\n\n\nreplacement\nStringValue\nString replacement\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nReplaced string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"bac\", \"bca\"]})\n&gt;&gt;&gt; t.s.replace(\"b\", \"z\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringReplace(s, 'b', 'z') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                     │\n├────────────────────────────┤\n│ azc                        │\n│ zac                        │\n│ zca                        │\n└────────────────────────────┘\n\n\n\n\n\n\nreverse\nreverse()\nReverse the characters of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nReversed string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghi\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ abc    │\n│ def    │\n│ ghi    │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.reverse()\n\n┏━━━━━━━━━━━━┓\n┃ Reverse(s) ┃\n┡━━━━━━━━━━━━┩\n│ string     │\n├────────────┤\n│ cba        │\n│ fed        │\n│ ihg        │\n└────────────┘\n\n\n\n\n\n\nright\nright(nchars)\nReturn up to nchars from the end of each string.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nnchars\nint | ir.IntegerValue\nMaximum number of characters to return\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nCharacters from the end\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.right(2)\n\n┏━━━━━━━━━━━━━━━━┓\n┃ StrRight(s, 2) ┃\n┡━━━━━━━━━━━━━━━━┩\n│ string         │\n├────────────────┤\n│ bc             │\n│ fg             │\n│ lk             │\n└────────────────┘\n\n\n\n\n\n\nrpad\nrpad(length, pad=' ')\nPad self by truncating or padding on the right.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nself\n\nString to pad\nrequired\n\n\nlength\nint | ir.IntegerValue\nLength of output string\nrequired\n\n\npad\nstr | StringValue\nPad character\n' '\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRight-padded string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"def\", \"ghij\"]})\n&gt;&gt;&gt; t.s.rpad(5, \"-\")\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ RPad(s, 5, '-') ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ abc--           │\n│ def--           │\n│ ghij-           │\n└─────────────────┘\n\n\n\n\n\n\nrstrip\nrstrip()\nRemove whitespace from the right side of string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nRight-stripped string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.rstrip()\n\n┏━━━━━━━━━━━┓\n┃ RStrip(s) ┃\n┡━━━━━━━━━━━┩\n│ string    │\n├───────────┤\n│ \\ta       │\n│ \\nb       │\n│ \\vc       │\n└───────────┘\n\n\n\n\n\n\nsplit\nsplit(delimiter)\nSplit as string on delimiter.\n\n\n\n\n\n\nThis API only works on backends with array support.\n\n\n\n\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndelimiter\nstr | StringValue\nValue to split by\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nArrayValue\nThe string split by delimiter\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"col\": [\"a,b,c\", \"d,e\", \"f\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ col    ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ a,b,c  │\n│ d,e    │\n│ f      │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.col.split(\",\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringSplit(col, ',') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ array&lt;string&gt;         │\n├───────────────────────┤\n│ ['a', 'b', ... +1]    │\n│ ['d', 'e']            │\n│ ['f']                 │\n└───────────────────────┘\n\n\n\n\n\n\nstartswith\nstartswith(start)\nDetermine whether self starts with end.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nstr | StringValue\nprefix to check for\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nBooleanValue\nBoolean indicating whether self starts with start\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"Ibis project\", \"GitHub\"]})\n&gt;&gt;&gt; t.s.startswith(\"Ibis\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StartsWith(s, 'Ibis') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ boolean               │\n├───────────────────────┤\n│ True                  │\n│ False                 │\n└───────────────────────┘\n\n\n\n\n\n\nstrip\nstrip()\nRemove whitespace from left and right sides of a string.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nStripped string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"\\ta\\t\", \"\\nb\\n\", \"\\vc\\t\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ \\ta\\t  │\n│ \\nb\\n  │\n│ \\vc\\t  │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.strip()\n\n┏━━━━━━━━━━┓\n┃ Strip(s) ┃\n┡━━━━━━━━━━┩\n│ string   │\n├──────────┤\n│ a        │\n│ b        │\n│ c        │\n└──────────┘\n\n\n\n\n\n\nsubstr\nsubstr(start, length=None)\nExtract a substring.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nstart\nint | ir.IntegerValue\nFirst character to start splitting, indices start at 0\nrequired\n\n\nlength\nint | ir.IntegerValue | None\nMaximum length of each substring. If not supplied, searches the entire string\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nFound substring\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"abc\", \"defg\", \"hijlk\"]})\n&gt;&gt;&gt; t.s.substr(2)\n\n┏━━━━━━━━━━━━━━━━━┓\n┃ Substring(s, 2) ┃\n┡━━━━━━━━━━━━━━━━━┩\n│ string          │\n├─────────────────┤\n│ c               │\n│ fg              │\n│ jlk             │\n└─────────────────┘\n\n\n\n\n\n\nto_timestamp\nto_timestamp(format_str)\nParse a string and return a timestamp.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat_str\nstr\nFormat string in strptime format\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nParsed timestamp value\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"ts\": [\"20170206\"]})\n&gt;&gt;&gt; t.ts.to_timestamp(\"%Y%m%d\")\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ StringToTimestamp(ts, '%Y%m%d') ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp('UTC')                │\n├─────────────────────────────────┤\n│ 2017-02-06 00:00:00+00:00       │\n└─────────────────────────────────┘\n\n\n\n\n\n\ntranslate\ntranslate(from_str, to_str)\nReplace from_str characters in self characters in to_str.\nTo avoid unexpected behavior, from_str should be shorter than to_str.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfrom_str\nStringValue\nCharacters in arg to replace\nrequired\n\n\nto_str\nStringValue\nCharacters to use for replacement\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nTranslated string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; table = ibis.table(dict(string_col=\"string\"))\n&gt;&gt;&gt; result = table.string_col.translate(\"a\", \"b\")\n\n\n\n\nupper\nupper()\nConvert string to all uppercase.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nUppercase string\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable({\"s\": [\"aaa\", \"A\", \"aa\"]})\n&gt;&gt;&gt; t\n\n┏━━━━━━━━┓\n┃ s      ┃\n┡━━━━━━━━┩\n│ string │\n├────────┤\n│ aaa    │\n│ A      │\n│ aa     │\n└────────┘\n\n\n\n\n&gt;&gt;&gt; t.s.upper()\n\n┏━━━━━━━━━━━━━━┓\n┃ Uppercase(s) ┃\n┡━━━━━━━━━━━━━━┩\n│ string       │\n├──────────────┤\n│ AAA          │\n│ A            │\n│ AA           │\n└──────────────┘\n\n\n\n\n\n\nuserinfo\nuserinfo()\nParse a URL and extract user info.\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; url = ibis.literal(\"https://user:pass@example.com:80/docs/books\")\n&gt;&gt;&gt; result = url.userinfo()  # user:pass\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nExtracted string value"
  },
  {
    "objectID": "reference/BackendTest.html",
    "href": "reference/BackendTest.html",
    "title": "BackendTest",
    "section": "",
    "text": "BackendTest(self, *, data_dir, tmpdir, worker_id, **kw)\nThe base class for managing configuration and data loading for a backend that does not require Docker for testing (this includes both in-process backends and cloud backends like Snowflake and BigQuery).\n\n\n\n\n\nName\nDescription\n\n\n\n\ncheck_dtype\nCheck that dtypes match when comparing Pandas Series\n\n\ncheck_names\nCheck that column name matches when comparing Pandas Series\n\n\ndeps\nA list of dependencies that must be present to run tests.\n\n\ndriver_supports_multiple_statements\nWhether the driver supports executing multiple statements in a single call.\n\n\nforce_sort\nSort results before comparing against reference computation.\n\n\nnative_bool\nWhether backend has native boolean types\n\n\nreduction_tolerance\nUsed for a single test in test_aggregation.py. You should not need to touch this.\n\n\nrounding_method\nName of round method to use for rounding test comparisons.\n\n\nstateful\nWhether special handling is needed for running a multi-process pytest run.\n\n\nsupports_arrays\nWhether backend supports Arrays / Lists\n\n\nsupports_json\nWhether backend supports operating on JSON\n\n\nsupports_map\nWhether backend supports mappings (currently DuckDB, Snowflake, and Trino)\n\n\nsupports_structs\nWhether backend supports Structs\n\n\nsupports_tpcds\nChild class defines a load_tpcds method that loads the required TPC-DS tables into a connection.\n\n\nsupports_tpch\nChild class defines a load_tpch method that loads the required TPC-H tables into a connection.\n\n\ntpc_absolute_tolerance\nAbsolute tolerance for floating point comparisons with pytest.approx in TPC correctness tests.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\nassert_frame_equal\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\n\n\nassert_series_equal\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\n\n\nconnect\nReturn a connection with data loaded from data_dir.\n\n\nload_data\nLoad testdata from data_dir.\n\n\nload_tpcds\nLoad TPC-DS data.\n\n\nload_tpch\nLoad TPC-H data.\n\n\npostload\nCode to execute after loading data.\n\n\npreload\nCode to execute before loading data.\n\n\nskip_if_missing_deps\nAdd an importorskip for any missing dependencies.\n\n\n\n\n\nassert_frame_equal(left, right, *args, **kwargs)\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\nforce_sort, and check_dtype are set as class-level variables.\n\n\n\nassert_series_equal(left, right, *args, **kwargs)\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\nforce_sort, check_dtype, and check_names are set as class-level variables.\n\n\n\nconnect(tmpdir, worker_id, **kw)\nReturn a connection with data loaded from data_dir.\n\n\n\nload_data(data_dir, tmpdir, worker_id, **kw)\nLoad testdata from data_dir.\n\n\n\nload_tpcds()\nLoad TPC-DS data.\n\n\n\nload_tpch()\nLoad TPC-H data.\n\n\n\npostload(**_)\nCode to execute after loading data.\n\n\n\npreload()\nCode to execute before loading data.\n\n\n\nskip_if_missing_deps()\nAdd an importorskip for any missing dependencies."
  },
  {
    "objectID": "reference/BackendTest.html#attributes",
    "href": "reference/BackendTest.html#attributes",
    "title": "BackendTest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncheck_dtype\nCheck that dtypes match when comparing Pandas Series\n\n\ncheck_names\nCheck that column name matches when comparing Pandas Series\n\n\ndeps\nA list of dependencies that must be present to run tests.\n\n\ndriver_supports_multiple_statements\nWhether the driver supports executing multiple statements in a single call.\n\n\nforce_sort\nSort results before comparing against reference computation.\n\n\nnative_bool\nWhether backend has native boolean types\n\n\nreduction_tolerance\nUsed for a single test in test_aggregation.py. You should not need to touch this.\n\n\nrounding_method\nName of round method to use for rounding test comparisons.\n\n\nstateful\nWhether special handling is needed for running a multi-process pytest run.\n\n\nsupports_arrays\nWhether backend supports Arrays / Lists\n\n\nsupports_json\nWhether backend supports operating on JSON\n\n\nsupports_map\nWhether backend supports mappings (currently DuckDB, Snowflake, and Trino)\n\n\nsupports_structs\nWhether backend supports Structs\n\n\nsupports_tpcds\nChild class defines a load_tpcds method that loads the required TPC-DS tables into a connection.\n\n\nsupports_tpch\nChild class defines a load_tpch method that loads the required TPC-H tables into a connection.\n\n\ntpc_absolute_tolerance\nAbsolute tolerance for floating point comparisons with pytest.approx in TPC correctness tests."
  },
  {
    "objectID": "reference/BackendTest.html#methods",
    "href": "reference/BackendTest.html#methods",
    "title": "BackendTest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nassert_frame_equal\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\n\n\nassert_series_equal\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\n\n\nconnect\nReturn a connection with data loaded from data_dir.\n\n\nload_data\nLoad testdata from data_dir.\n\n\nload_tpcds\nLoad TPC-DS data.\n\n\nload_tpch\nLoad TPC-H data.\n\n\npostload\nCode to execute after loading data.\n\n\npreload\nCode to execute before loading data.\n\n\nskip_if_missing_deps\nAdd an importorskip for any missing dependencies.\n\n\n\n\n\nassert_frame_equal(left, right, *args, **kwargs)\nCompare two Pandas DataFrames optionally ignoring order, and dtype.\nforce_sort, and check_dtype are set as class-level variables.\n\n\n\nassert_series_equal(left, right, *args, **kwargs)\nCompare two Pandas Series, optionally ignoring order, dtype, and column name.\nforce_sort, check_dtype, and check_names are set as class-level variables.\n\n\n\nconnect(tmpdir, worker_id, **kw)\nReturn a connection with data loaded from data_dir.\n\n\n\nload_data(data_dir, tmpdir, worker_id, **kw)\nLoad testdata from data_dir.\n\n\n\nload_tpcds()\nLoad TPC-DS data.\n\n\n\nload_tpch()\nLoad TPC-H data.\n\n\n\npostload(**_)\nCode to execute after loading data.\n\n\n\npreload()\nCode to execute before loading data.\n\n\n\nskip_if_missing_deps()\nAdd an importorskip for any missing dependencies."
  },
  {
    "objectID": "reference/operations.html",
    "href": "reference/operations.html",
    "title": "Operations",
    "section": "",
    "text": "Low level operation classes. Subject to change in non-major releases."
  },
  {
    "objectID": "reference/operations.html#classes",
    "href": "reference/operations.html#classes",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nAnalytic\nBase class for analytic window function operations.\n\n\nCumeDist\nCompute the cumulative distribution function of a column over a window.\n\n\nLag\nShift a column forward.\n\n\nLead\nShift a column backward.\n\n\nNTile\nCompute the percentile of a column over a window.\n\n\nNthValue\nRetrieve the Nth element of a column over a window.\n\n\nPercentRank\nCompute the percentile rank over a window.\n\n\nRankBase\nBase class for ranking operations.\n\n\nRowNumber\nCompute the row number over a window, starting from 0.\n\n\nShiftBase\nBase class for shift operations.\n\n\n\n\nAnalytic\nAnalytic(self, **kwargs)\nBase class for analytic window function operations.\n\n\nCumeDist\nCumeDist(self, **kwargs)\nCompute the cumulative distribution function of a column over a window.\n\n\nLag\nLag(self, **kwargs)\nShift a column forward.\n\n\nLead\nLead(self, **kwargs)\nShift a column backward.\n\n\nNTile\nNTile(self, **kwargs)\nCompute the percentile of a column over a window.\n\n\nNthValue\nNthValue(self, **kwargs)\nRetrieve the Nth element of a column over a window.\n\n\nPercentRank\nPercentRank(self, **kwargs)\nCompute the percentile rank over a window.\n\n\nRankBase\nRankBase(self, **kwargs)\nBase class for ranking operations.\n\n\nRowNumber\nRowNumber(self, **kwargs)\nCompute the row number over a window, starting from 0.\n\n\nShiftBase\nShiftBase(self, **kwargs)\nBase class for shift operations."
  },
  {
    "objectID": "reference/operations.html#classes-1",
    "href": "reference/operations.html#classes-1",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nArray\nConstruct an array.\n\n\nArrayAll\nCompute whether all array elements are true.\n\n\nArrayAny\nCompute whether any array element is true.\n\n\nArrayConcat\nConcatenate two or more arrays into a single array.\n\n\nArrayContains\nReturn whether an array contains a specific value.\n\n\nArrayDistinct\nReturn the unique elements of an array.\n\n\nArrayFilter\nFilter array elements with a function.\n\n\nArrayFlatten\nFlatten a nested array one level.\n\n\nArrayIndex\nReturn the element of an array at some index.\n\n\nArrayIntersect\nReturn the intersection of two arrays.\n\n\nArrayLength\nCompute the length of an array.\n\n\nArrayMap\nApply a function to every element of an array.\n\n\nArrayMax\nCompute the maximum value of an array.\n\n\nArrayMean\nCompute the average of an array.\n\n\nArrayMin\nCompute the minimum value of an array.\n\n\nArrayPosition\nReturn the position of a specific value in an array.\n\n\nArrayRemove\nRemove an element from an array.\n\n\nArrayRepeat\nRepeat the elements of an array.\n\n\nArraySlice\nSlice an array element.\n\n\nArraySort\nSort the values of an array.\n\n\nArraySum\nCompute the sum of an array.\n\n\nArrayUnion\nReturn the union of two arrays.\n\n\nArrayZip\nZip two or more arrays into an array of structs.\n\n\nIntegerRange\nProduce an array of integers from start to stop, moving by step.\n\n\nRange\nBase class for range-generating operations.\n\n\nTimestampRange\nProduce an array of timestamps from start to stop, moving by step.\n\n\nUnnest\nUnnest an array value into a column.\n\n\n\n\nArray\nArray(self, **kwargs)\nConstruct an array.\n\n\nArrayAll\nArrayAll(self, **kwargs)\nCompute whether all array elements are true.\n\n\nArrayAny\nArrayAny(self, **kwargs)\nCompute whether any array element is true.\n\n\nArrayConcat\nArrayConcat(self, **kwargs)\nConcatenate two or more arrays into a single array.\n\n\nArrayContains\nArrayContains(self, **kwargs)\nReturn whether an array contains a specific value.\n\n\nArrayDistinct\nArrayDistinct(self, **kwargs)\nReturn the unique elements of an array.\n\n\nArrayFilter\nArrayFilter(self, **kwargs)\nFilter array elements with a function.\n\n\nArrayFlatten\nArrayFlatten(self, **kwargs)\nFlatten a nested array one level.\nThe input expression must have at least one level of nesting for flattening to make sense.\n\n\nArrayIndex\nArrayIndex(self, **kwargs)\nReturn the element of an array at some index.\n\n\nArrayIntersect\nArrayIntersect(self, **kwargs)\nReturn the intersection of two arrays.\n\n\nArrayLength\nArrayLength(self, **kwargs)\nCompute the length of an array.\n\n\nArrayMap\nArrayMap(self, **kwargs)\nApply a function to every element of an array.\n\n\nArrayMax\nArrayMax(self, **kwargs)\nCompute the maximum value of an array.\n\n\nArrayMean\nArrayMean(self, **kwargs)\nCompute the average of an array.\n\n\nArrayMin\nArrayMin(self, **kwargs)\nCompute the minimum value of an array.\n\n\nArrayPosition\nArrayPosition(self, **kwargs)\nReturn the position of a specific value in an array.\n\n\nArrayRemove\nArrayRemove(self, **kwargs)\nRemove an element from an array.\n\n\nArrayRepeat\nArrayRepeat(self, **kwargs)\nRepeat the elements of an array.\n\n\nArraySlice\nArraySlice(self, **kwargs)\nSlice an array element.\n\n\nArraySort\nArraySort(self, **kwargs)\nSort the values of an array.\n\n\nArraySum\nArraySum(self, **kwargs)\nCompute the sum of an array.\n\n\nArrayUnion\nArrayUnion(self, **kwargs)\nReturn the union of two arrays.\n\n\nArrayZip\nArrayZip(self, **kwargs)\nZip two or more arrays into an array of structs.\n\n\nIntegerRange\nIntegerRange(self, **kwargs)\nProduce an array of integers from start to stop, moving by step.\n\n\nRange\nRange(self, **kwargs)\nBase class for range-generating operations.\n\n\nTimestampRange\nTimestampRange(self, **kwargs)\nProduce an array of timestamps from start to stop, moving by step.\n\n\nUnnest\nUnnest(self, **kwargs)\nUnnest an array value into a column."
  },
  {
    "objectID": "reference/operations.html#classes-2",
    "href": "reference/operations.html#classes-2",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nCast\nExplicitly cast a value to a specific data type.\n\n\nCoalesce\nReturn the first non-null expression from a tuple of expressions.\n\n\nConstant\nA function that produces a constant.\n\n\nDateNow\nReturn the current date.\n\n\nE\nThe mathematical constant e.\n\n\nGreatest\nReturn the largest value from a tuple of expressions.\n\n\nHash\nReturn the hash of a value.\n\n\nHexDigest\nReturn the hexadecimal digest of a value.\n\n\nIsNull\nReturn true if values are null.\n\n\nLeast\nReturn the smallest value from a tuple of expressions.\n\n\nLiteral\nA constant value.\n\n\nNotNull\nReturns true if values are not null.\n\n\nNullIf\nReturn NULL if an expression equals some specific value.\n\n\nPi\nThe mathematical constant pi.\n\n\nRandomScalar\nReturn a random scalar between 0 and 1.\n\n\nRandomUUID\nReturn a random UUID.\n\n\nRowID\nThe row number of the returned result.\n\n\nSearchedCase\nSearched case statement.\n\n\nSimpleCase\nSimple case statement.\n\n\nTimestampNow\nReturn the current timestamp.\n\n\nTryCast\nTry to cast a value to a specific data type.\n\n\nTypeOf\nReturn the database data type of the input expression.\n\n\n\n\nCast\nCast(self, **kwargs)\nExplicitly cast a value to a specific data type.\n\n\nCoalesce\nCoalesce(self, **kwargs)\nReturn the first non-null expression from a tuple of expressions.\n\n\nConstant\nConstant()\nA function that produces a constant.\n\n\nDateNow\nDateNow()\nReturn the current date.\n\n\nE\nE()\nThe mathematical constant e.\n\n\nGreatest\nGreatest(self, **kwargs)\nReturn the largest value from a tuple of expressions.\n\n\nHash\nHash(self, **kwargs)\nReturn the hash of a value.\n\n\nHexDigest\nHexDigest(self, **kwargs)\nReturn the hexadecimal digest of a value.\n\n\nIsNull\nIsNull(self, **kwargs)\nReturn true if values are null.\n\n\nLeast\nLeast(self, **kwargs)\nReturn the smallest value from a tuple of expressions.\n\n\nLiteral\nLiteral(self, value, dtype)\nA constant value.\n\n\nNotNull\nNotNull(self, **kwargs)\nReturns true if values are not null.\n\n\nNullIf\nNullIf(self, **kwargs)\nReturn NULL if an expression equals some specific value.\n\n\nPi\nPi()\nThe mathematical constant pi.\n\n\nRandomScalar\nRandomScalar(self, **kwargs)\nReturn a random scalar between 0 and 1.\n\n\nRandomUUID\nRandomUUID(self, **kwargs)\nReturn a random UUID.\n\n\nRowID\nRowID(self, **kwargs)\nThe row number of the returned result.\n\n\nSearchedCase\nSearchedCase(self, cases, results, default)\nSearched case statement.\n\n\nSimpleCase\nSimpleCase(self, base, cases, results, default)\nSimple case statement.\n\n\nTimestampNow\nTimestampNow()\nReturn the current timestamp.\n\n\nTryCast\nTryCast(self, **kwargs)\nTry to cast a value to a specific data type.\n\n\nTypeOf\nTypeOf(self, **kwargs)\nReturn the database data type of the input expression."
  },
  {
    "objectID": "reference/operations.html#classes-3",
    "href": "reference/operations.html#classes-3",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nGeoArea\nArea of the geo spatial data.\n\n\nGeoAsBinary\nReturn the Well-Known Binary (WKB) representation of the input, without SRID meta data.\n\n\nGeoAsEWKB\nReturn the Well-Known Binary representation of the input, with SRID meta data.\n\n\nGeoAsEWKT\nReturn the Well-Known Text representation of the input, with SRID meta data.\n\n\nGeoAsText\nReturn the Well-Known Text (WKT) representation of the input, without SRID metadata.\n\n\nGeoAzimuth\nReturn the angle in radians from the horizontal of the vector defined by the two inputs.\n\n\nGeoBuffer\nReturn all points whose distance from this geometry is less than or equal to radius.\n\n\nGeoCentroid\nReturns the geometric center of a geometry.\n\n\nGeoContains\nCheck if the first geo spatial data contains the second one.\n\n\nGeoContainsProperly\nCheck if the left value contains the right one, with no shared no boundary points.\n\n\nGeoConvert\nReturns a transformed version of the given geometry from source crs/srid to a target crs/srid.\n\n\nGeoCoveredBy\nCheck if no point in the left operand is outside that of the right.\n\n\nGeoCovers\nCheck if no point in the right operand is outside that of the left.\n\n\nGeoCrosses\nCheck if the inputs have some but not all interior points in common.\n\n\nGeoDFullyWithin\nCheck if the geometries are fully within distance of one another.\n\n\nGeoDWithin\nCheck if the geometries are within distance of one another.\n\n\nGeoDifference\nReturn a geometry that is the delta between the left and right inputs.\n\n\nGeoDisjoint\nCheck if the Geometries do not spatially intersect.\n\n\nGeoDistance\nReturns minimum distance between two geospatial operands.\n\n\nGeoEndPoint\nReturn the last point of a LINESTRING geometry as a POINT.\n\n\nGeoEnvelope\nThe bounding box of the supplied geometry.\n\n\nGeoEquals\nReturns True if the given geometries represent the same geometry.\n\n\nGeoFlipCoordinates\nReturns a new geometry with the coordinates of the input geometry “flipped” so that x = y and y = x.\n\n\nGeoGeometryN\nReturns the Nth Geometry of a Multi geometry.\n\n\nGeoGeometryType\nReturns the type of the geometry.\n\n\nGeoIntersection\nReturn a geometry that represents the point-set intersection of the inputs.\n\n\nGeoIntersects\nReturns True if the Geometries/Geography “spatially intersect in 2D”.\n\n\nGeoIsValid\nReturns true if the geometry is well-formed.\n\n\nGeoLength\nLength of geo spatial data.\n\n\nGeoLineLocatePoint\nLocate the distance a point falls along the length of a line.\n\n\nGeoLineMerge\nMerge a MultiLineString into a LineString.\n\n\nGeoLineSubstring\nClip a substring from a LineString.\n\n\nGeoMaxDistance\nReturns the 2-dimensional max distance between two geometries in projected units.\n\n\nGeoNPoints\nReturn the number of points in a geometry.\n\n\nGeoNRings\nReturn the number of rings for polygons or multipolygons.\n\n\nGeoOrderingEquals\nCheck if two geometries are equal and have the same point ordering.\n\n\nGeoOverlaps\nCheck if the inputs are of the same dimension but are not completely contained by each other.\n\n\nGeoPerimeter\nPerimeter of the geo spatial data.\n\n\nGeoPoint\nReturn a point constructed from the input coordinate values.\n\n\nGeoPointN\nReturn the Nth point in a single linestring in the geometry.\n\n\nGeoSRID\nReturns the spatial reference identifier for the ST_Geometry.\n\n\nGeoSetSRID\nSet the spatial reference identifier for the ST_Geometry.\n\n\nGeoSimplify\nReturns a simplified version of the given geometry.\n\n\nGeoSpatialBinOp\nGeo Spatial base binary.\n\n\nGeoSpatialUnOp\nGeo Spatial base unary.\n\n\nGeoStartPoint\nReturn the first point of a LINESTRING geometry as a POINT.\n\n\nGeoTouches\nCheck if the inputs have at least one point in common but their interiors do not intersect.\n\n\nGeoTransform\nReturns a transformed version of the given geometry into a new SRID.\n\n\nGeoUnaryUnion\nReturns the pointwise union of the geometries in the column.\n\n\nGeoUnion\nReturns the pointwise union of the two geometries.\n\n\nGeoWithin\nReturns True if the geometry A is completely inside geometry B.\n\n\nGeoX\nReturn the X coordinate of the point, or NULL if not available.\n\n\nGeoXMax\nReturns X maxima of a bounding box 2d or 3d or a geometry.\n\n\nGeoXMin\nReturns Y minima of a bounding box 2d or 3d or a geometry.\n\n\nGeoY\nReturn the Y coordinate of the point, or NULL if not available.\n\n\nGeoYMax\nReturns Y maxima of a bounding box 2d or 3d or a geometry.\n\n\nGeoYMin\nReturns Y minima of a bounding box 2d or 3d or a geometry.\n\n\n\n\nGeoArea\nGeoArea(self, **kwargs)\nArea of the geo spatial data.\n\n\nGeoAsBinary\nGeoAsBinary(self, **kwargs)\nReturn the Well-Known Binary (WKB) representation of the input, without SRID meta data.\n\n\nGeoAsEWKB\nGeoAsEWKB(self, **kwargs)\nReturn the Well-Known Binary representation of the input, with SRID meta data.\n\n\nGeoAsEWKT\nGeoAsEWKT(self, **kwargs)\nReturn the Well-Known Text representation of the input, with SRID meta data.\n\n\nGeoAsText\nGeoAsText(self, **kwargs)\nReturn the Well-Known Text (WKT) representation of the input, without SRID metadata.\n\n\nGeoAzimuth\nGeoAzimuth(self, **kwargs)\nReturn the angle in radians from the horizontal of the vector defined by the two inputs.\nAngle is computed clockwise from down-to-up: on the clock: 12=0; 3=PI/2; 6=PI; 9=3PI/2.\n\n\nGeoBuffer\nGeoBuffer(self, **kwargs)\nReturn all points whose distance from this geometry is less than or equal to radius.\nCalculations are in the Spatial Reference System of this geometry.\n\n\nGeoCentroid\nGeoCentroid(self, **kwargs)\nReturns the geometric center of a geometry.\n\n\nGeoContains\nGeoContains(self, **kwargs)\nCheck if the first geo spatial data contains the second one.\n\n\nGeoContainsProperly\nGeoContainsProperly(self, **kwargs)\nCheck if the left value contains the right one, with no shared no boundary points.\n\n\nGeoConvert\nGeoConvert(self, **kwargs)\nReturns a transformed version of the given geometry from source crs/srid to a target crs/srid.\n\n\nGeoCoveredBy\nGeoCoveredBy(self, **kwargs)\nCheck if no point in the left operand is outside that of the right.\n\n\nGeoCovers\nGeoCovers(self, **kwargs)\nCheck if no point in the right operand is outside that of the left.\n\n\nGeoCrosses\nGeoCrosses(self, **kwargs)\nCheck if the inputs have some but not all interior points in common.\n\n\nGeoDFullyWithin\nGeoDFullyWithin(self, **kwargs)\nCheck if the geometries are fully within distance of one another.\n\n\nGeoDWithin\nGeoDWithin(self, **kwargs)\nCheck if the geometries are within distance of one another.\n\n\nGeoDifference\nGeoDifference(self, **kwargs)\nReturn a geometry that is the delta between the left and right inputs.\n\n\nGeoDisjoint\nGeoDisjoint(self, **kwargs)\nCheck if the Geometries do not spatially intersect.\n\n\nGeoDistance\nGeoDistance(self, **kwargs)\nReturns minimum distance between two geospatial operands.\n\n\nGeoEndPoint\nGeoEndPoint(self, **kwargs)\nReturn the last point of a LINESTRING geometry as a POINT.\nReturns NULL if the input is not a LINESTRING.\n\n\nGeoEnvelope\nGeoEnvelope(self, **kwargs)\nThe bounding box of the supplied geometry.\n\n\nGeoEquals\nGeoEquals(self, **kwargs)\nReturns True if the given geometries represent the same geometry.\n\n\nGeoFlipCoordinates\nGeoFlipCoordinates(self, **kwargs)\nReturns a new geometry with the coordinates of the input geometry “flipped” so that x = y and y = x.\n\n\nGeoGeometryN\nGeoGeometryN(self, **kwargs)\nReturns the Nth Geometry of a Multi geometry.\n\n\nGeoGeometryType\nGeoGeometryType(self, **kwargs)\nReturns the type of the geometry.\n\n\nGeoIntersection\nGeoIntersection(self, **kwargs)\nReturn a geometry that represents the point-set intersection of the inputs.\n\n\nGeoIntersects\nGeoIntersects(self, **kwargs)\nReturns True if the Geometries/Geography “spatially intersect in 2D”.\n\n(share any portion of space) and False if they don`t (they are Disjoint).\n\n\n\nGeoIsValid\nGeoIsValid(self, **kwargs)\nReturns true if the geometry is well-formed.\n\n\nGeoLength\nGeoLength(self, **kwargs)\nLength of geo spatial data.\n\n\nGeoLineLocatePoint\nGeoLineLocatePoint(self, **kwargs)\nLocate the distance a point falls along the length of a line.\nReturns a float between zero and one representing the location of the closest point on the linestring to the given point, as a fraction of the total 2d line length.\n\n\nGeoLineMerge\nGeoLineMerge(self, **kwargs)\nMerge a MultiLineString into a LineString.\nReturns a (set of) LineString(s) formed by sewing together the constituent line work of a multilinestring. If a geometry other than a linestring or multilinestring is given, this will return an empty geometry collection.\n\n\nGeoLineSubstring\nGeoLineSubstring(self, **kwargs)\nClip a substring from a LineString.\nReturns a linestring that is a substring of the input one, starting and ending at the given fractions of the total 2d length. The second and third arguments are floating point values between zero and one. This only works with linestrings.\n\n\nGeoMaxDistance\nGeoMaxDistance(self, **kwargs)\nReturns the 2-dimensional max distance between two geometries in projected units.\nIf g1 and g2 is the same geometry the function will return the distance between the two vertices most far from each other in that geometry\n\n\nGeoNPoints\nGeoNPoints(self, **kwargs)\nReturn the number of points in a geometry.\n\n\nGeoNRings\nGeoNRings(self, **kwargs)\nReturn the number of rings for polygons or multipolygons.\nOuter rings are counted.\n\n\nGeoOrderingEquals\nGeoOrderingEquals(self, **kwargs)\nCheck if two geometries are equal and have the same point ordering.\nReturns true if the two geometries are equal and the coordinates are in the same order.\n\n\nGeoOverlaps\nGeoOverlaps(self, **kwargs)\nCheck if the inputs are of the same dimension but are not completely contained by each other.\n\n\nGeoPerimeter\nGeoPerimeter(self, **kwargs)\nPerimeter of the geo spatial data.\n\n\nGeoPoint\nGeoPoint(self, **kwargs)\nReturn a point constructed from the input coordinate values.\nConstant coordinates result in construction of a POINT literal.\n\n\nGeoPointN\nGeoPointN(self, **kwargs)\nReturn the Nth point in a single linestring in the geometry.\nNegative values are counted backwards from the end of the LineString, so that -1 is the last point. Returns NULL if there is no linestring in the geometry\n\n\nGeoSRID\nGeoSRID(self, **kwargs)\nReturns the spatial reference identifier for the ST_Geometry.\n\n\nGeoSetSRID\nGeoSetSRID(self, **kwargs)\nSet the spatial reference identifier for the ST_Geometry.\n\n\nGeoSimplify\nGeoSimplify(self, **kwargs)\nReturns a simplified version of the given geometry.\n\n\nGeoSpatialBinOp\nGeoSpatialBinOp(self, **kwargs)\nGeo Spatial base binary.\n\n\nGeoSpatialUnOp\nGeoSpatialUnOp(self, **kwargs)\nGeo Spatial base unary.\n\n\nGeoStartPoint\nGeoStartPoint(self, **kwargs)\nReturn the first point of a LINESTRING geometry as a POINT.\nReturns NULL if the input is not a LINESTRING.\n\n\nGeoTouches\nGeoTouches(self, **kwargs)\nCheck if the inputs have at least one point in common but their interiors do not intersect.\n\n\nGeoTransform\nGeoTransform(self, **kwargs)\nReturns a transformed version of the given geometry into a new SRID.\n\n\nGeoUnaryUnion\nGeoUnaryUnion(self, **kwargs)\nReturns the pointwise union of the geometries in the column.\n\n\nGeoUnion\nGeoUnion(self, **kwargs)\nReturns the pointwise union of the two geometries.\n\n\nGeoWithin\nGeoWithin(self, **kwargs)\nReturns True if the geometry A is completely inside geometry B.\n\n\nGeoX\nGeoX(self, **kwargs)\nReturn the X coordinate of the point, or NULL if not available.\nInput must be a point\n\n\nGeoXMax\nGeoXMax(self, **kwargs)\nReturns X maxima of a bounding box 2d or 3d or a geometry.\n\n\nGeoXMin\nGeoXMin(self, **kwargs)\nReturns Y minima of a bounding box 2d or 3d or a geometry.\n\n\nGeoY\nGeoY(self, **kwargs)\nReturn the Y coordinate of the point, or NULL if not available.\nInput must be a point\n\n\nGeoYMax\nGeoYMax(self, **kwargs)\nReturns Y maxima of a bounding box 2d or 3d or a geometry.\n\n\nGeoYMin\nGeoYMin(self, **kwargs)\nReturns Y minima of a bounding box 2d or 3d or a geometry."
  },
  {
    "objectID": "reference/operations.html#classes-4",
    "href": "reference/operations.html#classes-4",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nBucket\nCompute the bucket number of a numeric column.\n\n\n\n\nBucket\nBucket(self, buckets, include_under, include_over, **kwargs)\nCompute the bucket number of a numeric column."
  },
  {
    "objectID": "reference/operations.html#classes-5",
    "href": "reference/operations.html#classes-5",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nJSONGetItem\nGet a value from a JSON object or array.\n\n\nToJSONArray\nConvert a value to an array of JSON objects.\n\n\nToJSONMap\nConvert a value to a map of string to JSON.\n\n\nUnwrapJSONBoolean\nUnwrap a JSON bool into an engine-native bool.\n\n\nUnwrapJSONFloat64\nUnwrap a JSON number into an engine-native float64.\n\n\nUnwrapJSONInt64\nUnwrap a JSON number into an engine-native int64.\n\n\nUnwrapJSONString\nUnwrap a JSON string into an engine-native string.\n\n\n\n\nJSONGetItem\nJSONGetItem()\nGet a value from a JSON object or array.\n\n\nToJSONArray\nToJSONArray()\nConvert a value to an array of JSON objects.\n\n\nToJSONMap\nToJSONMap()\nConvert a value to a map of string to JSON.\n\n\nUnwrapJSONBoolean\nUnwrapJSONBoolean()\nUnwrap a JSON bool into an engine-native bool.\n\n\nUnwrapJSONFloat64\nUnwrapJSONFloat64()\nUnwrap a JSON number into an engine-native float64.\n\n\nUnwrapJSONInt64\nUnwrapJSONInt64()\nUnwrap a JSON number into an engine-native int64.\n\n\nUnwrapJSONString\nUnwrapJSONString()\nUnwrap a JSON string into an engine-native string."
  },
  {
    "objectID": "reference/operations.html#classes-6",
    "href": "reference/operations.html#classes-6",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nAnd\nLogical AND.\n\n\nBetween\nCheck if a value is within a range.\n\n\nComparison\nBase class for comparison operations.\n\n\nEquals\nEquality comparison.\n\n\nGreater\nGreater than comparison.\n\n\nGreaterEqual\nGreater than or equal to comparison.\n\n\nIdenticalTo\nIdentity comparison. Considers two NULL values equal.\n\n\nIfElse\nTernary case expression.\n\n\nInValues\nCheck if a value is in a set of values.\n\n\nLess\nLess than comparison.\n\n\nLessEqual\nLess than or equal to comparison.\n\n\nLogicalBinary\nBase class for logical binary operations.\n\n\nNot\nLogical negation.\n\n\nNotEquals\nInequality comparison.\n\n\nOr\nLogical OR.\n\n\nXor\nLogical XOR.\n\n\n\n\nAnd\nAnd(self, **kwargs)\nLogical AND.\n\n\nBetween\nBetween(self, arg, lower_bound, upper_bound)\nCheck if a value is within a range.\n\n\nComparison\nComparison(self, left, right)\nBase class for comparison operations.\n\n\nEquals\nEquals(self, left, right)\nEquality comparison.\n\n\nGreater\nGreater(self, left, right)\nGreater than comparison.\n\n\nGreaterEqual\nGreaterEqual(self, left, right)\nGreater than or equal to comparison.\n\n\nIdenticalTo\nIdenticalTo(self, left, right)\nIdentity comparison. Considers two NULL values equal.\n\n\nIfElse\nIfElse(self, **kwargs)\nTernary case expression.\nEquivalent to\nbool_expr.case().when(True, true_expr).else_(false_or_null_expr)\nMany backends implement this as a built-in function.\n\n\nInValues\nInValues(self, **kwargs)\nCheck if a value is in a set of values.\n\n\nLess\nLess(self, left, right)\nLess than comparison.\n\n\nLessEqual\nLessEqual(self, left, right)\nLess than or equal to comparison.\n\n\nLogicalBinary\nLogicalBinary(self, **kwargs)\nBase class for logical binary operations.\n\n\nNot\nNot(self, **kwargs)\nLogical negation.\n\n\nNotEquals\nNotEquals(self, left, right)\nInequality comparison.\n\n\nOr\nOr(self, **kwargs)\nLogical OR.\n\n\nXor\nXor(self, **kwargs)\nLogical XOR."
  },
  {
    "objectID": "reference/operations.html#classes-7",
    "href": "reference/operations.html#classes-7",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nMap\nConstruct a map.\n\n\nMapContains\nCheck if a map contains a key.\n\n\nMapGet\nGet a value from a map by key.\n\n\nMapKeys\nGet the keys of a map as an array.\n\n\nMapLength\nCompute the number of unique keys in a map.\n\n\nMapMerge\nCombine two maps into one.\n\n\nMapValues\nGet the values of a map as an array.\n\n\n\n\nMap\nMap(self, **kwargs)\nConstruct a map.\n\n\nMapContains\nMapContains(self, **kwargs)\nCheck if a map contains a key.\n\n\nMapGet\nMapGet(self, **kwargs)\nGet a value from a map by key.\n\n\nMapKeys\nMapKeys(self, **kwargs)\nGet the keys of a map as an array.\n\n\nMapLength\nMapLength(self, **kwargs)\nCompute the number of unique keys in a map.\n\n\nMapMerge\nMapMerge(self, **kwargs)\nCombine two maps into one.\nIf a key is present in both maps, the value from the first is kept.\n\n\nMapValues\nMapValues(self, **kwargs)\nGet the values of a map as an array."
  },
  {
    "objectID": "reference/operations.html#classes-8",
    "href": "reference/operations.html#classes-8",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nAbs\nAbsolute value.\n\n\nAcos\nReturns the arc cosine of x.\n\n\nAdd\nAdd two values.\n\n\nAsin\nReturns the arc sine of x.\n\n\nAtan\nReturns the arc tangent of x.\n\n\nAtan2\nReturns the arc tangent of x and y.\n\n\nBaseConvert\nConvert a number from one base to another.\n\n\nBitwiseAnd\nBitwise AND operation.\n\n\nBitwiseBinary\nBase class for bitwise binary operations.\n\n\nBitwiseLeftShift\nBitwise left shift operation.\n\n\nBitwiseNot\nBitwise NOT operation.\n\n\nBitwiseOr\nBitwise OR operation.\n\n\nBitwiseRightShift\nBitwise right shift operation.\n\n\nBitwiseXor\nBitwise XOR operation.\n\n\nCeil\nRound up to the nearest integer value greater than or equal to this value.\n\n\nClip\nClip a value to a specified range.\n\n\nCos\nReturns the cosine of x.\n\n\nCot\nReturns the cotangent of x.\n\n\nDegrees\nConverts radians to degrees.\n\n\nDivide\nDivide the left value by the right value.\n\n\nExp\nExponential function.\n\n\nFloor\nRound down to the nearest integer value less than or equal to this value.\n\n\nFloorDivide\nDivide the left value by the right value and round down to the nearest integer.\n\n\nIsInf\nCheck if the value is infinite.\n\n\nIsNan\nCheck if the value is NaN.\n\n\nLn\nNatural logarithm.\n\n\nLog\nLogarithm with a specific base.\n\n\nLog10\nLogarithm base 10.\n\n\nLog2\nLogarithm base 2.\n\n\nLogarithm\nBase class for logarithmic operations.\n\n\nMathUnary\nBase class for unary math operations.\n\n\nModulus\nReturn the remainder after the division of the left value by the right value.\n\n\nMultiply\nMultiply two values.\n\n\nNegate\nNegate the value.\n\n\nPower\nRaise the left value to the power of the right value.\n\n\nRadians\nConverts degrees to radians.\n\n\nRound\nRound a value.\n\n\nSign\nSign of the value.\n\n\nSin\nReturns the sine of x.\n\n\nSqrt\nSquare root of the value.\n\n\nSubtract\nSubtract the right value from the left value.\n\n\nTan\nReturns the tangent of x.\n\n\nTrigonometricBinary\nTrigonometric base binary.\n\n\nTrigonometricUnary\nTrigonometric base unary.\n\n\n\n\nAbs\nAbs(self, **kwargs)\nAbsolute value.\n\n\nAcos\nAcos(self, **kwargs)\nReturns the arc cosine of x.\n\n\nAdd\nAdd(self, **kwargs)\nAdd two values.\n\n\nAsin\nAsin(self, **kwargs)\nReturns the arc sine of x.\n\n\nAtan\nAtan(self, **kwargs)\nReturns the arc tangent of x.\n\n\nAtan2\nAtan2(self, **kwargs)\nReturns the arc tangent of x and y.\n\n\nBaseConvert\nBaseConvert(self, **kwargs)\nConvert a number from one base to another.\n\n\nBitwiseAnd\nBitwiseAnd(self, **kwargs)\nBitwise AND operation.\n\n\nBitwiseBinary\nBitwiseBinary(self, **kwargs)\nBase class for bitwise binary operations.\n\n\nBitwiseLeftShift\nBitwiseLeftShift(self, **kwargs)\nBitwise left shift operation.\n\n\nBitwiseNot\nBitwiseNot(self, **kwargs)\nBitwise NOT operation.\n\n\nBitwiseOr\nBitwiseOr(self, **kwargs)\nBitwise OR operation.\n\n\nBitwiseRightShift\nBitwiseRightShift(self, **kwargs)\nBitwise right shift operation.\n\n\nBitwiseXor\nBitwiseXor(self, **kwargs)\nBitwise XOR operation.\n\n\nCeil\nCeil(self, **kwargs)\nRound up to the nearest integer value greater than or equal to this value.\n\n\nClip\nClip(self, **kwargs)\nClip a value to a specified range.\n\n\nCos\nCos(self, **kwargs)\nReturns the cosine of x.\n\n\nCot\nCot(self, **kwargs)\nReturns the cotangent of x.\n\n\nDegrees\nDegrees(self, **kwargs)\nConverts radians to degrees.\n\n\nDivide\nDivide(self, **kwargs)\nDivide the left value by the right value.\n\n\nExp\nExp(self, **kwargs)\nExponential function.\n\n\nFloor\nFloor(self, **kwargs)\nRound down to the nearest integer value less than or equal to this value.\n\n\nFloorDivide\nFloorDivide(self, **kwargs)\nDivide the left value by the right value and round down to the nearest integer.\n\n\nIsInf\nIsInf(self, **kwargs)\nCheck if the value is infinite.\n\n\nIsNan\nIsNan(self, **kwargs)\nCheck if the value is NaN.\n\n\nLn\nLn(self, **kwargs)\nNatural logarithm.\n\n\nLog\nLog(self, **kwargs)\nLogarithm with a specific base.\n\n\nLog10\nLog10(self, **kwargs)\nLogarithm base 10.\n\n\nLog2\nLog2(self, **kwargs)\nLogarithm base 2.\n\n\nLogarithm\nLogarithm(self, **kwargs)\nBase class for logarithmic operations.\n\n\nMathUnary\nMathUnary(self, **kwargs)\nBase class for unary math operations.\n\n\nModulus\nModulus(self, **kwargs)\nReturn the remainder after the division of the left value by the right value.\n\n\nMultiply\nMultiply(self, **kwargs)\nMultiply two values.\n\n\nNegate\nNegate(self, **kwargs)\nNegate the value.\n\n\nPower\nPower(self, **kwargs)\nRaise the left value to the power of the right value.\n\n\nRadians\nRadians(self, **kwargs)\nConverts degrees to radians.\n\n\nRound\nRound(self, **kwargs)\nRound a value.\n\n\nSign\nSign(self, **kwargs)\nSign of the value.\n\n\nSin\nSin(self, **kwargs)\nReturns the sine of x.\n\n\nSqrt\nSqrt(self, **kwargs)\nSquare root of the value.\n\n\nSubtract\nSubtract(self, **kwargs)\nSubtract the right value from the left value.\n\n\nTan\nTan(self, **kwargs)\nReturns the tangent of x.\n\n\nTrigonometricBinary\nTrigonometricBinary(self, **kwargs)\nTrigonometric base binary.\n\n\nTrigonometricUnary\nTrigonometricUnary(self, **kwargs)\nTrigonometric base unary."
  },
  {
    "objectID": "reference/operations.html#classes-9",
    "href": "reference/operations.html#classes-9",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nAll\nCheck if all values in a column are true.\n\n\nAny\nCheck if any value in a column is true.\n\n\nApproxCountDistinct\nApproximate number of unique values.\n\n\nApproxMedian\nCompute the approximate median of a set of comparable values.\n\n\nArbitrary\nRetrieve an arbitrary element.\n\n\nArgMax\nCompute the index of the maximum value in a column.\n\n\nArgMin\nCompute the index of the minimum value in a column.\n\n\nArrayCollect\nCollect values into an array.\n\n\nBitAnd\nAggregate bitwise AND operation.\n\n\nBitOr\nAggregate bitwise OR operation.\n\n\nBitXor\nAggregate bitwise XOR operation.\n\n\nCorrelation\nCorrelation coefficient of two columns.\n\n\nCount\nCount the number of non-null elements of a column.\n\n\nCountDistinct\nCount the number of distinct values in a column.\n\n\nCountDistinctStar\nCount the number of distinct rows of a relation.\n\n\nCountStar\nCount the number of rows of a relation.\n\n\nCovariance\nCovariance of two columns.\n\n\nFirst\nRetrieve the first element.\n\n\nGroupConcat\nConcatenate strings in a group with a given separator character.\n\n\nLast\nRetrieve the last element.\n\n\nMax\nCompute the maximum of a column.\n\n\nMean\nCompute the mean of a column.\n\n\nMedian\nCompute the median of a column.\n\n\nMin\nCompute the minimum of a column.\n\n\nMode\nCompute the mode of a column.\n\n\nMultiQuantile\nCompute multiple quantiles of a column.\n\n\nQuantile\nCompute the quantile of a column.\n\n\nReduction\nBase class for reduction operations.\n\n\nStandardDev\nCompute the standard deviation of a column.\n\n\nSum\nCompute the sum of a column.\n\n\nVariance\nCompute the variance of a column.\n\n\nVarianceBase\nBase class for variance and standard deviation.\n\n\n\n\nAll\nAll(self, **kwargs)\nCheck if all values in a column are true.\n\n\nAny\nAny(self, **kwargs)\nCheck if any value in a column is true.\n\n\nApproxCountDistinct\nApproxCountDistinct(self, **kwargs)\nApproximate number of unique values.\n\n\nApproxMedian\nApproxMedian(self, **kwargs)\nCompute the approximate median of a set of comparable values.\n\n\nArbitrary\nArbitrary(self, **kwargs)\nRetrieve an arbitrary element.\nReturns a non-null value unless the column is empty or all values are NULL.\n\n\nArgMax\nArgMax(self, **kwargs)\nCompute the index of the maximum value in a column.\n\n\nArgMin\nArgMin(self, **kwargs)\nCompute the index of the minimum value in a column.\n\n\nArrayCollect\nArrayCollect(self, **kwargs)\nCollect values into an array.\n\n\nBitAnd\nBitAnd(self, **kwargs)\nAggregate bitwise AND operation.\nAll elements in an integer column are ANDed together.\nThis can be used to determine which bit flags are set on all elements.\n\nSee Also\n\nBigQuery BIT_AND\nMySQL BIT_AND\n\n\n\n\nBitOr\nBitOr(self, **kwargs)\nAggregate bitwise OR operation.\nAll elements in an integer column are ORed together. This can be used to determine which bit flags are set on any element.\n\nSee Also\n\nBigQuery BIT_OR\nMySQL BIT_OR\n\n\n\n\nBitXor\nBitXor(self, **kwargs)\nAggregate bitwise XOR operation.\nAll elements in an integer column are XORed together. This can be used as a parity checksum of element values.\n\nSee Also\n\nBigQuery BIT_XOR\nMySQL BIT_XOR\n\n\n\n\nCorrelation\nCorrelation(self, **kwargs)\nCorrelation coefficient of two columns.\n\n\nCount\nCount(self, **kwargs)\nCount the number of non-null elements of a column.\n\n\nCountDistinct\nCountDistinct(self, **kwargs)\nCount the number of distinct values in a column.\n\n\nCountDistinctStar\nCountDistinctStar(self, **kwargs)\nCount the number of distinct rows of a relation.\n\n\nCountStar\nCountStar(self, **kwargs)\nCount the number of rows of a relation.\n\n\nCovariance\nCovariance(self, **kwargs)\nCovariance of two columns.\n\n\nFirst\nFirst(self, **kwargs)\nRetrieve the first element.\n\n\nGroupConcat\nGroupConcat(self, **kwargs)\nConcatenate strings in a group with a given separator character.\n\n\nLast\nLast(self, **kwargs)\nRetrieve the last element.\n\n\nMax\nMax(self, **kwargs)\nCompute the maximum of a column.\n\n\nMean\nMean(self, **kwargs)\nCompute the mean of a column.\n\n\nMedian\nMedian(self, **kwargs)\nCompute the median of a column.\n\n\nMin\nMin(self, **kwargs)\nCompute the minimum of a column.\n\n\nMode\nMode(self, **kwargs)\nCompute the mode of a column.\n\n\nMultiQuantile\nMultiQuantile(self, **kwargs)\nCompute multiple quantiles of a column.\n\n\nQuantile\nQuantile(self, **kwargs)\nCompute the quantile of a column.\n\n\nReduction\nReduction(self, **kwargs)\nBase class for reduction operations.\n\n\nStandardDev\nStandardDev(self, **kwargs)\nCompute the standard deviation of a column.\n\n\nSum\nSum(self, **kwargs)\nCompute the sum of a column.\n\n\nVariance\nVariance(self, **kwargs)\nCompute the variance of a column.\n\n\nVarianceBase\nVarianceBase(self, **kwargs)\nBase class for variance and standard deviation."
  },
  {
    "objectID": "reference/operations.html#classes-10",
    "href": "reference/operations.html#classes-10",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nAggregate\nAggregate a table by a set of group by columns and metrics.\n\n\nDatabaseTable\nA table that is bound to a specific backend.\n\n\nDifference\nSubtract one table from another.\n\n\nDistinct\nCompute the distinct rows of a table.\n\n\nDropNull\nDrop null values in the table.\n\n\nDummyTable\nA table constructed from literal values.\n\n\nField\nA field of a relation.\n\n\nFillNull\nFill null values in the table.\n\n\nFilter\nFilter a table by a set of predicates.\n\n\nInMemoryTable\nA table whose data is stored in memory.\n\n\nIntersection\nIntersect two tables.\n\n\nLimit\nLimit and/or offset the number of records in a table.\n\n\nNamespace\nObject to model namespaces for tables.\n\n\nPhysicalTable\nBase class for tables with a name.\n\n\nProject\nProject a subset of columns from a relation.\n\n\nRelation\nBase class for relational operations.\n\n\nSQLQueryResult\nA table sourced from the result set of a SQL SELECT statement.\n\n\nSQLStringView\nA view created from a SQL string.\n\n\nSample\nSample performs random sampling of records in a table.\n\n\nSet\nBase class for set operations.\n\n\nSort\nSort a table by a set of keys.\n\n\nTableUnnest\nCross join unnest operation.\n\n\nUnboundTable\nA table that is not bound to a specific backend.\n\n\nUnion\nUnion two tables.\n\n\nView\nA view created from an expression.\n\n\n\n\nAggregate\nAggregate(self, parent, groups, metrics)\nAggregate a table by a set of group by columns and metrics.\n\n\nDatabaseTable\nDatabaseTable(self, **kwargs)\nA table that is bound to a specific backend.\n\n\nDifference\nDifference(self, left, right, **kwargs)\nSubtract one table from another.\n\n\nDistinct\nDistinct(self, **kwargs)\nCompute the distinct rows of a table.\n\n\nDropNull\nDropNull(self, **kwargs)\nDrop null values in the table.\n\n\nDummyTable\nDummyTable(self, **kwargs)\nA table constructed from literal values.\n\n\nField\nField(self, rel, name)\nA field of a relation.\n\n\nFillNull\nFillNull(self, **kwargs)\nFill null values in the table.\n\n\nFilter\nFilter(self, parent, predicates)\nFilter a table by a set of predicates.\n\n\nInMemoryTable\nInMemoryTable(self, **kwargs)\nA table whose data is stored in memory.\n\n\nIntersection\nIntersection(self, left, right, **kwargs)\nIntersect two tables.\n\n\nLimit\nLimit(self, **kwargs)\nLimit and/or offset the number of records in a table.\n\n\nNamespace\nNamespace(self, **kwargs)\nObject to model namespaces for tables.\nMaps to the concept of database and/or catalog in SQL databases that support them.\n\n\nPhysicalTable\nPhysicalTable(self, **kwargs)\nBase class for tables with a name.\n\n\nProject\nProject(self, parent, values)\nProject a subset of columns from a relation.\n\n\nRelation\nRelation(self, **kwargs)\nBase class for relational operations.\n\nAttributes\n\n\n\nName\nDescription\n\n\n\n\nfields\nA mapping of column names to fields of the relation.\n\n\nschema\nThe schema of the relation.\n\n\nvalues\nA mapping of column names to expressions which build up the relation.\n\n\n\n\n\n\nSQLQueryResult\nSQLQueryResult(self, **kwargs)\nA table sourced from the result set of a SQL SELECT statement.\n\n\nSQLStringView\nSQLStringView(self, **kwargs)\nA view created from a SQL string.\n\n\nSample\nSample(self, **kwargs)\nSample performs random sampling of records in a table.\n\n\nSet\nSet(self, left, right, **kwargs)\nBase class for set operations.\n\n\nSort\nSort(self, parent, keys)\nSort a table by a set of keys.\n\n\nTableUnnest\nTableUnnest(self, **kwargs)\nCross join unnest operation.\n\n\nUnboundTable\nUnboundTable(self, **kwargs)\nA table that is not bound to a specific backend.\n\n\nUnion\nUnion(self, left, right, **kwargs)\nUnion two tables.\n\n\nView\nView(self, **kwargs)\nA view created from an expression."
  },
  {
    "objectID": "reference/operations.html#classes-11",
    "href": "reference/operations.html#classes-11",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nSortKey\nA sort key.\n\n\n\n\nSortKey\nSortKey(self, **kwargs)\nA sort key."
  },
  {
    "objectID": "reference/operations.html#classes-12",
    "href": "reference/operations.html#classes-12",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nArrayStringJoin\nJoin strings in an array with a separator.\n\n\nCapitalize\nCapitalize the first letter of a string.\n\n\nEndsWith\nCheck if a string ends with another string.\n\n\nExtractAuthority\nExtract the authority from a URL.\n\n\nExtractFile\nExtract the file from a URL.\n\n\nExtractFragment\nExtract the fragment from a URL.\n\n\nExtractHost\nExtract the host from a URL.\n\n\nExtractPath\nExtract the path from a URL.\n\n\nExtractProtocol\nExtract the protocol from a URL.\n\n\nExtractQuery\nExtract the query from a URL.\n\n\nExtractUserInfo\nExtract the user info from a URL.\n\n\nFindInSet\nFind the position of a string in a list of comma-separated strings.\n\n\nLPad\nPad a string on the left.\n\n\nLStrip\nStrip leading whitespace.\n\n\nLevenshtein\nCompute the Levenshtein distance between two strings.\n\n\nLowercase\nConvert a string to lowercase.\n\n\nRPad\nPad a string on the right.\n\n\nRStrip\nStrip trailing whitespace.\n\n\nRegexExtract\nExtract a substring from a string using a regular expression.\n\n\nRegexReplace\nReplace a substring in a string using a regular expression.\n\n\nRegexSearch\nSearch a string with a regular expression.\n\n\nRegexSplit\nSplit a string using a regular expression.\n\n\nRepeat\nRepeat a string.\n\n\nReverse\nReverse a string.\n\n\nStartsWith\nCheck if a string starts with another string.\n\n\nStrRight\nExtract a substring starting from the right of a string.\n\n\nStringAscii\nCompute the ASCII code of the first character of a string.\n\n\nStringConcat\nConcatenate strings.\n\n\nStringContains\nCheck if a string contains a substring.\n\n\nStringFind\nFind the position of a substring in a string.\n\n\nStringJoin\nJoin strings with a separator.\n\n\nStringLength\nCompute the length of a string.\n\n\nStringReplace\nReplace a substring in a string with another string.\n\n\nStringSQLILike\nCase-insensitive SQL LIKE string match operation.\n\n\nStringSQLLike\nSQL LIKE string match operation.\n\n\nStringSlice\nExtract a substring from a string.\n\n\nStringSplit\nSplit a string using a delimiter.\n\n\nStringUnary\nBase class for string operations accepting one argument.\n\n\nStrip\nStrip leading and trailing whitespace.\n\n\nSubstring\nExtract a substring from a string.\n\n\nTranslate\nTranslate characters in a string.\n\n\nUppercase\nConvert a string to uppercase.\n\n\n\n\nArrayStringJoin\nArrayStringJoin(self, **kwargs)\nJoin strings in an array with a separator.\n\n\nCapitalize\nCapitalize(self, **kwargs)\nCapitalize the first letter of a string.\n\n\nEndsWith\nEndsWith(self, **kwargs)\nCheck if a string ends with another string.\n\n\nExtractAuthority\nExtractAuthority(self, **kwargs)\nExtract the authority from a URL.\n\n\nExtractFile\nExtractFile(self, **kwargs)\nExtract the file from a URL.\n\n\nExtractFragment\nExtractFragment(self, **kwargs)\nExtract the fragment from a URL.\n\n\nExtractHost\nExtractHost(self, **kwargs)\nExtract the host from a URL.\n\n\nExtractPath\nExtractPath(self, **kwargs)\nExtract the path from a URL.\n\n\nExtractProtocol\nExtractProtocol(self, **kwargs)\nExtract the protocol from a URL.\n\n\nExtractQuery\nExtractQuery(self, **kwargs)\nExtract the query from a URL.\n\n\nExtractUserInfo\nExtractUserInfo(self, **kwargs)\nExtract the user info from a URL.\n\n\nFindInSet\nFindInSet(self, **kwargs)\nFind the position of a string in a list of comma-separated strings.\n\n\nLPad\nLPad(self, **kwargs)\nPad a string on the left.\n\n\nLStrip\nLStrip(self, **kwargs)\nStrip leading whitespace.\n\n\nLevenshtein\nLevenshtein(self, **kwargs)\nCompute the Levenshtein distance between two strings.\n\n\nLowercase\nLowercase(self, **kwargs)\nConvert a string to lowercase.\n\n\nRPad\nRPad(self, **kwargs)\nPad a string on the right.\n\n\nRStrip\nRStrip(self, **kwargs)\nStrip trailing whitespace.\n\n\nRegexExtract\nRegexExtract(self, **kwargs)\nExtract a substring from a string using a regular expression.\n\n\nRegexReplace\nRegexReplace(self, **kwargs)\nReplace a substring in a string using a regular expression.\n\n\nRegexSearch\nRegexSearch(self, **kwargs)\nSearch a string with a regular expression.\n\n\nRegexSplit\nRegexSplit(self, **kwargs)\nSplit a string using a regular expression.\n\n\nRepeat\nRepeat(self, **kwargs)\nRepeat a string.\n\n\nReverse\nReverse(self, **kwargs)\nReverse a string.\n\n\nStartsWith\nStartsWith(self, **kwargs)\nCheck if a string starts with another string.\n\n\nStrRight\nStrRight(self, **kwargs)\nExtract a substring starting from the right of a string.\n\n\nStringAscii\nStringAscii(self, **kwargs)\nCompute the ASCII code of the first character of a string.\n\n\nStringConcat\nStringConcat(self, **kwargs)\nConcatenate strings.\n\n\nStringContains\nStringContains(self, **kwargs)\nCheck if a string contains a substring.\n\n\nStringFind\nStringFind(self, **kwargs)\nFind the position of a substring in a string.\n\n\nStringJoin\nStringJoin(self, **kwargs)\nJoin strings with a separator.\n\n\nStringLength\nStringLength(self, **kwargs)\nCompute the length of a string.\n\n\nStringReplace\nStringReplace(self, **kwargs)\nReplace a substring in a string with another string.\n\n\nStringSQLILike\nStringSQLILike(self, **kwargs)\nCase-insensitive SQL LIKE string match operation.\nSimilar to case-insensitive globbing.\n\n\nStringSQLLike\nStringSQLLike(self, **kwargs)\nSQL LIKE string match operation.\nSimilar to globbing.\n\n\nStringSlice\nStringSlice(self, **kwargs)\nExtract a substring from a string.\n\n\nStringSplit\nStringSplit(self, **kwargs)\nSplit a string using a delimiter.\n\n\nStringUnary\nStringUnary(self, **kwargs)\nBase class for string operations accepting one argument.\n\n\nStrip\nStrip(self, **kwargs)\nStrip leading and trailing whitespace.\n\n\nSubstring\nSubstring(self, **kwargs)\nExtract a substring from a string.\n\n\nTranslate\nTranslate(self, **kwargs)\nTranslate characters in a string.\n\n\nUppercase\nUppercase(self, **kwargs)\nConvert a string to uppercase."
  },
  {
    "objectID": "reference/operations.html#classes-13",
    "href": "reference/operations.html#classes-13",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nStructColumn\nConstruct a struct column from literals or expressions.\n\n\nStructField\nExtract a field from a struct value.\n\n\n\n\nStructColumn\nStructColumn(self, names, values)\nConstruct a struct column from literals or expressions.\n\n\nStructField\nStructField(self, **kwargs)\nExtract a field from a struct value."
  },
  {
    "objectID": "reference/operations.html#classes-14",
    "href": "reference/operations.html#classes-14",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nExistsSubquery\nCheck if a subquery returns any rows.\n\n\nInSubquery\nCheck if a value is in the result of a subquery.\n\n\nScalarSubquery\nA subquery that returns a single scalar value.\n\n\nSubquery\nBase class for subquery operations.\n\n\n\n\nExistsSubquery\nExistsSubquery(self, **kwargs)\nCheck if a subquery returns any rows.\n\n\nInSubquery\nInSubquery(self, rel, needle)\nCheck if a value is in the result of a subquery.\n\n\nScalarSubquery\nScalarSubquery(self, rel)\nA subquery that returns a single scalar value.\n\n\nSubquery\nSubquery(self, **kwargs)\nBase class for subquery operations."
  },
  {
    "objectID": "reference/operations.html#classes-15",
    "href": "reference/operations.html#classes-15",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nBetweenTime\nCheck if a time is between two bounds.\n\n\nDate\nExtract the date from a timestamp.\n\n\nDateAdd\nAdd an interval to a date.\n\n\nDateDelta\nCompute the difference between two dates as integer number of requested units.\n\n\nDateDiff\nCompute the difference between two dates.\n\n\nDateFromYMD\nConstruct a date from year, month, and day.\n\n\nDateSub\nSubtract an interval from a date.\n\n\nDateTruncate\nTruncate a date to a specified unit.\n\n\nDayOfWeekIndex\nExtract the index of the day of the week from a date or timestamp.\n\n\nDayOfWeekName\nExtract the name of the day of the week from a date or timestamp.\n\n\nExtractDateField\nExtract a field from a date.\n\n\nExtractDay\nExtract the day from a date or timestamp.\n\n\nExtractDayOfYear\nExtract the day of the year from a date or timestamp.\n\n\nExtractEpochSeconds\nExtract seconds since the UNIX epoch from a date or timestamp.\n\n\nExtractHour\nExtract the hour from a time or timestamp.\n\n\nExtractIsoYear\nExtract the ISO year from a date or timestamp.\n\n\nExtractMicrosecond\nExtract microseconds from a time or timestamp.\n\n\nExtractMillisecond\nExtract milliseconds from a time or timestamp.\n\n\nExtractMinute\nExtract the minute from a time or timestamp.\n\n\nExtractMonth\nExtract the month from a date or timestamp.\n\n\nExtractQuarter\nExtract the quarter from a date or timestamp.\n\n\nExtractSecond\nExtract the second from a time or timestamp.\n\n\nExtractTemporalField\nExtract a field from a temporal value.\n\n\nExtractTimeField\nExtract a field from a time.\n\n\nExtractWeekOfYear\nExtract the week of the year from a date or timestamp.\n\n\nExtractYear\nExtract the year from a date or timestamp.\n\n\nIntervalAdd\nAdd two intervals.\n\n\nIntervalBinary\nBase class for interval binary operations.\n\n\nIntervalFloorDivide\nDivide an interval by a scalar, rounding down.\n\n\nIntervalFromInteger\nConstruct an interval from an integer.\n\n\nIntervalMultiply\nMultiply an interval by a scalar.\n\n\nIntervalSubtract\nSubtract one interval from another.\n\n\nStrftime\nFormat a temporal value as a string.\n\n\nStringToDate\nConvert a string to a date.\n\n\nStringToTimestamp\nConvert a string to a timestamp.\n\n\nTemporalDelta\nBase class for temporal delta operations.\n\n\nTime\nExtract the time from a timestamp.\n\n\nTimeAdd\nAdd an interval to a time.\n\n\nTimeDelta\nCompute the difference between two times as integer number of requested units.\n\n\nTimeDiff\nCompute the difference between two times.\n\n\nTimeFromHMS\nConstruct a time from hours, minutes, and seconds.\n\n\nTimeSub\nSubtract an interval from a time.\n\n\nTimeTruncate\nTruncate a time to a specified unit.\n\n\nTimestampAdd\nAdd an interval to a timestamp.\n\n\nTimestampBucket\nBucketize a timestamp to a specified interval.\n\n\nTimestampDelta\nCompute the difference between two timestamps as integer number of requested units.\n\n\nTimestampDiff\nCompute the difference between two timestamps.\n\n\nTimestampFromUNIX\nConstruct a timestamp from a UNIX timestamp.\n\n\nTimestampFromYMDHMS\nConstruct a timestamp from components.\n\n\nTimestampSub\nSubtract an interval from a timestamp.\n\n\nTimestampTruncate\nTruncate a timestamp to a specified unit.\n\n\n\n\nBetweenTime\nBetweenTime(self, arg, lower_bound, upper_bound)\nCheck if a time is between two bounds.\n\n\nDate\nDate(self, **kwargs)\nExtract the date from a timestamp.\n\n\nDateAdd\nDateAdd(self, **kwargs)\nAdd an interval to a date.\n\n\nDateDelta\nDateDelta(self, **kwargs)\nCompute the difference between two dates as integer number of requested units.\n\n\nDateDiff\nDateDiff(self, **kwargs)\nCompute the difference between two dates.\n\n\nDateFromYMD\nDateFromYMD(self, **kwargs)\nConstruct a date from year, month, and day.\n\n\nDateSub\nDateSub(self, **kwargs)\nSubtract an interval from a date.\n\n\nDateTruncate\nDateTruncate(self, **kwargs)\nTruncate a date to a specified unit.\n\n\nDayOfWeekIndex\nDayOfWeekIndex(self, **kwargs)\nExtract the index of the day of the week from a date or timestamp.\n\n\nDayOfWeekName\nDayOfWeekName(self, **kwargs)\nExtract the name of the day of the week from a date or timestamp.\n\n\nExtractDateField\nExtractDateField(self, **kwargs)\nExtract a field from a date.\n\n\nExtractDay\nExtractDay(self, **kwargs)\nExtract the day from a date or timestamp.\n\n\nExtractDayOfYear\nExtractDayOfYear(self, **kwargs)\nExtract the day of the year from a date or timestamp.\n\n\nExtractEpochSeconds\nExtractEpochSeconds(self, **kwargs)\nExtract seconds since the UNIX epoch from a date or timestamp.\n\n\nExtractHour\nExtractHour(self, **kwargs)\nExtract the hour from a time or timestamp.\n\n\nExtractIsoYear\nExtractIsoYear(self, **kwargs)\nExtract the ISO year from a date or timestamp.\n\n\nExtractMicrosecond\nExtractMicrosecond(self, **kwargs)\nExtract microseconds from a time or timestamp.\n\n\nExtractMillisecond\nExtractMillisecond(self, **kwargs)\nExtract milliseconds from a time or timestamp.\n\n\nExtractMinute\nExtractMinute(self, **kwargs)\nExtract the minute from a time or timestamp.\n\n\nExtractMonth\nExtractMonth(self, **kwargs)\nExtract the month from a date or timestamp.\n\n\nExtractQuarter\nExtractQuarter(self, **kwargs)\nExtract the quarter from a date or timestamp.\n\n\nExtractSecond\nExtractSecond(self, **kwargs)\nExtract the second from a time or timestamp.\n\n\nExtractTemporalField\nExtractTemporalField(self, **kwargs)\nExtract a field from a temporal value.\n\n\nExtractTimeField\nExtractTimeField(self, **kwargs)\nExtract a field from a time.\n\n\nExtractWeekOfYear\nExtractWeekOfYear(self, **kwargs)\nExtract the week of the year from a date or timestamp.\n\n\nExtractYear\nExtractYear(self, **kwargs)\nExtract the year from a date or timestamp.\n\n\nIntervalAdd\nIntervalAdd(self, **kwargs)\nAdd two intervals.\n\n\nIntervalBinary\nIntervalBinary(self, **kwargs)\nBase class for interval binary operations.\n\n\nIntervalFloorDivide\nIntervalFloorDivide(self, **kwargs)\nDivide an interval by a scalar, rounding down.\n\n\nIntervalFromInteger\nIntervalFromInteger(self, **kwargs)\nConstruct an interval from an integer.\n\n\nIntervalMultiply\nIntervalMultiply(self, **kwargs)\nMultiply an interval by a scalar.\n\n\nIntervalSubtract\nIntervalSubtract(self, **kwargs)\nSubtract one interval from another.\n\n\nStrftime\nStrftime(self, **kwargs)\nFormat a temporal value as a string.\n\n\nStringToDate\nStringToDate(self, **kwargs)\nConvert a string to a date.\n\n\nStringToTimestamp\nStringToTimestamp(self, **kwargs)\nConvert a string to a timestamp.\n\n\nTemporalDelta\nTemporalDelta(self, **kwargs)\nBase class for temporal delta operations.\n\n\nTime\nTime(self, **kwargs)\nExtract the time from a timestamp.\n\n\nTimeAdd\nTimeAdd(self, **kwargs)\nAdd an interval to a time.\n\n\nTimeDelta\nTimeDelta(self, **kwargs)\nCompute the difference between two times as integer number of requested units.\n\n\nTimeDiff\nTimeDiff(self, **kwargs)\nCompute the difference between two times.\n\n\nTimeFromHMS\nTimeFromHMS(self, **kwargs)\nConstruct a time from hours, minutes, and seconds.\n\n\nTimeSub\nTimeSub(self, **kwargs)\nSubtract an interval from a time.\n\n\nTimeTruncate\nTimeTruncate(self, **kwargs)\nTruncate a time to a specified unit.\n\n\nTimestampAdd\nTimestampAdd(self, **kwargs)\nAdd an interval to a timestamp.\n\n\nTimestampBucket\nTimestampBucket(self, **kwargs)\nBucketize a timestamp to a specified interval.\n\n\nTimestampDelta\nTimestampDelta(self, **kwargs)\nCompute the difference between two timestamps as integer number of requested units.\n\n\nTimestampDiff\nTimestampDiff(self, **kwargs)\nCompute the difference between two timestamps.\n\n\nTimestampFromUNIX\nTimestampFromUNIX(self, **kwargs)\nConstruct a timestamp from a UNIX timestamp.\n\n\nTimestampFromYMDHMS\nTimestampFromYMDHMS(self, **kwargs)\nConstruct a timestamp from components.\n\n\nTimestampSub\nTimestampSub(self, **kwargs)\nSubtract an interval from a timestamp.\n\n\nTimestampTruncate\nTimestampTruncate(self, **kwargs)\nTruncate a timestamp to a specified unit."
  },
  {
    "objectID": "reference/operations.html#classes-16",
    "href": "reference/operations.html#classes-16",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nagg\nAggregate user-defined functions.\n\n\nscalar\nScalar user-defined functions.\n\n\n\n\nagg\nagg()\nAggregate user-defined functions.\n\n\n\n\n\n\nThe agg class itself is not a public API, its methods are.\n\n\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nbuiltin\nConstruct an aggregate user-defined function that is built-in to the backend.\n\n\n\n\nbuiltin\nbuiltin(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct an aggregate user-defined function that is built-in to the backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which the builtin function resides.\nNone\n\n\ncatalog\n\nThe catalog in which the builtin function resides.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; @ibis.udf.agg.builtin\n... def favg(a: float) -&gt; float:\n...     '''Compute the average of a column using Kahan summation.'''\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; expr = favg(t.bill_length_mm)\n&gt;&gt;&gt; expr\n\n\n\n\n\n43.9219298245614\n\n\n\n\n\n\n\n\nscalar\nscalar()\nScalar user-defined functions.\n\n\n\n\n\n\nThe scalar class itself is not a public API, its methods are.\n\n\n\n\n\n\n\nMethods\n\n\n\nName\nDescription\n\n\n\n\nbuiltin\nConstruct a scalar user-defined function that is built-in to the backend.\n\n\npandas\nConstruct a vectorized scalar user-defined function that accepts pandas Series’ as inputs.\n\n\npyarrow\nConstruct a vectorized scalar user-defined function that accepts PyArrow Arrays as input.\n\n\npython\nConstruct a non-vectorized scalar user-defined function that accepts Python scalar values as inputs.\n\n\n\n\nbuiltin\nbuiltin(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct a scalar user-defined function that is built-in to the backend.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which the builtin function resides.\nNone\n\n\ncatalog\n\nThe catalog in which the builtin function resides.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function. For builtin UDFs, only the return type annotation is required. See the user guide for more information.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; @ibis.udf.scalar.builtin\n... def hamming(a: str, b: str) -&gt; int:\n...     '''Compute the Hamming distance between two strings.'''\n&gt;&gt;&gt; expr = hamming(\"duck\", \"luck\")\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n&gt;&gt;&gt; con.execute(expr)\n\n1\n\n\n\n\n\npandas\npandas(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct a vectorized scalar user-defined function that accepts pandas Series’ as inputs.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which to create the UDF.\nNone\n\n\ncatalog\n\nThe catalog in which to create the UDF.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(dict(int_col=[1, 2, 3], str_col=[\"a\", \"b\", \"c\"]))\n&gt;&gt;&gt; t\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ int_col ┃ str_col ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ int64   │ string  │\n├─────────┼─────────┤\n│       1 │ a       │\n│       2 │ b       │\n│       3 │ c       │\n└─────────┴─────────┘\n&gt;&gt;&gt; @ibis.udf.scalar.pandas\n... def str_cap(x: str) -&gt; str:\n...     # note usage of pandas `str` method\n...     return x.str.capitalize()\n&gt;&gt;&gt; str_cap(t.str_col)  # doctest: +SKIP\n┏━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ string_cap_0(str_col) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━┩\n│ string                │\n├───────────────────────┤\n│ A                     │\n│ B                     │\n│ C                     │\n└───────────────────────┘\n\n\nSee Also\n\npython\npyarrow\n\n\n\n\npyarrow\npyarrow(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct a vectorized scalar user-defined function that accepts PyArrow Arrays as input.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which to create the UDF.\nNone\n\n\ncatalog\n\nThe catalog in which to create the UDF.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; import pyarrow.compute as pc\n&gt;&gt;&gt; from datetime import date\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     dict(start_col=[date(2024, 4, 29)], end_col=[date(2025, 4, 29)]),\n... )\n&gt;&gt;&gt; @ibis.udf.scalar.pyarrow\n... def weeks_between(start: date, end: date) -&gt; int:\n...     return pc.weeks_between(start, end)\n&gt;&gt;&gt; weeks_between(t.start_col, t.end_col)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ weeks_between(start_col, end_col) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ int64                             │\n├───────────────────────────────────┤\n│                                52 │\n└───────────────────────────────────┘\n\n\n\n\n\nSee Also\n\npython\npandas\n\n\n\n\npython\npython(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct a non-vectorized scalar user-defined function that accepts Python scalar values as inputs.\n\n\n\n\n\n\npython UDFs are likely to be slow\n\n\n\n\n\npython UDFs are not vectorized: they are executed row by row with one Python function call per row\nThis calling pattern tends to be much slower than pandas or pyarrow-based vectorized UDFs.\n\n\n\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which to create the UDF.\nNone\n\n\ncatalog\n\nThe catalog in which to create the UDF.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(dict(int_col=[1, 2, 3], str_col=[\"a\", \"b\", \"c\"]))\n&gt;&gt;&gt; t\n\n┏━━━━━━━━━┳━━━━━━━━━┓\n┃ int_col ┃ str_col ┃\n┡━━━━━━━━━╇━━━━━━━━━┩\n│ int64   │ string  │\n├─────────┼─────────┤\n│       1 │ a       │\n│       2 │ b       │\n│       3 │ c       │\n└─────────┴─────────┘\n\n\n\n\n&gt;&gt;&gt; @ibis.udf.scalar.python\n... def str_magic(x: str) -&gt; str:\n...     return f\"{x}_magic\"\n&gt;&gt;&gt; @ibis.udf.scalar.python\n... def add_one_py(x: int) -&gt; int:\n...     return x + 1\n&gt;&gt;&gt; str_magic(t.str_col)\n\n┏━━━━━━━━━━━━━━━━━━━━┓\n┃ str_magic(str_col) ┃\n┡━━━━━━━━━━━━━━━━━━━━┩\n│ string             │\n├────────────────────┤\n│ a_magic            │\n│ b_magic            │\n│ c_magic            │\n└────────────────────┘\n\n\n\n\n&gt;&gt;&gt; add_one_py(t.int_col)\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ add_one_py(int_col) ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ int64               │\n├─────────────────────┤\n│                   2 │\n│                   3 │\n│                   4 │\n└─────────────────────┘\n\n\n\n\n\nSee Also\n\npandas\npyarrow"
  },
  {
    "objectID": "reference/operations.html#classes-17",
    "href": "reference/operations.html#classes-17",
    "title": "Operations",
    "section": "Classes",
    "text": "Classes\n\n\n\nName\nDescription\n\n\n\n\nWindowBoundary\nWindow boundary object.\n\n\nWindowFunction\nWindow function operation.\n\n\n\n\nWindowBoundary\nWindowBoundary(self, **kwargs)\nWindow boundary object.\n\n\nWindowFunction\nWindowFunction(self, how, start, end, **kwargs)\nWindow function operation."
  },
  {
    "objectID": "reference/schemas.html",
    "href": "reference/schemas.html",
    "title": "Schemas",
    "section": "",
    "text": "Table Schemas"
  },
  {
    "objectID": "reference/schemas.html#parameters",
    "href": "reference/schemas.html#parameters",
    "title": "Schemas",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\npairs\nSupportsSchema | None\nList or dictionary of name, type pairs. Mutually exclusive with names and types arguments.\nNone\n\n\nnames\nIterable[str] | None\nField names. Mutually exclusive with pairs.\nNone\n\n\ntypes\nIterable[str | dt.DataType] | None\nField types. Mutually exclusive with pairs.\nNone"
  },
  {
    "objectID": "reference/schemas.html#returns",
    "href": "reference/schemas.html#returns",
    "title": "Schemas",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nAn ibis schema"
  },
  {
    "objectID": "reference/schemas.html#examples",
    "href": "reference/schemas.html#examples",
    "title": "Schemas",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; from ibis import schema, Schema\n&gt;&gt;&gt; sc = schema([(\"foo\", \"string\"), (\"bar\", \"int64\"), (\"baz\", \"boolean\")])\n&gt;&gt;&gt; sc = schema(names=[\"foo\", \"bar\", \"baz\"], types=[\"string\", \"int64\", \"boolean\"])\n&gt;&gt;&gt; sc = schema(dict(foo=\"string\"))\n&gt;&gt;&gt; sc = schema(Schema(dict(foo=\"string\")))  # no-op"
  },
  {
    "objectID": "reference/schemas.html#attributes",
    "href": "reference/schemas.html#attributes",
    "title": "Schemas",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nfields\nA mapping of str to"
  },
  {
    "objectID": "reference/schemas.html#methods",
    "href": "reference/schemas.html#methods",
    "title": "Schemas",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nequals\nReturn whether other is equal to self.\n\n\nfrom_dask\nReturn the equivalent ibis schema.\n\n\nfrom_numpy\nReturn the equivalent ibis schema.\n\n\nfrom_pandas\nReturn the equivalent ibis schema.\n\n\nfrom_pyarrow\nReturn the equivalent ibis schema.\n\n\nfrom_tuples\nConstruct a Schema from an iterable of pairs.\n\n\nname_at_position\nReturn the name of a schema column at position i.\n\n\nto_dask\nReturn the equivalent dask dtypes.\n\n\nto_numpy\nReturn the equivalent numpy dtypes.\n\n\nto_pandas\nReturn the equivalent pandas datatypes.\n\n\nto_pyarrow\nReturn the equivalent pyarrow schema.\n\n\n\n\nequals\nequals(other)\nReturn whether other is equal to self.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\nSchema\nSchema to compare self to.\nrequired\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; first = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; second = ibis.schema({\"a\": \"int\"})\n&gt;&gt;&gt; assert first.equals(second)\n&gt;&gt;&gt; third = ibis.schema({\"a\": \"array&lt;int&gt;\"})\n&gt;&gt;&gt; assert not first.equals(third)\n\n\n\n\nfrom_dask\nfrom_dask(dask_schema)\nReturn the equivalent ibis schema.\n\n\nfrom_numpy\nfrom_numpy(numpy_schema)\nReturn the equivalent ibis schema.\n\n\nfrom_pandas\nfrom_pandas(pandas_schema)\nReturn the equivalent ibis schema.\n\n\nfrom_pyarrow\nfrom_pyarrow(pyarrow_schema)\nReturn the equivalent ibis schema.\n\n\nfrom_tuples\nfrom_tuples(values)\nConstruct a Schema from an iterable of pairs.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalues\nIterable[tuple[str, str | dt.DataType]]\nAn iterable of pairs of name and type.\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nSchema\nA new schema\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.Schema.from_tuples([(\"a\", \"int\"), (\"b\", \"string\")])\n\nibis.Schema {\n  a  int64\n  b  string\n}\n\n\n\n\n\nname_at_position\nname_at_position(i)\nReturn the name of a schema column at position i.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ni\nint\nThe position of the column\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nstr\nThe name of the column in the schema at position i.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; sch = ibis.Schema({\"a\": \"int\", \"b\": \"string\"})\n&gt;&gt;&gt; sch.name_at_position(0)\n\n'a'\n\n\n\n&gt;&gt;&gt; sch.name_at_position(1)\n\n'b'\n\n\n\n\n\nto_dask\nto_dask()\nReturn the equivalent dask dtypes.\n\n\nto_numpy\nto_numpy()\nReturn the equivalent numpy dtypes.\n\n\nto_pandas\nto_pandas()\nReturn the equivalent pandas datatypes.\n\n\nto_pyarrow\nto_pyarrow()\nReturn the equivalent pyarrow schema."
  },
  {
    "objectID": "reference/ServiceBackendTest.html",
    "href": "reference/ServiceBackendTest.html",
    "title": "ServiceBackendTest",
    "section": "",
    "text": "ServiceBackendTest(self, *, data_dir, tmpdir, worker_id, **kw)\nParent class to use for backend test configuration if backend requires a Docker container(s) in order to run locally.\n\n\n\n\n\nName\nDescription\n\n\n\n\ndata_volume\nData volume defined in compose.yaml corresponding to backend.\n\n\nservice_name\nName of service defined in compose.yaml corresponding to backend.\n\n\ntest_files\nReturns an iterable of test files to load into a Docker container before testing.\n\n\n\n\n\n\n\n\n\nName\nDescription\n\n\n\n\npreload\nUse docker compose cp to copy all files from test_files into a container.\n\n\n\n\n\npreload()\nUse docker compose cp to copy all files from test_files into a container.\nservice_name and data_volume are set as class-level variables."
  },
  {
    "objectID": "reference/ServiceBackendTest.html#attributes",
    "href": "reference/ServiceBackendTest.html#attributes",
    "title": "ServiceBackendTest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndata_volume\nData volume defined in compose.yaml corresponding to backend.\n\n\nservice_name\nName of service defined in compose.yaml corresponding to backend.\n\n\ntest_files\nReturns an iterable of test files to load into a Docker container before testing."
  },
  {
    "objectID": "reference/ServiceBackendTest.html#methods",
    "href": "reference/ServiceBackendTest.html#methods",
    "title": "ServiceBackendTest",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\npreload\nUse docker compose cp to copy all files from test_files into a container.\n\n\n\n\n\npreload()\nUse docker compose cp to copy all files from test_files into a container.\nservice_name and data_volume are set as class-level variables."
  },
  {
    "objectID": "reference/aggregate-udfs.html",
    "href": "reference/aggregate-udfs.html",
    "title": "Aggregate UDFs (experimental)",
    "section": "",
    "text": "Aggregate user-defined function APIs"
  },
  {
    "objectID": "reference/aggregate-udfs.html#methods",
    "href": "reference/aggregate-udfs.html#methods",
    "title": "Aggregate UDFs (experimental)",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbuiltin\nConstruct an aggregate user-defined function that is built-in to the backend.\n\n\n\n\nbuiltin\nbuiltin(fn=None, *, name=None, database=None, catalog=None, signature=None, **kwargs)\nConstruct an aggregate user-defined function that is built-in to the backend.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfn\n\nThe function to wrap.\nNone\n\n\nname\n\nThe name of the UDF in the backend if different from the function name.\nNone\n\n\ndatabase\n\nThe database in which the builtin function resides.\nNone\n\n\ncatalog\n\nThe catalog in which the builtin function resides.\nNone\n\n\nsignature\n\nIf present, a tuple of the form ((arg0type, arg1type, ...), returntype). For example, a function taking an int and a float and returning a string would be ((int, float), str). If not present, the signature will be derived from the type annotations of the wrapped function.\nNone\n\n\nkwargs\n\nAdditional backend-specific configuration arguments for the UDF.\n{}\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; @ibis.udf.agg.builtin\n... def favg(a: float) -&gt; float:\n...     '''Compute the average of a column using Kahan summation.'''\n&gt;&gt;&gt; t = ibis.examples.penguins.fetch()\n&gt;&gt;&gt; expr = favg(t.bill_length_mm)\n&gt;&gt;&gt; expr\n\n\n\n\n\n43.9219298245614"
  },
  {
    "objectID": "reference/expression-temporal.html",
    "href": "reference/expression-temporal.html",
    "title": "Temporal expressions",
    "section": "",
    "text": "Dates, times, timestamps and intervals."
  },
  {
    "objectID": "reference/expression-temporal.html#attributes",
    "href": "reference/expression-temporal.html#attributes",
    "title": "Temporal expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdd an interval to a timestamp.\n\n\nradd\nAdd an interval to a timestamp.\n\n\nsub\nSubtract a timestamp or an interval from a timestamp."
  },
  {
    "objectID": "reference/expression-temporal.html#methods",
    "href": "reference/expression-temporal.html#methods",
    "title": "Temporal expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nbucket\nTruncate the timestamp to buckets of a specified interval.\n\n\ndate\nReturn the date component of the expression.\n\n\ndelta\nCompute the number of parts between two timestamps.\n\n\nstrftime\nFormat a timestamp according to format_str.\n\n\ntruncate\nTruncate timestamp expression to units of unit.\n\n\n\n\nbucket\nbucket(interval=None, *, years=None, quarters=None, months=None, weeks=None, days=None, hours=None, minutes=None, seconds=None, milliseconds=None, microseconds=None, nanoseconds=None, offset=None)\nTruncate the timestamp to buckets of a specified interval.\nThis is similar to truncate, but supports truncating to arbitrary intervals rather than a single unit. Buckets are computed as fixed intervals starting from the UNIX epoch. This origin may be offset by specifying offset.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ninterval\nAny\nThe bucket width as an interval. Alternatively may be specified via component keyword arguments.\nNone\n\n\noffset\nAny\nAn interval to use to offset the start of the bucket.\nNone\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nThe start of the bucket as a timestamp.\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; from ibis import _\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; t = ibis.memtable(\n...     [\n...         (\"2020-04-15 08:04:00\", 1),\n...         (\"2020-04-15 08:06:00\", 2),\n...         (\"2020-04-15 08:09:00\", 3),\n...         (\"2020-04-15 08:11:00\", 4),\n...     ],\n...     columns=[\"ts\", \"val\"],\n... ).cast({\"ts\": \"timestamp\"})\n\nBucket the data into 5 minute wide buckets:\n\n&gt;&gt;&gt; t.ts.bucket(minutes=5)\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ TimestampBucket(ts, 5m) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp               │\n├─────────────────────────┤\n│ 2020-04-15 08:00:00     │\n│ 2020-04-15 08:05:00     │\n│ 2020-04-15 08:05:00     │\n│ 2020-04-15 08:10:00     │\n└─────────────────────────┘\n\n\n\nBucket the data into 5 minute wide buckets, offset by 2 minutes:\n\n&gt;&gt;&gt; t.ts.bucket(minutes=5, offset=ibis.interval(minutes=2))\n\n┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃ TimestampBucket(ts, 5m, 2m) ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp                   │\n├─────────────────────────────┤\n│ 2020-04-15 08:02:00         │\n│ 2020-04-15 08:02:00         │\n│ 2020-04-15 08:07:00         │\n│ 2020-04-15 08:07:00         │\n└─────────────────────────────┘\n\n\n\nOne common use of timestamp bucketing is computing statistics per bucket. Here we compute the mean of val across 5 minute intervals:\n\n&gt;&gt;&gt; mean_by_bucket = (\n...     t.group_by(t.ts.bucket(minutes=5).name(\"bucket\"))\n...     .agg(mean=_.val.mean())\n...     .order_by(\"bucket\")\n... )\n&gt;&gt;&gt; mean_by_bucket\n\n┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━┓\n┃ bucket              ┃ mean    ┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━┩\n│ timestamp           │ float64 │\n├─────────────────────┼─────────┤\n│ 2020-04-15 08:00:00 │     1.0 │\n│ 2020-04-15 08:05:00 │     2.5 │\n│ 2020-04-15 08:10:00 │     4.0 │\n└─────────────────────┴─────────┘\n\n\n\n\n\n\ndate\ndate()\nReturn the date component of the expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDateValue\nThe date component of self\n\n\n\n\n\n\ndelta\ndelta(other, part)\nCompute the number of parts between two timestamps.\n\n\n\n\n\n\nThe order of operands matches standard subtraction\n\n\n\nThe second argument is subtracted from the first.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\ndatetime.datetime | Value[dt.Timestamp]\nA timestamp expression\nrequired\n\n\npart\nLiteral[‘year’, ‘quarter’, ‘month’, ‘week’, ‘day’, ‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’, ‘nanosecond’] | Value[dt.String]\nThe unit of time to compute the difference in\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe number of parts between self and other\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; start = ibis.time(\"01:58:00\")\n&gt;&gt;&gt; end = ibis.time(\"23:59:59\")\n&gt;&gt;&gt; end.delta(start, \"hour\")\n\n\n\n\n\n22\n\n\n\n\n&gt;&gt;&gt; data = '''tpep_pickup_datetime,tpep_dropoff_datetime\n... 2016-02-01T00:23:56,2016-02-01T00:42:28\n... 2016-02-01T00:12:14,2016-02-01T00:21:41\n... 2016-02-01T00:43:24,2016-02-01T00:46:14\n... 2016-02-01T00:55:11,2016-02-01T01:24:34\n... 2016-02-01T00:11:13,2016-02-01T00:16:59'''\n&gt;&gt;&gt; with open(\"/tmp/triptimes.csv\", \"w\") as f:\n...     nbytes = f.write(data)  # nbytes is unused\n...\n&gt;&gt;&gt; taxi = ibis.read_csv(\"/tmp/triptimes.csv\")\n&gt;&gt;&gt; ride_duration = taxi.tpep_dropoff_datetime.delta(\n...     taxi.tpep_pickup_datetime, \"minute\"\n... ).name(\"ride_minutes\")\n&gt;&gt;&gt; ride_duration\n\n┏━━━━━━━━━━━━━━┓\n┃ ride_minutes ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│           19 │\n│            9 │\n│            3 │\n│           29 │\n│            5 │\n└──────────────┘\n\n\n\n\n\n\nstrftime\nstrftime(format_str)\nFormat a timestamp according to format_str.\nFormat string may depend on the backend, but we try to conform to ANSI strftime.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat_str\nstr\nstrftime format string\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nFormatted version of arg\n\n\n\n\n\n\ntruncate\ntruncate(unit)\nTruncate timestamp expression to units of unit.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\nLiteral[‘Y’, ‘Q’, ‘M’, ‘W’, ‘D’, ‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, ‘ns’]\nUnit to truncate to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nTruncated timestamp expression"
  },
  {
    "objectID": "reference/expression-temporal.html#attributes-1",
    "href": "reference/expression-temporal.html#attributes-1",
    "title": "Temporal expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdd an interval to a date.\n\n\nradd\nAdd an interval to a date.\n\n\nsub\nSubtract a date or an interval from a date."
  },
  {
    "objectID": "reference/expression-temporal.html#methods-1",
    "href": "reference/expression-temporal.html#methods-1",
    "title": "Temporal expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ndelta\nCompute the number of parts between two dates.\n\n\nstrftime\nFormat a date according to format_str.\n\n\ntruncate\nTruncate date expression to units of unit.\n\n\n\n\ndelta\ndelta(other, part)\nCompute the number of parts between two dates.\n\n\n\n\n\n\nThe order of operands matches standard subtraction\n\n\n\nThe second argument is subtracted from the first.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\ndatetime.date | Value[dt.Date]\nA date expression\nrequired\n\n\npart\nLiteral[‘year’, ‘quarter’, ‘month’, ‘week’, ‘day’] | Value[dt.String]\nThe unit of time to compute the difference in\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe number of parts between self and other\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; start = ibis.date(\"1992-09-30\")\n&gt;&gt;&gt; end = ibis.date(\"1992-10-01\")\n&gt;&gt;&gt; end.delta(start, \"day\")\n\n\n\n\n\n1\n\n\n\n\n&gt;&gt;&gt; prez = ibis.examples.presidential.fetch()\n&gt;&gt;&gt; prez.mutate(\n...     years_in_office=prez.end.delta(prez.start, \"year\"),\n...     hours_in_office=prez.end.delta(prez.start, \"hour\"),\n... ).drop(\"party\")\n\n┏━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃ name       ┃ start      ┃ end        ┃ years_in_office ┃ hours_in_office ┃\n┡━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ string     │ date       │ date       │ int64           │ int64           │\n├────────────┼────────────┼────────────┼─────────────────┼─────────────────┤\n│ Eisenhower │ 1953-01-20 │ 1961-01-20 │               8 │           70128 │\n│ Kennedy    │ 1961-01-20 │ 1963-11-22 │               2 │           24864 │\n│ Johnson    │ 1963-11-22 │ 1969-01-20 │               6 │           45264 │\n│ Nixon      │ 1969-01-20 │ 1974-08-09 │               5 │           48648 │\n│ Ford       │ 1974-08-09 │ 1977-01-20 │               3 │           21480 │\n│ Carter     │ 1977-01-20 │ 1981-01-20 │               4 │           35064 │\n│ Reagan     │ 1981-01-20 │ 1989-01-20 │               8 │           70128 │\n│ Bush       │ 1989-01-20 │ 1993-01-20 │               4 │           35064 │\n│ Clinton    │ 1993-01-20 │ 2001-01-20 │               8 │           70128 │\n│ Bush       │ 2001-01-20 │ 2009-01-20 │               8 │           70128 │\n│ …          │ …          │ …          │               … │               … │\n└────────────┴────────────┴────────────┴─────────────────┴─────────────────┘\n\n\n\n\n\n\nstrftime\nstrftime(format_str)\nFormat a date according to format_str.\nFormat string may depend on the backend, but we try to conform to ANSI strftime.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat_str\nstr\nstrftime format string\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nFormatted version of arg\n\n\n\n\n\n\ntruncate\ntruncate(unit)\nTruncate date expression to units of unit.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\nLiteral[‘Y’, ‘Q’, ‘M’, ‘W’, ‘D’]\nUnit to truncate arg to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nDateValue\nTruncated date value expression"
  },
  {
    "objectID": "reference/expression-temporal.html#attributes-2",
    "href": "reference/expression-temporal.html#attributes-2",
    "title": "Temporal expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\nadd\nAdd an interval to a time expression.\n\n\nradd\nAdd an interval to a time expression.\n\n\nsub\nSubtract a time or an interval from a time expression."
  },
  {
    "objectID": "reference/expression-temporal.html#methods-2",
    "href": "reference/expression-temporal.html#methods-2",
    "title": "Temporal expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ndelta\nCompute the number of parts between two times.\n\n\nstrftime\nFormat a time according to format_str.\n\n\ntruncate\nTruncate the expression to a time expression in units of unit.\n\n\n\n\ndelta\ndelta(other, part)\nCompute the number of parts between two times.\n\n\n\n\n\n\nThe order of operands matches standard subtraction\n\n\n\nThe second argument is subtracted from the first.\n\n\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nother\ndatetime.time | Value[dt.Time]\nA time expression\nrequired\n\n\npart\nLiteral[‘hour’, ‘minute’, ‘second’, ‘millisecond’, ‘microsecond’, ‘nanosecond’] | Value[dt.String]\nThe unit of time to compute the difference in\nrequired\n\n\n\n\n\nReturns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe number of parts between self and other\n\n\n\n\n\nExamples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n&gt;&gt;&gt; start = ibis.time(\"01:58:00\")\n&gt;&gt;&gt; end = ibis.time(\"23:59:59\")\n&gt;&gt;&gt; end.delta(start, \"hour\")\n\n\n\n\n\n22\n\n\n\n\n&gt;&gt;&gt; data = '''tpep_pickup_datetime,tpep_dropoff_datetime\n... 2016-02-01T00:23:56,2016-02-01T00:42:28\n... 2016-02-01T00:12:14,2016-02-01T00:21:41\n... 2016-02-01T00:43:24,2016-02-01T00:46:14\n... 2016-02-01T00:55:11,2016-02-01T01:24:34\n... 2016-02-01T00:11:13,2016-02-01T00:16:59'''\n&gt;&gt;&gt; with open(\"/tmp/triptimes.csv\", \"w\") as f:\n...     nbytes = f.write(data)  # nbytes is unused\n...\n&gt;&gt;&gt; taxi = ibis.read_csv(\"/tmp/triptimes.csv\")\n&gt;&gt;&gt; ride_duration = (\n...     taxi.tpep_dropoff_datetime.time()\n...     .delta(taxi.tpep_pickup_datetime.time(), \"minute\")\n...     .name(\"ride_minutes\")\n... )\n&gt;&gt;&gt; ride_duration\n\n┏━━━━━━━━━━━━━━┓\n┃ ride_minutes ┃\n┡━━━━━━━━━━━━━━┩\n│ int64        │\n├──────────────┤\n│           19 │\n│            9 │\n│            3 │\n│           29 │\n│            5 │\n└──────────────┘\n\n\n\n\n\n\nstrftime\nstrftime(format_str)\nFormat a time according to format_str.\nFormat string may depend on the backend, but we try to conform to ANSI strftime.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nformat_str\nstr\nstrftime format string\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nFormatted version of arg\n\n\n\n\n\n\ntruncate\ntruncate(unit)\nTruncate the expression to a time expression in units of unit.\nCommonly used for time series resampling.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nunit\nLiteral[‘h’, ‘m’, ‘s’, ‘ms’, ‘us’, ‘ns’]\nThe unit to truncate to\nrequired\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nTimeValue\nself truncated to unit"
  },
  {
    "objectID": "reference/expression-temporal.html#attributes-3",
    "href": "reference/expression-temporal.html#attributes-3",
    "title": "Temporal expressions",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nDescription\n\n\n\n\ndays\nExtract the number of days from an interval.\n\n\nhours\nExtract the number of hours from an interval.\n\n\nmicroseconds\nExtract the number of microseconds from an interval.\n\n\nmilliseconds\nExtract the number of milliseconds from an interval.\n\n\nminutes\nExtract the number of minutes from an interval.\n\n\nmonths\nExtract the number of months from an interval.\n\n\nnanoseconds\nExtract the number of nanoseconds from an interval.\n\n\nquarters\nExtract the number of quarters from an interval.\n\n\nseconds\nExtract the number of seconds from an interval.\n\n\nweeks\nExtract the number of weeks from an interval.\n\n\nyears\nExtract the number of years from an interval."
  },
  {
    "objectID": "reference/expression-temporal.html#methods-3",
    "href": "reference/expression-temporal.html#methods-3",
    "title": "Temporal expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nnegate\nNegate an interval expression.\n\n\nto_unit\nConvert this interval to units of target_unit.\n\n\n\n\nnegate\nnegate()\nNegate an interval expression.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntervalValue\nA negated interval value expression\n\n\n\n\n\n\nto_unit\nto_unit(target_unit)\nConvert this interval to units of target_unit."
  },
  {
    "objectID": "reference/expression-temporal.html#methods-4",
    "href": "reference/expression-temporal.html#methods-4",
    "title": "Temporal expressions",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nfull_name\nGet the name of the day of the week.\n\n\nindex\nGet the index of the day of the week.\n\n\n\n\nfull_name\nfull_name()\nGet the name of the day of the week.\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nStringValue\nThe name of the day of the week\n\n\n\n\n\n\nindex\nindex()\nGet the index of the day of the week.\n\n\n\n\n\n\nIbis follows the pandas convention for day numbering: Monday = 0 and Sunday = 6.\n\n\n\n\n\n\n\nReturns\n\n\n\nType\nDescription\n\n\n\n\nIntegerValue\nThe index of the day of the week."
  },
  {
    "objectID": "reference/expression-temporal.html#returns-14",
    "href": "reference/expression-temporal.html#returns-14",
    "title": "Temporal expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTimestampScalar\nAn expression representing the current timestamp."
  },
  {
    "objectID": "reference/expression-temporal.html#parameters-10",
    "href": "reference/expression-temporal.html#parameters-10",
    "title": "Temporal expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue_or_year\n\nEither a string value or datetime.date to coerce to a date, or an integral value representing the date year component.\nrequired\n\n\nmonth\n\nThe date month component; required if value_or_year is a year.\nNone\n\n\nday\n\nThe date day component; required if value_or_year is a year.\nNone"
  },
  {
    "objectID": "reference/expression-temporal.html#returns-15",
    "href": "reference/expression-temporal.html#returns-15",
    "title": "Temporal expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nDateValue\nA date expression"
  },
  {
    "objectID": "reference/expression-temporal.html#examples-4",
    "href": "reference/expression-temporal.html#examples-4",
    "title": "Temporal expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n\nCreate a date scalar from a string\n\n&gt;&gt;&gt; ibis.date(\"2023-01-02\")\n\n\n\n\n\nTimestamp('2023-01-02 00:00:00')\n\n\n\nCreate a date scalar from year, month, and day\n\n&gt;&gt;&gt; ibis.date(2023, 1, 2)\n\n\n\n\n\nTimestamp('2023-01-02 00:00:00')\n\n\n\nCreate a date column from year, month, and day\n\n&gt;&gt;&gt; t = ibis.memtable({\"y\": [2001, 2002], \"m\": [1, 3], \"d\": [2, 4]})\n&gt;&gt;&gt; ibis.date(t.y, t.m, t.d).name(\"date\")\n\n┏━━━━━━━━━━━━┓\n┃ date       ┃\n┡━━━━━━━━━━━━┩\n│ date       │\n├────────────┤\n│ 2001-01-02 │\n│ 2002-03-04 │\n└────────────┘"
  },
  {
    "objectID": "reference/expression-temporal.html#parameters-11",
    "href": "reference/expression-temporal.html#parameters-11",
    "title": "Temporal expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue_or_hour\n\nEither a string value or datetime.time to coerce to a time, or an integral value representing the time hour component.\nrequired\n\n\nminute\n\nThe time minute component; required if value_or_hour is an hour.\nNone\n\n\nsecond\n\nThe time second component; required if value_or_hour is an hour.\nNone"
  },
  {
    "objectID": "reference/expression-temporal.html#returns-16",
    "href": "reference/expression-temporal.html#returns-16",
    "title": "Temporal expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nTimeValue\nA time expression"
  },
  {
    "objectID": "reference/expression-temporal.html#examples-5",
    "href": "reference/expression-temporal.html#examples-5",
    "title": "Temporal expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n\nCreate a time scalar from a string\n\n&gt;&gt;&gt; ibis.time(\"01:02:03\")\n\n\n\n\n\ndatetime.time(1, 2, 3)\n\n\n\nCreate a time scalar from hour, minute, and second\n\n&gt;&gt;&gt; ibis.time(1, 2, 3)\n\n\n\n\n\ndatetime.time(1, 2, 3)\n\n\n\nCreate a time column from hour, minute, and second\n\n&gt;&gt;&gt; t = ibis.memtable({\"h\": [1, 4], \"m\": [2, 5], \"s\": [3, 6]})\n&gt;&gt;&gt; ibis.time(t.h, t.m, t.s).name(\"time\")\n\n┏━━━━━━━━━━┓\n┃ time     ┃\n┡━━━━━━━━━━┩\n│ time     │\n├──────────┤\n│ 01:02:03 │\n│ 04:05:06 │\n└──────────┘"
  },
  {
    "objectID": "reference/expression-temporal.html#parameters-12",
    "href": "reference/expression-temporal.html#parameters-12",
    "title": "Temporal expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue_or_year\n\nEither a string value or datetime.datetime to coerce to a timestamp, or an integral value representing the timestamp year component.\nrequired\n\n\nmonth\n\nThe timestamp month component; required if value_or_year is a year.\nNone\n\n\nday\n\nThe timestamp day component; required if value_or_year is a year.\nNone\n\n\nhour\n\nThe timestamp hour component; required if value_or_year is a year.\nNone\n\n\nminute\n\nThe timestamp minute component; required if value_or_year is a year.\nNone\n\n\nsecond\n\nThe timestamp second component; required if value_or_year is a year.\nNone\n\n\ntimezone\n\nThe timezone name, or none for a timezone-naive timestamp.\nNone"
  },
  {
    "objectID": "reference/expression-temporal.html#returns-17",
    "href": "reference/expression-temporal.html#returns-17",
    "title": "Temporal expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nTimestampValue\nA timestamp expression"
  },
  {
    "objectID": "reference/expression-temporal.html#examples-6",
    "href": "reference/expression-temporal.html#examples-6",
    "title": "Temporal expressions",
    "section": "Examples",
    "text": "Examples\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.options.interactive = True\n\nCreate a timestamp scalar from a string\n\n&gt;&gt;&gt; ibis.timestamp(\"2023-01-02T03:04:05\")\n\n\n\n\n\nTimestamp('2023-01-02 03:04:05')\n\n\n\nCreate a timestamp scalar from components\n\n&gt;&gt;&gt; ibis.timestamp(2023, 1, 2, 3, 4, 5)\n\n\n\n\n\nTimestamp('2023-01-02 03:04:05')\n\n\n\nCreate a timestamp column from components\n\n&gt;&gt;&gt; t = ibis.memtable({\"y\": [2001, 2002], \"m\": [1, 4], \"d\": [2, 5], \"h\": [3, 6]})\n&gt;&gt;&gt; ibis.timestamp(t.y, t.m, t.d, t.h, 0, 0).name(\"timestamp\")\n\n┏━━━━━━━━━━━━━━━━━━━━━┓\n┃ timestamp           ┃\n┡━━━━━━━━━━━━━━━━━━━━━┩\n│ timestamp           │\n├─────────────────────┤\n│ 2001-01-02 03:00:00 │\n│ 2002-04-05 06:00:00 │\n└─────────────────────┘"
  },
  {
    "objectID": "reference/expression-temporal.html#parameters-13",
    "href": "reference/expression-temporal.html#parameters-13",
    "title": "Temporal expressions",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nvalue\nint | datetime.timedelta | None\nInterval value.\nNone\n\n\nunit\nstr\nUnit of value\n's'\n\n\nyears\nint | None\nNumber of years\nNone\n\n\nquarters\nint | None\nNumber of quarters\nNone\n\n\nmonths\nint | None\nNumber of months\nNone\n\n\nweeks\nint | None\nNumber of weeks\nNone\n\n\ndays\nint | None\nNumber of days\nNone\n\n\nhours\nint | None\nNumber of hours\nNone\n\n\nminutes\nint | None\nNumber of minutes\nNone\n\n\nseconds\nint | None\nNumber of seconds\nNone\n\n\nmilliseconds\nint | None\nNumber of milliseconds\nNone\n\n\nmicroseconds\nint | None\nNumber of microseconds\nNone\n\n\nnanoseconds\nint | None\nNumber of nanoseconds\nNone"
  },
  {
    "objectID": "reference/expression-temporal.html#returns-18",
    "href": "reference/expression-temporal.html#returns-18",
    "title": "Temporal expressions",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nIntervalScalar\nAn interval expression"
  },
  {
    "objectID": "reference/ContextAdjustment.html",
    "href": "reference/ContextAdjustment.html",
    "title": "ContextAdjustment",
    "section": "",
    "text": "ContextAdjustment()\nOptions related to time context adjustment.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ntime_col\nstr\nName of the timestamp column for execution with a timecontext. See ibis/expr/timecontext.py for details."
  },
  {
    "objectID": "reference/ContextAdjustment.html#attributes",
    "href": "reference/ContextAdjustment.html#attributes",
    "title": "ContextAdjustment",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ntime_col\nstr\nName of the timestamp column for execution with a timecontext. See ibis/expr/timecontext.py for details."
  },
  {
    "objectID": "reference/connection.html",
    "href": "reference/connection.html",
    "title": "Top-level connection APIs",
    "section": "",
    "text": "Create and manage backend connections."
  },
  {
    "objectID": "reference/connection.html#parameters",
    "href": "reference/connection.html#parameters",
    "title": "Top-level connection APIs",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nresource\nPath | str\nA URL or path to the resource to be connected to.\nrequired\n\n\nkwargs\nAny\nBackend specific keyword arguments\n{}"
  },
  {
    "objectID": "reference/connection.html#examples",
    "href": "reference/connection.html#examples",
    "title": "Top-level connection APIs",
    "section": "Examples",
    "text": "Examples\nConnect to an in-memory DuckDB database:\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; con = ibis.connect(\"duckdb://\")\n\nConnect to an on-disk SQLite database:\n\n&gt;&gt;&gt; con = ibis.connect(\"sqlite://relative.db\")\n&gt;&gt;&gt; con = ibis.connect(\"sqlite:///absolute/path/to/data.db\")\n\nConnect to a PostgreSQL server:\n&gt;&gt;&gt; con = ibis.connect(\n...     \"postgres://user:password@hostname:5432\"\n... )  # quartodoc: +SKIP\nConnect to BigQuery:\n&gt;&gt;&gt; con = ibis.connect(\n...     \"bigquery://my-project/my-dataset\"\n... )  # quartodoc: +SKIP"
  },
  {
    "objectID": "reference/connection.html#returns",
    "href": "reference/connection.html#returns",
    "title": "Top-level connection APIs",
    "section": "Returns",
    "text": "Returns\n\n\n\nType\nDescription\n\n\n\n\nBaseBackend\nThe Ibis backend."
  },
  {
    "objectID": "reference/connection.html#parameters-1",
    "href": "reference/connection.html#parameters-1",
    "title": "Top-level connection APIs",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbackend\nstr | BaseBackend\nMay be a backend name or URL, or an existing backend instance.\nrequired"
  },
  {
    "objectID": "reference/connection.html#examples-1",
    "href": "reference/connection.html#examples-1",
    "title": "Top-level connection APIs",
    "section": "Examples",
    "text": "Examples\nYou can pass the backend as a name:\n\n&gt;&gt;&gt; import ibis\n&gt;&gt;&gt; ibis.set_backend(\"polars\")\n\nOr as a URI\n&gt;&gt;&gt; ibis.set_backend(\n...     \"postgres://user:password@hostname:5432\"\n... )  # quartodoc: +SKIP\nOr as an existing backend instance\n\n&gt;&gt;&gt; ibis.set_backend(ibis.duckdb.connect())"
  },
  {
    "objectID": "reference/Options.html",
    "href": "reference/Options.html",
    "title": "Options",
    "section": "",
    "text": "Options()\nIbis configuration options.\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\ninteractive\nbool\nShow the first few rows of computing an expression when in a repl.\n\n\nrepr\nRepr\nOptions controlling expression printing.\n\n\nverbose\nbool\nRun in verbose mode if True\n\n\nverbose_log\nCallable[[str], None] | None\nA callable to use when logging.\n\n\ngraphviz_repr\nbool\nRender expressions as GraphViz PNGs when running in a Jupyter notebook.\n\n\ndefault_backend\nOptional[ibis.backends.base.BaseBackend], default None\nThe default backend to use for execution, defaults to DuckDB if not set.\n\n\ncontext_adjustment\nContextAdjustment\nOptions related to time context adjustment.\n\n\nsql\nSQL\nSQL-related options.\n\n\nclickhouse\nConfig | None\nClickhouse specific options.\n\n\ndask\nConfig | None\nDask specific options.\n\n\nimpala\nConfig | None\nImpala specific options.\n\n\npandas\nConfig | None\nPandas specific options.\n\n\npyspark\nConfig | None\nPySpark specific options."
  },
  {
    "objectID": "reference/Options.html#attributes",
    "href": "reference/Options.html#attributes",
    "title": "Options",
    "section": "",
    "text": "Name\nType\nDescription\n\n\n\n\ninteractive\nbool\nShow the first few rows of computing an expression when in a repl.\n\n\nrepr\nRepr\nOptions controlling expression printing.\n\n\nverbose\nbool\nRun in verbose mode if True\n\n\nverbose_log\nCallable[[str], None] | None\nA callable to use when logging.\n\n\ngraphviz_repr\nbool\nRender expressions as GraphViz PNGs when running in a Jupyter notebook.\n\n\ndefault_backend\nOptional[ibis.backends.base.BaseBackend], default None\nThe default backend to use for execution, defaults to DuckDB if not set.\n\n\ncontext_adjustment\nContextAdjustment\nOptions related to time context adjustment.\n\n\nsql\nSQL\nSQL-related options.\n\n\nclickhouse\nConfig | None\nClickhouse specific options.\n\n\ndask\nConfig | None\nDask specific options.\n\n\nimpala\nConfig | None\nImpala specific options.\n\n\npandas\nConfig | None\nPandas specific options.\n\n\npyspark\nConfig | None\nPySpark specific options."
  }
]