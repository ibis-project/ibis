services:
  clickhouse:
    image: clickhouse/clickhouse-server:23.11.2.11-alpine
    ports:
      - 8123:8123 # http port
      - 9000:9000 # native protocol port
    healthcheck:
      interval: 1s
      retries: 10
      test:
        - CMD-SHELL
        - wget -qO- 'http://localhost:8123/?query=SELECT%201' # SELECT 1
    volumes:
      - clickhouse:/var/lib/clickhouse/user_files/ibis
    networks:
      - clickhouse

  impala:
    depends_on:
      - impala-postgres
      - kudu
      - kudu-tserver
    environment:
      PGPASSWORD: postgres
    healthcheck:
      interval: 1s
      retries: 60
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 21050 && nc -z 127.0.0.1 50070
    hostname: localhost
    image: ibisproject/impala:latest
    ports:
      - 50070:50070 # namenode http (hdfs)
      - 50075:50075 # datanode http (hdfs)
      - 8020:8020 # namenode metadata (hdfs)
      - 21050:21050 # hiveserver2 (impala)
    networks:
      - impala

  impala-postgres:
    user: postgres
    hostname: postgres
    environment:
      POSTGRES_PASSWORD: postgres
    healthcheck:
      interval: 1s
      retries: 10
      test:
        - CMD
        - pg_isready
    image: postgres:13.13-alpine
    networks:
      - impala

  kudu:
    cap_add:
      - SYS_TIME
    image: apache/kudu:1.17.0
    networks:
      - impala
    command: kudu master run --fs_wal_dir=/var/lib/kudu/master --fs_data_dirs=/var/lib/kudu/master
    healthcheck:
      interval: 1s
      retries: 60
      test:
        - CMD-SHELL
        - kudu cluster ksck kudu:7051

  kudu-tserver:
    cap_add:
      - SYS_TIME
    image: apache/kudu:1.17.0
    depends_on:
      - kudu # tablet server won't start if it can't find the master kudu node
    networks:
      - impala
    command: kudu tserver run --fs_wal_dir=/var/lib/kudu/master --fs_data_dirs=/var/lib/kudu/master --tserver_master_addrs=kudu
    healthcheck:
      interval: 1s
      retries: 60
      test:
        - CMD-SHELL
        - kudu cluster ksck kudu:7051

  mysql:
    environment:
      MYSQL_ALLOW_EMPTY_PASSWORD: "true"
      MYSQL_DATABASE: ibis_testing
      MYSQL_PASSWORD: ibis
      MYSQL_USER: ibis
    healthcheck:
      interval: 1s
      retries: 20
      test:
        - CMD
        - mariadb-admin
        - ping
    image: mariadb:10.11.6
    ports:
      - 3306:3306
    networks:
      - mysql
    volumes:
      - mysql:/data
      - $PWD/docker/mysql:/docker-entrypoint-initdb.d:ro

  postgres:
    user: postgres
    environment:
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: ibis_testing
      POSTGRES_USER: postgres
    build: ./docker/postgres
    image: ibis-postgres
    healthcheck:
      interval: 1s
      retries: 20
      test:
        - CMD
        - pg_isready
    ports:
      - 5432:5432
    networks:
      - postgres
    volumes:
      - postgres:/data

  mssql:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      MSSQL_SA_PASSWORD: 1bis_Testing!
      ACCEPT_EULA: "Y"
    healthcheck:
      interval: 1s
      retries: 20
      test:
        - CMD-SHELL
        - /opt/mssql-tools/bin/sqlcmd -S localhost -U sa -P "$$MSSQL_SA_PASSWORD" -Q "IF DB_ID('ibis_testing') IS NULL BEGIN CREATE DATABASE [ibis_testing] END"
    ports:
      - 1433:1433
    volumes:
      - mssql:/data
    networks:
      - mssql

  hive-metastore-db:
    image: postgres:16.1-alpine
    environment:
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: admin
      POSTGRES_DB: metastore
    healthcheck:
      interval: 1s
      retries: 20
      test:
        - CMD
        - pg_isready
        - --port=23456
    command: -c port=23456
    networks:
      - trino

  minio:
    # TODO: healthcheck?
    image: minio/minio:RELEASE.2023-11-01T18-37-25Z
    environment:
      MINIO_ROOT_USER: accesskey
      MINIO_ROOT_PASSWORD: secretkey
    entrypoint: sh
    command: -c 'mkdir -p /data/warehouse && minio server /data'
    healthcheck:
      interval: 1s
      retries: 20
      test:
        - CMD-SHELL
        - mc ping --count 1 trino
    networks:
      - trino
    volumes:
      - minio:/opt/data/raw
      - $PWD/docker/minio/config.json:/tmp/.mc/config.json:ro

  hive-metastore:
    # TODO: healthcheck?
    image: starburstdata/hive:3.1.3-e.4
    environment:
      HIVE_METASTORE_DRIVER: org.postgresql.Driver
      HIVE_METASTORE_JDBC_URL: jdbc:postgresql://hive-metastore-db:23456/metastore
      HIVE_METASTORE_USER: admin
      HIVE_METASTORE_PASSWORD: admin
      HIVE_METASTORE_WAREHOUSE_DIR: s3://warehouse/
      HIVE_METASTORE_USERS_IN_ADMIN_ROLE: "admin"
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: accesskey
      S3_SECRET_KEY: secretkey
      S3_PATH_STYLE_ACCESS: "true"
      REGION: ""
      GOOGLE_CLOUD_KEY_FILE_PATH: ""
      AZURE_ADL_CLIENT_ID: ""
      AZURE_ADL_CREDENTIAL: ""
      AZURE_ADL_REFRESH_URL: ""
      AZURE_ABFS_STORAGE_ACCOUNT: ""
      AZURE_ABFS_ACCESS_KEY: ""
      AZURE_WASB_STORAGE_ACCOUNT: ""
      AZURE_ABFS_OAUTH: ""
      AZURE_ABFS_OAUTH_TOKEN_PROVIDER: ""
      AZURE_ABFS_OAUTH_CLIENT_ID: ""
      AZURE_ABFS_OAUTH_SECRET: ""
      AZURE_ABFS_OAUTH_ENDPOINT: ""
      AZURE_WASB_ACCESS_KEY: ""
    depends_on:
      - hive-metastore-db
      - minio
    networks:
      - trino

  trino:
    depends_on:
      - hive-metastore
    healthcheck:
      interval: 2s
      retries: 15
      test:
        - CMD-SHELL
        - trino --output-format null --execute 'show schemas in hive; show schemas in memory'
    image: trinodb/trino:434
    ports:
      - 8080:8080
    networks:
      - trino
    volumes:
      - $PWD/docker/trino/catalog/memory.properties:/etc/trino/catalog/memory.properties:ro
      - $PWD/docker/trino/catalog/hive.properties:/etc/trino/catalog/hive.properties:ro
      - $PWD/docker/trino/jvm.config:/etc/trino/jvm.config:ro

  druid-postgres:
    image: postgres:16.1-alpine
    container_name: druid-postgres
    environment:
      POSTGRES_PASSWORD: FoolishPassword
      POSTGRES_USER: druid
      POSTGRES_DB: druid
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - pg_isready
    networks:
      - druid

  # Need 3.5 or later for container nodes
  druid-zookeeper:
    hostname: zookeeper
    container_name: zookeeper
    image: zookeeper:3.9
    environment:
      ZOO_MY_ID: 1
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 2181
    networks:
      - druid

  druid-coordinator:
    image: apache/druid:26.0.0
    hostname: coordinator
    container_name: coordinator
    volumes:
      - druid:/opt/shared
      - coordinator_var:/opt/druid/var
    depends_on:
      - druid-zookeeper
      - druid-postgres
    command:
      - coordinator
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 8081
    env_file:
      - ./docker/druid/environment
    networks:
      - druid

  druid-broker:
    image: apache/druid:26.0.0
    hostname: broker
    container_name: broker
    volumes:
      - broker_var:/opt/druid/var
    depends_on:
      - druid-zookeeper
      - druid-postgres
      - druid-coordinator
    command:
      - broker
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 8082
    ports:
      - 8082:8082
    env_file:
      - ./docker/druid/environment
    networks:
      - druid

  druid-historical:
    image: apache/druid:26.0.0
    hostname: historical
    container_name: historical
    volumes:
      - druid:/opt/shared
      - historical_var:/opt/druid/var
    depends_on:
      - druid-zookeeper
      - druid-postgres
      - druid-coordinator
    command:
      - historical
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 8083
    env_file:
      - ./docker/druid/environment
    networks:
      - druid

  druid-middlemanager:
    image: apache/druid:26.0.0
    hostname: middlemanager
    container_name: middlemanager
    volumes:
      - druid:/opt/shared
      - middle_var:/opt/druid/var
      - druid-data:/data
    depends_on:
      - druid-zookeeper
      - druid-postgres
      - druid-coordinator
    command:
      - middleManager
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 8091
    env_file:
      - ./docker/druid/environment
    networks:
      - druid

  druid:
    image: apache/druid:26.0.0
    hostname: router
    container_name: router
    volumes:
      - router_var:/opt/druid/var
    depends_on:
      - druid-zookeeper
      - druid-postgres
      - druid-coordinator
      - druid-middlemanager
      - druid-historical
      - druid-broker
    ports:
      - 8888:8888
    command:
      - router
    healthcheck:
      interval: 2s
      retries: 30
      test:
        - CMD-SHELL
        - nc -z 127.0.0.1 8888
    env_file:
      - ./docker/druid/environment
    networks:
      - druid

  oracle:
    image: gvenzl/oracle-free:23.3-slim
    environment:
      ORACLE_PASSWORD: ibis
      ORACLE_DATABASE: IBIS_TESTING
      APP_USER: ibis
      APP_USER_PASSWORD: ibis
    ports:
      - 1521:1521
    healthcheck:
      interval: 2s
      retries: 25
      test:
        - CMD-SHELL
        - ./healthcheck.sh
    restart: on-failure
    networks:
      - oracle
    volumes:
      - oracle:/opt/oracle/data

  exasol:
    image: exasol/docker-db:7.1.23
    privileged: true
    ports:
      - 8563:8563
    healthcheck:
      interval: 10s
      retries: 9
      timeout: 90s
      test:
        - CMD-SHELL
        - /usr/opt/EXASuite-7/EXASolution-7.1.23/bin/Console/exaplus -c 127.0.0.1:8563 -u sys -p exasol -encryption OFF <<< 'SELECT 1'
    networks:
      - exasol
    volumes:
      - exasol:/data

  flink-jobmanager:
    build: ./docker/flink
    image: ibis-flink
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
    ports:
      - 8081:8081
    command: jobmanager
    networks:
      - flink

  flink:
    build: ./docker/flink
    image: ibis-flink
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        taskmanager.numberOfTaskSlots: 2
        taskmanager.memory.process.size: 2048m
        taskmanager.memory.network.fraction: 0.4
        taskmanager.memory.network.min: 512mb
        taskmanager.memory.network.max: 2gb
    depends_on:
      - flink-jobmanager
    command: taskmanager
    networks:
      - flink

networks:
  impala:
  mysql:
  mssql:
  clickhouse:
  postgres:
  trino:
  druid:
  oracle:
  exasol:
  flink:

volumes:
  broker_var:
  coordinator_var:
  druid:
  historical_var:
  middle_var:
  router_var:
  # test data volumes
  clickhouse:
  druid-data:
  mssql:
  mysql:
  oracle:
  postgres:
  minio:
  exasol:
