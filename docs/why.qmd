---
title: "Why Ibis?"
---

Ibis defines a Python dataframe API that executes on any query engine -- the
frontend for any backend data platform, with 20+ backends today.  This allows
Ibis to have excellent performance -- as good as the backend it is connected to
-- with a consistent user experience.

## What is Ibis?

Ibis is the portable Python dataframe library.

We can demonstrate this with a simple example on a few local query engines:

```{python}
import ibis

ibis.options.interactive = True
```

::: {.panel-tabset}

```{python}
#| echo: false
t = ibis.examples.penguins.fetch()
t.to_parquet("penguins.parquet")
```

## DuckDB

```{python}
con = ibis.connect("duckdb://")  # <1>

t = con.read_parquet("penguins.parquet")
t.limit(3)
```

1. Change only your connection to switch between backends.

```{python}
t.group_by(["species", "island"]).agg(count=t.count()).order_by("count")
```

## Polars

```{python}
con = ibis.connect("polars://")  # <1>

t = con.read_parquet("penguins.parquet")
t.limit(3)
```

1. Change only your connection to switch between backends.

```{python}
t.group_by(["species", "island"]).agg(count=t.count()).order_by("count")
```

## DataFusion

```{python}
con = ibis.connect("datafusion://")  # <1>

t = con.read_parquet("penguins.parquet")
t.limit(3)
```

1. Change only your connection to switch between backends.

```{python}
t.group_by(["species", "island"]).agg(count=t.count()).order_by("count")
```

## PySpark

```{python}
con = ibis.connect("pyspark://")  # <1>

t = con.read_parquet("penguins.parquet")
t.limit(3)
```

1. Change only your connection to switch between backends.

```{python}
t.group_by(["species", "island"]).agg(count=t.count()).order_by("count")
```

:::

## Who is Ibis for?

Ibis is for data engineers, data analysts, and data scientists (or any title
that needs to work with data!) to use directly with their data platform(s) of
choice. It also has benefits for [data platforms](#ibis-for-data-platforms),
[organizations](#ibis-for-organizations), and [library
developers](#ibis-for-library-developers).

### Ibis for practitioners

You can use Ibis at any stage of your data workflow, no matter your role.

**Data engineers** can use Ibis to:

- write and maintain complex ETL/ELT jobs
- replace fragile SQL string pipelines with a robust Python API
- replace PySpark with a more Pythonic API that supports Spark and many other
 backends

**Data analysts** can use Ibis to:

- use Ibis interactive mode for rapid exploration
- perform rapid exploratory data analysis using interactive mode
- [create end-to-end analytics workflows](./posts/ibis-analytics/index.qmd)
- work in a general-purpose, yet easy to learn, programming language without the
 need for formatting SQL strings

**Data scientists** can use Ibis to:

- extract a sample of data for local iteration with a fast local backend
- prototype with the same API that will be used in production
- preprocess and feature engineer data before training a machine learning model

### Ibis for data platforms

Data platforms can use Ibis to quickly bring a fully-featured Python dataframe
library with minimal effort to their platform. In addition to a great Python
dataframe experience for their users, they also get integrations into the
[broader Python and ML ecosystem](#ecosystem).

Often, data platforms evolve to support Python in some sequence like:

1. Develop a fast query engine with a SQL frontend
2. Gain popularity and need to support Python for data science and ML use cases
3. Develop a bespoke pandas or PySpark-like dataframe library and ML
 integrations

This third step is where Ibis comes in. Instead of spending a lot of time and
money developing a bespoke Python dataframe library, you can create an Ibis
backend for your data platform [in as little as four hours for an experienced
Ibis
developer](https://voltrondata.com/resources/new-ibis-backend-shipped-in-four-hours-druid)
or, more typically, on the order of
[one](https://github.com/ibis-project/ibis/pull/7954) or
[two](https://github.com/ibis-project/ibis/pull/7303) months for a new
contributor.

::: {.callout-warning title="Why not the pandas or PySpark APIs?" collapse="true"}
The pandas API inherently does not scale due to its single-threaded design,
ordered index, and a lot of API baggage. The creator of pandas (and Ibis!) has
[talked about the issues with pandas
publicly](https://wesmckinney.com/blog/apache-arrow-pandas-internals/). While
there have been projects attempting to scale the pandas API, they always result
in a dubious support matrix. You can see that with
[Modin](https://modin.readthedocs.io/en/stable/supported_apis/dataframe_supported.html)
or [pandas on Spark (formerly known as
Koalas)](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html).

[Google BigQuery
DataFrames](https://github.com/googleapis/python-bigquery-dataframes) is a more
modern attempt to scale the pandas API built on top of Ibis. If you are going
to build a pandas API we recommend you take a look at this project.

PySpark is a great API for Spark, but not very Pythonic and tightly coupled to
the Spark execution engine.

Ibis takes inspiration from pandas and PySpark -- and R and SQL -- but is designed to be
scalable from the start. If offers a neutral,
[self-governed](https://github.com/ibis-project/governance) open source option
for your data platform.
:::

### Ibis for organizations

Organizations can use Ibis to standardize the interface for SQL and Python data
practitioners. It also allows organizations to:

- transfer data between systems
- transform, analyze, and prepare data where it lives
- benchmark your workload(s) across data systems using the same code
- mix SQL and Python code seamlessly, with all the benefits of a general-purpose
 programming language, type checking, and expression validation

### Ibis for library developers

Python developers creating libraries can use Ibis to:

- instantly support 20+ data backends
- instantly support pandas, PyArrow, and Polars objects
- read and write from all common file formats (depending on the backend)
- trace column-level lineage through Ibis expressions
- compile Ibis expressions to SQL or Substrait
- perform cross-dialect SQL transpilation (powered by
 [SQLGlot](https://github.com/tobymao/sqlglot))

## How does Ibis work?

Most Python dataframes are tightly coupled to their execution engine. And many
databases only support SQL, with no Python API. Ibis solves this problem by
providing a common API for data manipulation in Python, and compiling that API
into the backend's native language. This means you can learn a single API and
use it across any supported backend (execution engine).

Ibis broadly supports two types of backend:

1. SQL-generating backends
2. DataFrame-generating backends

```{python}
#| echo: false
from backends_sankey import fig

fig.show()
```

As you can see, most backends generate SQL. Ibis uses
[SQLGlot](https://github.com/tobymao/sqlglot) to transform Ibis expressions into
SQL strings. You can also use the
[`.sql()`](./how-to/extending/sql.qmd#table.sql) methods to mix in SQL strings,
compiling them to Ibis expressions.

While portability with Ibis isn't perfect, commonalities across backends and
SQL dialects combined with years of engineering effort produce a full-featured
and robust framework for data manipulation in Python.

In the long-term, we aim for a standard query plan Intermediate Representation
(IR) like [Substrait](https://substrait.io) to simplify this further.

## Python + SQL: better together

For most backends, Ibis works by compiling Python expressions into SQL:

```{python}
g = t.group_by(["species", "island"]).agg(count=t.count()).order_by("count")
ibis.to_sql(g)
```

You can mix and match Python and SQL code:

```{python}
sql = """
SELECT
  species,
  island,
  COUNT(*) AS count
FROM penguins
GROUP BY species, island
""".strip()
```

::: {.panel-tabset}

## DuckDB

```{python}
con = ibis.connect("duckdb://")
t = con.read_parquet("penguins.parquet")
g = t.alias("penguins").sql(sql)
g
```

```{python}
g.order_by("count")
```

## DataFusion

```{python}
con = ibis.connect("datafusion://")
t = con.read_parquet("penguins.parquet")
g = t.alias("penguins").sql(sql)
g
```

```{python}
g.order_by("count")
```

## PySpark

```{python}
con = ibis.connect("pyspark://")
t = con.read_parquet("penguins.parquet")
g = t.alias("penguins").sql(sql)
g
```

```{python}
g.order_by("count")
```

:::

This allows you to combine the flexibility of Python with the scale and
performance of modern SQL.

## Scaling up and out

Out of the box, Ibis offers a great local experience for working with many file
formats. You can scale up with DuckDB (the default backend) or choose from other
great options like Polars and DataFusion to work locally with large datasets.
Once you hit scaling issues on a local machine, you can continue scaling up with
a larger machine in the cloud using the same backend and same code.

If you hit scaling issues on a large single-node machine, you can switch to a
distributed backend like PySpark, BigQuery, or Trino by simply changing your
connection string.

## Stream-batch unification

As of [Ibis 8.0](./posts/ibis-version-8.0.0-release/index.qmd), the first stream
processing backends have been added. Since these systems tend to support SQL, we
can with minimal changes to Ibis support both batch and streaming workloads with
a single API. We aim to further unify the batch and streaming paradigms going
forward.

## Ecosystem

Ibis is part of a larger ecosystem of Python data tools. It is designed to work
well with other tools in this ecosystem, and we continue to make it easier to
use Ibis with other tools over time.

Ibis already works with other Python dataframes like:

- [pandas](https://github.com/pandas-dev/pandas)
- [Dask](https://github.com/dask/dask)
- [Polars](https://github.com/pola-rs/polars)

Ibis already works well with visualization libraries like:

- [matplotlib](https://github.com/matplotlib/matplotlib)
- [seaborn](https://github.com/mwaskom/seaborn)
- [Plotly](https://github.com/plotly/plotly.py)
- [Vega-Altair](https://github.com/altair-viz/altair)
- [plotnine](https://github.com/has2k1/plotnine)

Ibis already works well with dashboarding libraries like:

- [Streamlit](https://github.com/streamlit/streamlit)
- [Dash](https://github.com/plotly/dash)
- [Quarto dashboards](https://github.com/quarto-dev/quarto-cli)

Ibis already works well with machine learning libraries like:

- [scikit-learn](https://github.com/scikit-learn/scikit-learn)
- [XGBoost](https://github.com/dmlc/xgboost)
- [LightGBM](https://github.com/microsoft/lightgbm)
- [PyTorch](https://github.com/pytorch/pytorch)

## Supported backends

{{< include ./_tabsets/install.qmd >}}

See the [backend support matrix](./backends/support/matrix.qmd) for details on operations
supported. [Open a feature
request](https://github.com/ibis-project/ibis/issues/new?assignees=&labels=feature&projects=&template=feature-request.yml&title=feat)
if you'd like to see support for an operation in a given backend. If the backend
supports it, we'll do our best to add it quickly!

## Community

Community discussions primarily take place on
[GitHub](https://github.com/ibis-project/ibis/discussions) and
[Zulip](https://ibis-project.zulipchat.com).

## Getting started

If you're interested in trying Ibis we recommend the [getting started
tutorial](./tutorials/getting_started.qmd).
