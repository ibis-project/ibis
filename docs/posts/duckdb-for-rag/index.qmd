---
title: "Using DuckDB (or any database Ibis supports) for RAG"
author: "Cody Peterson"
date: "2024-02-29"
categories:
    - blog
    - llms
    - duckdb
---

## Overview

## What is RAG?

```{mermaid}
graph LR
    style T fill:#1E90FF,stroke:#1E90FF,color:#ffffff
    style N fill:#1E90FF,stroke:#1E90FF,color:#ffffff
    style C fill:#1E90FF,stroke:#1E90FF,color:#ffffff

    T("text") --> N("numbers (embedding vector)")
    N --> C("compute similarity metric")
```

## RAG over Ibis documentation

### Setup code

First, we need to install and import the necessary packages:

```bash
pip install 'ibis-framework[duckdb,examples]' openai python-dotenv
```

```{python}
import os
import ibis
import shutil
import subprocess

from openai import OpenAI
from dotenv import load_dotenv
```

We need to set the `OPENAI_API_KEY` environment variable to your OpenAI API
key. You can do this by creating a `.env` file in the same directory as your
code and adding the following line:

```
OPENAI_API_KEY=<your-api-key>
```

Then, you can load the environment variables with the following code:

```{python}
load_dotenv()
```

Now, we're ready to make LLM calls. Let's setup Ibis for interactive use:

```{python}
ibis.options.interactive = True
```

### Getting the data into DuckDB

While we're already running this code within the Ibis repository, we'll write a function to clone any repository:

```{python}
def clone_repo(repo_url):
    subprocess.run(["git", "clone", "--depth", "1", repo_url], check=False)
```

Now, we will clone `ibis` and prepare to ingest all the Quarto markdown files into a DuckDB database called `rag.ddb`:

```{python}
org_url = "https://github.com/ibis-project"
repo_name = "ibis"

shutil.rmtree(repo_name, ignore_errors=True)
clone_repo(f"{org_url}/{repo_name}")

if os.path.exists("rag.ddb"):
    os.remove("rag.ddb")
if os.path.exists("rag.ddb.wal"):
    os.remove("rag.ddb.wal")

rag_con = ibis.connect("duckdb://rag.ddb")
rag_con.list_tables()
```

We'll walk through all the cloned files and for `.qmd` files, read the contents and insertthem into the `ibis` table:

```{python}
for root, _, files in os.walk(repo_name):
    for file in files:
        if file.endswith(".qmd"):
            with open(os.path.join(root, file)) as f:
                contents = f.read()

                data = {
                    "filepath": [os.path.join(root, file)],
                    "contents": [contents],
                }

                t = ibis.memtable(data)

                if repo_name not in rag_con.list_tables():
                    rag_con.create_table(repo_name, t)
                else:
                    rag_con.insert(repo_name, t)

for table in rag_con.list_tables():
    if "memtable" in table:
        rag_con.drop_view(table)

rag_con.list_tables()
```

```{python}
#| code-fold: true
#| echo: false
# this is here to avoid building all the docs in this post...
_ = shutil.rmtree(repo_name, ignore_errors=True)
```


Let's inspect the table:

```{python}
t = rag_con.table(repo_name)
t
```

Now we have a DuckDB database with the `ibis` table containing all the file paths and contents of the Quarto markdown files in the `ibis` repository. You can adjust this code to work with any repository and any file type(s)! For instance, we may also want to ingest all the `.py` files in the Ibis repsoitory.

### Embedding text to vectors

To search for similar text in traditional RAG fashion, we need to **embed** the text into vectors. We can use [OpenAI's embedding models](https://platform.openai.com/docs/guides/embeddings/embedding-models) to perform this step.

Note that the maximum length of input **tokens** is 8,191. A word typically consists of 1-3 tokens. To estimate tokens of a string in Python, we can roughly divide the length of that string by 4. This isn't perfect, but will work for now. Let's augmented our data with the estimated number of tokens and sort from largest to smallest:

```{python}
def _estimate_tokens(text: str) -> int:
    return len(text) // 4


@ibis.udf.scalar.python
def estimate_tokens(text: str) -> int:
    return _estimate_tokens(text)
```

```{python}
t = (
    t.mutate(contents_tokens_estimate=estimate_tokens(t.contents))
    .order_by(ibis._["contents_tokens_estimate"].desc())
    .relocate("filepath", "contents_tokens_estimate")
)
t
```

The longest text is certainly over our embedding model limit, and the second longest might be as well. In practice, we'd want to chunk our text into smaller pieces. This is its own challenge, but for the purposes of this demonstration we'll just ignore text that is probably too long.

Let's define our embedding functions:

```{python}
def _embed(text: str) -> list[float]:
    """Text to fixed-length array embedding."""
    try:
        model = "text-embedding-3-small"
        text = text.replace("\n", " ")
        client = OpenAI()
        return client.embeddings.create(input=[text], model=model).data[0].embedding
    except Exception as e:
        print(e)
        return None


@ibis.udf.scalar.python
def embed(text: str, tokens_estimate: int) -> list[float]:
    """Text to fixed-length array embedding."""
    if 0 < tokens_estimate < 8000:
        return _embed(text)
    return None
```

Cosine similarity over vectors is a common way to measure similarity for RAG applications. We'll explore its downsides below.

Let's embed the text:

```{python}
#| code-fold: true
#| echo: false
_ = ibis.get_backend().raw_sql("set enable_progress_bar = false")
```

```{python}
t = t.mutate(
    contents_embedding=embed(t["contents"], t["contents_tokens_estimate"])
).cache()
t
```

Notice there was one error -- that second row's content was indeed too long! Our token estimate is a bit off, but overall useful. We accounted for this possibility in our Python UDF above, returning `None` (mapped to `NULL` in DuckDB) when the text is too long or an error occurs.

### Searching with cosine similarity

We'll make Ibis aware of DuckDB's built-in `list_cosine_similarity` function:

```{python}
@ibis.udf.scalar.builtin
def list_cosine_similarity(x, y) -> float:
    """Compute cosine similarity between two vectors."""
```

::: {.callout-tip}
If using a different backend, you'll need to use that backend's corresponding cosine similarity function or define your own.
:::

Now we can search for similar text:

```{python}
def search_text(text):
    embedding = _embed(text)

    s = (
        t.mutate(similarity=list_cosine_similarity(t.contents_embedding, embedding))
        .relocate("similarity")
        .order_by(ibis._["similarity"].desc())
        .cache()
    )

    return s
```

```{python}
text = "where can I chat with the community about Ibis?"
search_text(text)
```

```{python}
text = "what do users say about Ibis?"
search_text(text)
```

```{python}
text = "can Ibis complete the one billion row challenge?"
search_text(text)
```

```{python}
text = "teach me about the Polars backend"
search_text(text)
```

```{python}
text = "why does Voltron Data support Ibis?"
search_text(text)
```

```{python}
text = "why should I use Ibis?"
search_text(text)
```

```{python}
text = "what is the Ibis roadmap?"
search_text(text)
```

## RAG pitfalls

As in traditional software engineering and machine learning, the architecture when building systems around language models is not a monolith. The best solution is often "**it depends**." Using cosine (or otherwise) similarity search over text transformed to numbers is not a silver bullet.

We can demonstrate this with the following scenario: let's assume we're building a chatbot that writes and runs SQL on the user's behalf based on their questions posed in English (or any language the language model understand sufficiently). We might store past queries, the SQL generated, and some evaluation criteria (upvote/downvote on the generated SQL from the user) in a database. Then we can use the same RAG approach above to augment our queries with the user's past queries and the generated SQL.

Take the following English query:

```{python}
a = """the first 10 rows of the penguins, ordered by body_mass_g from
lightest to heaviest"""
print(a)
```

We can compute the cosine similarity of this query against itself, resulting in 1.0 or close to it:

```{python}
list_cosine_similarity(_embed(a), _embed(a))
```

Now, let's flip the ordering. This would result in an entirely different SQL query and result set:

```{python}
b = """the first 10 rows of the penguins, ordered by body_mass_g from
heaviest to lightest"""
print(b)
```

And compute the cosine similarity of `a` and `b`:

```{python}
list_cosine_similarity(_embed(a), _embed(b))
```

Similarly, we can construct a semantically equivalent query that would result in the same SQL as example `a`, but has a much lower cosine similarity in the embedding space:

```{python}
c = """of the birds retrieve the initial ten opposite-of-columns,
sorted from biggest to smallest by weight"""
print(c)
```

```{python}
list_cosine_similarity(_embed(a), _embed(c))
```

## Discussion

TODO:

- **it depends**: what are you trying to accomplish?
- bespoke solutions introduce tech debt
- DuckDB or similar are fine for millions of rows, perhaps low-billions
- RAG is a silly term
- Hamel quote?
- cost (very low for embeddings, <$0.01 for this blog) and speed
- alternative approaches: traditional search, llm search, etc.

## Next steps

Try this out yourself! All you need is an OpenAI account and the code above.

It's never been a better time to get involved with Ibis. [Join us on Zulip and
introduce yourself!](https://ibis-project.zulipchat.com/) More Ibis + language
model content coming soon.
