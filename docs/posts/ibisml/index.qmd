---
title: "Using IbisML for a kagge competition: credit risk model stability"
author: "Jiting Xu"
date: "2024-07-02"
categories:
    - blog
    - machine learning
    - feature engineering
execute:
  freeze: true
---

## Introduction
In this post, we'll demonstrate how to use Ibis and IbisML end-to-end for a Kaggle competition - "[Home Credit - Credit Risk Model Stability](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability)"
1. Load data and perform feature engineering with Ibis on a duckdb backend
2. Conduct last-mile ML data preprocessing with IbisML
3. Train two popular models using different frameworks:
    - Neural network with PyTorch and PyTorch Lightning
    - XGBoost

The aim of this competition is to predict which clients are more likely to default on their loans by leveraging both internal and external information available for each client. The dataset comprises numerous files due to the utilization of diverse data sources and varying levels of data aggregation during preparation.

To get started with Ibis and IbisML, please refer to the websites:
- [Ibis](https://ibis-project.org/): An open source dataframe library that works with any data system
- [IbisML](https://ibis-project.github.io/ibis-ml/): A library for building scalable ML pipelines


## Prerequisites
Before diving into the competition, ensure you have the necessary tools and environment set up. This includes downloading the dataset and installing the required libraries. Follow the instructions below to get started.

### Download data
To get started with the Home Credit - Credit Risk Model Stability competition, follow the steps below to download the necessary data:
1. Option 1: Manual Download
     - Log into your kaggle account and download all data from this [link](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data), unzip the files, and save them to your local disk.
2. Option 2: Kaggle API
    - Go to your Kaggle account settings: [Kaggle Account Settings](https://www.kaggle.com/account).
    - Under the "API" section, click on "Create New API Token". This will download the `kaggle.json` file to your computer.
    - Place the `kaggle.json` file in the correct directory, normally it is under your home directory `~/.kaggle`:
        ```bash
        mkdir ~/.kaggle
        mv ~/Downloads/kaggle.json ~/.kaggle
        ```
    - Install Kaggle CLI and download the data:
        ```bash
        pip install kaggle
        kaggle competitions download -c home-credit-credit-risk-model-stability
        unzip home-credit-credit-risk-model-stability.zip
        ```


### Install libraries

To work with Ibis and IbisML using the DuckDB backend, you'll need to install the necessary packages:
```python
pip install "ibis-framework[duckdb]" ibis-ml
```

Additionally, when building models, ensure you have the required libraries installed:

```python
# PyTorch
pip install --upgrade torch torchvision pytorch-lightning

# XGBoost
pip install xgboost
```

## Data loading and processing
We'll use Ibis with the DuckDB backend to load the Parquet files (with the same data also available in CSV format) and perform the necessary processing for the next step.

::: {.callout-tip}
You can perform the same process with a different backend, such as Polars, Dask, BigQuery, Spark, or other Ibis-supported backends. Simply move the datasets to the desired database and set the Ibis backend using these references: [Work with multiple backends](https://ibis-project.org/how-to/input-output/multiple-backends).
:::


### Directory structure and tables

Since there are many data files, let's start by examining the directory structure and tables within the train directory, focusing on the Parquet files:

```bash
# Change this to your directory
tree -L 2 /Users/claypot/Downloads/home-credit-credit-risk-model-stability/parquet_files/train
```

The output will show:

```bash
/Users/claypot/Downloads/home-credit-credit-risk-model-stability/parquet_files/train
├── train_applprev_1_0.parquet
├── train_applprev_1_1.parquet
├── train_applprev_2.parquet
├── train_base.parquet
├── train_credit_bureau_a_1_0.parquet
├── train_credit_bureau_a_1_1.parquet
├── train_credit_bureau_a_1_3.parquet
├── train_credit_bureau_a_2_0.parquet
├── train_credit_bureau_a_2_1.parquet
├── train_credit_bureau_a_2_10.parquet
├── train_credit_bureau_a_2_2.parquet
├── train_credit_bureau_a_2_3.parquet
├── train_credit_bureau_a_2_4.parquet
├── train_credit_bureau_a_2_5.parquet
├── train_credit_bureau_a_2_6.parquet
├── train_credit_bureau_a_2_7.parquet
├── train_credit_bureau_a_2_8.parquet
├── train_credit_bureau_a_2_9.parquet
├── train_credit_bureau_b_1.parquet
├── train_credit_bureau_b_2.parquet
├── train_debitcard_1.parquet
├── train_deposit_1.parquet
├── train_other_1.parquet
├── train_person_1.parquet
├── train_person_2.parquet
├── train_static_0_0.parquet
├── train_static_0_1.parquet
├── train_static_cb_0.parquet
├── train_tax_registry_a_1.parquet
├── train_tax_registry_b_1.parquet
└── train_tax_registry_c_1.parquet
```
The `train_base.parquet` file is the base table, while the others are feature tables. Let's take a quick look at these tables.

#### Base table
The base table (`train_base.parquet`) contains the unique id, a binary target flag and other information for the training samples. This unique ID will serve as the linking key for joining with other feature tables.
- `case_id` - This is the unique identifier for each credit case. You'll need this ID to join relevant tables to the base table. There are about 1.5m unique loans.
- `date_decision` - This refers to the date when a decision was made regarding the approval of the loan.
- `WEEK_NUM` - This is the week number used for aggregation. In the test sample, `WEEK_NUM` continues sequentially from the last training value of `WEEK_NUM`.
- `MONTH` - This column represents the month and is intended for aggregation purposes.
- `target` - This is the target flag, determined after a certain period based on whether or not the client defaulted on the specific loan.

#### Feature tables

The remaining files contain feature data, consisting of approximately 450 unique features extracted from previous loan applications and external data sources. All features are usable as predictors, their definitions can be found in the file `feature_definitions.csv` from this [link](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data).

::: {.callout-note}
There are several things we want to mention:
1. **Union Datasets**: One dataset could be saved into multiple parquet files, such as `train_applprev_1_0.parquet` and `train_applprev_1_1.parquet`, We need to combine these files before moving forward.
2. **Dataset Levels**: Datasets may have different levels, which we will explain as follows:
     - **Depth 0**: For depth=0 tables, predictors can be directly joined with the base table and used as features.
     - **Depth > 0**: Tables with a depth greater than 0 require aggregation using `case_id`, which will condense the historical records associated with each `case_id` into a single feature.
:::

Here are several examples of tables with different levels.

```{python}
# Import libraries
import ibis
import ibis.expr.datatypes as dt
from ibis import _
import ibis_ml as ml
from pathlib import Path
from glob import glob

# enable interactive mode for ibis
ibis.options.interactive = True
```

```{python}
# Change the root path to yours
ROOT            = Path("/Users/claypot/Downloads/home-credit-credit-risk-model-stability")
TRAIN_DIR       = ROOT / "parquet_files" / "train"
TEST_DIR        = ROOT / "parquet_files" / "test"
```

```{python}
# Example of table with depth = 0
# Predictors can be directly joined with the base table and used as features.
ibis.read_parquet(TRAIN_DIR / "train_static_cb_0.parquet").head()
```

```{python}
# Example of a table with depth = 1
# We need to aggregate the features and collect statistics based on `case_id` and then join with the base table to use them as features
ibis.read_parquet(TRAIN_DIR / "train_credit_bureau_b_1.parquet").relocate("num_group1").order_by(["case_id", "num_group1"]).head()
```

For more details on features and its exploratory data analysis (EDA), you can refer to feature dictionary and these kaggle notebooks:
- [Feature dictionary](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/data#:~:text=calendar_view_week-,feature_definitions,-.csv)
- [Home credit risk prediction EDA](https://www.kaggle.com/code/loki97/home-credit-risk-prediction-eda)
- [Home credit CRMS 2024 EDA](https://www.kaggle.com/code/sergiosaharovskiy/home-credit-crms-2024-eda-and-submission)

### Data loading and processing

We will perform the following data processing steps using Ibis and IbisML:

- **Convert Data Types**: Ensure consistency by converting data types, as the same features in different sub-files may have different types.
- **Aggregate Features**: For tables with depth greater than 0, aggregate features based on `case_id`, including statistics calculation. You can collect statistics such as mean, median, mode, minimum, standard deviation, and others based on your needs.
- **Union and Join Datasets**: Combine multiple sub-files of the same dataset into one table by appending them, as some datasets are split into multiple sub-files with a common prefix. Afterward, join these tables with the base table to consolidate the data.

#### Convert data types

We'll use IbisML to create a series of `Cast` steps, forming a recipe for data type conversion across the dataset. This conversion is based on the provided information extracted from column names. Predictors that underwent similar transformations are indicated by a capital letter at the end of their names:

- P - Transform DPD (Days past due)
- M - Masking categories
- A - Transform amount
- D - Transform date
- T - Unspecified Transform
- L - Unspecified Transform

We'll define the a [recipe](https://ibis-project.github.io/ibis-ml/reference/core.html#ibis_ml.Recipe) for data processing, which will be used later to transform the dataset.

```{python}
#| execution: {iopub.status.busy: '2024-05-07T05:25:35.158024Z', iopub.status.idle: '2024-05-07T05:25:35.158484Z', shell.execute_reply: '2024-05-07T05:25:35.158258Z', shell.execute_reply.started: '2024-05-07T05:25:35.158243Z'}
#| trusted: true
# Convert columns ends with P to floating number
step_cast_P_to_float = ml.Cast(ml.endswith("P"), dt.float64)
# Convert columns ends with A to floating number
step_cast_A_to_float = ml.Cast(ml.endswith("A"), dt.float64)
# Convert columns ends with D to date
step_cast_D_to_date = ml.Cast(ml.endswith("D"), dt.date)
# Convert columns ends with M to str
step_cast_M_to_str = ml.Cast(ml.endswith("M"), dt.str)

data_type_recipes = ml.Recipe(
    step_cast_P_to_float,
    step_cast_D_to_date,
    step_cast_M_to_str,
    step_cast_A_to_float,
    # Cast some special columns
    ml.Cast(["date_decision"], "date"),
    ml.Cast(["case_id", "WEEK_NUM", "num_group1", "num_group2"], dt.int64),
    ml.Cast(
        [
            "cardtype_51L",
            "credacc_status_367L",
            "requesttype_4525192L",
            "riskassesment_302T",
            "max_periodicityofpmts_997L",
        ],
        dt.str
    ),
    ml.Cast(
        [
            "isbidproductrequest_292L",
            "isdebitcard_527L",
            "equalityempfrom_62L",
        ],
        dt.int64
    ),
)
```

::: {.callout-tip}
IbisML offers a powerful set of column selectors, allowing you to select columns based on names, types, and patterns. For more information, you can refer to the IbisML column selectors [documentation](https://ibis-project.github.io/ibis-ml/reference/selectors.html).
:::

#### Aggregate Features

For tables with a depth greater than 0 that cannot be directly joined with the base table, we need to aggregate the features by the `case_id`. You could compute the minimum, maximum, mean, median, and standard deviation for numeric columns, and the maximum and minimum for non-numeric columns.

Here, I use the maximum as an example.

```{python}
def agg_by_id(table):
    return table.group_by("case_id").agg(
            [
                table[col_name].max().name(f"max_{col_name}")
                for col_name in table.columns
                if col_name[-1] in ("T", "L", "P", "A", "D", "M")
            ]
    )
```

#### Read and process the data files

We will define a function reads Parquet files, optionally handles regex patterns for multiple files, applies data type transformations using `data_type_recipes`, and performs aggregation based on `case_id` if specified by the depth parameter.

```{python}

def read_and_process_files(file_path, depth=None, is_regex=False):
    """
    Read and process Parquet files.

    Args:
        file_path (str): Path to the file or regex pattern to match files.
        depth (int, optional): Depth of processing. If 1 or 2, additional aggregation is performed. Defaults to None.
        is_regex (bool, optional): Whether the file_path is a regex pattern. Defaults to False.

    Returns:
        ibis.Table: The processed Ibis table.
    """
    if is_regex:
        # Read and union multiple files
        chunks = []
        for path in glob(str(file_path)):
            chunk = ibis.read_parquet(path)
            # Transform table using IbisML
            chunk = data_type_recipes.fit(chunk).to_ibis(chunk)
            chunks.append(chunk)
        table = ibis.union(*chunks)
    else:
        # Read a single file
        table = ibis.read_parquet(file_path)
        # Transform table using IbisML
        table = data_type_recipes.fit(table).to_ibis(table)

    # Perform aggregation if depth is 1 or 2
    if depth in [1, 2]:
        table = agg_by_id(table)

    return table
```

Let's define two dictionaries, `train_data_store` and `test_data_store`, that organize and store processed datasets for training and testing purposes.

```{python}
train_data_store = {
    "df_base": read_and_process_files(TRAIN_DIR / "train_base.parquet"),
    "depth_0": [
        read_and_process_files(TRAIN_DIR / "train_static_cb_0.parquet"),
        read_and_process_files(TRAIN_DIR / "train_static_0_*.parquet", is_regex=True),
    ],
    "depth_1": [
        read_and_process_files(TRAIN_DIR / "train_applprev_1_*.parquet", 1, is_regex=True),
        read_and_process_files(TRAIN_DIR / "train_tax_registry_a_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_tax_registry_b_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_tax_registry_c_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_credit_bureau_b_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_other_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_person_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_deposit_1.parquet", 1),
        read_and_process_files(TRAIN_DIR / "train_debitcard_1.parquet", 1),
    ],
    "depth_2": [
        read_and_process_files(TRAIN_DIR / "train_credit_bureau_b_2.parquet", 2),
    ]
}

test_data_store = {
    "df_base": read_and_process_files(TEST_DIR / "test_base.parquet"),
    "depth_0": [
        read_and_process_files(TEST_DIR / "test_static_cb_0.parquet"),
        read_and_process_files(TEST_DIR / "test_static_0_*.parquet", is_regex=True),
    ],
    "depth_1": [
        read_and_process_files(TEST_DIR / "test_applprev_1_*.parquet", 1, is_regex=True),
        read_and_process_files(TEST_DIR / "test_tax_registry_a_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_tax_registry_b_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_tax_registry_c_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_credit_bureau_b_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_other_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_person_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_deposit_1.parquet", 1),
        read_and_process_files(TEST_DIR / "test_debitcard_1.parquet", 1),
    ],
    "depth_2": [
        read_and_process_files(TEST_DIR / "test_credit_bureau_b_2.parquet", 2),
    ]
}
```

####  Join features with the base data

Join all the features from different sources to the base table.

```{python}
#| execution: {iopub.execute_input: '2024-05-07T05:22:32.482803Z', iopub.status.busy: '2024-05-07T05:22:32.482279Z', iopub.status.idle: '2024-05-07T05:22:32.491094Z', shell.execute_reply: '2024-05-07T05:22:32.489898Z', shell.execute_reply.started: '2024-05-07T05:22:32.482768Z'}
#| trusted: true
def join_data(df_base, depth_0, depth_1, depth_2):
    for i, df in enumerate(depth_0 + depth_1 + depth_2):
        df_base = df_base.join(df, "case_id", how="left", rname="{name}_right" + f"_{i}" )
    return df_base
```

Generate train and test datasets

```{python}
#| execution: {iopub.execute_input: '2024-05-07T05:26:42.040000Z', iopub.status.busy: '2024-05-07T05:26:42.039573Z', iopub.status.idle: '2024-05-07T05:26:42.978902Z', shell.execute_reply: '2024-05-07T05:26:42.977736Z', shell.execute_reply.started: '2024-05-07T05:26:42.039968Z'}
#| trusted: true
df_train = join_data(**train_data_store)
df_test = join_data(**test_data_store)
```

#### Remove less meaningful columns

To filter out columns based on specific criteria, you can follow these steps:

- Remove columns where the average proportion of null values exceeds 0.95.
- Remove categorical columns where the number of unique values is more than 50.

::: {.callout-warning}
Running the following code may take up to 10 minutes to identify columns. You can skip this step and proceed to the next cell, where pre-identified columns are provided.
:::

```{python}
# use this to find the remoed_cols, it will take a couple of minutes, you could directly use the output in the next cell
# removed_cols = []
# for colname in df_train.columns:
# Todo(Jiting): rewrite this to make it more efficient
# Calculate all the stats in one run?
#     null_frac = df_train[colname].isnull().mean().execute()
#     freq = df_train[colname].nunique().execute()
#     if colname not in ["target", "case_id", "WEEK_NUM"] and null_frac > 0.95:
#         removed_cols.append(colname)
#     if colname not in ["target", "case_id", "WEEK_NUM"] and str(df_train[colname].type()) ==  "string":
#         if (freq == 1) | (freq > 50):
#             removed_cols.append(colname)
#     if (colname[-1] not in ["P", "A", "L", "M"]) and (('month_' in colname) or ('year_' in colname)):
#         removed_cols.append(colname)
```

```{python}
removed_cols = [
    'assignmentdate_4955616D',
    'dateofbirth_342D',
    'for3years_128L',
    'for3years_504L',
    'for3years_584L',
    'formonth_118L',
    'formonth_206L',
    'formonth_535L',
    'forquarter_1017L',
    'forquarter_462L',
    'forquarter_634L',
    'fortoday_1092L',
    'forweek_1077L',
    'forweek_528L',
    'forweek_601L',
    'foryear_618L',
    'foryear_818L',
    'foryear_850L',
    'pmtaverage_4955615A',
    'pmtcount_4955617L',
    'riskassesment_302T',
    'riskassesment_940T',
    'bankacctype_710L',
    'clientscnt_136L',
    'equalityempfrom_62L',
    'interestrategrace_34L',
    'isbidproductrequest_292L',
    'lastapprcommoditytypec_5251766M',
    'lastcancelreason_561M',
    'lastdependentsnum_448L',
    'lastotherinc_902A',
    'lastotherlnsexpense_631A',
    'lastrejectcommodtypec_5251769M',
    'lastrepayingdate_696D',
    'maxannuity_4075009A',
    'paytype1st_925L',
    'paytype_783L',
    'payvacationpostpone_4187118D',
    'previouscontdistrict_112M',
    'typesuite_864L',
    'max_cancelreason_3545846M',
    'max_district_544M',
    'max_profession_152M',
    'max_name_4527232M',
    'max_name_4917606M',
    'max_employername_160M',
    'case_id_right_6',
    'max_amount_1115A',
    'max_classificationofcontr_1114M',
    'max_contractdate_551D',
    'max_contractmaturitydate_151D',
    'max_contractst_516M',
    'max_contracttype_653M',
    'max_credlmt_1052A',
    'max_credlmt_228A',
    'max_credlmt_3940954A',
    'max_credor_3940957M',
    'max_credor_3940957M',
    'max_credquantity_1099L',
    'max_credquantity_984L',
    'max_debtpastduevalue_732A',
    'max_debtvalue_227A',
    'max_dpd_550P',
    'max_dpd_733P',
    'max_dpdmax_851P',
    'max_dpdmaxdatemonth_804T',
    'max_dpdmaxdatemonth_804T',
    'max_dpdmaxdateyear_742T',
    'max_dpdmaxdateyear_742T',
    'max_installmentamount_644A',
    'max_installmentamount_833A',
    'max_instlamount_892A',
    'max_interesteffectiverate_369L',
    'max_interestrateyearly_538L',
    'max_lastupdate_260D',
    'max_maxdebtpduevalodued_3940955A',
    'max_numberofinstls_810L',
    'max_overdueamountmax_950A',
    'max_overdueamountmaxdatemonth_494T',
    'max_overdueamountmaxdatemonth_494T',
    'max_overdueamountmaxdateyear_432T',
    'max_overdueamountmaxdateyear_432T',
    'max_periodicityofpmts_997L',
    'max_periodicityofpmts_997M',
    'max_pmtdaysoverdue_1135P',
    'max_pmtmethod_731M',
    'max_pmtnumpending_403L',
    'max_purposeofcred_722M',
    'max_residualamount_1093A',
    'max_residualamount_127A',
    'max_residualamount_3940956A',
    'max_subjectrole_326M',
    'max_subjectrole_43M',
    'max_totalamount_503A',
    'max_totalamount_881A',
    'case_id_right_7',
    'max_amtdebitincoming_4809443A',
    'max_amtdebitoutgoing_4809440A',
    'max_amtdepositbalance_4809441A',
    'max_amtdepositincoming_4809444A',
    'max_amtdepositoutgoing_4809442A',
    'max_birthdate_87D',
    'max_childnum_185L',
    'max_contaddr_district_15M',
    'max_contaddr_zipcode_807M',
    'max_gender_992L',
    'max_housingtype_772L',
    'max_isreference_387L',
    'max_maritalst_703L',
    'max_registaddr_district_1083M',
    'max_registaddr_zipcode_184M',
    'max_role_993L',
    'max_role_993L',
    'max_contractenddate_991D',
    'max_last180dayaveragebalance_704A',
    'max_last180dayturnover_1134A',
    'max_last30dayturnover_651A',
    'case_id_right_11',
    'max_pmts_date_1107D',
    'max_pmts_dpdvalue_108P',
    'max_pmts_pmtsoverdue_635A',
    "max_empl_employedtotal_800L",
    "max_empl_industry_691L",
 ]
```

```{python}
df_train = df_train.drop(removed_cols)
df_train
```

```{python}
#| execution: {iopub.status.busy: '2024-05-07T05:28:01.011452Z', iopub.status.idle: '2024-05-07T05:28:01.012576Z', shell.execute_reply: '2024-05-07T05:28:01.012257Z', shell.execute_reply.started: '2024-05-07T05:28:01.012228Z'}
#| trusted: true

df_test = df_test.drop(removed_cols)
df_test
```

## Last-mile data preprocessing

Last-mile preprocessing is crucial in the machine learning (ML) workflow because it ensures that the data fed into the model is in the most optimal format, facilitating accurate and efficient training and predictions. We will perform the following transformation before feeding the data to models:
- Dropping features with zero variance and redundant features
- Missing value imputation
- Encoding categorical variables
- Handling date variables
- Handling outliers
- Scaling and normalization

::: {.callout-note}
IbisML provides a set of transformations, you could find the roadmap in this github [thread](https://github.com/ibis-project/ibis-ml/issues/32) and the guides from the IbisML [website](https://ibis-project.github.io/ibis-ml/).
:::

### Drop features

```{python}
# Drop features with zero variance for all features
# For numerical columns, drop featuures with 0 variance
# For non-numerical columns, drop features with one or fewer unique values
step_drop_zero_variance = ml.DropZeroVariance(ml.everything())

# Drop redundant case_id_*
step_drop_reduncant_id = ml.Drop(ml.startswith("case_id_right_"))
```

### Impute features

```{python}
step_impute_mode = ml.ImputeMode(ml.string())
step_impute_median = ml.ImputeMedian(ml.numeric())
```

### Encode categorical features

```{python}
target_encoding_step = ml.TargetEncode([
    "education_1103M",
    "education_88M",
    "max_education_1138M",
    "max_employername_160M",
    "max_district_544M",
    "previouscontdistrict_112M",
    "lastapprcommoditycat_1041M",
    "lastcancelreason_561M",
    "lastrejectcommoditycat_161M",
    "lastrejectreason_759M",
])
```

```{python}
ohe_step = ml.OneHotEncode([
    "bankacctype_710L",
    "cardtype_51L",
    "credtype_322L",
    "disbursementtype_67L",
    "inittransactioncode_186L",
    "lastapprcommoditytypec_5251766M",
    "lastrejectcommodtypec_5251769M",
    "lastrejectreasonclient_4145040M",
    "lastst_736L",
    "paytype1st_925L",
    "paytype_783L",
    "twobodfilling_608L",
    "max_credtype_587L",
    "max_familystate_726L",
    "max_inittransactioncode_279L",
    "max_postype_4733339M",
    "max_profession_152M",
    "max_rejectreason_755M",
    "max_rejectreasonclient_4145042M",
    "max_status_219L",
    "max_classificationofcontr_1114M",
    "max_contractst_516M",
])
```

### Handle date variables

```{python}
# Calculate all the days difference between any date columns and the column `date_decision`
date_cols = [col_name for col_name in df_train.columns if col_name[-1] == "D"]
days_to_decision_expr = {
        # Difference in days
        f"{col}_date_decision_diff": (_.date_decision.epoch_seconds() - getattr(_, col).epoch_seconds()) / (60 * 60 * 24)
        for col in date_cols
}
days_to_decision_step = ml.Mutate(days_to_decision_expr)
```

```{python}
# Extract information from the date columns
expand_date_step = ml.ExpandDate(ml.date(), ["week", "day"]) # dow and month is set to catagoery
```

### Handle outliers

```{python}
step_handle_outliers = ml.HandleUnivariateOutliers(["days180_256L"])
```

### Construct recipe

We'll construct [Recipe](https://ibis-project.github.io/ibis-ml/reference/core.html#ibis_ml.Recipe) by chaining all transformation steps, which will be applied to the train and datasets later.

```{python}
last_mile_ml_recipes = ml.Recipe(
    # Drop cols with 0 variance
    step_drop_zero_variance,
    # remove extra case_id_right_*
    step_drop_reduncant_id,

    # cast bool to int
    ml.Cast(ml.has_type("bool"), "float32"),

    # handle date cols
    days_to_decision_step,
    expand_date_step,
    ml.Drop(ml.date()),
    ml.Drop(["MONTH", "WEEK_NUM"]),

    # handle string columns
    ohe_step,
    target_encoding_step,
    step_impute_mode,
    ml.Drop(ml.string()),

    # handle numeric cols
    # Capping outliers
    step_handle_outliers,
    step_impute_median,
    ml.ScaleMinMax(ml.numeric()),

    # Fill missing value
    ml.FillNA(ml.numeric(), 0),
    ml.Cast(ml.numeric(), "float32"),
)
```

## Modeling

After completing data preprocessing with Ibis and IbisML, we proceed to the modeling phase. Here are two approaches:

1. Utilize IbisML as a decomposable component within an ML pipeline and hand off the data to downstream modeling frameworks in various output formats:
     - Pandas Dataframe
     - Numpy Array
     - Polars Dataframe
     - Dask Dataframe
     - xgboost.DMatrix
     - Pyarrow Table
2. Alternatively, incorporate IbisML recipes as components within an sklearn Pipeline and train models similarly to how you would with sklearn.

We will focus on option 1 in this example. You could find option 2 from this [tutorial](https://ibis-project.github.io/ibis-ml/tutorial/xgboost.html).

### Train and test data splitting

We'll use hashing on the unique key to consistently assign data points to specific splits. Hashing is robust to underlying changes in the data, such as adding, deleting, or reordering rows. This deterministic process ensures that each data point is always assigned to the same split, thereby enhancing reproducibility.

```{python}
import random

# This enables the analysis to be reproducible when random numbers are used
random.seed(222)
random_key = str(random.getrandbits(256))

# Put 3/4 of the data into the training set
df_train = df_train.mutate(
    train=(df_train.case_id.cast(dt.str) + random_key).hash().abs() % 4 < 3
)

train_data = df_train[df_train.train].drop("train")
test_data = df_train[~df_train.train].drop("train")

X_train = train_data.drop("target")
y_train = train_data.target.cast(dt.int64).name("target")

X_test = test_data.drop("target")
y_test = test_data.target
```

::: {.callout-warning}
Hashing provides a consistent but pseudo-random distribution of data, which may not precisely align with the specified train/test ratio. While hash codes ensure reproducibility, they don't guarantee an exact split. Due to statistical variance, you might find a slight imbalance in the distribution, resulting in marginally more or fewer samples in either the training or test set than the target percentage. This minor deviation from the intended ratio is a normal consequence of hash-based partitioning.
:::

### Fit recipe

```{python}
# Train preprocessing recipe using training dataset
last_mile_ml_recipes.fit(X_train, y_train)
```


In the previous cell, we trained the recipe using the training dataset. Now, we will transform both the train and test datasets for later modeling. Data can be outputted by IbisML recipes in various formats, such as NumPy, pandas, polars, PyArrow, Dask DataFrame, and XGBoost DMatrix, making it compatible with various modeling frameworks. The default output format is `Numpy Array`

```{python}
# Transform train and test dataset using IbisML recipe
X_train_transformed = last_mile_ml_recipes.transform(X_train)
X_test_transformed = last_mile_ml_recipes.transform(X_test)
```

### Neural network classifier using pytorch

```{python}
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pytorch_lightning as pl
from pytorch_lightning import Trainer

class SimpleNeuralNetClassifier(pl.LightningModule):
    def __init__(self, input_dim, hidden_dim=64, output_dim=1):
        super().__init__()
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim)
        )
        self.loss = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss(y_hat.squeeze(), y)
        self.log('train_loss', loss)
        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self(x)
        loss = self.loss(y_hat.squeeze(), y)
        self.log('val_loss', loss)
        return loss

    def configure_optimizers(self):
        return optim.Adam(self.parameters(), lr=0.001)

    def predict_proba(self, x):
        # Return the probability predictions
        with torch.no_grad():
            self.eval()
            return self.sigmoid(self(x))

# Initialize your LightningModule
nn_classifier = SimpleNeuralNetClassifier(input_dim=X_train_transformed.shape[1])

# Create PyTorch Lightning data loaders
train_dataset = TensorDataset(
    torch.Tensor(X_train_transformed),
    torch.Tensor(y_train.to_pandas().to_numpy())
)
val_dataset = TensorDataset(
    torch.Tensor(X_test_transformed),
    torch.Tensor(y_test.to_pandas().to_numpy())
)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

# Initialize a Trainer
trainer = Trainer(max_epochs=2)

# Train the model
trainer.fit(nn_classifier, train_loader, val_loader)
```

```{python}
y_pred = nn_classifier.predict_proba( torch.Tensor(X_test_transformed))
y_pred
```

### xgboost

```{python}
import xgboost as xgb

# Build a simple xgboost
xgboost = xgb.XGBClassifier(
    n_estimators=100,
    max_depth=4,
    learning_rate=0.05,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)
# Fit the model using training dataset
xgboost.fit(X_train_transformed, y_train)
```

```{python}
# Predict
xgboost.predict_proba(X_train_transformed)[:, 1]
```

## Reference
- [1st Place Solution](https://www.kaggle.com/code/yuuniekiri/fork-of-home-credit-risk-lightgbm)
- [home-credit-2024-starter-notebook](https://www.kaggle.com/code/jetakow/home-credit-2024-starter-notebook)
- [EDA and Submission](https://www.kaggle.com/competitions/home-credit-credit-risk-model-stability/discussion/508337)
- [Home Credit Baseline](https://www.kaggle.com/code/greysky/home-credit-baseline)
