{
  "hash": "3757479a45bc71be4a3e76e0aebb7589",
  "result": {
    "markdown": "---\ntitle: \"Ibis goes real-time! Introducing the new Flink backend for Ibis\"\nauthor: \"Deepyaman Datta\"\ndate: \"2024-01-29\"\ncategories:\n    - blog\n    - flink\n    - stream processing\n---\n\n## Introduction\n\nIbis 8.0.0 marks the official release of the Apache Flink backend for Ibis.\nFlink is one of the most established stream-processing frameworks out there and\na central part of the real-time data infrastructure at companies like DoorDash,\nLinkedIn, Netflix, and Uber. The Flink backend is also the first streaming\nbackend Ibis supports. Follow along as we define and execute a simple streaming\njob using Ibis!\n\n## Installation prerequisites\n\n* **Docker Compose:** This tutorial uses Docker Compose to manage an Apache\n  Kafka environment (including sample data generation) and a Flink cluster (for\n  [remote execution](#remote-execution)). You can [download and install Docker\n  Compose from the official website](https://docs.docker.com/compose/install/).\n* **JDK 11:** Flink requires Java 11. If you don't already have JDK 11\n  installed, you can [get the appropriate Eclipse Temurin\n  release](https://adoptium.net/temurin/releases/?package=jdk&version=11).\n* **Python:** To follow along, you need Python 3.9 or 3.10.\n\n## Installing the Flink backend for Ibis\n\nWe use a Python client to explore data in Kafka topics. You can install it,\nalongside the Flink backend for Ibis, with `pip`, `conda`, `mamba`, or `pixi`:\n\n::: {.panel-tabset}\n\n## Using `pip`\n\n```bash\npip install ibis-framework apache-flink kafka-python\n```\n\n## Using `conda`\n\n::: {.callout-important}\n## PyFlink is not available on conda-forge; please use `pip` to install the PyFlink backend instead.\n:::\n\n## Using `mamba`\n\n::: {.callout-important}\n## PyFlink is not available on conda-forge; please use `pip` to install the PyFlink backend instead.\n:::\n\n## Using `pixi`\n\n::: {.callout-important}\n## PyFlink is not available on conda-forge; please use `pip` to install the PyFlink backend instead.\n:::\n\n:::\n\n## Spinning up the services using Docker Compose\n\nThe [claypotai/ibis-flink-example GitHub\nrepository](https://github.com/claypotai/ibis-flink-example) includes the\nrelevant Docker Compose configuration for this tutorial. Clone the repository,\nand run `docker compose up` from the cloned directory to create Kafka topics,\ngenerate sample data, and launch a Flink cluster:\n\n```bash\ngit clone https://github.com/claypotai/ibis-flink-example.git\ncd ibis-flink-example\ndocker compose up\n```\n\n::: {.callout-tip}\nIf you don't intend to try [remote execution](#remote-execution), you can start\nonly the Kafka-related services with `docker compose up kafka init-kafka\ndata-generator`.\n:::\n\nAfter a few seconds, you should see messages indicating your Kafka environment\nis ready:\n\n```bash\nibis-flink-example-init-kafka-1      | Successfully created the following topics:\nibis-flink-example-init-kafka-1      | payment_msg\nibis-flink-example-init-kafka-1      | sink\nibis-flink-example-init-kafka-1 exited with code 0\nibis-flink-example-data-generator-1  | Connected to Kafka\nibis-flink-example-data-generator-1  | Producing 20000 records to Kafka topic payment_msg\n```\n\nThis example uses mock payments data. The `payment_msg` Kafka topic contains\nmessages in the following format:\n\n```json\n{\n    \"createTime\": \"2023-09-20 22:19:02.224\",\n    \"orderId\": 1695248388,\n    \"payAmount\": 88694.71922270155,\n    \"payPlatform\": 0,\n    \"provinceId\": 6\n}\n```\n\nIn a separate terminal, we can explore what these messages look like:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom itertools import islice\n\nfrom kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(\"payment_msg\")\nfor msg in islice(consumer, 3):\n    print(msg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConsumerRecord(topic='payment_msg', partition=0, offset=147, timestamp=1706248229136, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-26 05:50:29.135\", \"orderId\": 1706248303, \"payAmount\": 81650.68080243719, \"payPlatform\": 0, \"provinceId\": 4}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=131, serialized_header_size=-1)\nConsumerRecord(topic='payment_msg', partition=0, offset=148, timestamp=1706248229637, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-26 05:50:29.636\", \"orderId\": 1706248304, \"payAmount\": 90616.05736160549, \"payPlatform\": 1, \"provinceId\": 5}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=131, serialized_header_size=-1)\nConsumerRecord(topic='payment_msg', partition=0, offset=149, timestamp=1706248230138, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-26 05:50:30.138\", \"orderId\": 1706248305, \"payAmount\": 64564.717152252706, \"payPlatform\": 0, \"provinceId\": 5}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=132, serialized_header_size=-1)\n```\n:::\n:::\n\n\n## Running the tutorial\n\nThis tutorial uses Ibis with the Flink backend to process the aforementioned\npayment messages. You can choose to either [run it locally](#local-execution) or\n[submit a job to an already-running Flink cluster](#remote-execution).\n\n### Local execution\n\nThe simpler option is to run the example using the Flink mini cluster.\n\n#### Create a table environment\n\nThe [table\nenvironment](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/table/table_environment/)\nserves as the main entry point for interacting with the Flink runtime. The\n`flink` backend does not create `TableEnvironment` objects; you must create a\n`TableEnvironment` and pass that to\n[`ibis.flink.connect`](../../backends/flink.qmd#ibis.flink.connect):\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport ibis\nfrom pyflink.table import EnvironmentSettings, TableEnvironment\n\nenv_settings = EnvironmentSettings.in_streaming_mode()\ntable_env = TableEnvironment.create(env_settings)\ntable_env.get_config().set(\"parallelism.default\", \"1\")  # <1>\n\ncon = ibis.flink.connect(table_env)\n```\n:::\n\n\n1. write all the data to one file\n\nFlink’s streaming connectors aren't part of the binary distribution. Link the\n[Kafka\nconnector](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/kafka/)\nfor cluster execution by adding the JAR file from the cloned repository. [Ibis\nexposes the `raw_sql` method for situations like this, where you need to run\narbitrary SQL that cannot be modeled as a table\nexpression](https://ibis-project.org/how-to/extending/sql#backend.raw_sql):\n\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncon.raw_sql(\"ADD JAR 'flink-sql-connector-kafka-3.0.2-1.18.jar'\")\n```\n:::\n\n\n#### Create the source and sink tables\n\nUse\n[`create_table`](../../backends/flink.qmd#ibis.backends.flink.Backend.create_table)\nto register tables. Notice the new top-level `ibis.watermark` API for\n[specifying a watermark\nstrategy](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/concepts/time/#event-time-and-watermarks).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nsource_schema = ibis.schema(                                                        # <1>\n    {                                                                               # <1>\n        \"createTime\": \"timestamp(3)\",                                               # <1>\n        \"orderId\": \"int64\",                                                         # <1>\n        \"payAmount\": \"float64\",                                                     # <1>\n        \"payPlatform\": \"int32\",                                                     # <1>\n        \"provinceId\": \"int32\",                                                      # <1>\n    }                                                                               # <1>\n)                                                                                   # <1>\n\nsource_configs = {                                                                  # <1>\n    \"connector\": \"kafka\",                                                           # <1>\n    \"topic\": \"payment_msg\",                                                         # <1>\n    \"properties.bootstrap.servers\": \"localhost:9092\",                               # <1>\n    \"properties.group.id\": \"test_3\",                                                # <1>\n    \"scan.startup.mode\": \"earliest-offset\",                                         # <1>\n    \"format\": \"json\",                                                               # <1>\n}                                                                                   # <1>\n\nt = con.create_table(                                                               # <1>\n    \"payment_msg\",                                                                  # <1>\n    schema=source_schema,                                                           # <1>\n    tbl_properties=source_configs,                                                  # <1>\n    watermark=ibis.watermark(                                                       # <1>\n        time_col=\"createTime\", allowed_delay=ibis.interval(seconds=15)              # <1>\n    ),                                                                              # <1>\n)                                                                                   # <1>\n\nsink_schema = ibis.schema(                                                          # <2>\n    {                                                                               # <2>\n        \"province_id\": \"int32\",                                                     # <2>\n        \"pay_amount\": \"float64\",                                                    # <2>\n    }                                                                               # <2>\n)                                                                                   # <2>\n\nsink_configs = {                                                                    # <2>\n    \"connector\": \"kafka\",                                                           # <3>\n    \"topic\": \"sink\",                                                                # <2>\n    \"properties.bootstrap.servers\": \"localhost:9092\",                               # <2>\n    \"format\": \"json\",                                                               # <2>\n}                                                                                   # <2>\n\ncon.create_table(                                                                   # <2>\n    \"total_amount_by_province_id\", schema=sink_schema, tbl_properties=sink_configs  # <2>\n)                                                                                   # <2>\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatabaseTable: total_amount_by_province_id\n  province_id int32\n  pay_amount  float64\n</pre>\n```\n:::\n:::\n\n\n1. create source Table\n2. create sink Table\n\n#### Perform calculations\n\nCompute the total pay amount per province in the past 10 seconds (as of each\nmessage, for the province in the incoming message):\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nagged = t.select(\n    province_id=t.provinceId,\n    pay_amount=t.payAmount.sum().over(\n        range=(-ibis.interval(seconds=10), 0),\n        group_by=t.provinceId,\n        order_by=t.createTime,\n    ),\n)\n```\n:::\n\n\nFinally, emit the query result to the sink table:\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncon.insert(\"total_amount_by_province_id\", agged)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<pyflink.table.table_result.TableResult at 0x147b737c0>\n```\n:::\n:::\n\n\n### Remote execution\n\nYou can also submit the example to the [remote cluster started using Docker\nCompose](#spinning-up-the-services-using-docker-compose). The\n`window_aggregation.py` file in the cloned repository contains the [same steps\nthat we performed for local execution](#local-execution). We will [use the\nmethod described in the official Flink\ndocumentation](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/cli/#submitting-pyflink-jobs).\n\n::: {.callout-tip}\nYou can find the `./bin/flink` executable with the following command:\n\n```bash\npython -c'from pathlib import Path; import pyflink; print(Path(pyflink.__spec__.origin).parent / \"bin\" / \"flink\")'\n```\n:::\n\nMy full command looks like this:\n\n```bash\n/opt/miniconda3/envs/ibis-dev/lib/python3.10/site-packages/pyflink/bin/flink run --jobmanager localhost:8081 --python window_aggregation.py\n```\n\nThe command will exit after displaying a submission message:\n\n```\nJob has been submitted with JobID b816faaf5ef9126ea5b9b6a37012cf56\n```\n\n## Viewing the results\n\nSimilar to how we viewed messages in the `payment_msg` topic, we can print\nresults from the `sink` topic:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nconsumer = KafkaConsumer(\"sink\")\nfor msg in islice(consumer, 10):\n    print(msg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConsumerRecord(topic='sink', partition=0, offset=0, timestamp=1706248236987, timestamp_type=0, key=None, value=b'{\"province_id\":5,\"pay_amount\":61581.650195910595}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=1, timestamp=1706248236991, timestamp_type=0, key=None, value=b'{\"province_id\":2,\"pay_amount\":94670.98549118037}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=2, timestamp=1706248236992, timestamp_type=0, key=None, value=b'{\"province_id\":0,\"pay_amount\":12981.518147468774}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=3, timestamp=1706248236992, timestamp_type=0, key=None, value=b'{\"province_id\":3,\"pay_amount\":1224.9347854971782}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=4, timestamp=1706248236992, timestamp_type=0, key=None, value=b'{\"province_id\":6,\"pay_amount\":37369.578488761756}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=5, timestamp=1706248236992, timestamp_type=0, key=None, value=b'{\"province_id\":2,\"pay_amount\":190372.0022648085}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=6, timestamp=1706248236992, timestamp_type=0, key=None, value=b'{\"province_id\":3,\"pay_amount\":54649.12171874394}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=7, timestamp=1706248236993, timestamp_type=0, key=None, value=b'{\"province_id\":4,\"pay_amount\":42218.386893476614}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=8, timestamp=1706248236993, timestamp_type=0, key=None, value=b'{\"province_id\":6,\"pay_amount\":120314.87462453957}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=9, timestamp=1706248236993, timestamp_type=0, key=None, value=b'{\"province_id\":4,\"pay_amount\":52183.18034203951}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\n```\n:::\n:::\n\n\nVoilà! You've run your first streaming application using Ibis.\n\n## Shutting down the Compose environment\n\nPress <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the Docker Compose containers. Once\nstopped, run `docker compose down` to remove the services created for this\ntutorial.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}