{
  "hash": "f2681ac285ea3af47fd82a1199bcc365",
  "result": {
    "markdown": "---\ntitle: \"Ibis goes real-time! Introducing the new Flink backend for Ibis\"\nauthor: \"Deepyaman Datta\"\ndate: \"2024-01-10\"\ncategories:\n    - blog\n    - flink\n    - stream processing\n---\n\n## Introduction\n\nIbis 8.0.0 marks the official release of the Apache Flink backend for Ibis. Flink is one of the most established stream-processing frameworks out there and a central part of the real-time data infrastructure at companies like DoorDash, LinkedIn, Netflix, and Uber. The Flink backend is also the first streaming backend Ibis supports. Follow along as we define and execute a simple streaming job using Ibis!\n\n## Installation prerequisites\n\n* **Docker Compose:** This tutorial uses Docker Compose to manage an Apache Kafka environment (including sample data generation) and a Flink cluster (for [remote execution](#remote-execution)). You can [download and install Docker Compose from the official website](https://docs.docker.com/compose/install/).\n* **JDK 11:** Flink requires Java 11. If you don't already have JDK 11 installed, you can [get the appropriate Eclipse Temurin release](https://adoptium.net/temurin/releases/?package=jdk&version=11).\n* **Python:** To follow along, you need Python 3.9 or 3.10.\n\n## Installing the Flink backend for Ibis\n\nWe use a Python client to explore data in Kafka topics. You can install it, alongside the Flink backend for Ibis, with `pip`, `conda`, `mamba`, or `pixi`:\n\n::: {.panel-tabset}\n\n## Using `pip`\n\n```bash\npip install ibis-framework apache-flink kafka-python\n```\n\n## Using `conda`\n\n::: {.callout-important}\nPyFlink is not available on conda-forge; please\nuse `pip` to install the PyFlink backend instead.\n:::\n\n## Using `mamba`\n\n::: {.callout-important}\nPyFlink is not available on conda-forge; please\nuse `pip` to install the PyFlink backend instead.\n:::\n\n## Using `pixi`\n\n::: {.callout-important}\nPyFlink is not available on conda-forge; please\nuse `pip` to install the PyFlink backend instead.\n:::\n\n:::\n\n## Spinning up the services using Docker Compose\n\nThe [claypotai/ibis-flink-example GitHub repository](https://github.com/claypotai/ibis-flink-example) includes the relevant Docker Compose configuration for this tutorial. Clone the repository, and run `docker compose up` from the cloned directory to create Kafka topics, generate sample data, and launch a Flink cluster.\n\n```bash\ngit clone https://github.com/claypotai/ibis-flink-example.git\ncd ibis-flink-example\ndocker compose up\n```\n\n::: {.callout-tip}\nIf you don't intend to try [remote execution](#remote-execution), you can start only the Kafka-related services with `docker compose up kafka init-kafka data-generator`.\n:::\n\nAfter a few seconds, you should see messages indicating your Kafka environment is ready:\n\n```bash\nibis-flink-example-init-kafka-1      | Successfully created the following topics:\nibis-flink-example-init-kafka-1      | payment_msg\nibis-flink-example-init-kafka-1      | sink\nibis-flink-example-init-kafka-1 exited with code 0\nibis-flink-example-data-generator-1  | Connected to Kafka\nibis-flink-example-data-generator-1  | Producing 20000 records to Kafka topic payment_msg\n```\n\nThe `payment_msg` Kafka topic contains messages in the following format:\n\n```json\n{\n    \"createTime\": \"2023-09-20 22:19:02.224\",\n    \"orderId\": 1695248388,\n    \"payAmount\": 88694.71922270155,\n    \"payPlatform\": 0,\n    \"provinceId\": 6\n}\n```\n\nIn a separate terminal, we can explore what these messages look like:\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom itertools import islice\n\nfrom kafka import KafkaConsumer\n\nconsumer = KafkaConsumer(\"payment_msg\")\nfor msg in islice(consumer, 3):\n    print(msg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConsumerRecord(topic='payment_msg', partition=0, offset=259, timestamp=1704902818253, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-10 16:06:58.252\", \"orderId\": 1704902948, \"payAmount\": 69487.90996696643, \"payPlatform\": 0, \"provinceId\": 5}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=131, serialized_header_size=-1)\nConsumerRecord(topic='payment_msg', partition=0, offset=260, timestamp=1704902818756, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-10 16:06:58.755\", \"orderId\": 1704902949, \"payAmount\": 33755.777322522496, \"payPlatform\": 0, \"provinceId\": 6}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=132, serialized_header_size=-1)\nConsumerRecord(topic='payment_msg', partition=0, offset=261, timestamp=1704902819261, timestamp_type=0, key=None, value=b'{\"createTime\": \"2024-01-10 16:06:59.260\", \"orderId\": 1704902950, \"payAmount\": 65273.42127921554, \"payPlatform\": 0, \"provinceId\": 0}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=131, serialized_header_size=-1)\n```\n:::\n:::\n\n\n## Running the tutorial\n\nThis tutorial uses the Flink backend for Ibis to process the aforementioned payment messages. You can choose to either [run it locally](#local-execution) or [submit a job to an already-running Flink cluster](#remote-execution).\n\n### Local execution\n\nThe simpler option is to run the example using the Flink mini cluster.\n\n#### Create a table environment\n\nThe [table environment](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/dev/python/table/table_environment/) serves as the main entry point for interacting with the Flink runtime. The `flink` backend does not create `TableEnvironment` objects; you must create a `TableEnvironment` and pass that to [`ibis.flink.connect`](../../backends/flink.qmd#ibis.flink.connect).\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport ibis\nfrom pyflink.table import EnvironmentSettings, TableEnvironment\n\nenv_settings = EnvironmentSettings.in_streaming_mode()\ntable_env = TableEnvironment.create(env_settings)\n# write all the data to one file\ntable_env.get_config().set(\"parallelism.default\", \"1\")\n\ncon = ibis.flink.connect(table_env)\n```\n:::\n\n\nFlink’s streaming connectors aren't part of the binary distribution. Link the [Kafka connector](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/connectors/table/kafka/) for cluster execution by adding the JAR file from the cloned repository.\n\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncon._exec_sql(\"ADD JAR 'flink-sql-connector-kafka-3.0.2-1.18.jar'\")\n```\n:::\n\n\n#### Create the source and sink tables\n\nUse [`create_table`](../../backends/flink.qmd#ibis.backends.flink.Backend.create_table) to register tables. Notice the new top-level `ibis.watermark` API for [specifying a watermark strategy](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/concepts/time/#event-time-and-watermarks).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\n# create source Table\nsource_schema = ibis.schema(\n    {\n        \"createTime\": \"timestamp(3)\",\n        \"orderId\": \"int64\",\n        \"payAmount\": \"float64\",\n        \"payPlatform\": \"int32\",\n        \"provinceId\": \"int32\",\n    }\n)\n\nsource_configs = {\n    \"connector\": \"kafka\",\n    \"topic\": \"payment_msg\",\n    \"properties.bootstrap.servers\": \"localhost:9092\",\n    \"properties.group.id\": \"test_3\",\n    \"scan.startup.mode\": \"earliest-offset\",\n    \"format\": \"json\",\n}\n\nt = con.create_table(\n    \"payment_msg\",\n    schema=source_schema,\n    tbl_properties=source_configs,\n    watermark=ibis.watermark(\n        time_col=\"createTime\", allowed_delay=ibis.interval(seconds=15)\n    ),\n)\n\n# create sink Table\nsink_schema = ibis.schema(\n    {\n        \"province_id\": \"int32\",\n        \"pay_amount\": \"float64\",\n    }\n)\n\nsink_configs = {\n    \"connector\": \"kafka\",\n    \"topic\": \"sink\",\n    \"properties.bootstrap.servers\": \"localhost:9092\",\n    \"format\": \"json\",\n}\n\ncon.create_table(\n    \"total_amount_by_province_id\", schema=sink_schema, tbl_properties=sink_configs\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">DatabaseTable: total_amount_by_province_id\n  province_id int32\n  pay_amount  float64\n</pre>\n```\n:::\n:::\n\n\n#### Perform calculations\n\nCompute the total pay amount per province in the past 10 seconds (as of each message, for the province in the incoming message).\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nagged = t.select(\n    province_id=t.provinceId,\n    pay_amount=t.payAmount.sum().over(\n        range=(-ibis.interval(seconds=10), 0),\n        group_by=t.provinceId,\n        order_by=t.createTime,\n    ),\n)\n```\n:::\n\n\nFinally, emit the query result to the sink table.\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ncon.insert(\"total_amount_by_province_id\", agged)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n<pyflink.table.table_result.TableResult at 0x139e88790>\n```\n:::\n:::\n\n\n### Remote execution\n\nYou can also submit the example to the [remote cluster started using Docker Compose](#spinning-up-the-services-using-docker-compose). The `window_aggregation.py` file in the cloned repository contains the [same steps that we performed for local execution](#local-execution). We will [use the method described in the official Flink documentation](https://nightlies.apache.org/flink/flink-docs-release-1.18/docs/deployment/cli/#submitting-pyflink-jobs).\n\n::: {.callout-tip}\nYou can find the `./bin/flink` executable with the following command:\n\n```bash\npython -c'from pathlib import Path; import pyflink; print(Path(pyflink.__spec__.origin).parent / \"bin\" / \"flink\")'\n```\n:::\n\nMy full command looks like this:\n\n```bash\n/opt/miniconda3/envs/ibis-dev/lib/python3.10/site-packages/pyflink/bin/flink run --jobmanager localhost:8081 --python window_aggregation.py\n```\n\nThe command will exit after displaying a submission message:\n\n```\nJob has been submitted with JobID b816faaf5ef9126ea5b9b6a37012cf56\n```\n\n## Viewing the results\n\nSimilar to how we viewed messages in the `payment_msg` topic, we can print results from the `sink` topic:\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nconsumer = KafkaConsumer(\"sink\")\nfor msg in islice(consumer, 10):\n    print(msg)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConsumerRecord(topic='sink', partition=0, offset=0, timestamp=1704902828420, timestamp_type=0, key=None, value=b'{\"province_id\":4,\"pay_amount\":69856.46269205776}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=1, timestamp=1704902828424, timestamp_type=0, key=None, value=b'{\"province_id\":6,\"pay_amount\":1789.143741678545}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=2, timestamp=1704902828425, timestamp_type=0, key=None, value=b'{\"province_id\":0,\"pay_amount\":11002.763696389351}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=3, timestamp=1704902828425, timestamp_type=0, key=None, value=b'{\"province_id\":1,\"pay_amount\":28179.067505530566}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=4, timestamp=1704902828425, timestamp_type=0, key=None, value=b'{\"province_id\":0,\"pay_amount\":87855.68887586275}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=5, timestamp=1704902828426, timestamp_type=0, key=None, value=b'{\"province_id\":2,\"pay_amount\":3813.798695925863}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=6, timestamp=1704902828426, timestamp_type=0, key=None, value=b'{\"province_id\":0,\"pay_amount\":134820.50950218836}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=7, timestamp=1704902828426, timestamp_type=0, key=None, value=b'{\"province_id\":5,\"pay_amount\":82583.1706681021}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=47, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=8, timestamp=1704902828426, timestamp_type=0, key=None, value=b'{\"province_id\":2,\"pay_amount\":23806.09667846494}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=48, serialized_header_size=-1)\nConsumerRecord(topic='sink', partition=0, offset=9, timestamp=1704902828426, timestamp_type=0, key=None, value=b'{\"province_id\":5,\"pay_amount\":144007.61702048077}', headers=[], checksum=None, serialized_key_size=-1, serialized_value_size=49, serialized_header_size=-1)\n```\n:::\n:::\n\n\nVoilà! You've run your first streaming application using Ibis.\n\n## Shutting down the Compose environment\n\nPress <kbd>Ctrl</kbd>+<kbd>C</kbd> to stop the Docker Compose containers. Once stopped, run `docker compose down` to remove the services created for this tutorial.\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}